# Gemini API Documentation Documentation

Generated on: 2025-11-19T10:06:24.571Z
Base URL: https://ai.google.dev/gemini-api/docs
URL Pattern: https://ai.google.dev/gemini-api

## Documentation Content

### Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 

# Gemini API

 The developer platform to build and scale with Google's latest AI models. Start in minutes. 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### C#

 

```
using System.Threading.Tasks;
using Google.GenAI;
using Google.GenAI.Types;

public class GenerateContentSimpleText {
  public static async Task main() {
    var client = new Client();
    var response = await client.Models.GenerateContentAsync(
      model: "gemini-2.5-flash", contents: "Explain how AI works in a few words"
    );
    Console.WriteLine(response.Candidates[0].Content.Parts[0].Text);
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 
 
 Start building 
 
 

Follow our Quickstart guide to get an API key and make your first API call in minutes.

 

 For most models, you can start with our free tier, without having to set up a billing account.
 

 
 

 

 
 

## Meet the models

 
 
 

 spark 
 Gemini 3 Pro
 

 

 Our most intelligent model, the best in the world for multimodal understanding, all built on state-of-the-art reasoning.
 

 
 
 

 spark 
 Gemini 2.5 Pro
 

 

 Our powerful reasoning model, which excels at coding and complex reasonings tasks.
 

 
 
 

 spark 
 Gemini 2.5 Flash
 

 

 Our most balanced model, with a 1 million token context window and more.
 

 
 
 

 spark 
 Gemini 2.5 Flash-Lite
 

 

 Our fastest and most cost-efficient multimodal model with great performance
 for high-frequency tasks.
 

 
 
 

 video_library 
 Veo 3.1
 

 

 Our state-of-the-art video generation model, with native audio.
 

 
 
 

 üçå 
 Gemini 2.5 Flash Image (Nano Banana)
 

 

 State-of-the-art image generation and editing model
 

 
 
 

## Explore Capabilities

 
 
 
 
 imagesmode 
 
 
 
 

 Native Image Generation (Nano Banana)
 

 

 Generate and edit highly contextual images natively with Gemini 2.5 Flash Image.
 

 
 
 
 
 
 article 
 
 
 
 

 Long Context
 

 

 Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.
 

 
 
 
 
 
 code 
 
 
 
 

 Structured Outputs
 

 

 Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.
 

 
 
 
 
 
 functions 
 
 
 
 

 Function Calling
 

 

 Build agentic workflows by connecting Gemini to external APIs and tools.
 

 
 
 
 
 
 videocam 
 
 
 
 

 Video Generation with Veo 3.1
 

 

 Create high-quality video content from text or image prompts with our state-of-the-art model.
 

 
 
 
 
 
 android_recorder 
 
 
 
 

 Voice Agents with Live API
 

 

 Build real-time voice applications and agents with the Live API.
 

 
 
 
 
 
 build 
 
 
 
 

 Tools
 

 

 Connect Gemini to the world through built-in tools like Google Search, URL Context, Google Maps, Code Execution and Computer Use.
 

 
 
 
 
 
 stacks 
 
 
 
 

 Document Understanding
 

 

 Process up to 1000 pages of PDF files.
 

 
 
 
 
 
 cognition_2 
 
 
 
 

 Thinking
 

 

 Explore how thinking capabilities improve reasoning for complex tasks and agents.
 

 
 
 
 

## Developer Toolkit

 
 
 
 

 
 AI Studio
 

 

 Test prompts, manage your API keys, monitor usage, and build prototypes in our web-based IDE.
 

 
 

 Open AI Studio
 

 
 
 
 

 group 
 Developer Community
 

 

 Ask questions and find solutions from other developers and Google engineers.
 

 
 

 Join the community
 

 
 
 
 

 menu_book 
 API Reference
 

 

 Find detailed information about the Gemini API in the official reference documentation.
 

 
 

 Access the API reference
 

 
 
 
 
 

 
 

 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 

# Gemini API

 The developer platform to build and scale with Google's latest AI models. Start in minutes. 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### C#

 

```
using System.Threading.Tasks;
using Google.GenAI;
using Google.GenAI.Types;

public class GenerateContentSimpleText {
  public static async Task main() {
    var client = new Client();
    var response = await client.Models.GenerateContentAsync(
      model: "gemini-2.5-flash", contents: "Explain how AI works in a few words"
    );
    Console.WriteLine(response.Candidates[0].Content.Parts[0].Text);
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 
 
 Start building 
 
 

Follow our Quickstart guide to get an API key and make your first API call in minutes.

 

 For most models, you can start with our free tier, without having to set up a billing account.
 

 
 

 

 
 

## Meet the models

 
 
 

 spark 
 Gemini 3 Pro
 

 

 Our most intelligent model, the best in the world for multimodal understanding, all built on state-of-the-art reasoning.
 

 
 
 

 spark 
 Gemini 2.5 Pro
 

 

 Our powerful reasoning model, which excels at coding and complex reasonings tasks.
 

 
 
 

 spark 
 Gemini 2.5 Flash
 

 

 Our most balanced model, with a 1 million token context window and more.
 

 
 
 

 spark 
 Gemini 2.5 Flash-Lite
 

 

 Our fastest and most cost-efficient multimodal model with great performance
 for high-frequency tasks.
 

 
 
 

 video_library 
 Veo 3.1
 

 

 Our state-of-the-art video generation model, with native audio.
 

 
 
 

 üçå 
 Gemini 2.5 Flash Image (Nano Banana)
 

 

 State-of-the-art image generation and editing model
 

 
 
 

## Explore Capabilities

 
 
 
 
 imagesmode 
 
 
 
 

 Native Image Generation (Nano Banana)
 

 

 Generate and edit highly contextual images natively with Gemini 2.5 Flash Image.
 

 
 
 
 
 
 article 
 
 
 
 

 Long Context
 

 

 Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.
 

 
 
 
 
 
 code 
 
 
 
 

 Structured Outputs
 

 

 Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.
 

 
 
 
 
 
 functions 
 
 
 
 

 Function Calling
 

 

 Build agentic workflows by connecting Gemini to external APIs and tools.
 

 
 
 
 
 
 videocam 
 
 
 
 

 Video Generation with Veo 3.1
 

 

 Create high-quality video content from text or image prompts with our state-of-the-art model.
 

 
 
 
 
 
 android_recorder 
 
 
 
 

 Voice Agents with Live API
 

 

 Build real-time voice applications and agents with the Live API.
 

 
 
 
 
 
 build 
 
 
 
 

 Tools
 

 

 Connect Gemini to the world through built-in tools like Google Search, URL Context, Google Maps, Code Execution and Computer Use.
 

 
 
 
 
 
 stacks 
 
 
 
 

 Document Understanding
 

 

 Process up to 1000 pages of PDF files.
 

 
 
 
 
 
 cognition_2 
 
 
 
 

 Thinking
 

 

 Explore how thinking capabilities improve reasoning for complex tasks and agents.
 

 
 
 
 

## Developer Toolkit

 
 
 
 

 
 AI Studio
 

 

 Test prompts, manage your API keys, monitor usage, and build prototypes in our web-based IDE.
 

 
 

 Open AI Studio
 

 
 
 
 

 group 
 Developer Community
 

 

 Ask questions and find solutions from other developers and Google engineers.
 

 
 

 Join the community
 

 
 
 
 

 menu_book 
 API Reference
 

 

 Find detailed information about the Gemini API in the official reference documentation.
 

 
 

 Access the API reference
 

 
 
 
 
 

 
 

 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini API quickstart &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/quickstart

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API quickstart  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This quickstart shows you how to install our libraries and make your first Gemini API request.

## Before you begin

You need a Gemini API key. If you don't already have one, you can get it for free in Google AI Studio .

## Install the Google GenAI SDK

 
 

### Python

Using Python 3.9+ , install the
 `google-genai` package 
using the following
 pip command :

 

```
pip install -q -U google-genai
```

 
 

### JavaScript

Using Node.js v18+ ,
install the
 Google Gen AI SDK for TypeScript and JavaScript 
using the following
 npm command :

 

```
npm install @google/genai
```

 
 

### Go

Install
 google.golang.org/genai in
your module directory using the go get command :

 

```
go get google.golang.org/genai
```

 
 

### Java

If you're using Maven, you can install
 google-genai by adding the
following to your dependencies:

 

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

 
 

### C#

Install
 googleapis/go-genai in
your module directory using the dotnet add command 

 

```
dotnet add package Google.GenAI
```

 
 

### Apps Script

- To create a new Apps Script project, go to
 script.new .

- Click Untitled project .

- Rename the Apps Script project AI Studio and click Rename .

- Set your API key 

 At the left, click Project Settings .

- Under Script Properties click Add script property .

- For Property , enter the key name: `GEMINI_API_KEY`.

- For Value , enter the value for the API key.

- Click Save script properties .

 
- Replace the `Code.gs` file contents with the following code:

 
 

## Make your first request

Here is an example that uses the
 `generateContent` method
to send a request to the Gemini API using the Gemini 2.5 Flash model.

If you set your API key as the
environment variable `GEMINI_API_KEY`, it will be picked up automatically by the
client when using the Gemini API libraries .
Otherwise you will need to pass your API key as
an argument when initializing the client.

Note that all code samples in the Gemini API docs assume that you have set the
environment variable `GEMINI_API_KEY`.

 
 

### Python

 

```
from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// The client gets the API key from the environment variable `GEMINI_API_KEY`.
const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### C#

 

```
using System.Threading.Tasks;
using Google.GenAI;
using Google.GenAI.Types;

public class GenerateContentSimpleText {
  public static async Task main() {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    var client = new Client();
    var response = await client.Models.GenerateContentAsync(
      model: "gemini-2.5-flash", contents: "Explain how AI works in a few words"
    );
    Console.WriteLine(response.Candidates[0].Content.Parts[0].Text);
  }
}
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');
function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 

## What's next

Now that you made your first API request, you might want to explore the
following guides that show Gemini in action:

- Text generation 

- Image generation 

- Image understanding 

- Thinking 

- Function calling 

- Long context 

- Embeddings 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Using Gemini API keys &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/api-key

- 
 
 
 
 
 
 
 
 
 
 
 Using Gemini API keys  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ÔøΩÔøΩÔøΩ‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Using Gemini API keys 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

To use the Gemini API, you need an API key. This page outlines how to create and
manage your keys in Google AI Studio as well as how to set up your environment
to use them in your code.

## API Keys

An API key is an
encrypted string that you can use when calling Google Cloud APIs.
You can create and manage all your Gemini API Keys from the
 Google AI Studio API Keys page.

Once you have an API key, you have the following options to connect to the
Gemini API:

- Setting your API key as an environment variable 

- Providing your API key explicitly 

For initial testing, you can hard code an API key, but this should only be
temporary since it's not secure. You can find examples for hard coding the API
key in Providing API key explicitly section.

## Google Cloud projects

 Google Cloud projects 
are fundamental to using Google Cloud services (such as the Gemini API),
managing billing, and controlling collaborators and permissions. Google AI
Studio provides a lightweight interface to your Google Cloud projects.

If you don't have
any projects created yet, you must either create a new project or import one
from Google Cloud into Google AI Studio. The Projects page in Google AI
Studio will display all keys that have sufficient permission to use the Gemini
API. Refer to the import projects section for instructions.

### Default project

For new users, after accepting Terms of Service, Google AI Studio creates a
default Google Cloud Project and API Key, for ease of use. You can rename this
project in Google AI Studio by navigating to Projects view in the
 Dashboard , clicking the 3 dots settings button next to a project and
choosing Rename project . Existing users, or users who already have Google
Cloud Accounts won't have a default project created.

## Import projects

Each Gemini API key is associated with a Google Cloud project. By default,
Google AI Studio does not show all of your Cloud Projects. You must import the
projects you want by searching for the name or project ID in the
 Import Projects dialog. To view a complete list of projects you have access
to, visit the Cloud Console.

If you don't have any projects imported yet, follow these steps to import a
Google Cloud project and create a key:

- Go to Google AI Studio .

- Open the Dashboard from the left side panel.

- Select Projects .

- Select the Import projects button in the Projects page.

- Search for and select the Google Cloud project you want to import and select
the Import button.

Once a project is imported, go to the API Keys page from the Dashboard 
menu and create an API key in the project you just imported.

## Limitations

The following are limitations of managing API keys and Google Cloud projects in
Google AI Studio.

- You can create a maximum of 10 project at a time from the Google AI Studio
 Projects page.

- You can name and rename projects and keys.

- The API keys and Projects pages display a maximum of 100 keys and
50 projects.

- Only API keys that have no restrictions, or are restricted to the Generative
Language API are displayed.

For additional management access to your projects, visit the Google Cloud Console.

## Setting the API key as an environment variable

If you set the environment variable `GEMINI_API_KEY` or `GOOGLE_API_KEY`, the
API key will automatically be picked up by the client when using one of the
 Gemini API libraries . It's recommended that you
set only one of those variables, but if both are set, `GOOGLE_API_KEY` takes
precedence.

If you're using the REST API, or JavaScript on the browser, you will need to
provide the API key explicitly.

Here is how you can set your API key locally as the environment variable
`GEMINI_API_KEY` with different operating systems.

 
 

### Linux/macOS - Bash

Bash is a common Linux and macOS terminal configuration. You can check if
you have a configuration file for it by running the following command:

 

```
~/.bashrc
```

 

If the response is "No such file or directory", you will need to create this
file and open it by running the following commands, or use `zsh`:

 

```
touch ~/.bashrc
open ~/.bashrc
```

 

Next, you need to set your API key by adding the following export command:

 

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

 

After saving the file, apply the changes by running:

 

```
source ~/.bashrc
```

 
 

### macOS - Zsh

Zsh is a common Linux and macOS terminal configuration. You can check if
you have a configuration file for it by running the following command:

 

```
~/.zshrc
```

 

If the response is "No such file or directory", you will need to create this
file and open it by running the following commands, or use `bash`:

 

```
touch ~/.zshrc
open ~/.zshrc
```

 

Next, you need to set your API key by adding the following export command:

 

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

 

After saving the file, apply the changes by running:

 

```
source ~/.zshrc
```

 
 

### Windows

- Search for "Environment Variables" in the search bar.

- Choose to modify System Settings . You may have to confirm you want to
do this.

- In the system settings dialog, click the button labeled Environment
Variables .

- Under either User variables (for the current user) or System
variables (applies to all users who use the machine), click New... 

- Specify the variable name as `GEMINI_API_KEY`. Specify your Gemini API
Key as the variable value.

- Click OK to apply the changes.

- Open a new terminal session (cmd or Powershell) to get the new variable.

 
 

## Providing the API key explicitly

In some cases, you may want to explicitly provide an API key. For example:

- You're doing a simple API call and prefer hard coding the API key.

- You want explicit control without having to rely on automatic discovery of
environment variables by the Gemini API libraries

- You're using an environment where environment variables are not supported
(e.g web) or you are making REST calls.

Below are examples for how you can provide an API key explicitly:

 
 

### Python

 

```
from google import genai

client = genai.Client(api_key="YOUR_API_KEY")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  "YOUR_API_KEY",
        Backend: genai.BackendGeminiAPI,
    })
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = Client.builder().apiKey("YOUR_API_KEY").build();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: YOUR_API_KEY" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 

## Keep your API key secure

Treat your Gemini API key like a password. If compromised, others can use your
project's quota, incur charges (if billing is enabled), and access your
private data, such as files.

### Critical security rules

- 

 Never commit API keys to source control. Do not check your API key into version control systems like Git.

- 

 Never expose API keys on the client-side. Do not use your API key directly
in web or mobile apps in production. Keys in client-side code
(including our JavaScript/TypeScript libraries and REST calls) can be
extracted.

### Best practices

- 

 Use server-side calls with API keys The most secure way to use your API
key is to call the Gemini API from a server-side application where the key
can be kept confidential.

- 

 Use ephemeral tokens for client-side access (Live API only): For direct
client-side access to the Live API, you can use ephemeral tokens. They come with
lower security risks and can be suitable for production use. Review
 ephemeral tokens guide for more information.

- 

 Consider adding restrictions to your key: You can limit a key's permissions
by adding API key restrictions .
This minimizes the potential damage if the key is ever leaked.

For some general best practices, you can also review this
 support article .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Build mode in Google AI Studio &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/aistudio-build-mode

- 
 
 
 
 
 
 
 
 
 
 
 Build mode in Google AI Studio  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Build mode in Google AI Studio 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page describes how you can use the Build mode in Google AI Studio to
quickly build (or vibe code) and deploy apps that test out the latest
capabilities of Gemini like nano banana and the Live API .

## Get started

Start vibe coding in Google AI Studio's Build mode . You can
start building in a few ways:

- Start with a prompt : In Build mode, use the input box to enter a
description of what you want to build. Select AI Chips to add specific
features like image generation or Google Maps data to your prompt. You can
even say what you want using the speech-to-text button.

- "I'm Feeling Lucky" button : If you need a creative spark, use the "I'm
Feeling Lucky" button, and Gemini will generate a prompt with a project idea
to get you started.

- Remix a project from the Gallery : Open a project from the App
Gallery and select Copy App .

Once you run the prompt, you'll see the necessary code and files get generated,
with a live preview of your app appearing on the right-hand side.

## What is created?

When you run your prompt, AI Studio creates a web app. By default, it will
create a React web app but you can choose to create an Angular app in the
 Settings menu . You can view the code that gets generated by selecting the
 Code tab in the right-hand preview pane.

The following are files to note:

- geminiService.ts : This file contains the main logic for your app, from
constructing prompts to calling the Gemini API and parsing its responses. You
can edit the base prompt in this file or modify any component functionality
directly or by interactively chatting with Gemini in Build mode. Note that the
code in this file uses the GenAI TS SDK to interact with
the Gemini API.

## Continue building

Once Google AI Studio generates the initial code for your web application, you
have two primary options for continuing your project: Build in AI Studio or
 Develop Externally .

### Build in Google AI Studio

You can continue refining and expanding your application directly within the
Google AI Studio environment:

- Iterate with Gemini : Use the chat panel in Build mode to ask Gemini to
make modifications, add new features, or change the styling. For instance, you
could ask, "Add a button that alerts the user" or "Change the color scheme to
blue and white."

- Edit the Code Directly : Open the Code tab in the preview panel to make
live edits. You can save your project to GitHub to utilize version control
while developing.

### Develop externally

For more advanced workflows, you can export the code and work in your preferred
environment:

- Download and Develop Locally : Export the generated code as a ZIP file 
and import it into your code editor. This lets you use your familiar tools,
build systems, and local version control practices to continue building beyond
the initial prototype.

- Push to GitHub : Integrate the code with your existing development and
deployment processes by pushing it to a GitHub repository .

## Key features

Google AI Studio includes several features to make the building process
intuitive and visual:

- Annotation mode : Instead of writing code to change your app's appearance,
Annotation Mode lets you highlight any part of your app's UI and describe the
change you want. For example, you can select a component and type, "Make this
button blue," or "Animate this image to slide in from the left." When you
select Add to chat , a prompt is generated with a screenshot of the
annotated app.

- Share your app : You can share your creations with others to
collaborate or showcase your work.

- App Gallery : The App Gallery provides a visual library of project ideas.
You can browse what's possible with Gemini, preview applications instantly,
and remix them to make them your own.

## Deploy or archive your app

Once your application is ready, you can deploy it directly from AI Studio.
Options for deployment include:

- Google Cloud Run : Deploy your application as a scalable service. Note that
pricing for Google Cloud Run may apply based on usage.

- GitHub : Export your project to a GitHub repository to integrate it into
your existing development and deployment workflows.

## Limitations

This section outlines important limitations when using Build mode in Google AI
Studio.

API Key security and exposure

- The code for shared apps is visible to anyone who views them. Never use a real
API key directly in your app's code.

- By default, apps use a placeholder (e.g., process.env.GEMINI_API_KEY) for the
API key. When a user runs your shared app within AI Studio, AI Studio acts as a
proxy, replacing the placeholder with the end user's API key, ensuring your
key remains private.

App visibility and sharing

- Apps are stored in Google Drive and inherit its permissions model, meaning
they are private by default.

- Sharing Permissions: When you share an app with other users:

 Shared users can see the code and fork the app for their own use.

- If granted edit permission, shared users can modify the app's code.

 

Deployment outside AI Studio

- While you can deploy your app to Cloud Run for a public URL, this setup will
use your API key for all users' Gemini API calls.

 JavaScript apps are run client side, so ensure API keys only have minimal
access to prevent data leaks or misuse. For example, other File Search
Stores from the same project may be accessible to users via this mechanism.

 
- Secure external deployment: To run an app securely outside of AI Studio (e.g.,
after downloading the zip file), you must move the logic that uses the API key
to a server-side component to prevent key exposure to end users. This is not
needed if you deploy using Cloud Run.

- Key exposure warning: Simply replacing the placeholder with a real API key in a
client-side environment is strongly discouraged, as the key will become
visible to any user.

Tool and feature support

- Local development import: Currently, you cannot develop apps locally with
external tools and import them into AI Studio.

## What's Next?

- See what others have built and get inspired by remixing an existing project in
the App Gallery .

- Check out the YouTube playlist for a collection of AI Studio
vibe coding tutorials to help you get started.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini API libraries &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/libraries

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API libraries  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API libraries 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

When building with the Gemini API, we recommend using the Google GenAI SDK .
These are the official, production-ready libraries that we develop and maintain
for the most popular languages. They are in General Availability and used in all our official
documentation and examples.

If you're new to the Gemini API, follow our quickstart guide to get started.

## Language support and installation

The Google GenAI SDK is available for the Python, JavaScript/TypeScript, Go and
Java languages. You can install each language's library using package managers,
or visit their GitHub repos for further engagement:

 
 

### Python

- 

Library: `google-genai` 

- 

GitHub Repository: googleapis/python-genai 

- 

Installation: `pip install google-genai`

 
 

### JavaScript

- 

Library: `@google/genai` 

- 

GitHub Repository: googleapis/js-genai 

- 

Installation: `npm install @google/genai`

 
 

### Go

- 

Library: `google.golang.org/genai` 

- 

GitHub Repository: googleapis/go-genai 

- 

Installation: `go get google.golang.org/genai`

 
 

### Java

- 

Library: `google-genai`

- 

GitHub Repository: googleapis/java-genai 

- 

Installation: If you're using Maven, add the following to your dependencies:

 

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

 
 

### C#

- 

Library: `Google.GenAI`

- 

GitHub Repository: googleapis/go-genai 

- 

Installation: `dotnet add package Google.GenAI`

 
 

## General availability

We started rolling out Google GenAI SDK, a new set of libraries to access Gemini
API, in late 2024 when we launched Gemini 2.0.

As of May 2025, they reached General Availability (GA) across all supported
platforms and are the recommended libraries to access the Gemini API. They are
stable, fully supported for production use, and are actively maintained. They
provide access to the latest features, and offer the best performance working
with Gemini.

If you're using one of our legacy libraries,
we strongly recommend you migrate so that you can access the latest features and
get the best performance working with Gemini. Review the legacy libraries section for more information.

## Legacy libraries and migration

If you are using one of our legacy libraries, we recommend that you
 migrate to the new libraries .

The legacy libraries don't provide access to recent features (such as
 Live API and Veo ) and are on
a deprecation path. They will stop receiving updates on November 30th,
2025, the feature gaps will grow and potential bugs may no longer get fixed.

Each legacy library's support status varies, detailed in the following table:

 
 
 
 
 
 
 
 
 
 Language 
 Legacy library 
 Support status 
 Recommended library 
 
 
 
 
 Python 
 `google-generativeai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `google-genai` 
 
 
 JavaScript/TypeScript 
 `@google/generativeai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `@google/genai` 
 
 
 Go 
 `google.golang.org/generative-ai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `google.golang.org/genai` 
 
 
 Dart and Flutter 
 `google_generative_ai` 
 Not actively maintained 
 Use trusted community or third party libraries, like firebase_ai , or access using REST API 
 
 
 Swift 
 `generative-ai-swift` 
 Not actively maintained 
 Use Firebase AI Logic 
 
 
 Android 
 `generative-ai-android` 
 Not actively maintained 
 Use Firebase AI Logic 
 
 
 

 Note for Java developers: There was no legacy Google-provided Java SDK for
the Gemini API, so no migration from a previous Google library is required. You
can start directly with the new library in the
 Language support and installation section.

## Prompt templates for code generation

Generative models (e.g., Gemini, Claude) and AI-powered IDEs (e.g., Cursor) may
produce code for the Gemini API using outdated or deprecated libraries due to
their training data cutoff. For the generated code to use the latest,
recommended libraries, provide version and usage guidance directly in your
prompts. You can use the templates below to provide the necessary context:

- 

 Python 

- 

 JavaScript/TypeScript 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### OpenAI compatibility &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/openai

- 
 
 
 
 
 
 
 
 
 
 
 OpenAI compatibility  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 OpenAI compatibility 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models are accessible using the OpenAI libraries (Python and TypeScript /
Javascript) along with the REST API, by updating three lines of code
and using your Gemini API key . If you
aren't already using the OpenAI libraries, we recommend that you call the
 Gemini API directly .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ]
    }'
```

 
 

What changed? Just three lines!

- 

 `api_key="GEMINI_API_KEY"` : Replace "`GEMINI_API_KEY`" with your actual Gemini
API key, which you can get in Google AI Studio .

- 

 

```
base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
```

: This
tells the OpenAI library to send requests to the Gemini API endpoint instead of
the default URL.

- 

 `model="gemini-2.5-flash"` : Choose a compatible Gemini model

## Thinking

Gemini 3 and 2.5 models are trained to think through complex problems, leading
to significantly improved reasoning. The Gemini API comes with thinking
parameters which give fine grain
control over how much the model will think.

Gemini 3 uses `"low"` and `"high"` thinking levels, and Gemini 2.5 models use
exact thinking budgets. These map to OpenAI's reasoning efforts as follows:

 
 
 `reasoning_effort` (OpenAI) 
 `thinking_level` (Gemini 3) 
 `thinking_budget` (Gemini 2.5) 
 
 
 `minimal` 
 `low` 
 `1,024` 
 
 
 `low` 
 `low` 
 `1,024` 
 
 
 `medium` 
 `high` 
 `8,192` 
 
 
 `high` 
 `high` 
 `24,576` 
 
 

If no `reasoning_effort` is specified, Gemini uses the model's
default level or budget .

If you want to disable thinking, you can set `reasoning_effort` to `"none"` for
2.5 models. Reasoning cannot be turned off for Gemini 2.5 Pro or 3 models.

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    reasoning_effort="low",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    reasoning_effort: "low",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
    "reasoning_effort": "low",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
      ]
    }'
```

 
 

Gemini thinking models also produce thought summaries .
You can use the `extra_body` field to include Gemini fields
in your request.

Note that `reasoning_effort` and `thinking_level`/`thinking_budget` overlap
functionality, so they can't be used at the same time.

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "Explain to me how AI works"}],
    extra_body={
      'extra_body': {
        "google": {
          "thinking_config": {
            "thinking_budget": "low",
            "include_thoughts": True
          }
        }
      }
    }
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    messages: [{role: "user", content: "Explain to me how AI works",}],
    extra_body: {
      "google": {
        "thinking_config": {
          "thinking_budget": "low",
          "include_thoughts": true
        }
      }
    }
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
      "messages": [{"role": "user", "content": "Explain to me how AI works"}],
      "extra_body": {
        "google": {
           "thinking_config": {
             "include_thoughts": true
           }
        }
      }
    }'
```

 
 

Gemini 3 supports OpenAI compatibility for thought signatures in chat completion
APIs. You can find the full example on the thought signatures page.

## Streaming

The Gemini API supports streaming responses .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  stream=True
)

for chunk in response:
    print(chunk.choices[0].delta)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    stream: true,
  });

  for await (const chunk of completion) {
    console.log(chunk.choices[0].delta.content);
  }
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ],
    "stream": true
  }'
```

 
 

## Function calling

Function calling makes it easier for you to get structured data outputs from
generative models and is supported in the Gemini API .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Get the weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. Chicago, IL",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]

messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}]
response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=messages,
  tools=tools,
  tool_choice="auto"
)

print(response)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}];
  const tools = [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get the weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. Chicago, IL",
              },
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
          },
        }
      }
  ];

  const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: messages,
    tools: tools,
    tool_choice: "auto",
  });

  console.log(response);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
  "model": "gemini-2.0-flash",
  "messages": [
    {
      "role": "user",
      "content": "What'\''s the weather like in Chicago today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. Chicago, IL"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}'
```

 
 

## Image understanding

Gemini models are natively multimodal and provide best in class performance on
 many common vision tasks .

 
 

### Python

 

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

 
 

### JavaScript

 

```
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

 
 

### REST

 

```
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

 
 

## Generate an image

Generate an image:

 
 

### Python

 

```
import base64
from openai import OpenAI
from PIL import Image
from io import BytesIO

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

response = client.images.generate(
    model="imagen-3.0-generate-002",
    prompt="a portrait of a sheepadoodle wearing a cape",
    response_format='b64_json',
    n=1,
)

for image_data in response.data:
  image = Image.open(BytesIO(base64.b64decode(image_data.b64_json)))
  image.show()
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const image = await openai.images.generate(
    {
      model: "imagen-3.0-generate-002",
      prompt: "a portrait of a sheepadoodle wearing a cape",
      response_format: "b64_json",
      n: 1,
    }
  );

  console.log(image.data);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/images/generations" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer GEMINI_API_KEY" \
  -d '{
        "model": "imagen-3.0-generate-002",
        "prompt": "a portrait of a sheepadoodle wearing a cape",
        "response_format": "b64_json",
        "n": 1,
      }'
```

 
 

## Audio understanding

Analyze audio input:

 
 

### Python

 

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

with open("/path/to/your/audio/file.wav", "rb") as audio_file:
  base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Transcribe this audio",
        },
        {
              "type": "input_audio",
              "input_audio": {
                "data": base64_audio,
                "format": "wav"
          }
        }
      ],
    }
  ],
)

print(response.choices[0].message.content)
```

 
 

### JavaScript

 

```
import fs from "fs";
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

const audioFile = fs.readFileSync("/path/to/your/audio/file.wav");
const base64Audio = Buffer.from(audioFile).toString("base64");

async function main() {
  const response = await client.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Transcribe this audio",
          },
          {
            type: "input_audio",
            input_audio: {
              data: base64Audio,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

  console.log(response.choices[0].message.content);
}

main();
```

 
 

### REST

 

```
bash -c '
  base64_audio=$(base64 -i "/path/to/your/audio/file.wav");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"Transcribe this audio file.\" },
            {
              \"type\": \"input_audio\",
              \"input_audio\": {
                \"data\": \"${base64_audio}\",
                \"format\": \"wav\"
              }
            }
          ]
        }
      ]
    }"
'
```

 
 

## Structured output

Gemini models can output JSON objects in any structure you define .

 
 

### Python

 

```
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "John and Susan are going to an AI conference on Friday."},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
```

 
 

## Embeddings

Text embeddings measure the relatedness of text strings and can be generated
using the Gemini API .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.embeddings.create(
    input="Your text string goes here",
    model="gemini-embedding-001"
)

print(response.data[0].embedding)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const embedding = await openai.embeddings.create({
    model: "gemini-embedding-001",
    input: "Your text string goes here",
  });

  console.log(embedding);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/embeddings" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "input": "Your text string goes here",
    "model": "gemini-embedding-001"
  }'
```

 
 

## Batch API

You can create batch jobs , submit them, and check
their status using the OpenAI library.

You'll need to prepare the JSONL file in OpenAI input format. For example:

 

```
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "Tell me a one-sentence joke."}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "Why is the sky blue?"}]}}
```

 

OpenAI compatibility for Batch supports creating a batch,
monitoring job status, and viewing batch results.

Compatibility for upload and download is currently not supported. Instead, the
following example uses the `genai` client for uploading and downloading
 files , the same as when using the Gemini Batch API .

 
 

### Python

 

```
from openai import OpenAI

# Regular genai client for uploads & downloads
from google import genai
client = genai.Client()

openai_client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Upload the JSONL file in OpenAI input format, using regular genai SDK
uploaded_file = client.files.upload(
    file='my-batch-requests.jsonl',
    config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')
)

# Create batch
batch = openai_client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/chat/completions",
    completion_window="24h"
)

# Wait for batch to finish (up to 24h)
while True:
    batch = client.batches.retrieve(batch.id)
    if batch.status in ('completed', 'failed', 'cancelled', 'expired'):
        break
    print(f"Batch not finished. Current state: {batch.status}. Waiting 30 seconds...")
    time.sleep(30)
print(f"Batch finished: {batch}")

# Download results in OpenAI output format, using regular genai SDK
file_content = genai_client.files.download(file=batch.output_file_id).decode('utf-8')

# See batch_output JSONL in OpenAI output format
for line in file_content.splitlines():
    print(line)
```

 
 

The OpenAI SDK also supports generating embeddings with the Batch API . To do so, switch out the
`create` method's `endpoint` field for an embeddings endpoint, as well as the
`url` and `model` keys in the JSONL file:

 

```
# JSONL file using embeddings model and endpoint
# {"custom_id": "request-1", "method": "POST", "url": "/v1/embeddings", "body": {"model": "ggemini-embedding-001", "messages": [{"role": "user", "content": "Tell me a one-sentence joke."}]}}
# {"custom_id": "request-2", "method": "POST", "url": "/v1/embeddings", "body": {"model": "gemini-embedding-001", "messages": [{"role": "user", "content": "Why is the sky blue?"}]}}

# ...

# Create batch step with embeddings endpoint
batch = openai_client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/embeddings",
    completion_window="24h"
)
```

 

See the Batch embedding generation 
section of the OpenAI compatibility cookbook for a complete example.

## `extra_body`

There are several features supported by Gemini that are not available in OpenAI
models but can be enabled using the `extra_body` field.

 `extra_body` features 

 
 
 `cached_content` 
 Corresponds to Gemini's `GenerateContentRequest.cached_content`. 
 
 
 `thinking_config` 
 Corresponds to Gemini's `ThinkingConfig`. 
 
 

### `cached_content`

Here's an example of using `extra_body` to set `cached_content`:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key=MY_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

stream = client.chat.completions.create(
    model="gemini-2.5-pro",
    n=1,
    messages=[
        {
            "role": "user",
            "content": "Summarize the video"
        }
    ],
    stream=True,
    stream_options={'include_usage': True},
    extra_body={
        'extra_body':
        {
            'google': {
              'cached_content': "cachedContents/0000aaaa1111bbbb2222cccc3333dddd4444eeee"
          }
        }
    }
)

for chunk in stream:
    print(chunk)
    print(chunk.usage.to_dict())
```

 
 

## List models

Get a list of available Gemini models:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

models = client.models.list()
for model in models:
  print(model.id)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models \
-H "Authorization: Bearer GEMINI_API_KEY"
```

 
 

## Retrieve a model

Retrieve a Gemini model:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

model = client.models.retrieve("gemini-2.0-flash")
print(model.id)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const model = await openai.models.retrieve("gemini-2.0-flash");
  console.log(model.id);
}

main();
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models/gemini-2.0-flash \
-H "Authorization: Bearer GEMINI_API_KEY"
```

 
 

## Current limitations

Support for the OpenAI libraries is still in beta while we extend feature support.

If you have questions about supported parameters, upcoming features, or run into
any issues getting started with Gemini, join our Developer Forum .

## What's next

Try our OpenAI Compatibility Colab to work through more detailed
examples.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini models &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/models

- 
 
 
 
 
 
 
 
 
 
 
 Gemini models  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini models 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 

OUR MOST INTELLIGENT MODEL

 

## Gemini 3 Pro

 

 The best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 3 Pro Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-3-pro-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, Image, Video, Audio, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Preview: gemini-3-pro-preview`

 

 
 
 
 
 calendar_month Latest update 
 November 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

 
 
 

OUR ADVANCED THINKING MODEL

 

## Gemini 2.5 Pro

 

 Our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Pro

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, text, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Stable: gemini-2.5-pro`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Pro TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-pro-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

FAST AND INTELLIGENT

 

## Gemini 2.5 Flash

 

 Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL Context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-image` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Images and text

 
 
 

 Output 

 

Images and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

65,536

 
 
 

 Output token limit 

 

32,768

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-image`

 - Preview: `gemini-2.5-flash-image-preview`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 cognition_2 Knowledge cutoff 
 June 2025 
 
 
 
 
 

### Gemini 2.5 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, text

 
 
 

 Output 

 

Audio and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

131,072

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-native-audio-preview-09-2025`

 - Preview: `gemini-live-2.5-flash-preview`

 

 gemini-live-2.5-flash-preview will be deprecated on December 09, 2025 

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-flash-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

ULTRA FAST

 

## Gemini 2.5 Flash-Lite

 

 Our fastest flash model optimized for cost-efficiency and high throughput.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash-Lite

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-lite`

 

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash-Lite Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-lite-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

## Previous Gemini models

 
 
 

OUR SECOND GENERATION WORKHORSE MODEL

 

## Gemini 2.0 Flash

 

 Our second generation workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

 Gemini 2.0 Flash delivers next-gen features and improved capabilities,
 including superior speed, native tool use, and a 1M token
 context window.
 

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.0 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Experimental 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash`

 - Stable: `gemini-2.0-flash-001`

 - Experimental: `gemini-2.0-flash-exp`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-preview-image-generation` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text and images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

32,768

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-preview-image-generation`

 

 gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa 

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.0-flash-live-001`
 

 gemini-2.0-flash-live-001 will be deprecated on December 09, 2025 

 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, and text

 
 
 

 Output 

 

Text, and audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-live-001`

 

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 
 
 

 
 
 

OUR SECOND GENERATION FAST MODEL

 

## Gemini 2.0 Flash-Lite

 

 Our second generation small workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

 

 Try in Google AI Studio 

 

#### Model details

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash-lite`

 - Stable: `gemini-2.0-flash-lite-001`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 

## Model version name patterns

Gemini models are available in either stable , preview , latest , or
 experimental versions.

### Stable

Points to a specific stable model. Stable models usually don't change. Most
production apps should use a specific stable model.

For example: `gemini-2.5-flash`.

### Preview

Points to a preview model which may be used for production. Preview models will
typically have billing enabled, might come with more restrictive rate limits and
will be deprecated with at least 2 weeks notice.

For example: `gemini-2.5-flash-preview-09-2025`.

### Latest

Points to the latest release for a specific model variation. This can be a
stable, preview or experimental release. This alias will get hot-swapped with
every new release of a specific model variation. A 2-week notice will
be provided through email before the version behind latest is changed.

For example: `gemini-flash-latest`.

### Experimental

Points to an experimental model which will typically be not be suitable for
production use and come with more restrictive rate limits. We release
experimental models to gather feedback and get our latest updates into the hands
of developers quickly.

Experimental models are not stable and availability of model endpoints is
subject to change.

## Model deprecations

For information about model deprecations, visit the Gemini deprecations page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini 3 Developer Guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/gemini-3

- 
 
 
 
 
 
 
 
 
 
 
 Gemini 3 Developer Guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini 3 Developer Guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini 3 is our most intelligent model family to date, built on a foundation of state-of-the-art reasoning. It is designed to bring any idea to life by mastering agentic workflows, autonomous coding, and complex multimodal tasks. This guide covers key features of the Gemini 3 model family and how to get the most out of it.

 
 
 
 

Gemini 3 Pro uses dynamic thinking by default to reason through prompts. For faster, lower-latency responses when complex reasoning isn't required, you can constrain the model's thinking level to `low`.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Find the race condition in this multi-threaded C++ snippet: [code here]"}]
    }]
  }'
```

 
 

## Explore

 

Explore our collection of Gemini 3 apps to see how the model handles advanced reasoning, autonomous coding, and complex multimodal tasks.

## Meet Gemini 3

Gemini 3 Pro is the first model in the new series. `gemini-3-pro-preview` is best for your complex tasks that require broad world knowledge and advanced reasoning across modalities.

 
 
 
 Model ID 
 Context Window (In / Out) 
 Knowledge Cutoff 
 Pricing (Input / Output)* 
 
 

 
 
 gemini-3-pro-preview 
 1M / 64k 
 Jan 2025 
 $2 / $12 (<200k tokens) 
 $4 / $18 (>200k tokens) 
 
 
 

 * Pricing is per 1 million tokens. Prices listed are for standard text; multimodal input rates may vary. 

For detailed rate limits, batch pricing, and additional information, see the models page .

## New API features in Gemini 3

Gemini 3 introduces new parameters designed to give developers more control over latency, cost, and multimodal fidelity.

### Thinking level

The `thinking_level` parameter controls the maximum depth of the model's internal reasoning process before it produces a response. Gemini 3 treats these levels as relative allowances for thinking rather than strict token guarantees. If `thinking_level` is not specified, Gemini 3 Pro will default to `high`.

- `low`: Minimizes latency and cost. Best for simple instruction following, chat, or high-throughput applications

- `medium`: (Coming soon), not supported at launch 

- `high` (Default): Maximizes reasoning depth. The model may take significantly longer to reach a first token, but the output will be more carefully reasoned.

### Media resolution

Gemini 3 introduces granular control over multimodal vision processing via the `media_resolution` parameter. Higher resolutions improve the model's ability to read fine text or identify small details, but increase token usage and latency. The `media_resolution` parameter determines the maximum number of tokens allocated per input image or video frame. 

You can now set the resolution to `media_resolution_low`, `media_resolution_medium`, or `media_resolution_high` per individual media part or globally (via `generation_config`). If unspecified, the model uses optimal defaults based on the media type. 

 Recommended settings 

 
 
 
 Media Type 
 Recommended Setting 
 Max Tokens 
 Usage Guidance 
 
 

 
 
 Images 
 `media_resolution_high` 
 1120 
 Recommended for most image analysis tasks to ensure maximum quality. 
 
 
 PDFs 
 `media_resolution_medium` 
 560 
 Optimal for document understanding; quality typically saturates at `medium`. Increasing to `high` rarely improves OCR results for standard documents. 
 
 
 Video (General) 
 `media_resolution_low` (or `media_resolution_medium`) 
 70 (per frame) 
 Note: For video, `low` and `medium` settings are treated identically (70 tokens) to optimize context usage. This is sufficient for most action recognition and description tasks. 
 
 
 Video (Text-heavy) 
 `media_resolution_high` 
 280 (per frame) 
 Required only when the use case involves reading dense text (OCR) or small details within video frames. 
 
 
 
 
 

### Python

 

```
from google import genai
from google.genai import types
import base64

# The media_resolution parameter is currently only available in the v1alpha API version.
client = genai.Client(http_options={'api_version': 'v1alpha'})

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents=[
        types.Content(
            parts=[
                types.Part(text="What is in this image?"),
                types.Part(
                    inline_data=types.Blob(
                        mime_type="image/jpeg",
                        data=base64.b64decode("..."),
                    ),
                    media_resolution={"level": "media_resolution_high"}
                )
            ]
        )
    ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// The media_resolution parameter is currently only available in the v1alpha API version.
const ai = new GoogleGenAI({ apiVersion: "v1alpha" });

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: [
      {
        parts: [
          { text: "What is in this image?" },
          {
            inlineData: {
              mimeType: "image/jpeg",
              data: "...",
            },
            mediaResolution: {
              level: "media_resolution_high"
            }
          }
        ]
      }
    ]
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [
        { "text": "What is in this image?" },
        {
          "inlineData": {
            "mimeType": "image/jpeg",
            "data": "..."
          },
          "mediaResolution": {
            "level": "media_resolution_high"
          }
        }
      ]
    }]
  }'
```

 
 

### Temperature

For Gemini 3, we strongly recommend keeping the temperature parameter at its default value of `1.0`.

While previous models often benefited from tuning temperature to control creativity versus determinism, Gemini 3's reasoning capabilities are optimized for the default setting. Changing the temperature (setting it below 1.0) may lead to unexpected behavior, such as looping or degraded performance, particularly in complex mathematical or reasoning tasks.

### Thought signatures

Gemini 3 uses Thought signatures to maintain reasoning context across API calls. These signatures are encrypted representations of the model's internal thought process. To ensure the model maintains its reasoning capabilities you must return these signatures back to the model in your request exactly as they were received:

- Function Calling (Strict): The API enforces strict validation on the "Current Turn". Missing signatures will result in a 400 error.

- Text/Chat: Validation is not strictly enforced, but omitting signatures will degrade the model's reasoning and answer quality.

#### Function calling (strict validation)

When Gemini generates a `functionCall`, it relies on the `thoughtSignature` to process the tool's output correctly in the next turn. The "Current Turn" includes all Model (`functionCall`) and User (`functionResponse`) steps that occurred since the last standard User `text` message.

- Single Function Call: The `functionCall` part contains a signature. You must return it. 

- Parallel Function Calls: Only the first `functionCall` part in the list will contain the signature. You must return the parts in the exact order received.

- Multi-Step (Sequential): If the model calls a tool, receives a result, and calls another tool (within the same turn), both function calls have signatures. You must return all accumulated signatures in the history.

#### Text and streaming

For standard chat or text generation, the presence of a signature is not guaranteed.

- Non-Streaming : The final content part of the response may contain a `thoughtSignature`, though it is not always present. If one is returned, you should send it back to maintain best performance. 

- Streaming : If a signature is generated, it may arrive in a final chunk that contains an empty text part. Ensure your stream parser checks for signatures even if the text field is empty.

#### Code examples

 
 
 

#### Multi-step Function Calling (Sequential)

 

The user asks a question requiring two separate steps (Check Flight -> Book Taxi) in one turn. 

 Step 1: Model calls Flight Tool. 

 The model returns a signature ` `

 

```
// Model Response (Turn 1, Step 1)
  {
    "role": "model",
    "parts": [
      {
        "functionCall": { "name": "check_flight", "args": {...} },
        "thoughtSignature": "<Sig_A>" // SAVE THIS
      }
    ]
  }
```

 
 

 Step 2: User sends Flight Result 

 We must send back ` ` to keep the model's train of thought.

 

```
// User Request (Turn 1, Step 2)
[
  { "role": "user", "parts": [{ "text": "Check flight AA100..." }] },
  { 
    "role": "model", 
    "parts": [
      { 
        "functionCall": { "name": "check_flight", "args": {...} }, 
        "thoughtSignature": "<Sig_A>" // REQUIRED
      } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": { "name": "check_flight", "response": {...} } }] }
]
```

 
 

 Step 3: Model calls Taxi Tool 

 The model remembers the flight delay via ` ` and now decides to book a taxi. It generates a new signature ` `.

 

```
// Model Response (Turn 1, Step 3)
{
  "role": "model",
  "parts": [
    {
      "functionCall": { "name": "book_taxi", "args": {...} },
      "thoughtSignature": "<Sig_B>" // SAVE THIS
    }
  ]
}
```

 
 

 Step 4: User sends Taxi Result 

 To complete the turn, you must send back the entire chain: ` ` AND ` `.

 

```
// User Request (Turn 1, Step 4)
[
  // ... previous history ...
  { 
    "role": "model", 
    "parts": [
       { "functionCall": { "name": "check_flight", ... }, "thoughtSignature": "<Sig_A>" } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": {...} }] },
  { 
    "role": "model", 
    "parts": [
       { "functionCall": { "name": "book_taxi", ... }, "thoughtSignature": "<Sig_B>" } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": {...} }] }
]
```

 
 
 

 
 
 

#### Parallel Function Calling

 

The user asks: "Check the weather in Paris and London." The model returns two function calls in one response. 

 

```
// User Request (Sending Parallel Results)
[
  {
    "role": "user",
    "parts": [
      { "text": "Check the weather in Paris and London." }
    ]
  },
  {
    "role": "model",
    "parts": [
      // 1. First Function Call has the signature
      {
        "functionCall": { "name": "check_weather", "args": { "city": "Paris" } },
        "thoughtSignature": "<Signature_A>" 
      },
      // 2. Subsequent parallel calls DO NOT have signatures
      {
        "functionCall": { "name": "check_weather", "args": { "city": "London" } }
      } 
    ]
  },
  {
    "role": "user",
    "parts": [
      // 3. Function Responses are grouped together in the next block
      {
        "functionResponse": { "name": "check_weather", "response": { "temp": "15C" } }
      },
      {
        "functionResponse": { "name": "check_weather", "response": { "temp": "12C" } }
      }
    ]
  }
]
```

 
 
 

 
 
 

#### Text/In-Context Reasoning (No Validation)

 The user asks a question that requires in-context reasoning without external tools. While not strictly validated, including the signature helps the model maintain the reasoning chain for follow-up questions.

 

```
// User Request (Follow-up question)
[
  { 
    "role": "user", 
    "parts": [{ "text": "What are the risks of this investment?" }] 
  },
  { 
    "role": "model", 
    "parts": [
      {
        "text": "I need to calculate the risk step-by-step. First, I'll look at volatility...",
        "thoughtSignature": "<Signature_C>" // Recommended to include
      }
    ]
  },
  { 
    "role": "user", 
    "parts": [{ "text": "Summarize that in one sentence." }] 
  }
]
```

 
 
 

#### Migrating from other models

If you are transferring a conversation trace from another model (e.g., Gemini 2.5) or injecting a custom function call that was not generated by Gemini 3, you will not have a valid signature.

To bypass strict validation in these specific scenarios, populate the field with this specific dummy string: 

```
"thoughtSignature": "context_engineering_is_the_way_to_go"
```



### Structured Outputs with tools

Gemini 3 allows you to combine Structured Outputs with built-in tools, including Grounding with Google Search , URL Context , and Code Execution .

 
 

### Python

 

```
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from typing import List

class MatchResult(BaseModel):
    winner: str = Field(description="The name of the winner.")
    final_match_score: str = Field(description="The final match score.")
    scorers: List[str] = Field(description="The name of the scorer.")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Search for all details for the latest Euro.",
    config={
        "tools": [
            {"google_search": {}},
            {"url_context": {}}
        ],
        "response_mime_type": "application/json",
        "response_json_schema": MatchResult.model_json_schema(),
    },  
)

result = MatchResult.model_validate_json(response.text)
print(result)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});

const matchSchema = z.object({
  winner: z.string().describe("The name of the winner."),
  final_match_score: z.string().describe("The final score."),
  scorers: z.array(z.string()).describe("The name of the scorer.")
});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Search for all details for the latest Euro.",
    config: {
      tools: [
        { googleSearch: {} },
        { urlContext: {} }
      ],
      responseMimeType: "application/json",
      responseJsonSchema: zodToJsonSchema(matchSchema),
    },
  });

  const match = matchSchema.parse(JSON.parse(response.text));
  console.log(match);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Search for all details for the latest Euro."}]
    }],
    "tools": [
      {"googleSearch": {}},
      {"urlContext": {}}
    ],
    "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
            "type": "object",
            "properties": {
                "winner": {"type": "string", "description": "The name of the winner."},
                "final_match_score": {"type": "string", "description": "The final score."},
                "scorers": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "The name of the scorer."
                }
            },
            "required": ["winner", "final_match_score", "scorers"]
        }
    }
  }'
```

 
 

## Migrating from Gemini 2.5

Gemini 3 is our most capable model family to date and offers a stepwise improvement over Gemini 2.5 Pro. When migrating, consider the following:

- Thinking: If you were previously using complex prompt engineering (like Chain-of-thought) to force Gemini 2.5 to reason, try Gemini 3 with `thinking_level: "high"` and simplified prompts. 

- Temperature settings: If your existing code explicitly sets temperature (especially to low values for deterministic outputs), we recommend removing this parameter and using the Gemini 3 default of 1.0 to avoid potential looping issues or performance degradation on complex tasks. 

- PDF & document understanding: Default OCR resolution for PDFs has changed. If you relied on specific behavior for dense document parsing, test the new `media_resolution_high` setting to ensure continued accuracy. 

- Token consumption: Migrating to Gemini 3 Pro defaults may increase token usage for PDFs but decrease token usage for video. If requests now exceed the context window due to higher default resolutions, we recommend explicitly reducing the media resolution. 

- Image segmentation: Image segmentation capabilities (returning pixel-level masks for objects) are not supported in Gemini 3 Pro. For workloads requiring native image segmentation, we recommend continuing to utilize Gemini 2.5 Flash with thinking turned off or Gemini Robotics-ER 1.5 .

## OpenAI compatibility

For users utilizing the OpenAI compatibility layer, standard parameters are automatically mapped to Gemini equivalents:

- `reasoning_effort` (OAI) maps to `thinking_level` (Gemini). Note that `reasoning_effort` medium maps to `thinking_level` high.

## Prompting best practices

Gemini 3 is a reasoning model, which changes how you should prompt.

- Precise instructions: Be concise in your input prompts. Gemini 3 responds best to direct, clear instructions. It may over-analyze verbose or overly complex prompt engineering techniques used for older models. 

- Output verbosity: By default, Gemini 3 is less verbose and prefers providing direct, efficient answers. If your use case requires a more conversational or "chatty" persona, you must explicitly steer the model in the prompt (e.g., "Explain this as a friendly, talkative assistant"). 

- Context management: When working with large datasets (e.g., entire books, codebases, or long videos), place your specific instructions or questions at the end of the prompt, after the data context. Anchor the model's reasoning to the provided data by starting your question with a phrase like, "Based on the information above...".

Learn more about prompt design strategies in the prompt engineering guide . 

## FAQ

- 

 What is the knowledge cutoff for Gemini 3 Pro? Gemini 3 has a knowledge cutoff of January 2025. For more recent information, use the Search Grounding tool. 

- 

 What are the context window limits? Gemini 3 Pro supports a 1 million token input context window and up to 64k tokens of output. 

- 

 Is there a free tier for Gemini 3 Pro? You can try the model for free in Google AI Studio, but currently, there is no free tier available for `gemini-3-pro-preview` in the Gemini API. 

- 

 Will my old `thinking_budget` code still work? Yes, `thinking_budget` is still supported for backward compatibility, but we recommend migrating to `thinking_level` for more predictable performance. Do not use both in the same request.

- 

 Does Gemini 3 support the Batch API? Yes, Gemini 3 supports the Batch API. 

- 

 Is Context Caching supported? Yes, Context Caching is supported for Gemini 3. The minimum token count required to initiate caching is 2,048 tokens.

- 

 Which tools are supported in Gemini 3? Gemini 3 supports Google Search , File Search , Code Execution , and URL Context . It also supports standard Function Calling for your own custom tools. Please note that Google Maps and Computer Use are currently not supported.

## Next steps

- Get started with the Gemini 3 Cookbook 

- Check the dedicated Cookbook guide on thinking levels and how to migrate from thinking budget to thinking levels.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Generate images using Imagen &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/imagen

- 
 
 
 
 
 
 
 
 
 
 
 Generate images using Imagen  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ÔøΩÔøΩ‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Generate images using Imagen 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Imagen is Google's high-fidelity image generation model, capable of generating
realistic and high quality images from text prompts. All generated images
include a SynthID watermark. To learn more about the available Imagen model
variants, see the Model versions section.

## Generate images using the Imagen models

This example demonstrates generating images with an Imagen model :

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-001',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-001",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

 
 

### REST

 

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-001:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

 
 
 
 
 AI-generated image of a robot holding a red skateboard 
 

### Imagen configuration

Imagen supports English only prompts at this time and the following parameters:

- `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive).
The default is 4.

- `imageSize`: The size of the generated image. This is only supported for
the Standard and Ultra models. The supported values are `1K` and `2K`.
Default is `1K`.

- `aspectRatio`: Changes the aspect ratio of the generated image. Supported
values are `"1:1"`, `"3:4"`, `"4:3"`, `"9:16"`, and `"16:9"`. The default is
`"1:1"`.

- 

`personGeneration`: Allow the model to generate images of people. The
following values are supported:

 `"dont_allow"`: Block generation of images of people.

- `"allow_adult"`: Generate images of adults, but not children. This is
the default.

- `"allow_all"`: Generate images that include adults and children.

 

## Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt
can produce different results, along with examples of images you can create.

### Prompt writing basics

A good prompt is descriptive and clear, and makes use of meaningful keywords and
modifiers. Start by thinking of your subject , context , and style .

 
 
 Image text: A sketch ( style ) of a modern apartment building 
( subject ) surrounded by skyscrapers ( context and background ). 
 

- 

 Subject : The first thing to think about with any prompt is the
 subject : the object, person, animal, or scenery you want an image of.

- 

 Context and background: Just as important is the background or context 
in which the subject will be placed. Try placing your subject in a variety
of backgrounds. For example, a studio with a white background, outdoors, or
indoor environments.

- 

 Style: Finally, add the style of image you want. Styles can be general
(painting, photograph, sketches) or very specific (pastel painting, charcoal
drawing, isometric 3D). You can also combine styles.

After you write a first version of your prompt, refine your prompt by adding
more details until you get to the image that you want. Iteration is important.
Start by establishing your core idea, and then refine and expand upon that core
idea until the generated image is close to your vision.

 
 
 
 
 
 Prompt: A park in the spring next to a lake 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour, red wildflowers 
 
 
 
 

Imagen models can transform your ideas into detailed images, whether
your prompts are short or long and detailed. Refine your vision
through iterative prompting, adding details until you achieve the perfect
result.

 
 
 
 
 

Short prompts let you generate an image quickly.

 
 
 Prompt: close-up photo of a woman in her 20s, street photography,
 movie still, muted orange warm tones
 
 
 
 
 
 
 

Longer prompts let you add specific details and build your image.

 
 
 Prompt: captivating photo of a woman in her 20s utilizing a street
 photography style. The image should look like a movie still with muted orange
 warm tones.
 
 
 
 
 
 

Additional advice for Imagen prompt writing:

- Use descriptive language : Employ detailed adjectives and adverbs to
paint a clear picture for Imagen.

- Provide context : If necessary, include background information to aid the
AI's understanding.

- Reference specific artists or styles : If you have a particular aesthetic
in mind, referencing specific artists or art movements can be helpful.

- Use prompt engineering tools : Consider exploring prompt engineering
tools or resources to help you refine your prompts and achieve optimal
results.

- Enhancing the facial details in your personal and group images : Specify facial details as a focus of the photo (for example, use the
 word "portrait" in the prompt).

### Generate text in images

Imagen models can add text into images, opening up more creative image generation
possibilities. Use the following guidance to get the most out of this feature:

- Iterate with confidence : You might have to regenerate images until you
achieve the look you want. Imagen's text integration is still
evolving, and sometimes multiple attempts yield the best results.

- Keep it short : Limit text to 25 characters or less for optimal
generation.

- 

 Multiple phrases : Experiment with two or three distinct phrases to
provide additional information. Avoid exceeding three phrases for cleaner
compositions.

 
 
 Prompt: A poster with the text "Summerland" in bold font as a
title, underneath this text is the slogan "Summer never felt so good"
 
 

- 

 Guide Placement : While Imagen can attempt to position text
as directed, expect occasional variations. This feature is continually
improving.

- 

 Inspire font style : Specify a general font style to subtly influence
Imagen's choices. Don't rely on precise font replication, but expect
creative interpretations.

- 

 Font size : Specify a font size or a general indication of size (for
example, small , medium , large ) to influence the font size generation.

### Prompt parameterization

To better control output results, you might find it helpful to parameterize the
inputs into Imagen. For example, suppose you
want your customers to be able to generate logos for their business, and you
want to make sure logos are always generated on a solid color background. You
also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the
following:

 

```
A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.
```

 

In your custom user interface, the customer can input the parameters using
a menu, and their chosen value populates the prompt Imagen receives.

For example:

- 

Prompt: 

```
A minimalist logo for a health care company on a solid color background. Include the text Journey.
```



 

- 

Prompt: 

```
A modern logo for a software company on a solid color background. Include the text Silo.
```



 

- 

Prompt: 

```
A traditional logo for a baking company on a solid color background. Include the text Seed.
```



 

### Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes
like photography descriptors, shapes and materials, historical art
movements, and image quality modifiers.

#### Photography

- Prompt includes: "A photo of..." 

To use this style, start with using keywords that clearly tell
Imagen that you're looking for a photograph. Start your prompts with
 "A photo of. . ." . For example:

 
 
 
 
 
 Prompt: A photo of coffee beans in a kitchen on a wooden surface 
 
 
 
 
 
 Prompt: A photo of a chocolate bar on a kitchen counter 
 
 
 
 
 
 Prompt: A photo of a modern building with water in the background 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

 Photography modifiers 

In the following examples, you can see several photography-specific modifiers
and parameters. You can combine multiple modifiers for more precise control.

- 

 Camera Proximity - Close up, taken from far away 

 
 
 
 
 
 Prompt: A close-up photo of coffee beans 
 
 
 
 
 
 Prompt: A zoomed out photo of a small bag of
 coffee beans in a messy kitchen 
 
 
 
 

- 

 Camera Position - aerial, from below 

 
 
 
 
 
 Prompt: aerial photo of urban city with skyscrapers 
 
 
 
 
 
 Prompt: A photo of a forest canopy with blue skies from below 
 
 
 
 

- 

 Lighting - natural, dramatic, warm, cold 

 
 
 
 
 
 Prompt: studio photo of a modern arm chair, natural lighting 
 
 
 
 
 
 Prompt: studio photo of a modern arm chair, dramatic lighting 
 
 
 
 

- 

 Camera Settings - motion blur, soft focus, bokeh, portrait 

 
 
 
 
 
 Prompt: photo of a city with skyscrapers from the inside of a car with motion blur 
 
 
 
 
 
 Prompt: soft focus photograph of a bridge in an urban city at night 
 
 
 
 

- 

 Lens types - 35mm, 50mm, fisheye, wide angle, macro 

 
 
 
 
 
 Prompt: photo of a leaf, macro lens 
 
 
 
 
 
 Prompt: street photography, new york city, fisheye lens 
 
 
 
 

- 

 Film types - black and white, polaroid 

 
 
 
 
 
 Prompt: a polaroid portrait of a dog wearing sunglasses 
 
 
 
 
 
 Prompt: black and white photo of a dog wearing sunglasses 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

### Illustration and art

- Prompt includes: "A painting of..." , "A sketch of..." 

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic
digital art. For example, the following images use the same prompt with
different styles:

 "An [art style or creation technique] of an angular
sporty electric sedan with skyscrapers in the background" 

 
 
 
 
 
 Prompt: A technical pencil drawing of an angular... 
 
 
 
 
 
 Prompt: A charcoal drawing of an angular... 
 
 
 
 
 
 Prompt: A color pencil drawing of an angular... 
 
 
 
 

 
 
 
 
 
 Prompt: A pastel painting of an angular... 
 
 
 
 
 
 Prompt: A digital art of an angular... 
 
 
 
 
 
 Prompt: An art deco (poster) of an angular... 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model. 

 Shapes and materials 

- Prompt includes: "...made of..." , "...in the shape of..." 

One of the strengths of this technology is that you can create imagery that
is otherwise difficult or impossible. For example, you can recreate
your company logo in different materials and textures.

 
 
 
 
 
 Prompt: a duffle bag made of cheese 
 
 
 
 
 
 Prompt: neon tubes in the shape of a bird 
 
 
 
 
 
 Prompt: an armchair made of paper , studio photo, origami style 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Historical art references

- Prompt includes: "...in the style of..." 

Certain styles have become iconic over the years. The following are some ideas
of historical painting or art styles that you can try.

 "generate an image in the style of [art period or movement]
 : a wind farm" 

 
 
 
 
 
 Prompt: generate an image in the style of an impressionist painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of a renaissance painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of pop art : a wind farm 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality
asset. Examples of quality modifiers include the following:

- General Modifiers - high-quality, beautiful, stylized 

- Photos - 4K, HDR, Studio Photo 

- Art, Illustration - by a professional, detailed 

The following are a few examples of prompts without quality modifiers and
the same prompt with quality modifiers.

 
 
 
 
 
 Prompt (no quality modifiers): a photo of a corn stalk 
 
 
 
 
 
 Prompt (with quality modifiers): 4k HDR beautiful 

 photo of a corn stalk taken by a 
 professional photographer 
 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Aspect ratios

Imagen image generation lets you set five distinct image aspect
ratios.

- Square (1:1, default) - A standard square photo. Common uses for this
aspect ratio include social media posts.

- 

 Fullscreen (4:3) - This aspect ratio is commonly used in media or film.
It is also the dimensions of most old (non-widescreen) TVs and medium format
cameras. It captures more of the scene horizontally (compared to 1:1),
making it a preferred aspect ratio for photography.

 
 
 
 
 
 Prompt: close up of a musician's fingers
playing the piano, black and white film, vintage (4:3 aspect ratio)
 
 
 
 
 
 
 Prompt: A professional studio photo of
french fries for a high end restaurant, in the style of a food magazine
 (4:3 aspect ratio)
 
 
 
 
 

- 

 Portrait full screen (3:4) - This is the fullscreen aspect ratio rotated
90 degrees. This lets to capture more of the scene vertically compared to
the 1:1 aspect ratio.

 
 
 
 
 
 Prompt: a woman hiking, close of her
boots reflected in a puddle, large mountains in the background, in the
style of an advertisement, dramatic angles (3:4 aspect ratio)
 
 
 
 
 
 
 Prompt: aerial shot of a river flowing
up a mystical valley (3:4 aspect ratio)
 
 
 
 
 

- 

 Widescreen (16:9) - This ratio has replaced 4:3 and is now the most
common aspect ratio for TVs, monitors, and mobile phone screens (landscape).
Use this aspect ratio when you want to capture more of the background (for
example, scenic landscapes).

 
 
 Prompt: a man wearing all white
clothing sitting on the beach, close up, golden hour lighting (16:9
aspect ratio)
 
 

- 

 Portrait (9:16) - This ratio is widescreen but rotated. This a
relatively new aspect ratio that has been popularized by short form video
apps (for example, YouTube shorts). Use this for tall objects with strong
vertical orientations such as buildings, trees, waterfalls, or other similar
objects.

 
 
 Prompt: a digital render of a massive skyscraper, modern,
grand, epic with a beautiful sunset in the background (9:16 aspect ratio)
 
 

#### Photorealistic images

Different versions of the image generation
model might offer a mix of artistic and photorealistic output. Use the following
wording in prompts to generate more photorealistic output, based on the subject
you want to generate.

 
 
 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 
 

 Portraits 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 

Using several keywords from the table, Imagen can generate the following
portraits:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, blue and grey duotones 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, film noir 

Model: `imagen-3.0-generate-002`

 Objects 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 

Using several keywords from the table, Imagen can
generate the following object images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: leaf of a prayer plant, macro lens, 60mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a plate of pasta, 100mm Macro lens 

Model: `imagen-3.0-generate-002`

 Motion 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 

Using several keywords from the table, Imagen can
generate the following motion images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a winning touchdown, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A deer running in the forest, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 Wide-angle 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 

Using several keywords from the table, Imagen can
generate the following wide-angle images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: an expansive mountain range, landscape wide angle 10mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a photo of the moon, astro photography, wide angle 10mm 

Model: `imagen-3.0-generate-002`

## Model versions

 
 
 
 

### Imagen 4

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-4.0-generate-001`

 `imagen-4.0-ultra-generate-001`

 `imagen-4.0-fast-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

480 tokens (text)

 
 
 

 Output images 

 

1 to 4 (Ultra/Standard/Fast)

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 
 
 

### Imagen 3

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-3.0-generate-002`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

N/A

 
 
 

 Output images 

 

Up to 4

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-03 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-03 UTC."],[],[]]

---

### Gemini API quickstart &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/quickstart#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API quickstart  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This quickstart shows you how to install our libraries and make your first Gemini API request.

## Before you begin

You need a Gemini API key. If you don't already have one, you can get it for free in Google AI Studio .

## Install the Google GenAI SDK

 
 

### Python

Using Python 3.9+ , install the
 `google-genai` package 
using the following
 pip command :

 

```
pip install -q -U google-genai
```

 
 

### JavaScript

Using Node.js v18+ ,
install the
 Google Gen AI SDK for TypeScript and JavaScript 
using the following
 npm command :

 

```
npm install @google/genai
```

 
 

### Go

Install
 google.golang.org/genai in
your module directory using the go get command :

 

```
go get google.golang.org/genai
```

 
 

### Java

If you're using Maven, you can install
 google-genai by adding the
following to your dependencies:

 

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

 
 

### C#

Install
 googleapis/go-genai in
your module directory using the dotnet add command 

 

```
dotnet add package Google.GenAI
```

 
 

### Apps Script

- To create a new Apps Script project, go to
 script.new .

- Click Untitled project .

- Rename the Apps Script project AI Studio and click Rename .

- Set your API key 

 At the left, click Project Settings .

- Under Script Properties click Add script property .

- For Property , enter the key name: `GEMINI_API_KEY`.

- For Value , enter the value for the API key.

- Click Save script properties .

 
- Replace the `Code.gs` file contents with the following code:

 
 

## Make your first request

Here is an example that uses the
 `generateContent` method
to send a request to the Gemini API using the Gemini 2.5 Flash model.

If you set your API key as the
environment variable `GEMINI_API_KEY`, it will be picked up automatically by the
client when using the Gemini API libraries .
Otherwise you will need to pass your API key as
an argument when initializing the client.

Note that all code samples in the Gemini API docs assume that you have set the
environment variable `GEMINI_API_KEY`.

 
 

### Python

 

```
from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// The client gets the API key from the environment variable `GEMINI_API_KEY`.
const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### C#

 

```
using System.Threading.Tasks;
using Google.GenAI;
using Google.GenAI.Types;

public class GenerateContentSimpleText {
  public static async Task main() {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    var client = new Client();
    var response = await client.Models.GenerateContentAsync(
      model: "gemini-2.5-flash", contents: "Explain how AI works in a few words"
    );
    Console.WriteLine(response.Candidates[0].Content.Parts[0].Text);
  }
}
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');
function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 

## What's next

Now that you made your first API request, you might want to explore the
following guides that show Gemini in action:

- Text generation 

- Image generation 

- Image understanding 

- Thinking 

- Function calling 

- Long context 

- Embeddings 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Using Gemini API keys &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/api-key#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Using Gemini API keys  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Using Gemini API keys 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

To use the Gemini API, you need an API key. This page outlines how to create and
manage your keys in Google AI Studio as well as how to set up your environment
to use them in your code.

## API Keys

An API key is an
encrypted string that you can use when calling Google Cloud APIs.
You can create and manage all your Gemini API Keys from the
 Google AI Studio API Keys page.

Once you have an API key, you have the following options to connect to the
Gemini API:

- Setting your API key as an environment variable 

- Providing your API key explicitly 

For initial testing, you can hard code an API key, but this should only be
temporary since it's not secure. You can find examples for hard coding the API
key in Providing API key explicitly section.

## Google Cloud projects

 Google Cloud projects 
are fundamental to using Google Cloud services (such as the Gemini API),
managing billing, and controlling collaborators and permissions. Google AI
Studio provides a lightweight interface to your Google Cloud projects.

If you don't have
any projects created yet, you must either create a new project or import one
from Google Cloud into Google AI Studio. The Projects page in Google AI
Studio will display all keys that have sufficient permission to use the Gemini
API. Refer to the import projects section for instructions.

### Default project

For new users, after accepting Terms of Service, Google AI Studio creates a
default Google Cloud Project and API Key, for ease of use. You can rename this
project in Google AI Studio by navigating to Projects view in the
 Dashboard , clicking the 3 dots settings button next to a project and
choosing Rename project . Existing users, or users who already have Google
Cloud Accounts won't have a default project created.

## Import projects

Each Gemini API key is associated with a Google Cloud project. By default,
Google AI Studio does not show all of your Cloud Projects. You must import the
projects you want by searching for the name or project ID in the
 Import Projects dialog. To view a complete list of projects you have access
to, visit the Cloud Console.

If you don't have any projects imported yet, follow these steps to import a
Google Cloud project and create a key:

- Go to Google AI Studio .

- Open the Dashboard from the left side panel.

- Select Projects .

- Select the Import projects button in the Projects page.

- Search for and select the Google Cloud project you want to import and select
the Import button.

Once a project is imported, go to the API Keys page from the Dashboard 
menu and create an API key in the project you just imported.

## Limitations

The following are limitations of managing API keys and Google Cloud projects in
Google AI Studio.

- You can create a maximum of 10 project at a time from the Google AI Studio
 Projects page.

- You can name and rename projects and keys.

- The API keys and Projects pages display a maximum of 100 keys and
50 projects.

- Only API keys that have no restrictions, or are restricted to the Generative
Language API are displayed.

For additional management access to your projects, visit the Google Cloud Console.

## Setting the API key as an environment variable

If you set the environment variable `GEMINI_API_KEY` or `GOOGLE_API_KEY`, the
API key will automatically be picked up by the client when using one of the
 Gemini API libraries . It's recommended that you
set only one of those variables, but if both are set, `GOOGLE_API_KEY` takes
precedence.

If you're using the REST API, or JavaScript on the browser, you will need to
provide the API key explicitly.

Here is how you can set your API key locally as the environment variable
`GEMINI_API_KEY` with different operating systems.

 
 

### Linux/macOS - Bash

Bash is a common Linux and macOS terminal configuration. You can check if
you have a configuration file for it by running the following command:

 

```
~/.bashrc
```

 

If the response is "No such file or directory", you will need to create this
file and open it by running the following commands, or use `zsh`:

 

```
touch ~/.bashrc
open ~/.bashrc
```

 

Next, you need to set your API key by adding the following export command:

 

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

 

After saving the file, apply the changes by running:

 

```
source ~/.bashrc
```

 
 

### macOS - Zsh

Zsh is a common Linux and macOS terminal configuration. You can check if
you have a configuration file for it by running the following command:

 

```
~/.zshrc
```

 

If the response is "No such file or directory", you will need to create this
file and open it by running the following commands, or use `bash`:

 

```
touch ~/.zshrc
open ~/.zshrc
```

 

Next, you need to set your API key by adding the following export command:

 

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

 

After saving the file, apply the changes by running:

 

```
source ~/.zshrc
```

 
 

### Windows

- Search for "Environment Variables" in the search bar.

- Choose to modify System Settings . You may have to confirm you want to
do this.

- In the system settings dialog, click the button labeled Environment
Variables .

- Under either User variables (for the current user) or System
variables (applies to all users who use the machine), click New... 

- Specify the variable name as `GEMINI_API_KEY`. Specify your Gemini API
Key as the variable value.

- Click OK to apply the changes.

- Open a new terminal session (cmd or Powershell) to get the new variable.

 
 

## Providing the API key explicitly

In some cases, you may want to explicitly provide an API key. For example:

- You're doing a simple API call and prefer hard coding the API key.

- You want explicit control without having to rely on automatic discovery of
environment variables by the Gemini API libraries

- You're using an environment where environment variables are not supported
(e.g web) or you are making REST calls.

Below are examples for how you can provide an API key explicitly:

 
 

### Python

 

```
from google import genai

client = genai.Client(api_key="YOUR_API_KEY")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  "YOUR_API_KEY",
        Backend: genai.BackendGeminiAPI,
    })
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = Client.builder().apiKey("YOUR_API_KEY").build();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: YOUR_API_KEY" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 

## Keep your API key secure

Treat your Gemini API key like a password. If compromised, others can use your
project's quota, incur charges (if billing is enabled), and access your
private data, such as files.

### Critical security rules

- 

 Never commit API keys to source control. Do not check your API key into version control systems like Git.

- 

 Never expose API keys on the client-side. Do not use your API key directly
in web or mobile apps in production. Keys in client-side code
(including our JavaScript/TypeScript libraries and REST calls) can be
extracted.

### Best practices

- 

 Use server-side calls with API keys The most secure way to use your API
key is to call the Gemini API from a server-side application where the key
can be kept confidential.

- 

 Use ephemeral tokens for client-side access (Live API only): For direct
client-side access to the Live API, you can use ephemeral tokens. They come with
lower security risks and can be suitable for production use. Review
 ephemeral tokens guide for more information.

- 

 Consider adding restrictions to your key: You can limit a key's permissions
by adding API key restrictions .
This minimizes the potential damage if the key is ever leaked.

For some general best practices, you can also review this
 support article .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Generate videos with Veo 3.1 in Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/video

- 
 
 
 
 
 
 
 
 
 
 
 Generate videos with Veo 3.1 in Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Generate videos with Veo 3.1 in Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 Veo 3.1 is Google's state-of-the-art
model for generating high-fidelity, 8-second 720p or 1080p videos featuring
stunning realism and natively generated audio. You can access
this model programmatically using the Gemini API. To learn more about the
available Veo model variants, see the Model Versions section.

Veo 3.1 excels at a wide range of visual and cinematic styles and introduces
several new capabilities:

- Video extension : Extend videos that were previously
generated using Veo.

- Frame-specific generation : Generate a video by
specifying the first and last frames.

- Image-based direction : Use up to three reference images to guide
the content of your generated video.

For more information about writing effective text prompts for video generation,
see the Veo prompt guide 

## Text to video generation

Choose an example to see how to generate a video with dialogue, cinematic
realism, or creative animation:

 
 
 
 
 

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"""

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("dialogue_example.mp4")
print("Generated video saved to dialogue_example.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.1-generate-preview",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### REST

 

```
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

 
 

 

## Image to video generation

The following code demonstrates generating an image using
 Gemini 2.5 Flash Image aka Nano Banana ,
then using that image as the
starting frame for generating a video with Veo 3.1.

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Nano Banana.
image = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=prompt,
    config={"response_modalities":['IMAGE']}
)

# Step 2: Generate video with Veo 3.1 using the image.
operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    image=image.parts[0].as_image(),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3_with_image_input.mp4")
print("Generated video saved to veo3_with_image_input.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Nano Banana.
const imageResponse = await ai.models.generateContent({
  model: "gemini-2.5-flash-image",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3.1 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: prompt,
  image: {
    imageBytes: imageResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Nano Banana.
    imageResponse, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3.1 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        prompt,
        imageResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### Using reference images

Veo 3.1 now accepts up to 3 reference images to guide your generated video's
content. Provide images of a person, character, or product to
preserve the subject's appearance in the output video.

For example, using these three images generated with
 Nano Banana as references with a
 well-written prompt creates the following video:

 
 
 
 ``dress_image`` 
 ``woman_image`` 
 ``glasses_image`` 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "The video opens with a medium, eye-level shot of a beautiful woman with dark hair and warm brown eyes. She wears a magnificent, high-fashion flamingo dress with layers of pink and fuchsia feathers, complemented by whimsical pink, heart-shaped sunglasses. She walks with serene confidence through the crystal-clear, shallow turquoise water of a sun-drenched lagoon. The camera slowly pulls back to a medium-wide shot, revealing the breathtaking scene as the dress's long train glides and floats gracefully on the water's surface behind her. The cinematic, dreamlike atmosphere is enhanced by the vibrant colors of the dress against the serene, minimalist landscape, capturing a moment of pure elegance and high-fashion fantasy."

dress_reference = types.VideoGenerationReferenceImage(
  image=dress_image, # Generated separately with Nano Banana
  reference_type="asset"
)

sunglasses_reference = types.VideoGenerationReferenceImage(
  image=glasses_image, # Generated separately with Nano Banana
  reference_type="asset"
)

woman_reference = types.VideoGenerationReferenceImage(
  image=woman_image, # Generated separately with Nano Banana
  reference_type="asset"
)

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    config=types.GenerateVideosConfig(
      reference_images=[dress_reference, glasses_reference, woman_reference],
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_with_reference_images.mp4")
print("Generated video saved to veo3.1_with_reference_images.mp4")
```

 
 

 

### Using first and last frames

Veo 3.1 lets you create videos using interpolation, or specifying the first and
last frames of the video. For information about writing effective text prompts
for video generation, see the Veo prompt guide .

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "A cinematic, haunting video. A ghostly woman with long white hair and a flowing dress swings gently on a rope swing beneath a massive, gnarled tree in a foggy, moonlit clearing. The fog thickens and swirls around her, and she slowly fades away, vanishing completely. The empty swing is left swaying rhythmically on its own in the eerie silence."

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    image=first_image, # Generated separately with Nano Banana
    config=types.GenerateVideosConfig(
      last_frame=last_image # Generated separately with Nano Banana
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_with_interpolation.mp4")
print("Generated video saved to veo3.1_with_interpolation.mp4")
```

 
 
 
 
 
 ``first_image`` 
 ``last_image`` 
 veo3.1_with_interpolation.mp4 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

## Extending Veo videos

Use Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds
and up to 20 times.

Input video limitations:

- Veo-generated videos only up to 141 seconds long.

- Gemini API only supports video extensions for Veo-generated videos.

- The video should come from a previous generation, like `operation.response.generated_videos[0].video`

- Input videos are expected to have a certain length, aspect ratio, and dimensions:

 Aspect ratio: 9:16 or 16:9

- Resolution: 720p

- Video length: 141 seconds or less

 

The output of the extension is a single video combining the user input video and
the generated extended video for up to 148 seconds of video.

This example takes the a Veo-generated video, shown here with
its original prompt, and extends it using the `video` parameter and a new
prompt:

 
 
 
 Prompt 
 Output: `butterfly_video` 
 
 
 
 
 
 An origami butterfly flaps its wings and flies out of the french doors into the garden.
 
 
 
 
 
 
 
 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower."

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    video=operation.response.generated_videos[0].video, # This must be a video from a previous generation
    prompt=prompt,
    config=types.GenerateVideosConfig(
        number_of_videos=1,
        resolution="720p"
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_extension.mp4")
print("Generated video saved to veo3.1_extension.mp4")
```

 
 

 

For information about writing effective text prompts for video generation, see
the Veo prompt guide .

## Handling asynchronous operations

Video generation is a computationally intensive task. When you send a request
to the API, it starts a long-running job and immediately returns an `operation`
object. You must then poll until the video is ready, which is indicated by the
`done` status being true.

The core of this process is a polling loop, which periodically checks the job's
status.

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)

# Once done, the result is in operation.response.
# ... process and download your video ...
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

// After starting the job, you get an operation object.
let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
});

// Alternatively, you can use operation.name to get the operation.
// operation = types.GenerateVideosOperation(name=operation.name)

// This loop checks the job status every 10 seconds.
while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    // Refresh the operation object to get the latest status.
    operation = await ai.operations.getVideosOperation({ operation });
}

// Once done, the result is in operation.response.
// ... process and download your video ...
```

 
 

## Veo API parameters and specifications

These are the parameters you can set in your API request to control the video
generation process.

 
 
 
 Parameter 
 Description 
 Veo 3.1 & Veo 3.1 Fast 
 Veo 3 & Veo 3 Fast 
 Veo 2 
 
 
 
 
 `prompt` 
 The text description for the video. Supports audio cues. 
 `string` 
 `string` 
 `string` 
 
 
 `negativePrompt` 
 Text describing what not to include in the video. 
 `string` 
 `string` 
 `string` 
 
 
 `image` 
 An initial image to animate. 
 `Image` object 
 `Image` object 
 `Image` object 
 
 
 `lastFrame` 
 The final image for an interpolation video to transition. Must be used in combination with the `image` parameter. 
 `Image` object 
 `Image` object 
 `Image` object 
 
 
 `referenceImages` 
 Up to three images to be used as style and content references. 
 `VideoGenerationReferenceImage` object (Veo 3.1 only) 
 n/a 
 n/a 
 
 
 `video` 
 Video to be used for video extension. 
 `Video` object 
 n/a 
 n/a 
 
 
 `aspectRatio` 
 The video's aspect ratio. 
 `"16:9"` (default, 720p & 1080p),
`"9:16"`(720p & 1080p)

 
 `"16:9"` (default, 720p & 1080p),
`"9:16"` (720p & 1080p) 
 `"16:9"` (default, 720p),
`"9:16"` (720p) 
 
 
 `resolution` 
 The video's aspect ratio. 
 `"720p"` (default), 
`"1080p"` (only supports 8s duration)

 `"720p"` only for extension 
 `"720p"` (default), 
`"1080p"` (16:9 only) 
 Unsupported 
 
 
 `durationSeconds` 
 Length of the generated video. 
 `"4"`, `"6"`, `"8"`.

 Must be "8" when using extension or interpolation (supports both 16:9 and 9:16), and when using `referenceImages` (only supports 16:9) 
 `"4"`, `"6"`, `"8"` 
 `"5"`, `"6"`, `"8"` 
 
 
 `personGeneration` 
 Controls the generation of people.

 (See Limitations for region restrictions) 
 
 Text-to-video & Extension:
`"allow_all"` only

 Image-to-video, Interpolation, & Reference images:
`"allow_adult"` only
 
 
 Text-to-video:
`"allow_all"` only

 Image-to-video:
`"allow_adult"` only
 
 
 Text-to-video: 
`"allow_all"`, `"allow_adult"`, `"dont_allow"`
 
Image-to-video: 
`"allow_adult"`, and `"dont_allow"`
 
 
 
 

Note that the `seed` parameter is also available for Veo 3 models.
It doesn't guarantee determinism, but slightly improves it.

You can customize your video generation by setting parameters in your request.
For example you can specify `negativePrompt` to guide the model.

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt="A cinematic shot of a majestic lion in the savannah.",
    config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "parameters_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### REST

 

```
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

 
 

## Veo prompt guide

This section contains examples of videos you can create using Veo, and shows you
how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that
generated videos and uploaded photos don't contain offensive content.
Prompts that violate our terms and guidelines are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get the most out of Veo, start with
identifying your core idea, refine your idea by adding keywords and modifiers,
and incorporate video-specific terminology into your prompts.

The following elements should be included in your prompt:

- Subject : The object, person, animal, or scenery that you want in your
video, such as cityscape , nature , vehicles , or puppies .

- Action : What the subject is doing (for example, walking , running , or
 turning their head ).

- Style : Specify creative direction using specific film
style keywords, such as sci-fi , horror film , film noir , or animated
styles like cartoon .

- Camera positioning and motion : [Optional] Control the camera's location
and movement using terms like aerial view , eye-level , top-down shot ,
 dolly shot , or worms eye .

- Composition : [Optional] How the shot is framed, such as wide shot ,
 close-up , single-shot or two-shot .

- Focus and lens effects : [Optional] Use terms like shallow focus ,
 deep focus , soft focus , macro lens , and wide-angle lens to achieve
specific visual effects.

- Ambiance : [Optional] How the color and light contribute to the scene,
such as blue tones , night , or warm tones .

#### More tips for writing prompts

- Use descriptive language : Use adjectives and adverbs to paint a clear
picture for Veo.

- Enhance the facial details : Specify
facial details as a focus of the photo like using the word portrait in
the prompt.

 For more comprehensive prompting strategies, visit Introduction to
prompt design . 

### Prompting for audio

With Veo 3, you can provide cues for sound effects, ambient noise, and dialogue.
The model captures the nuance of these cues to generate a synchronized
soundtrack.

- Dialogue: Use quotes for specific speech. (Example: "This must be the
key," he murmured.)

- Sound Effects (SFX): Explicitly describe sounds. (Example: tires
screeching loudly, engine roaring.)

- Ambient Noise: Describe the environment's soundscape. (Example: A faint,
eerie hum resonates in the background.)

These videos demonstrate prompting Veo 3's audio generation with increasing
levels of detail.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 More detail (Dialogue and ambience) 
A wide shot of a misty Pacific Northwest forest. Two exhausted hikers, a man and a woman, push through ferns when the man stops abruptly, staring at a tree. Close-up: Fresh, deep claw marks are gouged into the tree's bark. Man: (Hand on his hunting knife) "That's no ordinary bear." Woman: (Voice tight with fear, scanning the woods) "Then what is it?" A rough bark, snapping twigs, footsteps on the damp earth. A lone bird chirps. 
 
 
 
 
 
 Less detail (Dialogue) 
Paper Cut-Out Animation. New Librarian: "Where do you keep the forbidden books?" Old Curator: "We don't. They keep us." 
 
 
 
 
 
 

Try out these prompts yourself to hear the audio!

 Try Veo 3 

### Prompting with reference images

You can use one or more images as inputs to guide your generated videos, using
Veo's image-to-video 
capabilities. Veo uses the input image as the initial frame. Select an image
closest to what you envision as the first scene of your video to animate
everyday objects, bring drawings and paintings to life, and add movement and
sound to nature scenes.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Input image (Generated by Nano Banana) 
A hyperrealistic macro photo of tiny, miniature surfers riding ocean waves inside a rustic stone bathroom sink. A vintage brass faucet is running, creating the perpetual surf. Surreal, whimsical, bright natural lighting. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
A surreal, cinematic macro video. Tiny surfers ride perpetual, rolling waves inside a stone bathroom sink. A running vintage brass faucet generates the endless surf. The camera slowly pans across the whimsical, sunlit scene as the miniature figures expertly carve the turquoise water. 
 
 
 
 
 
 

Veo 3.1 lets you reference images or ingredients to direct your generated
video's content. Provide up to three asset images of a single person, character,
or product. Veo preserves the subject's appearance in the output video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Reference image (Generated by Nano Banana) 
A deep sea angler fish lurks in the deep dark water, teeth bared and bait glowing. 
 
 
 
 
 
 Reference image (Generated by Nano Banana) 
A pink child's princess costume complete with a wand and tiara, on a plain product background. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Create a silly cartoon version of the fish wearing the costume, swimming and waving the wand around. 
 
 
 
 
 
 

Using Veo 3.1, you can also generate videos by specifying the first and last
frames of the video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 First image (Generated by Nano Banana) 
A high quality photorealistic front image of a ginger cat driving a red convertible racing car on the French riviera coast. 
 
 
 
 
 
 Last image (Generated by Nano Banana) 
Show what happens when the car takes off from a cliff. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Optional 
 
 
 
 
 
 

This feature gives you precise control over your shot's composition by letting
you define the starting and ending frame. Upload an image or use a frame from a
previous video generation to make sure your scene begins and concludes exactly
as you envision it.

### Prompting for extension

To extend your Veo-generated video with Veo 3.1, use the video as an input along
with an optional text prompt. Extend finalizes the final second or 24 frames of
your video and continues the action.

Note that voice is not able to be effectively extended if it's not present in
the last 1 second of video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Input video (Generated by Veo 3.1) 
The paraglider takes off from the top of the mountain and starts gliding down the mountains overlooking the flower covered valleys below. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Extend this video with the paraglider slowly descending. 
 
 
 
 
 
 

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can
elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of
 prompt writing basics in your prompt.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Close up shot (composition) of melting icicles (subject) on a frozen
 rock wall (context) with cool blue tones (ambiance), zoomed in
 (camera motion) maintaining close-up detail of water drips (action).
 
 
 
 
 
 
 

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly
specific details to get Veo to refine the output to your liking.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Less detail 
The camera dollies to show a close up of a desperate man in
 a green trench coat. He's making a call on a rotary-style wall phone with a
 green neon light. It looks like a movie scene. 
 
 
 
 
 
 More detail 
A close-up cinematic
 shot follows a desperate man in a weathered green trench coat as he dials a
 rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a
 green neon sign. The camera dollies in, revealing the tension in his jaw and
 the desperation etched on his face as he struggles to make the call. The
 shallow depth of field focuses on his furrowed brow and the black rotary
 phone, blurring the background into a sea of neon colors and indistinct
 shadows, creating a sense of urgency and isolation. 
 
 
 
 
 
 

#### Snow leopard

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Simple prompt: 
A cute creature with snow leopard-like fur is walking in winter
forest, 3D cartoon style render. 
 
 
 
 
 
 Detailed prompt: 
Create a short 3D animated scene in a joyful cartoon style. A cute
creature with snow leopard-like fur, large expressive eyes, and a friendly,
rounded form happily prances through a whimsical winter forest. The scene should
feature rounded, snow-covered trees, gentle falling snowflakes, and warm
sunlight filtering through the branches. The creature's bouncy movements and
wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with
bright, cheerful colors and playful animation. 
 
 
 
 
 
 

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject and context

Specify the main focus (subject) and the background or environment (context).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements 
 
 
 
 
 
 A satellite floating through outer space with the moon and some
stars in the background. 
 
 
 
 
 
 

#### Action

Specify what the subject is doing (e.g., walking, running, or turning their
head).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A wide shot of a woman walking along the beach, looking content and
relaxed towards the horizon at sunset. 
 
 
 
 
 
 

#### Style

Add keywords to steer the generation toward a specific aesthetic (e.g., surreal,
vintage, futuristic, film noir).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Film noir style, man and woman walk on the street, mystery,
cinematic, black and white. 
 
 
 
 
 
 

#### Camera motion and composition

Specify how the camera moves (POV shot, aerial view, tracking drone view) and
how the shot is framed (wide shot, close-up, low angle).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A POV shot from a vintage car driving in the rain, Canada at night,
cinematic. 
 
 
 
 
 
 Extreme close-up of a an eye with city reflected in it. 
 
 
 
 
 
 

#### Ambiance

Color palettes and lighting influence the mood. Try terms like "muted orange
warm tones," "natural light," "sunrise," or "cool blue tones."

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A close-up of a girl holding adorable golden retriever puppy in the park, sunlight. 
 
 
 
 
 
 Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood. 
 
 
 
 
 
 

### Negative prompts

Negative prompts specify elements you don't want in the video.

- ‚ùå Don't use instructive language like no or don't . (e.g., "No walls").

- ‚úÖ Do describe what you don't want to see. (e.g., "wall, frame").

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Without Negative Prompt: 
Generate a short, stylized animation of a large, solitary oak tree
with leaves blowing vigorously in a strong wind... [truncated] 
 
 
 
 
 
 With Negative Prompt: 
[Same prompt]

Negative prompt: urban background, man-made structures,
dark, stormy, or threatening atmosphere. 
 
 
 
 
 
 

### Aspect ratios

Veo lets you specify the aspect ratio for your video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Widescreen (16:9) 
Create a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows. 
 
 
 
 
 
 Portrait (9:16) 
Create a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest. 
 
 
 
 
 
 

## Limitations

- Request latency: Min: 11 seconds; Max: 6 minutes (during peak hours).

- Regional limitations: In EU, UK, CH, MENA locations, the following
are the allowed values for `personGeneration`:

 Veo 3: `allow_adult` only.

- Veo 2: `dont_allow` and `allow_adult`. Default is `dont_allow`.

 
- Video retention: Generated videos are stored on the server for 2 days,
after which they are removed. To save a local copy, you must download your
video within 2 days of generation. Extended videos are treated as newly
generated videos.

- Watermarking: Videos created by Veo are watermarked using SynthID , our tool for watermarking
and identifying AI-generated content. Videos can be verified using the
 SynthID verification platform.

- Safety: Generated videos are passed through safety filters and
memorization checking processes that help mitigate privacy, copyright and
bias risks.

- Audio error: Veo 3.1 will sometimes block a video from generating
because of safety filters or other processing issues with the audio. You
will not be charged if your video is blocked from generating.

## Model features

 
 
 
 Feature 
 Description 
 Veo 3.1 & Veo 3.1 Fast 
 Veo 3 & Veo 3 Fast 
 Veo 2 
 
 
 
 
 Audio 
 Natively generates audio with video. 
 Natively generates audio with video. 
 ‚úîÔ∏è Always on 
 ‚ùå Silent only 
 
 
 Input Modalities 
 The type of input used for generation. 
 Text-to-Video, Image-to-Video, Video-to-Video 
 Text-to-Video, Image-to-Video 
 Text-to-Video, Image-to-Video 
 
 
 Resolution 
 The output resolution of the video. 
 720p & 1080p (8s length only) 

720p only when using video extension. 
 720p & 1080p (16:9 only) 
 720p 
 
 
 Frame Rate 
 The output frame rate of the video. 
 24fps 
 24fps 
 24fps 
 
 
 Video Duration 
 Length of the generated video. 
 8 seconds, 6 seconds, 4 seconds 
8 seconds only when using reference images 
 8 seconds 
 5-8 seconds 
 
 
 Videos per Request 
 Number of videos generated per request. 
 1 
 1 
 1 or 2 
 
 
 Status & Details 
 Model availability and further details. 
 Preview 
 Stable 
 Stable 
 
 
 

## Model versions

Check out the Pricing and Rate limits pages for more Veo model-specific usage
details.

Veo Fast versions allow developers to create videos with sound while maintaining
high quality and optimizing for speed and business use cases. They're ideal for
backend services that programmatically generate ads, tools for rapid A/B testing
of creative concepts, or apps that need to quickly produce social media content.

 
 
 
 

### Veo 3.1 Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.1-generate-preview`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 
 
 

### Veo 3.1 Fast Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.1-fast-generate-preview`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 
 
 

### Veo 3

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.0-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 
 
 

### Veo 3 Fast

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.0-fast-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 
 
 

### Veo 2

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-2.0-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, image

 
 
 

 Output 

 

Video

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

N/A

 
 
 

 Image input 

 

Any image resolution and aspect ratio up to 20MB file size

 
 
 

 Output video 

 

Up to 2

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 
 
 

## What's next

- Get started with the Veo 3.1 API by experimenting in the Veo Quickstart Colab 
and the Veo 3.1 applet .

- Learn how to write even better prompts with our Introduction to prompt design .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-13 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-13 UTC."],[],[]]

---

### Build mode in Google AI Studio &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/aistudio-build-mode#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Build mode in Google AI Studio  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Build mode in Google AI Studio 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page describes how you can use the Build mode in Google AI Studio to
quickly build (or vibe code) and deploy apps that test out the latest
capabilities of Gemini like nano banana and the Live API .

## Get started

Start vibe coding in Google AI Studio's Build mode . You can
start building in a few ways:

- Start with a prompt : In Build mode, use the input box to enter a
description of what you want to build. Select AI Chips to add specific
features like image generation or Google Maps data to your prompt. You can
even say what you want using the speech-to-text button.

- "I'm Feeling Lucky" button : If you need a creative spark, use the "I'm
Feeling Lucky" button, and Gemini will generate a prompt with a project idea
to get you started.

- Remix a project from the Gallery : Open a project from the App
Gallery and select Copy App .

Once you run the prompt, you'll see the necessary code and files get generated,
with a live preview of your app appearing on the right-hand side.

## What is created?

When you run your prompt, AI Studio creates a web app. By default, it will
create a React web app but you can choose to create an Angular app in the
 Settings menu . You can view the code that gets generated by selecting the
 Code tab in the right-hand preview pane.

The following are files to note:

- geminiService.ts : This file contains the main logic for your app, from
constructing prompts to calling the Gemini API and parsing its responses. You
can edit the base prompt in this file or modify any component functionality
directly or by interactively chatting with Gemini in Build mode. Note that the
code in this file uses the GenAI TS SDK to interact with
the Gemini API.

## Continue building

Once Google AI Studio generates the initial code for your web application, you
have two primary options for continuing your project: Build in AI Studio or
 Develop Externally .

### Build in Google AI Studio

You can continue refining and expanding your application directly within the
Google AI Studio environment:

- Iterate with Gemini : Use the chat panel in Build mode to ask Gemini to
make modifications, add new features, or change the styling. For instance, you
could ask, "Add a button that alerts the user" or "Change the color scheme to
blue and white."

- Edit the Code Directly : Open the Code tab in the preview panel to make
live edits. You can save your project to GitHub to utilize version control
while developing.

### Develop externally

For more advanced workflows, you can export the code and work in your preferred
environment:

- Download and Develop Locally : Export the generated code as a ZIP file 
and import it into your code editor. This lets you use your familiar tools,
build systems, and local version control practices to continue building beyond
the initial prototype.

- Push to GitHub : Integrate the code with your existing development and
deployment processes by pushing it to a GitHub repository .

## Key features

Google AI Studio includes several features to make the building process
intuitive and visual:

- Annotation mode : Instead of writing code to change your app's appearance,
Annotation Mode lets you highlight any part of your app's UI and describe the
change you want. For example, you can select a component and type, "Make this
button blue," or "Animate this image to slide in from the left." When you
select Add to chat , a prompt is generated with a screenshot of the
annotated app.

- Share your app : You can share your creations with others to
collaborate or showcase your work.

- App Gallery : The App Gallery provides a visual library of project ideas.
You can browse what's possible with Gemini, preview applications instantly,
and remix them to make them your own.

## Deploy or archive your app

Once your application is ready, you can deploy it directly from AI Studio.
Options for deployment include:

- Google Cloud Run : Deploy your application as a scalable service. Note that
pricing for Google Cloud Run may apply based on usage.

- GitHub : Export your project to a GitHub repository to integrate it into
your existing development and deployment workflows.

## Limitations

This section outlines important limitations when using Build mode in Google AI
Studio.

API Key security and exposure

- The code for shared apps is visible to anyone who views them. Never use a real
API key directly in your app's code.

- By default, apps use a placeholder (e.g., process.env.GEMINI_API_KEY) for the
API key. When a user runs your shared app within AI Studio, AI Studio acts as a
proxy, replacing the placeholder with the end user's API key, ensuring your
key remains private.

App visibility and sharing

- Apps are stored in Google Drive and inherit its permissions model, meaning
they are private by default.

- Sharing Permissions: When you share an app with other users:

 Shared users can see the code and fork the app for their own use.

- If granted edit permission, shared users can modify the app's code.

 

Deployment outside AI Studio

- While you can deploy your app to Cloud Run for a public URL, this setup will
use your API key for all users' Gemini API calls.

 JavaScript apps are run client side, so ensure API keys only have minimal
access to prevent data leaks or misuse. For example, other File Search
Stores from the same project may be accessible to users via this mechanism.

 
- Secure external deployment: To run an app securely outside of AI Studio (e.g.,
after downloading the zip file), you must move the logic that uses the API key
to a server-side component to prevent key exposure to end users. This is not
needed if you deploy using Cloud Run.

- Key exposure warning: Simply replacing the placeholder with a real API key in a
client-side environment is strongly discouraged, as the key will become
visible to any user.

Tool and feature support

- Local development import: Currently, you cannot develop apps locally with
external tools and import them into AI Studio.

## What's Next?

- See what others have built and get inspired by remixing an existing project in
the App Gallery .

- Check out the YouTube playlist for a collection of AI Studio
vibe coding tutorials to help you get started.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Music generation using Lyria RealTime &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/music-generation

- 
 
 
 
 
 
 
 
 
 
 
 Music generation using Lyria RealTime  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Music generation using Lyria RealTime 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API, using
 Lyria RealTime ,
provides access to a state-of-the-art, real-time, streaming music
generation model. It allows developers to build applications where users
can interactively create, continuously steer, and perform instrumental
music.

To experience what can be built using Lyria RealTime, try it on AI Studio
using the Prompt DJ or the
 MIDI DJ apps!

## How music generation works

Lyria RealTime music generation uses a persistent, bidirectional,
low-latency streaming connection using
 WebSocket .

## Generate and control music

Lyria RealTime works a bit like the Live API 
in the sense that it is using websockets to keep a real-time communication with
the model. It's still not exactly the same as you can't talk to the model and
you have to use a specific format to prompt it.

The following code demonstrates how to generate music:

 
 

### Python

This example initializes the Lyria RealTime session using
`client.aio.live.music.connect()`, then sends an
initial prompt with `session.set_weighted_prompts()` along with an initial
configuration using `session.set_music_generation_config`, starts the music
generation using `session.play()` and sets up
`receive_audio()` to process the audio chunks it receives.

 

```
  import asyncio
  from google import genai
  from google.genai import types

  client = genai.Client(http_options={'api_version': 'v1alpha'})

  async def main():
      async def receive_audio(session):
        """Example background task to process incoming audio."""
        while True:
          async for message in session.receive():
            audio_data = message.server_content.audio_chunks[0].data
            # Process audio...
            await asyncio.sleep(10**-12)

      async with (
        client.aio.live.music.connect(model='models/lyria-realtime-exp') as session,
        asyncio.TaskGroup() as tg,
      ):
        # Set up task to receive server messages.
        tg.create_task(receive_audio(session))

        # Send initial prompts and config
        await session.set_weighted_prompts(
          prompts=[
            types.WeightedPrompt(text='minimal techno', weight=1.0),
          ]
        )
        await session.set_music_generation_config(
          config=types.LiveMusicGenerationConfig(bpm=90, temperature=1.0)
        )

        # Start streaming music
        await session.play()
  if __name__ == "__main__":
      asyncio.run(main())
```

 
 
 
 
 

### JavaScript

This example initializes the Lyria RealTime session using
`client.live.music.connect()`, then sends an
initial prompt with `session.setWeightedPrompts()` along with an initial
configuration using `session.setMusicGenerationConfig`, starts the music
generation using `session.play()` and sets up an
`onMessage` callback to process the audio chunks it receives.

 

```
import { GoogleGenAI } from "@google/genai";
import Speaker from "speaker";
import { Buffer } from "buffer";

const client = new GoogleGenAI({
  apiKey: GEMINI_API_KEY,
    apiVersion: "v1alpha" ,
});

async function main() {
  const speaker = new Speaker({
    channels: 2,       // stereo
    bitDepth: 16,      // 16-bit PCM
    sampleRate: 44100, // 44.1 kHz
  });

  const session = await client.live.music.connect({
    model: "models/lyria-realtime-exp",
    callbacks: {
      onmessage: (message) => {
        if (message.serverContent?.audioChunks) {
          for (const chunk of message.serverContent.audioChunks) {
            const audioBuffer = Buffer.from(chunk.data, "base64");
            speaker.write(audioBuffer);
          }
        }
      },
      onerror: (error) => console.error("music session error:", error),
      onclose: () => console.log("Lyria RealTime stream closed."),
    },
  });

  await session.setWeightedPrompts({
    weightedPrompts: [
      { text: "Minimal techno with deep bass, sparse percussion, and atmospheric synths", weight: 1.0 },
    ],
  });

  await session.setMusicGenerationConfig({
    musicGenerationConfig: {
      bpm: 90,
      temperature: 1.0,
      audioFormat: "pcm16",  // important so we know format
      sampleRateHz: 44100,
    },
  });

  await session.play();
}

main().catch(console.error);
```

 
 

 
 

You can then use `session.play()`, `session.pause()`, `session.stop()` and
`session.reset_context()` to start, pause, stop or reset the session.

## Steer music in real-time

### Prompt Lyria RealTime

While the stream is active, you can send new `WeightedPrompt` messages at any
time to alter the generated music. The model will smoothly transition based
on the new input.

The prompts need to follow the right format with a `text` (the
actual prompt), and a `weight`. The `weight` can take any value except `0`. `1.0`
is usually a good starting point.

 
 

### Python

 

```
  from google.genai import types

  await session.set_weighted_prompts(
    prompts=[
      {"text": "Piano", "weight": 2.0},
      types.WeightedPrompt(text="Meditation", weight=0.5),
      types.WeightedPrompt(text="Live Performance", weight=1.0),
    ]
  )
```

 
 

### JavaScript

 

```
  await session.setMusicGenerationConfig({
    weightedPrompts: [
      { text: 'Harmonica', weight: 0.3 },
      { text: 'Afrobeat', weight: 0.7 }
    ],
  });
```

 
 

Note that the model transitions can be a bit abrupt when drastically changing
the prompts so it's recommended to implement some kind of cross-fading by
sending intermediate weight values to the model.

### Update the configuration

You can also update the music generation parameters in real time. You can't just
update a parameter, you need to set the whole configuration otherwise the other
fields will be reset back to their default values.

Since updating the bpm or the scale is a drastic change for the model you'll
also need to tell it to reset its context using `reset_context()` to take the
new config into account. It won't stop the stream, but it will be a hard
transition. You don't need to do it for the other parameters.

 
 

### Python

 

```
  from google.genai import types

  await session.set_music_generation_config(
    config=types.LiveMusicGenerationConfig(
      bpm=128,
      scale=types.Scale.D_MAJOR_B_MINOR,
      music_generation_mode=types.MusicGenerationMode.QUALITY
    )
  )
  await session.reset_context();
```

 
 

### JavaScript

 

```
  await session.setMusicGenerationConfig({
    musicGenerationConfig: { 
      bpm: 120,
      density: 0.75,
      musicGenerationMode: MusicGenerationMode.QUALITY
    },
  });
  await session.reset_context();
```

 
 

## Prompt guide for Lyria RealTime

Here's a non-exhaustive list of prompts you can use to prompt Lyria RealTime:

- Instruments: 

```
303 Acid Bass, 808 Hip Hop Beat, Accordion, Alto Saxophone,
Bagpipes, Balalaika Ensemble, Banjo, Bass Clarinet, Bongos, Boomy Bass,
Bouzouki, Buchla Synths, Cello, Charango, Clavichord, Conga Drums,
Didgeridoo, Dirty Synths, Djembe, Drumline, Dulcimer, Fiddle, Flamenco
Guitar, Funk Drums, Glockenspiel, Guitar, Hang Drum, Harmonica, Harp,
Harpsichord, Hurdy-gurdy, Kalimba, Koto, Lyre, Mandolin, Maracas, Marimba,
Mbira, Mellotron, Metallic Twang, Moog Oscillations, Ocarina, Persian Tar,
Pipa, Precision Bass, Ragtime Piano, Rhodes Piano, Shamisen, Shredding
Guitar, Sitar, Slide Guitar, Smooth Pianos, Spacey Synths, Steel Drum, Synth
Pads, Tabla, TR-909 Drum Machine, Trumpet, Tuba, Vibraphone, Viola Ensemble,
Warm Acoustic Guitar, Woodwinds, ...
```



- Music Genre: 

```
Acid Jazz, Afrobeat, Alternative Country, Baroque, Bengal Baul,
Bhangra, Bluegrass, Blues Rock, Bossa Nova, Breakbeat, Celtic Folk, Chillout,
Chiptune, Classic Rock, Contemporary R&B, Cumbia, Deep House, Disco Funk,
Drum & Bass, Dubstep, EDM, Electro Swing, Funk Metal, G-funk, Garage Rock,
Glitch Hop, Grime, Hyperpop, Indian Classical, Indie Electronic, Indie Folk,
Indie Pop, Irish Folk, Jam Band, Jamaican Dub, Jazz Fusion, Latin Jazz, Lo-Fi
Hip Hop, Marching Band, Merengue, New Jack Swing, Minimal Techno, Moombahton,
Neo-Soul, Orchestral Score, Piano Ballad, Polka, Post-Punk, 60s Psychedelic
Rock, Psytrance, R&B, Reggae, Reggaeton, Renaissance Music, Salsa, Shoegaze,
Ska, Surf Rock, Synthpop, Techno, Trance, Trap Beat, Trip Hop, Vaporwave,
Witch house, ...
```



- Mood/Description: 

```
Acoustic Instruments, Ambient, Bright Tones, Chill,
Crunchy Distortion, Danceable, Dreamy, Echo, Emotional, Ethereal Ambience,
Experimental, Fat Beats, Funky, Glitchy Effects, Huge Drop, Live Performance,
Lo-fi, Ominous Drone, Psychedelic, Rich Orchestration, Saturated Tones,
Subdued Melody, Sustained Chords, Swirling Phasers, Tight Groove,
Unsettling, Upbeat, Virtuoso, Weird Noises, ...
```



These are just some examples, Lyria RealTime can do much more. Experiment
with your own prompts!

## Best practices

- Client applications must implement robust audio buffering to ensure smooth
playback. This helps account for network jitter and slight variations in
generation latency.

- Effective prompting:

 Be descriptive. Use adjectives describing mood, genre, and instrumentation.

- Iterate and steer gradually. Rather than completely changing the prompt,
try adding or modifying elements to morph the music more smoothly.

- Experiment with weight on `WeightedPrompt` to influence how strongly a new
prompt affects the ongoing generation.

 

## Technical details

This section describes the specifics of how to use Lyria RealTime music
generation.

### Specifications

- Output format: Raw 16-bit PCM Audio

- Sample rate: 48kHz

- Channels: 2 (stereo)

### Controls

Music generation can be influenced in real time by sending messages containing:

- `WeightedPrompt`: A text string describing a musical idea, genre, instrument,
mood, or characteristic. Multiple prompts can potentially be supplied to blend
influences. See above for more details on how to best prompt
Lyria RealTime.

- `MusicGenerationConfig`: Configuration for the music generation process,
influencing the characteristics of the output audio.). Parameters
include:

 `guidance`: (float) Range: `[0.0, 6.0]`. Default: `4.0`.
Controls how strictly the model follows the prompts. Higher guidance
improves adherence to the prompt, but makes transitions more abrupt.

- `bpm`: (int) Range: `[60, 200]`.
Sets the Beats Per Minute you want for the generated music. You need to
stop/play or reset the context for the model it take into account the new
bpm.

- `density`: (float) Range: `[0.0, 1.0]`.
Controls the density of musical notes/sounds. Lower values produce sparser
music; higher values produce "busier" music.

- `brightness`: (float) Range: `[0.0, 1.0]`.
Adjusts the tonal quality. Higher values produce "brighter" sounding
audio, generally emphasizing higher frequencies.

- `scale`: (Enum)
Sets the musical scale (Key and Mode) for the generation. Use the
 `Scale` enum values provided by the SDK. You need to
stop/play or reset the context for the model it take into account the new
scale.

- `mute_bass`: (bool) Default: `False`.
Controls whether the model reduces the outputs' bass.

- `mute_drums`: (bool) Default: `False`.
Controls whether the model outputs reduces the outputs' drums.

- `only_bass_and_drums`: (bool) Default: `False`.
Steer the model to try to only output bass and drums.

- `music_generation_mode`: (Enum)
Indicates to the model if it should focus on `QUALITY` (default value) or
`DIVERSITY` of music. It can also be set to `VOCALIZATION` to let the
model generate vocalizations as another instrument (add them as new
pompts).

 
- `PlaybackControl`: Commands to control playback aspects, such as play, pause,
stop or reset the context.

For `bpm`, `density`, `brightness` and `scale`, if no value is provided, the
model will decide what's best according to your initial prompts.

More classical parameters like `temperature` (0.0 to 3.0, default 1.1), `top_k`
(1 to 1000, default 40), and `seed` (0 to 2 147 483 647, randomly selected by
default) are also customizable in the `MusicGenerationConfig`.

#### Scale Enum Values

Here are all the scale values that the model can accept:

 
 
 
 Enum Value 
 Scale / Key 
 
 

 
 
 `C_MAJOR_A_MINOR` 
 C major / A minor 
 
 
 `D_FLAT_MAJOR_B_FLAT_MINOR` 
 D‚ô≠ major / B‚ô≠ minor 
 
 
 `D_MAJOR_B_MINOR` 
 D major / B minor 
 
 
 `E_FLAT_MAJOR_C_MINOR` 
 E‚ô≠ major / C minor 
 
 
 `E_MAJOR_D_FLAT_MINOR` 
 E major / C‚ôØ/D‚ô≠ minor 
 
 
 `F_MAJOR_D_MINOR` 
 F major / D minor 
 
 
 `G_FLAT_MAJOR_E_FLAT_MINOR` 
 G‚ô≠ major / E‚ô≠ minor 
 
 
 `G_MAJOR_E_MINOR` 
 G major / E minor 
 
 
 `A_FLAT_MAJOR_F_MINOR` 
 A‚ô≠ major / F minor 
 
 
 `A_MAJOR_G_FLAT_MINOR` 
 A major / F‚ôØ/G‚ô≠ minor 
 
 
 `B_FLAT_MAJOR_G_MINOR` 
 B‚ô≠ major / G minor 
 
 
 `B_MAJOR_A_FLAT_MINOR` 
 B major / G‚ôØ/A‚ô≠ minor 
 
 
 `SCALE_UNSPECIFIED` 
 Default / The model decides 
 
 
 

The model is capable of guiding the notes that are played, but does
not distinguish between relative keys. Thus each enum corresponds both to the
relative major and minor. For example, `C_MAJOR_A_MINOR` would correspond to all
the white keys of a piano, and `F_MAJOR_D_MINOR` would be all the white keys
except B flat.

### Limitations

- Instrumental only: The model generates instrumental music only.

- Safety: Prompts are checked by safety filters. Prompts triggering the filters
will be ignored in which case an explanation will be written in the output's
`filtered_prompt` field.

- Watermarking: Output audio is always watermarked for identification following
our Responsible AI principles.

## What's next

- Instead of music, learn how to generate multi-speakers conversation using 
the TTS models ,

- Discover how to generate images or videos ,

- Instead of generation music or audio, find out how to Gemini can
 understand Audio files ,

- Have a real-time conversation with Gemini using the
 Live API .

Explore the Cookbook for more
code examples and tutorials.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-26 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-26 UTC."],[],[]]

---

### Gemini API libraries &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/libraries#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API libraries  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API libraries 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

When building with the Gemini API, we recommend using the Google GenAI SDK .
These are the official, production-ready libraries that we develop and maintain
for the most popular languages. They are in General Availability and used in all our official
documentation and examples.

If you're new to the Gemini API, follow our quickstart guide to get started.

## Language support and installation

The Google GenAI SDK is available for the Python, JavaScript/TypeScript, Go and
Java languages. You can install each language's library using package managers,
or visit their GitHub repos for further engagement:

 
 

### Python

- 

Library: `google-genai` 

- 

GitHub Repository: googleapis/python-genai 

- 

Installation: `pip install google-genai`

 
 

### JavaScript

- 

Library: `@google/genai` 

- 

GitHub Repository: googleapis/js-genai 

- 

Installation: `npm install @google/genai`

 
 

### Go

- 

Library: `google.golang.org/genai` 

- 

GitHub Repository: googleapis/go-genai 

- 

Installation: `go get google.golang.org/genai`

 
 

### Java

- 

Library: `google-genai`

- 

GitHub Repository: googleapis/java-genai 

- 

Installation: If you're using Maven, add the following to your dependencies:

 

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

 
 

### C#

- 

Library: `Google.GenAI`

- 

GitHub Repository: googleapis/go-genai 

- 

Installation: `dotnet add package Google.GenAI`

 
 

## General availability

We started rolling out Google GenAI SDK, a new set of libraries to access Gemini
API, in late 2024 when we launched Gemini 2.0.

As of May 2025, they reached General Availability (GA) across all supported
platforms and are the recommended libraries to access the Gemini API. They are
stable, fully supported for production use, and are actively maintained. They
provide access to the latest features, and offer the best performance working
with Gemini.

If you're using one of our legacy libraries,
we strongly recommend you migrate so that you can access the latest features and
get the best performance working with Gemini. Review the legacy libraries section for more information.

## Legacy libraries and migration

If you are using one of our legacy libraries, we recommend that you
 migrate to the new libraries .

The legacy libraries don't provide access to recent features (such as
 Live API and Veo ) and are on
a deprecation path. They will stop receiving updates on November 30th,
2025, the feature gaps will grow and potential bugs may no longer get fixed.

Each legacy library's support status varies, detailed in the following table:

 
 
 
 
 
 
 
 
 
 Language 
 Legacy library 
 Support status 
 Recommended library 
 
 
 
 
 Python 
 `google-generativeai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `google-genai` 
 
 
 JavaScript/TypeScript 
 `@google/generativeai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `@google/genai` 
 
 
 Go 
 `google.golang.org/generative-ai` 
 All support, including bug fixes, ends on November 30th, 2025. 
 `google.golang.org/genai` 
 
 
 Dart and Flutter 
 `google_generative_ai` 
 Not actively maintained 
 Use trusted community or third party libraries, like firebase_ai , or access using REST API 
 
 
 Swift 
 `generative-ai-swift` 
 Not actively maintained 
 Use Firebase AI Logic 
 
 
 Android 
 `generative-ai-android` 
 Not actively maintained 
 Use Firebase AI Logic 
 
 
 

 Note for Java developers: There was no legacy Google-provided Java SDK for
the Gemini API, so no migration from a previous Google library is required. You
can start directly with the new library in the
 Language support and installation section.

## Prompt templates for code generation

Generative models (e.g., Gemini, Claude) and AI-powered IDEs (e.g., Cursor) may
produce code for the Gemini API using outdated or deprecated libraries due to
their training data cutoff. For the generated code to use the latest,
recommended libraries, provide version and usage guidance directly in your
prompts. You can use the templates below to provide the necessary context:

- 

 Python 

- 

 JavaScript/TypeScript 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Embeddings &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/embeddings

- 
 
 
 
 
 
 
 
 
 
 
 Embeddings  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Embeddings 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API offers text embedding models to generate embeddings for words,
phrases, sentences, and code. These foundational embeddings power advanced NLP
tasks such as semantic search, classification, and clustering, providing more
accurate, context-aware results than keyword-based approaches.

Building Retrieval Augmented Generation (RAG) systems is a common use case for
embeddings. Embeddings play a key role in significantly enhancing model outputs
with improved factual accuracy, coherence, and contextual richness. They
efficiently retrieve relevant information from knowledge bases, represented by
embeddings, which are then passed as additional context in the input prompt to
language models, guiding it to generate more informed and accurate responses.

To learn more about the available embedding model variants, see the Model
versions section. For higher throughput serving at half the
price, try Batch API Embedding .

## Generating embeddings

Use the `embedContent` method to generate text embeddings:

 
 

### Python

 

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents="What is the meaning of life?")

print(result.embeddings)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: 'What is the meaning of life?',
    });

    console.log(response.embeddings);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-001",
     "content": {"parts":[{"text": "What is the meaning of life?"}]}
    }'
```

 
 

You can also generate embeddings for multiple chunks at once by passing them in
as a list of strings.

 
 

### Python

 

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents= [
            "What is the meaning of life?",
            "What is the purpose of existence?",
            "How do I bake a cake?"
        ])

for embedding in result.embeddings:
    print(embedding)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: [
            'What is the meaning of life?',
            'What is the purpose of existence?',
            'How do I bake a cake?'
        ],
    });

    console.log(response.embeddings);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?"),
        genai.NewContentFromText("How does photosynthesis work?"),
        genai.NewContentFromText("Tell me about the history of the internet."),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"requests": [{
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "What is the meaning of life?"}]}, },
    {
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "How much wood would a woodchuck chuck?"}]}, },
    {
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "How does the brain work?"}]}, }, ]}' 2> /dev/null | grep -C 5 values
    ```
```

 
 

## Specify task type to improve performance

You can use embeddings for a wide range of tasks from classification to document
search. Specifying the right task type helps optimize the embeddings for the
intended relationships, maximizing accuracy and efficiency. For a complete list
of supported task types, see the Supported task types 
table.

The following example shows how you can use
`SEMANTIC_SIMILARITY` to check how similar in meaning strings of texts are.

 
 

### Python

 

```
from google import genai
from google.genai import types
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

client = genai.Client()

texts = [
    "What is the meaning of life?",
    "What is the purpose of existence?",
    "How do I bake a cake?"]

result = [
    np.array(e.values) for e in client.models.embed_content(
        model="gemini-embedding-001",
        contents=texts,
        config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")).embeddings
]

# Calculate cosine similarity. Higher scores = greater semantic similarity.

embeddings_matrix = np.array(result)
similarity_matrix = cosine_similarity(embeddings_matrix)

for i, text1 in enumerate(texts):
    for j in range(i + 1, len(texts)):
        text2 = texts[j]
        similarity = similarity_matrix[i, j]
        print(f"Similarity between '{text1}' and '{text2}': {similarity:.4f}")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as cosineSimilarity from "compute-cosine-similarity";

async function main() {
    const ai = new GoogleGenAI({});

    const texts = [
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    ];

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: texts,
        taskType: 'SEMANTIC_SIMILARITY'
    });

    const embeddings = response.embeddings.map(e => e.values);

    for (let i = 0; i < texts.length; i++) {
        for (let j = i + 1; j < texts.length; j++) {
            const text1 = texts[i];
            const text2 = texts[j];
            const similarity = cosineSimilarity(embeddings[i], embeddings[j]);
            console.log(`Similarity between '${text1}' and '${text2}': ${similarity.toFixed(4)}`);
        }
    }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "math"

    "google.golang.org/genai"
)

// cosineSimilarity calculates the similarity between two vectors.
func cosineSimilarity(a, b []float32) (float64, error) {
    if len(a) != len(b) {
        return 0, fmt.Errorf("vectors must have the same length")
    }

    var dotProduct, aMagnitude, bMagnitude float64
    for i := 0; i < len(a); i++ {
        dotProduct += float64(a[i] * b[i])
        aMagnitude += float64(a[i] * a[i])
        bMagnitude += float64(b[i] * b[i])
    }

    if aMagnitude == 0 || bMagnitude == 0 {
        return 0, nil
    }

    return dotProduct / (math.Sqrt(aMagnitude) * math.Sqrt(bMagnitude)), nil
}

func main() {
    ctx := context.Background()
    client, _ := genai.NewClient(ctx, nil)
    defer client.Close()

    texts := []string{
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    }

    var contents []*genai.Content
    for _, text := range texts {
        contents = append(contents, genai.NewContentFromText(text, genai.RoleUser))
    }

    result, _ := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{TaskType: genai.TaskTypeSemanticSimilarity},
    )

    embeddings := result.Embeddings

    for i := 0; i < len(texts); i++ {
        for j := i + 1; j < len(texts); j++ {
            similarity, _ := cosineSimilarity(embeddings[i].Values, embeddings[j].Values)
            fmt.Printf("Similarity between '%s' and '%s': %.4f\n", texts[i], texts[j], similarity)
        }
    }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"task_type": "SEMANTIC_SIMILARITY",
    "content": {
    "parts":[{
    "text": "What is the meaning of life?"}, {"text": "How much wood would a woodchuck chuck?"}, {"text": "How does the brain work?"}]}
    }'
```

 
 

The following shows an example output from this code snippet:

 

```
Similarity between 'What is the meaning of life?' and 'What is the purpose of existence?': 0.9481

Similarity between 'What is the meaning of life?' and 'How do I bake a cake?': 0.7471

Similarity between 'What is the purpose of existence?' and 'How do I bake a cake?': 0.7371
```

 

### Supported task types

 

 Task type 
 Description 
 Examples 
 
 SEMANTIC_SIMILARITY 
 Embeddings optimized to assess text similarity. 
 Recommendation systems, duplicate detection 
 
 
 CLASSIFICATION 
 Embeddings optimized to classify texts according to preset labels. 
 Sentiment analysis, spam detection 
 
 
 CLUSTERING 
 Embeddings optimized to cluster texts based on their similarities. 
 Document organization, market research, anomaly detection 
 
 
 RETRIEVAL_DOCUMENT 
 Embeddings optimized for document search. 
 Indexing articles, books, or web pages for search. 
 
 
 RETRIEVAL_QUERY 
 
 Embeddings optimized for general search queries.
 Use `RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for documents to be retrieved.
 
 Custom search 
 
 
 CODE_RETRIEVAL_QUERY 
 
 Embeddings optimized for retrieval of code blocks based on natural language queries.
 Use `CODE_RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for code blocks to be retrieved.
 
 Code suggestions and search 
 
 
 QUESTION_ANSWERING 
 
 Embeddings for questions in a question-answering system, optimized for finding documents that answer the question.
 Use `QUESTION_ANSWERING` for questions; `RETRIEVAL_DOCUMENT` for documents to be retrieved.
 
 Chatbox 
 
 
 FACT_VERIFICATION 
 
 Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement.
 Use `FACT_VERIFICATION` for the target text; `RETRIEVAL_DOCUMENT` for documents to be retrieved
 
 Automated fact-checking systems 
 
 

## Controlling embedding size

The Gemini embedding model, `gemini-embedding-001`, is trained using the
Matryoshka Representation Learning (MRL) technique which teaches a model to
learn high-dimensional embeddings that have initial segments (or prefixes) which
are also useful, simpler versions of the same data.

Use the `output_dimensionality` parameter to control the size of
the output embedding vector. Selecting a smaller output dimensionality can save
storage space and increase computational efficiency for downstream applications,
while sacrificing little in terms of quality. By default, it outputs a
3072-dimensional embedding, but you can truncate it to a smaller size without
losing quality to save storage space. We recommend using 768, 1536, or 3072
output dimensions.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

result = client.models.embed_content(
    model="gemini-embedding-001",
    contents="What is the meaning of life?",
    config=types.EmbedContentConfig(output_dimensionality=768)
)

[embedding_obj] = result.embeddings
embedding_length = len(embedding_obj.values)

print(f"Length of embedding: {embedding_length}")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {
    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        content: 'What is the meaning of life?',
        outputDimensionality: 768,
    });

    const embeddingLength = response.embedding.values.length;
    console.log(`Length of embedding: ${embeddingLength}`);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client uses Application Default Credentials.
    // Authenticate with 'gcloud auth application-default login'.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }
    defer client.Close()

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }

    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{OutputDimensionality: 768},
    )
    if err != nil {
        log.Fatal(err)
    }

    embedding := result.Embeddings[0]
    embeddingLength := len(embedding.Values)
    fmt.Printf("Length of embedding: %d\n", embeddingLength)
}
```

 
 

### REST

 

```
curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
        "content": {"parts":[{ "text": "What is the meaning of life?"}]},
        "output_dimensionality": 768
    }'
```

 
 

Example output from the code snippet:

 

```
Length of embedding: 768
```

 

## Ensuring quality for smaller dimensions

The 3072 dimension embedding is normalized. Normalized embeddings produce more
accurate semantic similarity by comparing vector direction, not magnitude. For
other dimensions, including 768 and 1536, you need to normalize the embeddings
as follows:

 
 

### Python

 

```
import numpy as np
from numpy.linalg import norm

embedding_values_np = np.array(embedding_obj.values)
normed_embedding = embedding_values_np / np.linalg.norm(embedding_values_np)

print(f"Normed embedding length: {len(normed_embedding)}")
print(f"Norm of normed embedding: {np.linalg.norm(normed_embedding):.6f}") # Should be very close to 1
```

 
 

Example output from this code snippet:

 

```
Normed embedding length: 768
Norm of normed embedding: 1.000000
```

 

The following table shows the MTEB scores, a commonly used benchmark for
embeddings, for different dimensions. Notably, the result shows that performance
is not strictly tied to the size of the embedding dimension, with lower
dimensions achieving scores comparable to their higher dimension counterparts.

 
 MRL Dimension 
 MTEB Score 
 
 2048

 
 68.16

 
 
 
 1536

 
 68.17

 
 
 
 768

 
 67.99

 
 
 
 512

 
 67.55

 
 
 
 256

 
 66.19

 
 
 
 128

 
 63.31

 
 
 

## Use cases

Text embeddings are crucial for a variety of common AI use cases, such as:

- Retrieval-Augmented Generation (RAG): Embeddings enhance the quality
of generated text by retrieving and incorporating relevant information into
the context of a model.

- 

 Information retrieval: Search for the most semantically similar text or
documents given a piece of input text.

 
 Document search tutorial task 
 

- 

 Search reranking : Prioritize the most relevant items by semantically
scoring initial results against the query.

 
 Search reranking tutorial task 
 

- 

 Anomaly detection: Comparing groups of embeddings can help identify
hidden trends or outliers.

 
 Anomaly detection tutorial bubble_chart 
 

- 

 Classification: Automatically categorize text based on its content, such
as sentiment analysis or spam detection

 
 Classification tutorial token 
 

- 

 Clustering: Effectively grasp complex relationships by creating clusters
and visualizations of your embeddings.

 
 Clustering visualization tutorial bubble_chart 
 

## Storing embeddings

As you take embeddings to production, it is common to
use vector databases to efficiently store, index, and retrieve
high-dimensional embeddings. Google Cloud offers managed data services that
can be used for this purpose including
 BigQuery ,
 AlloyDB , and
 Cloud SQL .

The following tutorials show how to use other third party vector databases
with Gemini Embedding.

- 
 ChromaDB tutorials bolt 
 

- 
 QDrant tutorials bolt 
 

- 
 Weaviate tutorials bolt 
 

- 
 Pinecone tutorials bolt 
 

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`gemini-embedding-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Text embeddings

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

2,048

 
 
 

 Output dimension size 

 

Flexible, supports: 128 - 3072, Recommended: 768, 1536, 3072

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-embedding-001`

 - Experimental: `gemini-embedding-exp-03-07` (deprecating in Oct of 2025)

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 

## Batch embeddings

If latency is not a concern, try using the Gemini Embeddings model with
 Batch API . This
allows for much higher throughput at 50% of interactive Embedding pricing.
Find examples on how to get started in the Batch API cookbook .

## Responsible use notice

Unlike generative AI models that create new content, the Gemini Embedding model
is only intended to transform the format of your input data into a numerical
representation. While Google is responsible for providing an embedding model
that transforms the format of your input data to the numerical-format requested,
users retain full responsibility for the data they input and the resulting
embeddings. By using the Gemini Embedding model you confirm that you have the
necessary rights to any content that you upload. Do not generate content that
infringes on others' intellectual property or privacy rights. Your use of this
service is subject to our Prohibited Use
Policy and
 Google's Terms of Service .

## Start building with embeddings

Check out the embeddings quickstart
notebook 
to explore the model capabilities and learn how to customize and visualize your
embeddings.

## Deprecation notice for legacy models

The following models will be deprecated in October, 2025:
 - `embedding-001`
 - `embedding-gecko-001`
 - `gemini-embedding-exp-03-07` (`gemini-embedding-exp`)

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### OpenAI compatibility &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/openai#main-content

- 
 
 
 
 
 
 
 
 
 
 
 OpenAI compatibility  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 OpenAI compatibility 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models are accessible using the OpenAI libraries (Python and TypeScript /
Javascript) along with the REST API, by updating three lines of code
and using your Gemini API key . If you
aren't already using the OpenAI libraries, we recommend that you call the
 Gemini API directly .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ]
    }'
```

 
 

What changed? Just three lines!

- 

 `api_key="GEMINI_API_KEY"` : Replace "`GEMINI_API_KEY`" with your actual Gemini
API key, which you can get in Google AI Studio .

- 

 

```
base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
```

: This
tells the OpenAI library to send requests to the Gemini API endpoint instead of
the default URL.

- 

 `model="gemini-2.5-flash"` : Choose a compatible Gemini model

## Thinking

Gemini 3 and 2.5 models are trained to think through complex problems, leading
to significantly improved reasoning. The Gemini API comes with thinking
parameters which give fine grain
control over how much the model will think.

Gemini 3 uses `"low"` and `"high"` thinking levels, and Gemini 2.5 models use
exact thinking budgets. These map to OpenAI's reasoning efforts as follows:

 
 
 `reasoning_effort` (OpenAI) 
 `thinking_level` (Gemini 3) 
 `thinking_budget` (Gemini 2.5) 
 
 
 `minimal` 
 `low` 
 `1,024` 
 
 
 `low` 
 `low` 
 `1,024` 
 
 
 `medium` 
 `high` 
 `8,192` 
 
 
 `high` 
 `high` 
 `24,576` 
 
 

If no `reasoning_effort` is specified, Gemini uses the model's
default level or budget .

If you want to disable thinking, you can set `reasoning_effort` to `"none"` for
2.5 models. Reasoning cannot be turned off for Gemini 2.5 Pro or 3 models.

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    reasoning_effort="low",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    reasoning_effort: "low",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
    "reasoning_effort": "low",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
      ]
    }'
```

 
 

Gemini thinking models also produce thought summaries .
You can use the `extra_body` field to include Gemini fields
in your request.

Note that `reasoning_effort` and `thinking_level`/`thinking_budget` overlap
functionality, so they can't be used at the same time.

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "Explain to me how AI works"}],
    extra_body={
      'extra_body': {
        "google": {
          "thinking_config": {
            "thinking_budget": "low",
            "include_thoughts": True
          }
        }
      }
    }
)

print(response.choices[0].message)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    messages: [{role: "user", content: "Explain to me how AI works",}],
    extra_body: {
      "google": {
        "thinking_config": {
          "thinking_budget": "low",
          "include_thoughts": true
        }
      }
    }
});

console.log(response.choices[0].message);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
      "messages": [{"role": "user", "content": "Explain to me how AI works"}],
      "extra_body": {
        "google": {
           "thinking_config": {
             "include_thoughts": true
           }
        }
      }
    }'
```

 
 

Gemini 3 supports OpenAI compatibility for thought signatures in chat completion
APIs. You can find the full example on the thought signatures page.

## Streaming

The Gemini API supports streaming responses .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  stream=True
)

for chunk in response:
    print(chunk.choices[0].delta)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    stream: true,
  });

  for await (const chunk of completion) {
    console.log(chunk.choices[0].delta.content);
  }
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ],
    "stream": true
  }'
```

 
 

## Function calling

Function calling makes it easier for you to get structured data outputs from
generative models and is supported in the Gemini API .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Get the weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. Chicago, IL",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]

messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}]
response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=messages,
  tools=tools,
  tool_choice="auto"
)

print(response)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}];
  const tools = [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get the weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. Chicago, IL",
              },
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
          },
        }
      }
  ];

  const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: messages,
    tools: tools,
    tool_choice: "auto",
  });

  console.log(response);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
  "model": "gemini-2.0-flash",
  "messages": [
    {
      "role": "user",
      "content": "What'\''s the weather like in Chicago today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. Chicago, IL"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}'
```

 
 

## Image understanding

Gemini models are natively multimodal and provide best in class performance on
 many common vision tasks .

 
 

### Python

 

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

 
 

### JavaScript

 

```
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

 
 

### REST

 

```
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

 
 

## Generate an image

Generate an image:

 
 

### Python

 

```
import base64
from openai import OpenAI
from PIL import Image
from io import BytesIO

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

response = client.images.generate(
    model="imagen-3.0-generate-002",
    prompt="a portrait of a sheepadoodle wearing a cape",
    response_format='b64_json',
    n=1,
)

for image_data in response.data:
  image = Image.open(BytesIO(base64.b64decode(image_data.b64_json)))
  image.show()
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const image = await openai.images.generate(
    {
      model: "imagen-3.0-generate-002",
      prompt: "a portrait of a sheepadoodle wearing a cape",
      response_format: "b64_json",
      n: 1,
    }
  );

  console.log(image.data);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/images/generations" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer GEMINI_API_KEY" \
  -d '{
        "model": "imagen-3.0-generate-002",
        "prompt": "a portrait of a sheepadoodle wearing a cape",
        "response_format": "b64_json",
        "n": 1,
      }'
```

 
 

## Audio understanding

Analyze audio input:

 
 

### Python

 

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

with open("/path/to/your/audio/file.wav", "rb") as audio_file:
  base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Transcribe this audio",
        },
        {
              "type": "input_audio",
              "input_audio": {
                "data": base64_audio,
                "format": "wav"
          }
        }
      ],
    }
  ],
)

print(response.choices[0].message.content)
```

 
 

### JavaScript

 

```
import fs from "fs";
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

const audioFile = fs.readFileSync("/path/to/your/audio/file.wav");
const base64Audio = Buffer.from(audioFile).toString("base64");

async function main() {
  const response = await client.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Transcribe this audio",
          },
          {
            type: "input_audio",
            input_audio: {
              data: base64Audio,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

  console.log(response.choices[0].message.content);
}

main();
```

 
 

### REST

 

```
bash -c '
  base64_audio=$(base64 -i "/path/to/your/audio/file.wav");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"Transcribe this audio file.\" },
            {
              \"type\": \"input_audio\",
              \"input_audio\": {
                \"data\": \"${base64_audio}\",
                \"format\": \"wav\"
              }
            }
          ]
        }
      ]
    }"
'
```

 
 

## Structured output

Gemini models can output JSON objects in any structure you define .

 
 

### Python

 

```
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "John and Susan are going to an AI conference on Friday."},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
```

 
 

## Embeddings

Text embeddings measure the relatedness of text strings and can be generated
using the Gemini API .

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.embeddings.create(
    input="Your text string goes here",
    model="gemini-embedding-001"
)

print(response.data[0].embedding)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const embedding = await openai.embeddings.create({
    model: "gemini-embedding-001",
    input: "Your text string goes here",
  });

  console.log(embedding);
}

main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/embeddings" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "input": "Your text string goes here",
    "model": "gemini-embedding-001"
  }'
```

 
 

## Batch API

You can create batch jobs , submit them, and check
their status using the OpenAI library.

You'll need to prepare the JSONL file in OpenAI input format. For example:

 

```
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "Tell me a one-sentence joke."}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "Why is the sky blue?"}]}}
```

 

OpenAI compatibility for Batch supports creating a batch,
monitoring job status, and viewing batch results.

Compatibility for upload and download is currently not supported. Instead, the
following example uses the `genai` client for uploading and downloading
 files , the same as when using the Gemini Batch API .

 
 

### Python

 

```
from openai import OpenAI

# Regular genai client for uploads & downloads
from google import genai
client = genai.Client()

openai_client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Upload the JSONL file in OpenAI input format, using regular genai SDK
uploaded_file = client.files.upload(
    file='my-batch-requests.jsonl',
    config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')
)

# Create batch
batch = openai_client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/chat/completions",
    completion_window="24h"
)

# Wait for batch to finish (up to 24h)
while True:
    batch = client.batches.retrieve(batch.id)
    if batch.status in ('completed', 'failed', 'cancelled', 'expired'):
        break
    print(f"Batch not finished. Current state: {batch.status}. Waiting 30 seconds...")
    time.sleep(30)
print(f"Batch finished: {batch}")

# Download results in OpenAI output format, using regular genai SDK
file_content = genai_client.files.download(file=batch.output_file_id).decode('utf-8')

# See batch_output JSONL in OpenAI output format
for line in file_content.splitlines():
    print(line)
```

 
 

The OpenAI SDK also supports generating embeddings with the Batch API . To do so, switch out the
`create` method's `endpoint` field for an embeddings endpoint, as well as the
`url` and `model` keys in the JSONL file:

 

```
# JSONL file using embeddings model and endpoint
# {"custom_id": "request-1", "method": "POST", "url": "/v1/embeddings", "body": {"model": "ggemini-embedding-001", "messages": [{"role": "user", "content": "Tell me a one-sentence joke."}]}}
# {"custom_id": "request-2", "method": "POST", "url": "/v1/embeddings", "body": {"model": "gemini-embedding-001", "messages": [{"role": "user", "content": "Why is the sky blue?"}]}}

# ...

# Create batch step with embeddings endpoint
batch = openai_client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/embeddings",
    completion_window="24h"
)
```

 

See the Batch embedding generation 
section of the OpenAI compatibility cookbook for a complete example.

## `extra_body`

There are several features supported by Gemini that are not available in OpenAI
models but can be enabled using the `extra_body` field.

 `extra_body` features 

 
 
 `cached_content` 
 Corresponds to Gemini's `GenerateContentRequest.cached_content`. 
 
 
 `thinking_config` 
 Corresponds to Gemini's `ThinkingConfig`. 
 
 

### `cached_content`

Here's an example of using `extra_body` to set `cached_content`:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
    api_key=MY_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

stream = client.chat.completions.create(
    model="gemini-2.5-pro",
    n=1,
    messages=[
        {
            "role": "user",
            "content": "Summarize the video"
        }
    ],
    stream=True,
    stream_options={'include_usage': True},
    extra_body={
        'extra_body':
        {
            'google': {
              'cached_content': "cachedContents/0000aaaa1111bbbb2222cccc3333dddd4444eeee"
          }
        }
    }
)

for chunk in stream:
    print(chunk)
    print(chunk.usage.to_dict())
```

 
 

## List models

Get a list of available Gemini models:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

models = client.models.list()
for model in models:
  print(model.id)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models \
-H "Authorization: Bearer GEMINI_API_KEY"
```

 
 

## Retrieve a model

Retrieve a Gemini model:

 
 

### Python

 

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

model = client.models.retrieve("gemini-2.0-flash")
print(model.id)
```

 
 

### JavaScript

 

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const model = await openai.models.retrieve("gemini-2.0-flash");
  console.log(model.id);
}

main();
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models/gemini-2.0-flash \
-H "Authorization: Bearer GEMINI_API_KEY"
```

 
 

## Current limitations

Support for the OpenAI libraries is still in beta while we extend feature support.

If you have questions about supported parameters, upcoming features, or run into
any issues getting started with Gemini, join our Developer Forum .

## What's next

Try our OpenAI Compatibility Colab to work through more detailed
examples.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini Robotics-ER 1.5 &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/robotics-overview

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Robotics-ER 1.5  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini Robotics-ER 1.5 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini Robotics-ER 1.5 is a vision-language model (VLM) that brings Gemini's
agentic capabilities to robotics. It's designed for advanced reasoning in the
physical world, allowing robots to interpret complex visual data, perform
spatial reasoning, and plan actions from natural language commands.

Key features and benefits:

- Enhanced autonomy: Robots can reason, adapt, and respond to changes in
open-ended environments.

- Natural language interaction: Makes robots easier to use by enabling
complex task assignments using natural language.

- Task orchestration: Deconstructs natural language commands into subtasks
and integrates with existing robot controllers and behaviors to complete
long-horizon tasks.

- Versatile capabilities: Locates and identifies objects, understands
object relationships, plans grasps and trajectories, and interprets dynamic
scenes.

This document describes what the model does and takes you
through several examples that highlight the model's
agentic capabilities.

If you want to jump right in, you can try out the model in Google AI Studio.

 Try in Google AI Studio 

## Safety

While Gemini Robotics-ER 1.5 was built with safety in mind, it is your
responsibility to maintain a safe environment around the robot. Generative AI
models can make mistakes, and physical robots can cause damage. Safety is a
priority, and making generative AI models safe when used with real-world
robotics is an active and critical area of our research. To learn more, visit
the Google DeepMind robotics safety page .

## Getting started: Finding objects in a scene

The following example demonstrates a common robotics use case. It shows how to
pass an image and a text prompt to the model using the
 `generateContent` 
method to get a list of identified objects with their corresponding 2D points.
The model returns points for items it identified in an image, returning
their normalized 2D coordinates and labels.

You can use this output with a robotics API or call a vision-language-action
(VLA) model or any other third-party user-defined functions to generate actions
for a robot to perform.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
PROMPT = """
          Point to no more than 10 items in the image. The label returned
          should be an identifying name for the object detected.
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format
          normalized to 0-1000.
        """
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image
with open("my-image.png", 'rb') as f:
    image_bytes = f.read()

image_response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/png',
        ),
        PROMPT
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        thinking_config=types.ThinkingConfig(thinking_budget=0)
    )
)

print(image_response.text)
```

 
 

### REST

 

```
# First, ensure you have the image file locally.
# Encode the image to base64
IMAGE_BASE64=$(base64 -w 0 my-image.png)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-robotics-er-1.5-preview:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "inlineData": {
              "mimeType": "image/png",
              "data": "'"${IMAGE_BASE64}"'"
            }
          },
          {
            "text": "Point to no more than 10 items in the image. The label returned should be an identifying name for the object detected. The answer should follow the json format: [{\"point\": [y, x], \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000."
          }
        ]
      }
    ],
    "generationConfig": {
      "temperature": 0.5,
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

The output will be a JSON array containing objects, each with a `point`
(normalized `[y, x]` coordinates) and a `label` identifying the object.

 
 

### JSON

 

```
[
  {"point": [376, 508], "label": "small banana"},
  {"point": [287, 609], "label": "larger banana"},
  {"point": [223, 303], "label": "pink starfruit"},
  {"point": [435, 172], "label": "paper bag"},
  {"point": [270, 786], "label": "green plastic bowl"},
  {"point": [488, 775], "label": "metal measuring cup"},
  {"point": [673, 580], "label": "dark blue bowl"},
  {"point": [471, 353], "label": "light blue bowl"},
  {"point": [492, 497], "label": "bread"},
  {"point": [525, 429], "label": "lime"}
]
```

 
 

The following image is an example of how these points can be displayed:

 

## How it works

Gemini Robotics-ER 1.5 allows your robots to contextualize and work in the
physical world using spatial understanding. It takes image/video/audio input and
natural language prompts to:

- Understand objects and scene context : Identifies objects, and reasons
about their relationship to the scene, including their affordances.

- Understand task instructions : Interprets tasks given in natural
language, like "find the banana".

- Reason spatially and temporally : Understand sequences of actions and how
objects interact with a scene over time.

- Provide structured output : Returns coordinates (points or bounding
boxes) representing object locations.

This enables robots to "see" and "understand" their environment
programmatically.

Gemini Robotics-ER 1.5 is also agentic, which means it can break down complex
tasks (like "put the apple in the bowl") into sub-tasks to orchestrate long
horizon tasks:

- Sequencing subtasks : Decomposes commands into a logical sequence of
steps.

- Function calls/Code execution : Executes steps by calling your existing
robot functions/tools or executing generated code.

Read more about how function calling with Gemini works on the Function Calling
page .

### Using the thinking budget with Gemini Robotics-ER 1.5

Gemini Robotics-ER 1.5 has a flexible thinking budget that gives you control
over latency versus accuracy tradeoffs. For spatial understanding tasks like
object detection, the model can achieve high performance with a small thinking
budget. More complex reasoning tasks like counting and weight estimation benefit
from a larger thinking budget. This lets you balance the need for
low-latency responses with high-accuracy results for more challenging tasks.

To learn more about thinking budgets, see the
 Thinking 
core capabilities page.

## Agentic capabilities for robotics

This section walks through various capabilities of Gemini
Robotics-ER 1.5, demonstrating how to use the model for robotic perception,
reasoning, and planning applications.

The examples in this section demonstrate capabilities from pointing and finding
objects in an image to planning trajectories and orchestrating long horizon
tasks. For simplicity, the code snippets have been reduced to show the prompt
and the call to `generate_content` API. The full runnable code as well as
additional examples can be found in the
 Robotics cookbook .

### Pointing to objects

Pointing and finding objects in images or video frames is a common use case for
vision-and-language models (VLMs) in robotics. The following example asks the
model to find specific objects within an image and return their coordinates in
an image.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

queries = [
    "bread",
    "starfruit",
    "banana",
]

prompt = f"""
    Get all points matching the following objects: {', '.join(queries)}. The
    label returned should be an identifying name for the object detected.
    The answer should follow the json format:

    [{{"point": , "label": }}, ...]. The points are in

    [y, x] format normalized to 0-1000.
    """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The output would be similar to the getting started example, a JSON containing
the coordinates of the objects found and their labels.

 

```
[
  {"point": [671, 317], "label": "bread"},
  {"point": [738, 307], "label": "bread"},
  {"point": [702, 237], "label": "bread"},
  {"point": [629, 307], "label": "bread"},
  {"point": [833, 800], "label": "bread"},
  {"point": [609, 663], "label": "banana"},
  {"point": [770, 483], "label": "starfruit"}
]
```

 

 

Use the following prompt to request the model to interpret abstract categories
like "fruit" instead of specific objects and locate all instances in the image.

 
 

### Python

 

```
prompt = f"""
        Get all points for fruit. The label returned should be an identifying
        name for the object detected.
        """ + """The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...]. The points are in
        [y, x] format normalized to 0-1000."""
```

 
 

Visit the image understanding page for
other image processing techniques.

### Tracking objects in a video

Gemini Robotics-ER 1.5 can also analyze video frames to track objects
over time. See Video inputs 
for a list of supported video formats.

The following is the base prompt used to find specific objects in
each frame that the model analyzes:

 
 

### Python

 

```
# Define the objects to find
queries = [
    "pen (on desk)",
    "pen (in robot hand)",
    "laptop (opened)",
    "laptop (closed)",
]

base_prompt = f"""
  Point to the following objects in the provided image: {', '.join(queries)}.
  The answer should follow the json format:

  [{{"point": , "label": }}, ...].

  The points are in [y, x] format normalized to 0-1000.
  If no objects are found, return an empty JSON list [].
  """
```

 
 

The output shows a pen and laptop being tracked across the video frames.

 

For the full runnable code, see the
 Robotics cookbook .

### Object detection and bounding boxes

Beyond single points, the model can also return 2D bounding boxes, providing a
rectangular region enclosing an object.

This example requests 2D bounding boxes for identifiable objects on a table. The
model is instructed to limit the output to 25 objects and to name multiple
instances uniquely.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
      Return bounding boxes as a JSON array with labels. Never return masks
      or code fencing. Limit to 25 objects. Include as many objects as you
      can identify on the table.
      If an object is present multiple times, name them according to their
      unique characteristic (colors, size, position, unique characteristics, etc..).
      The format should be as follows: [{"box_2d": [ymin, xmin, ymax, xmax],
      "label": <label for the object>}] normalized to 0-1000. The values in
      box_2d must only be integers
      """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The following displays the boxes returned from the model.

 

For the full runnable code, see the Robotics
cookbook .
The Image understanding page also has
additional examples of visual tasks like segmentation and object detection.

Additional bounding box examples can be found in the
 Image understanding page.

### Trajectories

Gemini Robotics-ER 1.5 can generate sequences of points that define a
trajectory, useful for guiding robot movement.

This example requests a trajectory to move a red pen to an organizer, including
the starting point and a series of intermediate points.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

points_data = []
prompt = """
        Place a point on the red pen, then 15 points for the trajectory of
        moving the red pen to the top of the organizer on the left.
        The points should be labeled by order of the trajectory, from '0'
        (start point at left hand) to <n> (final point)
        The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...].
        The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
  )
)

print(image_response.text)
```

 
 

The response is a set of coordinates that describe the trajectory of the path
that the red pen should follow to complete the task of moving it on top of the
organizer:

 

```
[
  {"point": [550, 610], "label": "0"},
  {"point": [500, 600], "label": "1"},
  {"point": [450, 590], "label": "2"},
  {"point": [400, 580], "label": "3"},
  {"point": [350, 550], "label": "4"},
  {"point": [300, 520], "label": "5"},
  {"point": [250, 490], "label": "6"},
  {"point": [200, 460], "label": "7"},
  {"point": [180, 430], "label": "8"},
  {"point": [160, 400], "label": "9"},
  {"point": [140, 370], "label": "10"},
  {"point": [120, 340], "label": "11"},
  {"point": [110, 320], "label": "12"},
  {"point": [105, 310], "label": "13"},
  {"point": [100, 305], "label": "14"},
  {"point": [100, 300], "label": "15"}
]
```

 

 

### Orchestration

Gemini Robotics-ER 1.5 can perform higher-level spatial reasoning, inferring
actions or identifying optimal locations based on contextual understanding.

#### Making room for a laptop

This example shows how Gemini Robotics-ER can reason about a space. The prompt
asks the model to identify which object needs to be moved to create
space for another item.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Point to the object that I need to remove to make room for my laptop
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The response contains a 2D coordinate of the object that answers the user's
question, in this case, the object that should move to make room for a laptop.

 

```
[
  {"point": [672, 301], "label": "The object that I need to remove to make room for my laptop"}
]
```

 

 

#### Packing a Lunch

The model can also provide instructions for multi-step tasks and point to
relevant objects for each step. This example shows how the model plans a series
of steps to pack a lunch bag.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-of-lunch.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Explain how to pack the lunch box and lunch bag. Point to each
          object that you refer to. Each point should be in the format:
          [{"point": [y, x], "label": }], where the coordinates are
          normalized between 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The response of this prompt is a set of step by step instructions on how to pack
a lunch bag from the image input.

 Input image 

 

 Model output 

 

```
Based on the image, here is a plan to pack the lunch box and lunch bag:

1.  **Pack the fruit into the lunch box.** Place the [apple](apple), [banana](banana), [red grapes](red grapes), and [green grapes](green grapes) into the [blue lunch box](blue lunch box).
2.  **Add the spoon to the lunch box.** Put the [blue spoon](blue spoon) inside the lunch box as well.
3.  **Close the lunch box.** Secure the lid on the [blue lunch box](blue lunch box).
4.  **Place the lunch box inside the lunch bag.** Put the closed [blue lunch box](blue lunch box) into the [brown lunch bag](brown lunch bag).
5.  **Pack the remaining items into the lunch bag.** Place the [blue snack bar](blue snack bar) and the [brown snack bar](brown snack bar) into the [brown lunch bag](brown lunch bag).

Here is the list of objects and their locations:
*   [{"point": [899, 440], "label": "apple"}]
*   [{"point": [814, 363], "label": "banana"}]
*   [{"point": [727, 470], "label": "red grapes"}]
*   [{"point": [675, 608], "label": "green grapes"}]
*   [{"point": [706, 529], "label": "blue lunch box"}]
*   [{"point": [864, 517], "label": "blue spoon"}]
*   [{"point": [499, 401], "label": "blue snack bar"}]
*   [{"point": [614, 705], "label": "brown snack bar"}]
*   [{"point": [448, 501], "label": "brown lunch bag"}]
```

 

#### Calling a custom robot API

This example demonstrates task orchestration with a custom robot API. It
introduces a mock API designed for a pick-and-place operation. The task is to
pick up a blue block and place it in an orange bowl:

 

Similar to the other examples on this page, the full runnable code is available
in the Robotics cookbook .

First step is to locate both of the items with the following prompt:

 
 

### Python

 

```
prompt = """
            Locate and point to the blue block and the orange bowl. The label
            returned should be an identifying name for the object detected.
            The answer should follow the json format: [{"point": <point>, "label": <label1>}, ...].
            The points are in [y, x] format normalized to 0-1000.
          """
```

 
 

The model response includes the normalized coordinates of the block and the bowl:

 

```
[
  {"point": [389, 252], "label": "orange bowl"},
  {"point": [727, 659], "label": "blue block"}
]
```

 

This example uses the following mock robot API: 

 
 

### Python

 

```
def move(x, y, high):
  print(f"moving to coordinates: {x}, {y}, {15 if high else 5}")

def setGripperState(opened):
  print("Opening gripper" if opened else "Closing gripper")

def returnToOrigin():
  print("Returning to origin pose")
```

 
 

The next step is calling a sequence of API functions with the necessary logic to
execute the action. The following prompt includes a description of the robot
API that the model should use when orchestrating this task.

 
 

### Python

 

```
prompt = f"""
    You are a robotic arm with six degrees-of-freedom. You have the
    following functions available to you:

    def move(x, y, high):
      # moves the arm to the given coordinates. The boolean value 'high' set
      to True means the robot arm should be lifted above the scene for
      avoiding obstacles during motion. 'high' set to False means the robot
      arm should have the gripper placed on the surface for interacting with
      objects.

    def setGripperState(opened):
      # Opens the gripper if opened set to true, otherwise closes the gripper

    def returnToOrigin():
      # Returns the robot to an initial state. Should be called as a cleanup
      operation.

    The origin point for calculating the moves is at normalized point
    y={robot_origin_y}, x={robot_origin_x}. Use this as the new (0,0) for
    calculating moves, allowing x and y to be negative.
    Perform a pick and place operation where you pick up the blue block at
    normalized coordinates ({block_x}, {block_y}) (relative coordinates:
    {block_relative_x}, {block_relative_y}) and place it into the orange
    bowl at normalized coordinates ({bowl_x}, {bowl_y})
    (relative coordinates: {bowl_relative_x}, {bowl_relative_y}).
    Provide the sequence of function calls as a JSON list of objects, where
    each object has a "function" key (the function name) and an "args" key
    (a list of arguments for the function).
    Also, include your reasoning before the JSON output.
    For example:
    Reasoning: To pick up the block, I will first move the arm to a high
    position above the block, open the gripper, move down to the block,
    close the gripper, lift the arm, move to a high position above the bowl,
    move down to the bowl, open the gripper, and then lift the arm back to
    a high position.
"""
```

 
 

The following shows a possible output of the model based on the prompt and
the mock robot API. The output includes the model thinking process and the
tasks that it planned out as a result. It also shows the output of the robot
function calls that the model sequenced together.

 

```
Reasoning: To perform the pick and place operation, I will first move the
robotic arm to a high position above the blue block to ensure clearance. Then, I
will open the gripper in preparation for grasping. Next, I will lower the arm to
the surface level to position the gripper around the block and close the gripper
to secure it. After picking up the block, I will lift the arm to clear the
surface and move towards the orange bowl, maintaining a high position to avoid
collisions. Once positioned above the bowl, I will lower the arm to place the
block inside. I will then open the gripper to release the block. Finally, I will
lift the arm out of the bowl and call `returnToOrigin` to reset the robot's
state.

[
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      false
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      true
    ]
  },
  {
    "function": "returnToOrigin",
    "args": []
  }
]


Executing Function Calls:
moving to coordinates: 163, 427, 15
Opening gripper
moving to coordinates: 163, 427, 5
Closing gripper
moving to coordinates: 163, 427, 15
moving to coordinates: -247, 90, 15
moving to coordinates: -247, 90, 5
Opening gripper
moving to coordinates: -247, 90, 15
Returning to origin pose
```

 

### Code execution

Gemini Robotics-ER 1.5 can suggest and execute Python code to perform tasks that
require dynamic actions, such as zooming into an image region for better detail.

This example demonstrates how the model can suggest using the
 code execution tool to "zoom in" on a specific
area of an image, which it then carries out to answer the user's question.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client(api_key=YOUR_API_KEY)

MODEL_ID = "gemini-robotics-er-1.5-preview"

# Load your image and set up your prompt
with open('path/to/image-of-object.jpg', 'rb') as f:
    image_bytes = f.read()
prompt = """
          What is the air quality reading? Using the code execution feature,
          zoom in on the image to take a closer look.
        """

response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/jpeg',
        ),
        prompt
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

The following shows a possible model response for this task. The output shows
the model generating code for zooming into the image to read the screen better.
It also shows the thought process around the adjustments it made to the
cropping. For the complete sample output, see the Robotics
cookbook .

 

```
# Detect relevant objects and output in json format
# [{"box_2d": [379, 407, 481, 553], "label": "display"}]
from PIL import Image

# Open the image
img = Image.open("input_file_0.jpeg")

# Define the coordinates for cropping (normalized to 0-1000)
# Based on visual inspection, the display is roughly centered around [430, 430] to [500, 500] in a 1000x1000 scale.
# Let's try to zoom in on the display area.
# Bounding box for display: [ymin, xmin, ymax, xmax] in pixels.
# From visual inspection of the original image (size 90489 bytes, resolution
unknown but let's assume a standard aspect ratio and roughly 1000x1000 for
relative positioning),
# the display is located at approximately y=380-480, x=410-540.
# Let's define a larger area around it for context.
# Let's say we want to crop from y=350 to y=550 and x=350 to x=650.

...

thought
The crop was too tight and only captured part of the display. Let's adjust the bounding box.
The previous crop was `img.crop((400, 350, 600, 500))`. This means xmin=400, ymin=350, xmax=600, ymax=500.
The display is centered around x=480, y=430.
The previous crop captured the left side of the display.
Let's expand the x range to the right and y range to cover the whole circle.
New bounding box estimate: ymin=370, xmin=400, ymax=500, xmax=600.
Let's try again with a wider crop.
# Detect relevant objects and output in json format
# [{"box_2d": [379, 407, 481, 553], "label": "display"}]
from PIL import Image

# Open the image
img = Image.open("input_file_0.jpeg")

# Define the coordinates for cropping (normalized to 0-1000)
‚Ä¶

The reading of 14 falls into the "Moderate" category.
The prompt asks for the "air quality reading", which can be interpreted as
either the numerical value or the categorical description. Given the numerical
value is explicitly displayed, it's the primary reading.

Let's provide the numerical value as the reading.
The reading is 014.
Based on the display on the air purifier, the air quality reading is **014**.
```

 

## Best practices

To optimize the performance and accuracy of your robotics applications, it's
crucial to understand how to interact with the Gemini model effectively. This
section outlines best practices and key strategies for crafting prompts,
handling visual data, and structuring tasks to achieve the most reliable
results.

- 

Use clear and simple language.

 

 Embrace natural language : The Gemini model is designed to comprehend
natural, conversational language. Structure your prompts in a way that is
semantically clear and mirrors how a person would naturally give
instructions.

- 

 Use everyday terminology : Opt for common, everyday language over
technical or specialized jargon. If the model is not responding as
expected to a particular term, try rephrasing it with a more common
synonym.

 
- 

Optimize the visual input.

 

 Zoom in for detail : When dealing with objects that are small or
difficult to discern in a wider shot, use a bounding box function to
isolate the object of interest. You can then crop the image to this
selection and send the new, focused image to the model for a more
detailed analysis.

- 

 Experiment with lighting and color : The model's perception can be
affected by challenging lighting conditions and poor color contrast.

 
- 

Break down complex problems into smaller steps. By addressing each smaller
step individually, you can guide the model to a more precise and successful
outcome.

- 

Improve accuracy through consensus. For tasks that require a high degree of
precision, you can query the model multiple times with the same prompt. By
averaging the returned results, you can arrive at a "consensus" that is
often more accurate and reliable.

## Limitations

Consider the following limitations when developing with Gemini Robotics-ER 1.5:

- Preview status: The model is currently in Preview . APIs and
capabilities may change, and it may not be suitable for production-critical
applications without thorough testing.

- Latency: Complex queries, high-resolution inputs, or extensive
`thinking_budget` can lead to increased processing times.

- Hallucinations: Like all large language models, Gemini Robotics-ER 1.5
can occasionally "hallucinate" or provide incorrect information, especially
for ambiguous prompts or out-of-distribution inputs.

- Dependence on prompt quality: The quality of the model's output is
highly dependent on the clarity and specificity of the input prompt. Vague
or poorly structured prompts can lead to suboptimal results.

- Computational cost: Running the model, especially with video inputs or
high `thinking_budget`, consumes computational resources and incurs costs.
See the Thinking page for more details.

- Input types: See the following topics for details on limitations for each mode.

 Image inputs 

- Video inputs 

- Audio inputs 

 

## Privacy Notice

You acknowledge that the models referenced in this document (the "Robotics
Models") leverage video and audio data in order to operate and move your
hardware in accordance with your instructions. You therefore may operate the
Robotics Models such that data from identifiable persons, such as voice,
imagery, and likeness data ("Personal Data"), will be collected by the Robotics
Models. If you elect to operate the Robotics Models in a manner that collects
Personal Data, you agree that you will not permit any identifiable persons to
interact with, or be present in the area surrounding, the Robotics Models,
unless and until such identifiable persons have been sufficiently notified of
and consented to the fact that their Personal Data may be provided to and used
by Google as outlined in the Gemini API Additional Terms of Service found at
 https://ai.google.dev/gemini-api/terms 
(the "Terms"), including in accordance
with the section entitled "How Google Uses Your Data". You will ensure that such
notice permits the collection and use of Personal Data as outlined in the Terms,
and you will use commercially reasonable efforts to minimize the collection and
distribution of Personal Data by using techniques such as face blurring and
operating the Robotics Models in areas not containing identifiable persons to
the extent practicable.

## Pricing

For detailed information on pricing and available regions, refer to the
 pricing page.

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-robotics-er-1.5-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-robotics-er-1.5-preview`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 

## Next steps

- Explore other capabilities and continue experimenting with different prompts
and inputs to discover more applications for Gemini Robotics-ER 1.5.
See the Robotics cookbook 
for more examples.

- Learn about how Gemini Robotics models were built with safety in mind, visit
the Google DeepMind robotics safety
page .

- Read about the latest updates on Gemini Robotics models on the
 Gemini Robotics landing page .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### Gemini models &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/models#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini models  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini models 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 

OUR MOST INTELLIGENT MODEL

 

## Gemini 3 Pro

 

 The best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 3 Pro Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-3-pro-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, Image, Video, Audio, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Preview: gemini-3-pro-preview`

 

 
 
 
 
 calendar_month Latest update 
 November 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

 
 
 

OUR ADVANCED THINKING MODEL

 

## Gemini 2.5 Pro

 

 Our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Pro

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, text, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Stable: gemini-2.5-pro`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Pro TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-pro-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

FAST AND INTELLIGENT

 

## Gemini 2.5 Flash

 

 Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL Context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-image` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Images and text

 
 
 

 Output 

 

Images and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

65,536

 
 
 

 Output token limit 

 

32,768

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-image`

 - Preview: `gemini-2.5-flash-image-preview`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 cognition_2 Knowledge cutoff 
 June 2025 
 
 
 
 
 

### Gemini 2.5 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, text

 
 
 

 Output 

 

Audio and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

131,072

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-native-audio-preview-09-2025`

 - Preview: `gemini-live-2.5-flash-preview`

 

 gemini-live-2.5-flash-preview will be deprecated on December 09, 2025 

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-flash-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

ULTRA FAST

 

## Gemini 2.5 Flash-Lite

 

 Our fastest flash model optimized for cost-efficiency and high throughput.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash-Lite

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-lite`

 

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash-Lite Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-lite-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

## Previous Gemini models

 
 
 

OUR SECOND GENERATION WORKHORSE MODEL

 

## Gemini 2.0 Flash

 

 Our second generation workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

 Gemini 2.0 Flash delivers next-gen features and improved capabilities,
 including superior speed, native tool use, and a 1M token
 context window.
 

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.0 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Experimental 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash`

 - Stable: `gemini-2.0-flash-001`

 - Experimental: `gemini-2.0-flash-exp`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-preview-image-generation` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text and images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

32,768

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-preview-image-generation`

 

 gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa 

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.0-flash-live-001`
 

 gemini-2.0-flash-live-001 will be deprecated on December 09, 2025 

 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, and text

 
 
 

 Output 

 

Text, and audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-live-001`

 

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 
 
 

 
 
 

OUR SECOND GENERATION FAST MODEL

 

## Gemini 2.0 Flash-Lite

 

 Our second generation small workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

 

 Try in Google AI Studio 

 

#### Model details

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash-lite`

 - Stable: `gemini-2.0-flash-lite-001`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 

## Model version name patterns

Gemini models are available in either stable , preview , latest , or
 experimental versions.

### Stable

Points to a specific stable model. Stable models usually don't change. Most
production apps should use a specific stable model.

For example: `gemini-2.5-flash`.

### Preview

Points to a preview model which may be used for production. Preview models will
typically have billing enabled, might come with more restrictive rate limits and
will be deprecated with at least 2 weeks notice.

For example: `gemini-2.5-flash-preview-09-2025`.

### Latest

Points to the latest release for a specific model variation. This can be a
stable, preview or experimental release. This alias will get hot-swapped with
every new release of a specific model variation. A 2-week notice will
be provided through email before the version behind latest is changed.

For example: `gemini-flash-latest`.

### Experimental

Points to an experimental model which will typically be not be suitable for
production use and come with more restrictive rate limits. We release
experimental models to gather feedback and get our latest updates into the hands
of developers quickly.

Experimental models are not stable and availability of model endpoints is
subject to change.

## Model deprecations

For information about model deprecations, visit the Gemini deprecations page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini Developer API pricing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/pricing

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Developer API pricing  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 

# 
 Gemini Developer API pricing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Start building free of charge with generous limits, then scale up with pay-as-you-go
pricing for your production ready applications.

 
 
 
 

### Free

 

For developers and small projects getting started with the Gemini API.

 

 - check_circle Limited access to certain models

 - check_circle Free input & output tokens

 - check_circle Google AI Studio access

 - check_circle Content used to improve our products * 

 

 Get started for Free 
 
 
 

### Paid

 

For production applications that require higher volumes and advanced features.

 

 - check_circle Higher rate limits for production deployments

 - check_circle Access to Context caching

 - check_circle Batch API (50% cost reduction)

 - check_circle Access to Google's most advanced models

 - check_circle Content *not* used to improve our products * 

 

 Upgrade to Paid 
 
 
 

### Enterprise

 

For large-scale deployments with custom needs for security, support, and compliance, powered by Vertex AI .

 

 - check_circle All features in Paid, plus optional access to:

 - check_circle Dedicated support channels

 - check_circle Advanced security & compliance

 - check_circle Provisioned throughput

 - check_circle Volume-based discounts (based on usage)

 - check_circle ML ops, model garden and more

 

 Contact Sales 
 
 
 

 
 
 

## Gemini 3 Pro Preview

 `gemini-3-pro-preview` 
 
 

 Try it in Google AI Studio 
 

 

The best model in the world for multimodal understanding, and our most powerful
agentic and vibe-coding model yet.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $2.00, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $12.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.20, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then (Coming soon) $14 / 1,000 search queries 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.00, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $6.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.20, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then (Coming soon) $14 / 1,000 search queries 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Pro

 `gemini-2.5-pro` 
 
 

 Try it in Google AI Studio 
 

 

Our state-of-the-art multipurpose model, which excels at coding and complex
reasoning tasks.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $1.25, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $10.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.125, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 10,000 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.625, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $5.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.125, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash

 `gemini-2.5-flash` 
 
 

 Try it in Google AI Studio 
 

 

Our first hybrid reasoning model which supports a 1M token context window and
has thinking budgets.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image / video)
$0.50 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $1.25 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash Preview

 `gemini-2.5-flash-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

The latest model based on the 2.5 Flash model. 2.5 Flash Preview is best for
large scale processing, low-latency,
high volume tasks that require thinking, and agentic use cases.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image / video)
$0.50 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $1.25 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash-Lite

 `gemini-2.5-flash-lite` 
 
 

 Try it in Google AI Studio 
 

 

Our smallest and most cost effective model, built for at scale usage.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Free of charge 
 $0.10 (text / image / video)
$0.30 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash RPD) 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Not available 
 $0.05 (text / image / video)
$0.15 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash-Lite Preview

 `gemini-2.5-flash-lite-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

The latest model based on Gemini 2.5 Flash lite optimized for cost-efficiency,
high throughput and high quality.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Free of charge 
 $0.10 (text / image / video)
$0.30 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash RPD) 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Not available 
 $0.05 (text / image / video)
$0.15 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash Native Audio (Live API)

 `gemini-2.5-flash-native-audio-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

Our Live API native audio models optimized for higher
quality audio outputs with better pacing, voice naturalness, verbosity, and
mood.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.50 (text)
$3.00 (audio / video) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.00 (text)
$12.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

The Live API also includes half-cascade audio generation models:

- `gemini-live-2.5-flash-preview`: Same price as the native audio model.

- `gemini-2.0-flash-live-001`: Input $0.35 (text), $2.10 (audio / image / video),
Output: $1.50 (text), $8.50 (audio)

These models will be deprecated soon.

 
 
 

## Gemini 2.5 Flash Image üçå

 `gemini-2.5-flash-image` 
 
 

 Try it in Google AI Studio 
 

 

Our native image generation model, optimized for speed, flexibility, and
contextual understanding. Text input and output is priced the same as
 2.5 Flash .

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.30 (text / image) 
 
 
 Output price 
 Not available 
 $0.039 per image* 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image) 
 
 
 Output price 
 Not available 
 $0.0195 per image* 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

[*] Image output is priced at $30 per 1,000,000 tokens. Output images up to
1024x1024px consume 1290 tokens and are equivalent to $0.039 per image.

 
 
 

## Gemini 2.5 Flash Preview TTS

 `gemini-2.5-flash-preview-tts` 
 
 

 Try it in Google AI Studio 
 

 

Our 2.5 Flash text-to-speech audio model optimized for price-performant,
low-latency, controllable speech generation.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.50 (text) 
 
 
 Output price 
 Free of charge 
 $10.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.25 (text) 
 
 
 Output price 
 Not available 
 $5.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Pro Preview TTS

 `gemini-2.5-pro-preview-tts` 
 
 

 Try it in Google AI Studio 
 

 

Our 2.5 Pro text-to-speech audio model optimized for powerful, low-latency
speech generation for more natural outputs and easier to steer prompts.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.00 (text) 
 
 
 Output price 
 Not available 
 $20.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.50 (text) 
 
 
 Output price 
 Not available 
 $10.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.0 Flash

 `gemini-2.0-flash` 
 
 

 Try it in Google AI Studio 
 

 

Our most balanced multimodal model with great performance across all tasks, with
a 1 million token context window, and built for the era of Agents.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.10 (text / image / video)
$0.70 (audio) 
 
 
 Output price 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Free of charge 
 $0.025 / 1,000,000 tokens (text/image/video)
$0.175 / 1,000,000 tokens (audio) 
 
 
 Context caching (storage) 
 Not available 
 $1.00 / 1,000,000 tokens per hour 
 
 
 Image generation pricing 
 Free of charge 
 $0.039 per image* 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.05 (text / image / video)
$0.35 (audio) 
 
 
 Output price 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.025 / 1,000,000 tokens (text/image/video)
$0.175 / 1,000,000 tokens (audio) 
 
 
 Context caching (storage) 
 Not available 
 $1.00 / 1,000,000 tokens per hour 
 
 
 Image generation pricing 
 Not available 
 $0.0195 per image* 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

[*] Image output is priced at $30 per 1,000,000 tokens. Output images up to
1024x1024px consume 1290 tokens and are equivalent to $0.039 per image.

 
 
 

## Gemini 2.0 Flash-Lite

 `gemini-2.0-flash-lite` 
 
 

 Try it in Google AI Studio 
 

 

Our smallest and most cost effective model, built for at scale usage.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.075 
 
 
 Output price 
 Free of charge 
 $0.30 
 
 
 Context caching price 
 Not available 
 Not available 
 
 
 Context caching (storage) 
 Not available 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.0375 
 
 
 Output price 
 Not available 
 $0.15 
 
 
 Context caching price 
 Not available 
 Not available 
 
 
 Context caching (storage) 
 Not available 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Imagen 4

 `imagen-4.0-generate-001`, `imagen-4.0-ultra-generate-001`, `imagen-4.0-fast-generate-001` 
 
 

 Try it in Google AI Studio 
 

 

Our latest image generation model, with significantly better text rendering and
better overall image quality.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per Image in USD 
 
 
 
 
 Imagen 4 Fast image price 
 Not available 
 $0.02 
 
 
 Imagen 4 Standard image price 
 Not available 
 $0.04 
 
 
 Imagen 4 Ultra image price 
 Not available 
 $0.06 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Imagen 3

 `imagen-3.0-generate-002` 
 
 

 Try it in Google AI Studio 
 

 

Our state-of-the-art image generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per Image in USD 
 
 
 
 
 Image price 
 Not available 
 $0.03 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 3.1

 `veo-3.1-generate-preview`, `veo-3.1-fast-generate-preview` 
 
 

 Try Veo 3.1 
 

 

Our latest video generation model, available to developers on the
paid tier of the Gemini API.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Veo 3.1 Standard video with audio price (default) 
 Not available 
 $0.40 
 
 
 Veo 3.1 Fast video with audio price (default) 
 Not available 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 3

 `veo-3.0-generate-001`, `veo-3.0-fast-generate-001` 
 
 

 Try Veo 3 
 

 

Our stable video generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Veo 3 Standard video with audio price (default) 
 Not available 
 $0.40 
 
 
 Veo 3 Fast video with audio price (default) 
 Not available 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 2

 `veo-2.0-generate-001` 
 
 

 Try the API 
 

 

Our state-of-the-art video generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Video price 
 Not available 
 $0.35 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Gemini Embedding

 `gemini-embedding-001` 
 
 

 Try the API 
 

 

Our newest embeddings model, more stable and with higher rate limits than
previous versions, available to developers on the free and paid tiers of the Gemini API.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.075 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini Robotics-ER 1.5 Preview

 `gemini-robotics-er-1.5-preview` 
 
 

 Try it in Google AI Studio 
 

 

Gemini Robotics-ER, short for Gemini Robotics-Embodied Reasoning, is a thinking
model that enhances robots' abilities to understand and interact with the
physical world.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 Not available 
 
 
 Output price (including thinking tokens) 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Computer Use Preview

 `gemini-2.5-computer-use-preview-10-2025` 
 
 

Our Computer Use model optimized for building browser control agents that
automate tasks.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.25, prompts 200k token 
 
 
 Output price 
 Not available 
 $10.00, prompts 200k 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 

## Gemma 3

 

 Try Gemma 3 
 

 

Our lightweight, state-of the art, open model built from the same technology
that powers our Gemini models.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 Not available 
 
 
 Output price 
 Free of charge 
 Not available 
 
 
 Context caching price 
 Free of charge 
 Not available 
 
 
 Context caching (storage) 
 Free of charge 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 

## Gemma 3n

 

 Try Gemma 3n 
 

 

Our open model built for efficient performance on everyday devices like mobile
phones, laptops, and tablets.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 Not available 
 
 
 Output price 
 Free of charge 
 Not available 
 
 
 Context caching price 
 Free of charge 
 Not available 
 
 
 Context caching (storage) 
 Free of charge 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

## Pricing for Tools

Tools are priced at their own rates, applied to the model using them.
Check the Models page for which tools are available
to each model.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Google Search 
 500 RPD free (limit shared for Flash and Flash-Lite).
Not available for Pro. 
 1,500 RPD free (limit shared for Flash and Flash-Lite).
Then $35 / 1,000 grounded prompts
 
 
 
 Google Maps 
 500 RPD
Not available for Pro. 
 1,500 RPD free (limit shared for Flash and Flash-Lite)
10,000 RPD free for Pro.
Then $25 / 1,000 grounded prompts 
 
 
 Code execution 
 Free of charge 
 Free of charge 
 
 
 URL context 
 Free of charge 
 Charged as input tokens per model pricing. 
 
 
 Computer use 
 Not available 
 See Gemini 2.5 Computer Use Preview pricing table. 
 
 
 File search 
 Free of charge 
 Charged for embeddings at $0.15 / 1M tokens.
Retrieved document tokens charged as regular tokens per model pricing. 
 
 
 

[*] Google AI Studio usage is free of charge in all available regions .
See Billing FAQs for details.

[**] Prices may differ from the prices listed here and the prices offered on
Vertex AI. For Vertex prices, see the Vertex AI pricing page .

[***] If you are using dynamic retrieval to
optimize costs, only requests that contain at least one grounding support URL
from the web in their response are charged for Grounding with Google Search.
Costs for Gemini always apply. Rate limits are subject to change.

 
 

 
 

 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Gemini 3 Developer Guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/gemini-3#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini 3 Developer Guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini 3 Developer Guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini 3 is our most intelligent model family to date, built on a foundation of state-of-the-art reasoning. It is designed to bring any idea to life by mastering agentic workflows, autonomous coding, and complex multimodal tasks. This guide covers key features of the Gemini 3 model family and how to get the most out of it.

 
 
 
 

Gemini 3 Pro uses dynamic thinking by default to reason through prompts. For faster, lower-latency responses when complex reasoning isn't required, you can constrain the model's thinking level to `low`.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Find the race condition in this multi-threaded C++ snippet: [code here]"}]
    }]
  }'
```

 
 

## Explore

 

Explore our collection of Gemini 3 apps to see how the model handles advanced reasoning, autonomous coding, and complex multimodal tasks.

## Meet Gemini 3

Gemini 3 Pro is the first model in the new series. `gemini-3-pro-preview` is best for your complex tasks that require broad world knowledge and advanced reasoning across modalities.

 
 
 
 Model ID 
 Context Window (In / Out) 
 Knowledge Cutoff 
 Pricing (Input / Output)* 
 
 

 
 
 gemini-3-pro-preview 
 1M / 64k 
 Jan 2025 
 $2 / $12 (<200k tokens) 
 $4 / $18 (>200k tokens) 
 
 
 

 * Pricing is per 1 million tokens. Prices listed are for standard text; multimodal input rates may vary. 

For detailed rate limits, batch pricing, and additional information, see the models page .

## New API features in Gemini 3

Gemini 3 introduces new parameters designed to give developers more control over latency, cost, and multimodal fidelity.

### Thinking level

The `thinking_level` parameter controls the maximum depth of the model's internal reasoning process before it produces a response. Gemini 3 treats these levels as relative allowances for thinking rather than strict token guarantees. If `thinking_level` is not specified, Gemini 3 Pro will default to `high`.

- `low`: Minimizes latency and cost. Best for simple instruction following, chat, or high-throughput applications

- `medium`: (Coming soon), not supported at launch 

- `high` (Default): Maximizes reasoning depth. The model may take significantly longer to reach a first token, but the output will be more carefully reasoned.

### Media resolution

Gemini 3 introduces granular control over multimodal vision processing via the `media_resolution` parameter. Higher resolutions improve the model's ability to read fine text or identify small details, but increase token usage and latency. The `media_resolution` parameter determines the maximum number of tokens allocated per input image or video frame. 

You can now set the resolution to `media_resolution_low`, `media_resolution_medium`, or `media_resolution_high` per individual media part or globally (via `generation_config`). If unspecified, the model uses optimal defaults based on the media type. 

 Recommended settings 

 
 
 
 Media Type 
 Recommended Setting 
 Max Tokens 
 Usage Guidance 
 
 

 
 
 Images 
 `media_resolution_high` 
 1120 
 Recommended for most image analysis tasks to ensure maximum quality. 
 
 
 PDFs 
 `media_resolution_medium` 
 560 
 Optimal for document understanding; quality typically saturates at `medium`. Increasing to `high` rarely improves OCR results for standard documents. 
 
 
 Video (General) 
 `media_resolution_low` (or `media_resolution_medium`) 
 70 (per frame) 
 Note: For video, `low` and `medium` settings are treated identically (70 tokens) to optimize context usage. This is sufficient for most action recognition and description tasks. 
 
 
 Video (Text-heavy) 
 `media_resolution_high` 
 280 (per frame) 
 Required only when the use case involves reading dense text (OCR) or small details within video frames. 
 
 
 
 
 

### Python

 

```
from google import genai
from google.genai import types
import base64

# The media_resolution parameter is currently only available in the v1alpha API version.
client = genai.Client(http_options={'api_version': 'v1alpha'})

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents=[
        types.Content(
            parts=[
                types.Part(text="What is in this image?"),
                types.Part(
                    inline_data=types.Blob(
                        mime_type="image/jpeg",
                        data=base64.b64decode("..."),
                    ),
                    media_resolution={"level": "media_resolution_high"}
                )
            ]
        )
    ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// The media_resolution parameter is currently only available in the v1alpha API version.
const ai = new GoogleGenAI({ apiVersion: "v1alpha" });

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: [
      {
        parts: [
          { text: "What is in this image?" },
          {
            inlineData: {
              mimeType: "image/jpeg",
              data: "...",
            },
            mediaResolution: {
              level: "media_resolution_high"
            }
          }
        ]
      }
    ]
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [
        { "text": "What is in this image?" },
        {
          "inlineData": {
            "mimeType": "image/jpeg",
            "data": "..."
          },
          "mediaResolution": {
            "level": "media_resolution_high"
          }
        }
      ]
    }]
  }'
```

 
 

### Temperature

For Gemini 3, we strongly recommend keeping the temperature parameter at its default value of `1.0`.

While previous models often benefited from tuning temperature to control creativity versus determinism, Gemini 3's reasoning capabilities are optimized for the default setting. Changing the temperature (setting it below 1.0) may lead to unexpected behavior, such as looping or degraded performance, particularly in complex mathematical or reasoning tasks.

### Thought signatures

Gemini 3 uses Thought signatures to maintain reasoning context across API calls. These signatures are encrypted representations of the model's internal thought process. To ensure the model maintains its reasoning capabilities you must return these signatures back to the model in your request exactly as they were received:

- Function Calling (Strict): The API enforces strict validation on the "Current Turn". Missing signatures will result in a 400 error.

- Text/Chat: Validation is not strictly enforced, but omitting signatures will degrade the model's reasoning and answer quality.

#### Function calling (strict validation)

When Gemini generates a `functionCall`, it relies on the `thoughtSignature` to process the tool's output correctly in the next turn. The "Current Turn" includes all Model (`functionCall`) and User (`functionResponse`) steps that occurred since the last standard User `text` message.

- Single Function Call: The `functionCall` part contains a signature. You must return it. 

- Parallel Function Calls: Only the first `functionCall` part in the list will contain the signature. You must return the parts in the exact order received.

- Multi-Step (Sequential): If the model calls a tool, receives a result, and calls another tool (within the same turn), both function calls have signatures. You must return all accumulated signatures in the history.

#### Text and streaming

For standard chat or text generation, the presence of a signature is not guaranteed.

- Non-Streaming : The final content part of the response may contain a `thoughtSignature`, though it is not always present. If one is returned, you should send it back to maintain best performance. 

- Streaming : If a signature is generated, it may arrive in a final chunk that contains an empty text part. Ensure your stream parser checks for signatures even if the text field is empty.

#### Code examples

 
 
 

#### Multi-step Function Calling (Sequential)

 

The user asks a question requiring two separate steps (Check Flight -> Book Taxi) in one turn. 

 Step 1: Model calls Flight Tool. 

 The model returns a signature ` `

 

```
// Model Response (Turn 1, Step 1)
  {
    "role": "model",
    "parts": [
      {
        "functionCall": { "name": "check_flight", "args": {...} },
        "thoughtSignature": "<Sig_A>" // SAVE THIS
      }
    ]
  }
```

 
 

 Step 2: User sends Flight Result 

 We must send back ` ` to keep the model's train of thought.

 

```
// User Request (Turn 1, Step 2)
[
  { "role": "user", "parts": [{ "text": "Check flight AA100..." }] },
  { 
    "role": "model", 
    "parts": [
      { 
        "functionCall": { "name": "check_flight", "args": {...} }, 
        "thoughtSignature": "<Sig_A>" // REQUIRED
      } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": { "name": "check_flight", "response": {...} } }] }
]
```

 
 

 Step 3: Model calls Taxi Tool 

 The model remembers the flight delay via ` ` and now decides to book a taxi. It generates a new signature ` `.

 

```
// Model Response (Turn 1, Step 3)
{
  "role": "model",
  "parts": [
    {
      "functionCall": { "name": "book_taxi", "args": {...} },
      "thoughtSignature": "<Sig_B>" // SAVE THIS
    }
  ]
}
```

 
 

 Step 4: User sends Taxi Result 

 To complete the turn, you must send back the entire chain: ` ` AND ` `.

 

```
// User Request (Turn 1, Step 4)
[
  // ... previous history ...
  { 
    "role": "model", 
    "parts": [
       { "functionCall": { "name": "check_flight", ... }, "thoughtSignature": "<Sig_A>" } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": {...} }] },
  { 
    "role": "model", 
    "parts": [
       { "functionCall": { "name": "book_taxi", ... }, "thoughtSignature": "<Sig_B>" } 
    ]
  },
  { "role": "user", "parts": [{ "functionResponse": {...} }] }
]
```

 
 
 

 
 
 

#### Parallel Function Calling

 

The user asks: "Check the weather in Paris and London." The model returns two function calls in one response. 

 

```
// User Request (Sending Parallel Results)
[
  {
    "role": "user",
    "parts": [
      { "text": "Check the weather in Paris and London." }
    ]
  },
  {
    "role": "model",
    "parts": [
      // 1. First Function Call has the signature
      {
        "functionCall": { "name": "check_weather", "args": { "city": "Paris" } },
        "thoughtSignature": "<Signature_A>" 
      },
      // 2. Subsequent parallel calls DO NOT have signatures
      {
        "functionCall": { "name": "check_weather", "args": { "city": "London" } }
      } 
    ]
  },
  {
    "role": "user",
    "parts": [
      // 3. Function Responses are grouped together in the next block
      {
        "functionResponse": { "name": "check_weather", "response": { "temp": "15C" } }
      },
      {
        "functionResponse": { "name": "check_weather", "response": { "temp": "12C" } }
      }
    ]
  }
]
```

 
 
 

 
 
 

#### Text/In-Context Reasoning (No Validation)

 The user asks a question that requires in-context reasoning without external tools. While not strictly validated, including the signature helps the model maintain the reasoning chain for follow-up questions.

 

```
// User Request (Follow-up question)
[
  { 
    "role": "user", 
    "parts": [{ "text": "What are the risks of this investment?" }] 
  },
  { 
    "role": "model", 
    "parts": [
      {
        "text": "I need to calculate the risk step-by-step. First, I'll look at volatility...",
        "thoughtSignature": "<Signature_C>" // Recommended to include
      }
    ]
  },
  { 
    "role": "user", 
    "parts": [{ "text": "Summarize that in one sentence." }] 
  }
]
```

 
 
 

#### Migrating from other models

If you are transferring a conversation trace from another model (e.g., Gemini 2.5) or injecting a custom function call that was not generated by Gemini 3, you will not have a valid signature.

To bypass strict validation in these specific scenarios, populate the field with this specific dummy string: 

```
"thoughtSignature": "context_engineering_is_the_way_to_go"
```



### Structured Outputs with tools

Gemini 3 allows you to combine Structured Outputs with built-in tools, including Grounding with Google Search , URL Context , and Code Execution .

 
 

### Python

 

```
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from typing import List

class MatchResult(BaseModel):
    winner: str = Field(description="The name of the winner.")
    final_match_score: str = Field(description="The final match score.")
    scorers: List[str] = Field(description="The name of the scorer.")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Search for all details for the latest Euro.",
    config={
        "tools": [
            {"google_search": {}},
            {"url_context": {}}
        ],
        "response_mime_type": "application/json",
        "response_json_schema": MatchResult.model_json_schema(),
    },  
)

result = MatchResult.model_validate_json(response.text)
print(result)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});

const matchSchema = z.object({
  winner: z.string().describe("The name of the winner."),
  final_match_score: z.string().describe("The final score."),
  scorers: z.array(z.string()).describe("The name of the scorer.")
});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Search for all details for the latest Euro.",
    config: {
      tools: [
        { googleSearch: {} },
        { urlContext: {} }
      ],
      responseMimeType: "application/json",
      responseJsonSchema: zodToJsonSchema(matchSchema),
    },
  });

  const match = matchSchema.parse(JSON.parse(response.text));
  console.log(match);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Search for all details for the latest Euro."}]
    }],
    "tools": [
      {"googleSearch": {}},
      {"urlContext": {}}
    ],
    "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
            "type": "object",
            "properties": {
                "winner": {"type": "string", "description": "The name of the winner."},
                "final_match_score": {"type": "string", "description": "The final score."},
                "scorers": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "The name of the scorer."
                }
            },
            "required": ["winner", "final_match_score", "scorers"]
        }
    }
  }'
```

 
 

## Migrating from Gemini 2.5

Gemini 3 is our most capable model family to date and offers a stepwise improvement over Gemini 2.5 Pro. When migrating, consider the following:

- Thinking: If you were previously using complex prompt engineering (like Chain-of-thought) to force Gemini 2.5 to reason, try Gemini 3 with `thinking_level: "high"` and simplified prompts. 

- Temperature settings: If your existing code explicitly sets temperature (especially to low values for deterministic outputs), we recommend removing this parameter and using the Gemini 3 default of 1.0 to avoid potential looping issues or performance degradation on complex tasks. 

- PDF & document understanding: Default OCR resolution for PDFs has changed. If you relied on specific behavior for dense document parsing, test the new `media_resolution_high` setting to ensure continued accuracy. 

- Token consumption: Migrating to Gemini 3 Pro defaults may increase token usage for PDFs but decrease token usage for video. If requests now exceed the context window due to higher default resolutions, we recommend explicitly reducing the media resolution. 

- Image segmentation: Image segmentation capabilities (returning pixel-level masks for objects) are not supported in Gemini 3 Pro. For workloads requiring native image segmentation, we recommend continuing to utilize Gemini 2.5 Flash with thinking turned off or Gemini Robotics-ER 1.5 .

## OpenAI compatibility

For users utilizing the OpenAI compatibility layer, standard parameters are automatically mapped to Gemini equivalents:

- `reasoning_effort` (OAI) maps to `thinking_level` (Gemini). Note that `reasoning_effort` medium maps to `thinking_level` high.

## Prompting best practices

Gemini 3 is a reasoning model, which changes how you should prompt.

- Precise instructions: Be concise in your input prompts. Gemini 3 responds best to direct, clear instructions. It may over-analyze verbose or overly complex prompt engineering techniques used for older models. 

- Output verbosity: By default, Gemini 3 is less verbose and prefers providing direct, efficient answers. If your use case requires a more conversational or "chatty" persona, you must explicitly steer the model in the prompt (e.g., "Explain this as a friendly, talkative assistant"). 

- Context management: When working with large datasets (e.g., entire books, codebases, or long videos), place your specific instructions or questions at the end of the prompt, after the data context. Anchor the model's reasoning to the provided data by starting your question with a phrase like, "Based on the information above...".

Learn more about prompt design strategies in the prompt engineering guide . 

## FAQ

- 

 What is the knowledge cutoff for Gemini 3 Pro? Gemini 3 has a knowledge cutoff of January 2025. For more recent information, use the Search Grounding tool. 

- 

 What are the context window limits? Gemini 3 Pro supports a 1 million token input context window and up to 64k tokens of output. 

- 

 Is there a free tier for Gemini 3 Pro? You can try the model for free in Google AI Studio, but currently, there is no free tier available for `gemini-3-pro-preview` in the Gemini API. 

- 

 Will my old `thinking_budget` code still work? Yes, `thinking_budget` is still supported for backward compatibility, but we recommend migrating to `thinking_level` for more predictable performance. Do not use both in the same request.

- 

 Does Gemini 3 support the Batch API? Yes, Gemini 3 supports the Batch API. 

- 

 Is Context Caching supported? Yes, Context Caching is supported for Gemini 3. The minimum token count required to initiate caching is 2,048 tokens.

- 

 Which tools are supported in Gemini 3? Gemini 3 supports Google Search , File Search , Code Execution , and URL Context . It also supports standard Function Calling for your own custom tools. Please note that Google Maps and Computer Use are currently not supported.

## Next steps

- Get started with the Gemini 3 Cookbook 

- Check the dedicated Cookbook guide on thinking levels and how to migrate from thinking budget to thinking levels.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Rate limits &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/rate-limits

- 
 
 
 
 
 
 
 
 
 
 
 Rate limits  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Rate limits 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Rate limits regulate the number of requests you can make to the Gemini API
within a given timeframe. These limits help maintain fair usage, protect against
abuse, and help maintain system performance for all users.

 View your active rate limits in AI Studio 

## How rate limits work

Rate limits are usually measured across three dimensions:

- Requests per minute ( RPM )

- Tokens per minute (input) ( TPM )

- Requests per day ( RPD )

Your usage is evaluated against each limit, and exceeding any of them will
trigger a rate limit error. For example, if your RPM limit is 20, making 21
requests within a minute will result in an error, even if you haven't exceeded
your TPM or other limits.

Rate limits are applied per project, not per API key.

Requests per day ( RPD ) quotas reset at midnight Pacific time.

Limits vary depending on the specific model being used, and some limits only
apply to specific models. For example, Images per minute, or IPM, is only
calculated for models capable of generating images (Imagen 3), but is
conceptually similar to TPM. Other models might have a token per day limit (TPD).

Rate limits are more restricted for experimental and preview models.

## Usage tiers

Rate limits are tied to the project's usage tier. As your API usage and spending
increase, you'll have an option to upgrade to a higher tier with increased rate
limits.

The qualifications for Tiers 2 and 3 are based on the total cumulative spending
on Google Cloud services (including, but not limited to, the Gemini API) for the
billing account linked to your project.

 
 
 
 Tier 
 Qualifications 
 
 

 
 
 Free 
 Users in eligible countries 
 
 
 Tier 1 
 Billing account linked to the project 
 
 
 Tier 2 
 Total spend: > $250 and at least 30 days since successful payment 
 
 
 Tier 3 
 Total spend: > $1,000 and at least 30 days since successful payment 
 
 
 

When you request an upgrade, our automated abuse protection system performs
additional checks. While meeting the stated qualification criteria is generally
sufficient for approval, in rare cases an upgrade request may be denied based on
other factors identified during the review process.

This system helps maintain the security and integrity of the Gemini API platform
for all users.

## Standard API rate limits

The following table lists the rate limits for all standard Gemini API calls.

 
 
 
 

### Free Tier

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 
 
 
 
 Text-out models 
 
 
 Gemini 2.5 Pro 
 2 
 125,000 
 50 
 
 
 Gemini 2.5 Flash 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash Preview 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash-Lite 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.0 Flash 
 15 
 1,000,000 
 200 
 
 
 Gemini 2.0 Flash-Lite 
 30 
 1,000,000 
 200 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 500,000 
 * 
 
 
 Gemini 2.0 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 3 
 10,000 
 15 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 10 
 200,000 
 100 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 
 
 Gemini Embedding 
 100 
 30,000 
 1,000 
 
 
 Gemini Robotics-ER 1.5 Preview 
 10 
 250,000 
 250 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 15 
 250,000 
 50 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 15 
 250,000 
 50 
 
 
 
 
 
 

### Tier 1

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 50 
 1,000,000 
 1,000 
 50,000,000 
 
 
 Gemini 2.5 Pro 
 150 
 2,000,000 
 10,000 
 5,000,000 
 
 
 Gemini 2.5 Flash 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash Preview 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash 
 2,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 10 
 10,000 
 100 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 10 
 10,000 
 50 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 500 
 500,000 
 2,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 1,000 
 1,000,000 
 10,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 10 
 * 
 70 
 * 
 
 
 Imagen 4 Ultra 
 5 
 * 
 30 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 2 
 * 
 10 
 * 
 
 
 Veo 3.1 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 3 
 2 
 * 
 10 
 * 
 
 
 Veo 3 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 3,000 
 1,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 300 
 1,000,000 
 10,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 150 
 2,000,000 
 10,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 2

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Pro 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Flash 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash Preview 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.0 Flash 
 10,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 20,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 100,000 
 10,000 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 25,000 
 1,000 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 2,000 
 1,500,000 
 50,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 2,000 
 3,000,000 
 100,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 15 
 * 
 1000 
 * 
 
 
 Imagen 4 Ultra 
 10 
 * 
 400 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 4 
 * 
 50 
 * 
 
 
 Veo 3.1 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 3 
 4 
 * 
 50 
 * 
 
 
 Veo 3 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 400 
 3,000,000 
 100,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 1,000 
 5,000,000 
 50,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 3

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Pro 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash Preview 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Imagen 4 Standard/Fast 
 20 
 * 
 15,000 
 * 
 
 
 Imagen 4 Ultra 
 15 
 * 
 5,000 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 10 
 * 
 500 
 * 
 
 
 Veo 3.1 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 3 
 10 
 * 
 500 
 * 
 
 
 Veo 3 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 10,000 
 10,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 600 
 8,000,000 
 * 
 *1,000,000,000* 
 
 
 Gemini 2.5 Computer Use Preview 
 2,000 
 8,000,000 
 * 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

Specified rate limits are not guaranteed and actual capacity may vary.

## Batch API rate limits

 Batch API requests are subject to their own rate
limits, separate from the non-batch API calls.

- Concurrent batch requests: 100

- Input file size limit: 2GB

- File storage limit: 20GB

- Enqueued tokens per model: The Batch Enqueued Tokens column in the
rate limits table lists the maximum number of tokens that can be enqueued
for batch processing across all your active batch jobs for a given model.
See in the standard API rate limits table .

## How to upgrade to the next tier

The Gemini API uses Cloud Billing for all billing services. To transition from
the Free tier to a paid tier, you must first enable Cloud Billing for your
Google Cloud project.

Once your project meets the specified criteria, it becomes eligible for an
upgrade to the next tier. To request an upgrade, follow these steps:

- Navigate to the API keys page in AI Studio.

- Locate the project you want to upgrade and click "Upgrade". The "Upgrade" option
will only show up for projects that meet next tier qualifications .

After a quick validation, the project will be upgraded to the next tier.

## Request a rate limit increase

Each model variation has an associated rate limit (requests per minute, RPM).
For details on those rate limits, see Gemini models .

 Request paid tier rate limit increase 

We offer no guarantees about increasing your rate limit, but we'll do our best
to review your request.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Generate images using Imagen &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/imagen#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Generate images using Imagen  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Generate images using Imagen 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Imagen is Google's high-fidelity image generation model, capable of generating
realistic and high quality images from text prompts. All generated images
include a SynthID watermark. To learn more about the available Imagen model
variants, see the Model versions section.

## Generate images using the Imagen models

This example demonstrates generating images with an Imagen model :

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-001',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-001",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

 
 

### REST

 

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-001:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

 
 
 
 
 AI-generated image of a robot holding a red skateboard 
 

### Imagen configuration

Imagen supports English only prompts at this time and the following parameters:

- `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive).
The default is 4.

- `imageSize`: The size of the generated image. This is only supported for
the Standard and Ultra models. The supported values are `1K` and `2K`.
Default is `1K`.

- `aspectRatio`: Changes the aspect ratio of the generated image. Supported
values are `"1:1"`, `"3:4"`, `"4:3"`, `"9:16"`, and `"16:9"`. The default is
`"1:1"`.

- 

`personGeneration`: Allow the model to generate images of people. The
following values are supported:

 `"dont_allow"`: Block generation of images of people.

- `"allow_adult"`: Generate images of adults, but not children. This is
the default.

- `"allow_all"`: Generate images that include adults and children.

 

## Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt
can produce different results, along with examples of images you can create.

### Prompt writing basics

A good prompt is descriptive and clear, and makes use of meaningful keywords and
modifiers. Start by thinking of your subject , context , and style .

 
 
 Image text: A sketch ( style ) of a modern apartment building 
( subject ) surrounded by skyscrapers ( context and background ). 
 

- 

 Subject : The first thing to think about with any prompt is the
 subject : the object, person, animal, or scenery you want an image of.

- 

 Context and background: Just as important is the background or context 
in which the subject will be placed. Try placing your subject in a variety
of backgrounds. For example, a studio with a white background, outdoors, or
indoor environments.

- 

 Style: Finally, add the style of image you want. Styles can be general
(painting, photograph, sketches) or very specific (pastel painting, charcoal
drawing, isometric 3D). You can also combine styles.

After you write a first version of your prompt, refine your prompt by adding
more details until you get to the image that you want. Iteration is important.
Start by establishing your core idea, and then refine and expand upon that core
idea until the generated image is close to your vision.

 
 
 
 
 
 Prompt: A park in the spring next to a lake 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour, red wildflowers 
 
 
 
 

Imagen models can transform your ideas into detailed images, whether
your prompts are short or long and detailed. Refine your vision
through iterative prompting, adding details until you achieve the perfect
result.

 
 
 
 
 

Short prompts let you generate an image quickly.

 
 
 Prompt: close-up photo of a woman in her 20s, street photography,
 movie still, muted orange warm tones
 
 
 
 
 
 
 

Longer prompts let you add specific details and build your image.

 
 
 Prompt: captivating photo of a woman in her 20s utilizing a street
 photography style. The image should look like a movie still with muted orange
 warm tones.
 
 
 
 
 
 

Additional advice for Imagen prompt writing:

- Use descriptive language : Employ detailed adjectives and adverbs to
paint a clear picture for Imagen.

- Provide context : If necessary, include background information to aid the
AI's understanding.

- Reference specific artists or styles : If you have a particular aesthetic
in mind, referencing specific artists or art movements can be helpful.

- Use prompt engineering tools : Consider exploring prompt engineering
tools or resources to help you refine your prompts and achieve optimal
results.

- Enhancing the facial details in your personal and group images : Specify facial details as a focus of the photo (for example, use the
 word "portrait" in the prompt).

### Generate text in images

Imagen models can add text into images, opening up more creative image generation
possibilities. Use the following guidance to get the most out of this feature:

- Iterate with confidence : You might have to regenerate images until you
achieve the look you want. Imagen's text integration is still
evolving, and sometimes multiple attempts yield the best results.

- Keep it short : Limit text to 25 characters or less for optimal
generation.

- 

 Multiple phrases : Experiment with two or three distinct phrases to
provide additional information. Avoid exceeding three phrases for cleaner
compositions.

 
 
 Prompt: A poster with the text "Summerland" in bold font as a
title, underneath this text is the slogan "Summer never felt so good"
 
 

- 

 Guide Placement : While Imagen can attempt to position text
as directed, expect occasional variations. This feature is continually
improving.

- 

 Inspire font style : Specify a general font style to subtly influence
Imagen's choices. Don't rely on precise font replication, but expect
creative interpretations.

- 

 Font size : Specify a font size or a general indication of size (for
example, small , medium , large ) to influence the font size generation.

### Prompt parameterization

To better control output results, you might find it helpful to parameterize the
inputs into Imagen. For example, suppose you
want your customers to be able to generate logos for their business, and you
want to make sure logos are always generated on a solid color background. You
also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the
following:

 

```
A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.
```

 

In your custom user interface, the customer can input the parameters using
a menu, and their chosen value populates the prompt Imagen receives.

For example:

- 

Prompt: 

```
A minimalist logo for a health care company on a solid color background. Include the text Journey.
```



 

- 

Prompt: 

```
A modern logo for a software company on a solid color background. Include the text Silo.
```



 

- 

Prompt: 

```
A traditional logo for a baking company on a solid color background. Include the text Seed.
```



 

### Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes
like photography descriptors, shapes and materials, historical art
movements, and image quality modifiers.

#### Photography

- Prompt includes: "A photo of..." 

To use this style, start with using keywords that clearly tell
Imagen that you're looking for a photograph. Start your prompts with
 "A photo of. . ." . For example:

 
 
 
 
 
 Prompt: A photo of coffee beans in a kitchen on a wooden surface 
 
 
 
 
 
 Prompt: A photo of a chocolate bar on a kitchen counter 
 
 
 
 
 
 Prompt: A photo of a modern building with water in the background 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

 Photography modifiers 

In the following examples, you can see several photography-specific modifiers
and parameters. You can combine multiple modifiers for more precise control.

- 

 Camera Proximity - Close up, taken from far away 

 
 
 
 
 
 Prompt: A close-up photo of coffee beans 
 
 
 
 
 
 Prompt: A zoomed out photo of a small bag of
 coffee beans in a messy kitchen 
 
 
 
 

- 

 Camera Position - aerial, from below 

 
 
 
 
 
 Prompt: aerial photo of urban city with skyscrapers 
 
 
 
 
 
 Prompt: A photo of a forest canopy with blue skies from below 
 
 
 
 

- 

 Lighting - natural, dramatic, warm, cold 

 
 
 
 
 
 Prompt: studio photo of a modern arm chair, natural lighting 
 
 
 
 
 
 Prompt: studio photo of a modern arm chair, dramatic lighting 
 
 
 
 

- 

 Camera Settings - motion blur, soft focus, bokeh, portrait 

 
 
 
 
 
 Prompt: photo of a city with skyscrapers from the inside of a car with motion blur 
 
 
 
 
 
 Prompt: soft focus photograph of a bridge in an urban city at night 
 
 
 
 

- 

 Lens types - 35mm, 50mm, fisheye, wide angle, macro 

 
 
 
 
 
 Prompt: photo of a leaf, macro lens 
 
 
 
 
 
 Prompt: street photography, new york city, fisheye lens 
 
 
 
 

- 

 Film types - black and white, polaroid 

 
 
 
 
 
 Prompt: a polaroid portrait of a dog wearing sunglasses 
 
 
 
 
 
 Prompt: black and white photo of a dog wearing sunglasses 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

### Illustration and art

- Prompt includes: "A painting of..." , "A sketch of..." 

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic
digital art. For example, the following images use the same prompt with
different styles:

 "An [art style or creation technique] of an angular
sporty electric sedan with skyscrapers in the background" 

 
 
 
 
 
 Prompt: A technical pencil drawing of an angular... 
 
 
 
 
 
 Prompt: A charcoal drawing of an angular... 
 
 
 
 
 
 Prompt: A color pencil drawing of an angular... 
 
 
 
 

 
 
 
 
 
 Prompt: A pastel painting of an angular... 
 
 
 
 
 
 Prompt: A digital art of an angular... 
 
 
 
 
 
 Prompt: An art deco (poster) of an angular... 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model. 

 Shapes and materials 

- Prompt includes: "...made of..." , "...in the shape of..." 

One of the strengths of this technology is that you can create imagery that
is otherwise difficult or impossible. For example, you can recreate
your company logo in different materials and textures.

 
 
 
 
 
 Prompt: a duffle bag made of cheese 
 
 
 
 
 
 Prompt: neon tubes in the shape of a bird 
 
 
 
 
 
 Prompt: an armchair made of paper , studio photo, origami style 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Historical art references

- Prompt includes: "...in the style of..." 

Certain styles have become iconic over the years. The following are some ideas
of historical painting or art styles that you can try.

 "generate an image in the style of [art period or movement]
 : a wind farm" 

 
 
 
 
 
 Prompt: generate an image in the style of an impressionist painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of a renaissance painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of pop art : a wind farm 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality
asset. Examples of quality modifiers include the following:

- General Modifiers - high-quality, beautiful, stylized 

- Photos - 4K, HDR, Studio Photo 

- Art, Illustration - by a professional, detailed 

The following are a few examples of prompts without quality modifiers and
the same prompt with quality modifiers.

 
 
 
 
 
 Prompt (no quality modifiers): a photo of a corn stalk 
 
 
 
 
 
 Prompt (with quality modifiers): 4k HDR beautiful 

 photo of a corn stalk taken by a 
 professional photographer 
 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Aspect ratios

Imagen image generation lets you set five distinct image aspect
ratios.

- Square (1:1, default) - A standard square photo. Common uses for this
aspect ratio include social media posts.

- 

 Fullscreen (4:3) - This aspect ratio is commonly used in media or film.
It is also the dimensions of most old (non-widescreen) TVs and medium format
cameras. It captures more of the scene horizontally (compared to 1:1),
making it a preferred aspect ratio for photography.

 
 
 
 
 
 Prompt: close up of a musician's fingers
playing the piano, black and white film, vintage (4:3 aspect ratio)
 
 
 
 
 
 
 Prompt: A professional studio photo of
french fries for a high end restaurant, in the style of a food magazine
 (4:3 aspect ratio)
 
 
 
 
 

- 

 Portrait full screen (3:4) - This is the fullscreen aspect ratio rotated
90 degrees. This lets to capture more of the scene vertically compared to
the 1:1 aspect ratio.

 
 
 
 
 
 Prompt: a woman hiking, close of her
boots reflected in a puddle, large mountains in the background, in the
style of an advertisement, dramatic angles (3:4 aspect ratio)
 
 
 
 
 
 
 Prompt: aerial shot of a river flowing
up a mystical valley (3:4 aspect ratio)
 
 
 
 
 

- 

 Widescreen (16:9) - This ratio has replaced 4:3 and is now the most
common aspect ratio for TVs, monitors, and mobile phone screens (landscape).
Use this aspect ratio when you want to capture more of the background (for
example, scenic landscapes).

 
 
 Prompt: a man wearing all white
clothing sitting on the beach, close up, golden hour lighting (16:9
aspect ratio)
 
 

- 

 Portrait (9:16) - This ratio is widescreen but rotated. This a
relatively new aspect ratio that has been popularized by short form video
apps (for example, YouTube shorts). Use this for tall objects with strong
vertical orientations such as buildings, trees, waterfalls, or other similar
objects.

 
 
 Prompt: a digital render of a massive skyscraper, modern,
grand, epic with a beautiful sunset in the background (9:16 aspect ratio)
 
 

#### Photorealistic images

Different versions of the image generation
model might offer a mix of artistic and photorealistic output. Use the following
wording in prompts to generate more photorealistic output, based on the subject
you want to generate.

 
 
 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 
 

 Portraits 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 

Using several keywords from the table, Imagen can generate the following
portraits:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, blue and grey duotones 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, film noir 

Model: `imagen-3.0-generate-002`

 Objects 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 

Using several keywords from the table, Imagen can
generate the following object images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: leaf of a prayer plant, macro lens, 60mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a plate of pasta, 100mm Macro lens 

Model: `imagen-3.0-generate-002`

 Motion 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 

Using several keywords from the table, Imagen can
generate the following motion images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a winning touchdown, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A deer running in the forest, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 Wide-angle 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 

Using several keywords from the table, Imagen can
generate the following wide-angle images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: an expansive mountain range, landscape wide angle 10mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a photo of the moon, astro photography, wide angle 10mm 

Model: `imagen-3.0-generate-002`

## Model versions

 
 
 
 

### Imagen 4

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-4.0-generate-001`

 `imagen-4.0-ultra-generate-001`

 `imagen-4.0-fast-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

480 tokens (text)

 
 
 

 Output images 

 

1 to 4 (Ultra/Standard/Fast)

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 
 
 

### Imagen 3

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-3.0-generate-002`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

N/A

 
 
 

 Output images 

 

Up to 4

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-03 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-03 UTC."],[],[]]

---

### Text generation &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/text-generation

- 
 
 
 
 
 
 
 
 
 
 
 Text generation  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Text generation 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can generate text output from various inputs, including text,
images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithTextInput {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

 

## Thinking with Gemini 2.5

2.5 Flash and Pro models have "thinking" enabled by default to enhance quality, which may take longer to run and increase token usage. 

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero. 

For more details, see the thinking guide .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.ThinkingConfig;

public class GenerateContentWithThinkingConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            // Disables thinking
            .thinkingConfig(ThinkingConfig.builder().thinkingBudget(0))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so,
pass a `GenerateContentConfig` 
object.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithSystemInstruction {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            .systemInstruction(
                Content.fromParts(Part.fromText("You are a cat. Your name is Neko.")))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Hello there", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

The `GenerateContentConfig` 
object also lets you override default generation parameters, such as
 temperature .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config = GenerateContentConfig.builder().temperature(0.1f).build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Explain how AI works", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Refer to the `GenerateContentConfig` 
in our API reference for a complete list of configurable parameters and their
descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with
media files. The following example demonstrates providing an image:

 
 

### Python

 

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.Content;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithMultiModalInputs {
  public static void main(String[] args) {

    Client client = new Client();

    Content content =
      Content.fromParts(
          Part.fromText("Tell me about this instrument"),
          Part.fromUri("/path/to/organ.jpg", "image/jpeg"));

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", content, null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

 
 

For alternative methods of providing images and more advanced image processing,
see our image understanding guide .
The API also supports document , video , and audio 
inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation 
process is complete.

For more fluid interactions, use streaming to receive `GenerateContentResponse` instances incrementally
as they're generated.

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentStream {
  public static void main(String[] args) {

    Client client = new Client();

    ResponseStream<GenerateContentResponse> responseStream =
      client.models.generateContentStream(
          "gemini-2.5-flash", "Write a story about a magic backpack.", null);

    for (GenerateContentResponse res : responseStream) {
      System.out.print(res.text());
    }

    // To save resources and avoid connection leaks, it is recommended to close the response
    // stream after consumption (or using try block to get the response stream).
    responseStream.close();
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Multi-turn conversations (chat)

Our SDKs provide functionality to collect multiple rounds of prompts and
responses into a chat, giving you an easy way to keep track of the conversation
history.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversation {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    GenerateContentResponse response =
        chatSession.sendMessage("I have 2 dogs in my house.");
    System.out.println("First response: " + response.text());

    response = chatSession.sendMessage("How many paws are in my house?");
    System.out.println("Second response: " + response.text());

    // Get the history of the chat session.
    // Passing 'true' to getHistory() returns the curated history, which excludes
    // empty or invalid parts.
    // Passing 'false' here would return the comprehensive history, including
    // empty or invalid parts.
    ImmutableList<Content> history = chatSession.getHistory(true);
    System.out.println("History: " + history);
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Streaming can also be used for multi-turn conversations.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversationWithStreaming {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    ResponseStream<GenerateContentResponse> responseStream =
        chatSession.sendMessageStream("I have 2 dogs in my house.", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    responseStream = chatSession.sendMessageStream("How many paws are in my house?", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    // Get the history of the chat session. History is added after the stream
    // is consumed and includes the aggregated response from the stream.
    System.out.println("History: " + chatSession.getHistory(false));
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Supported models

All models in the Gemini family support text generation. To learn more
about the models and their capabilities, visit the
 Models page.

## Best practices

### Prompting tips

For basic text generation, a zero-shot 
prompt often suffices without needing examples, system instructions or specific
formatting.

For more tailored outputs:

- Use System instructions to guide the model.

- Provide few example inputs and outputs to guide the model. This is often referred to as few-shot prompting.

Consult our prompt engineering guide for
more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our
 structured output guide to learn how.

## What's next

- Try the Gemini API getting started Colab .

- Explore Gemini's image ,
 video , audio 
and document understanding capabilities.

- Learn about multimodal
 file prompting strategies .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Image generation with Gemini (aka Nano Banana üçå) &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/image-generation

- 
 
 
 
 
 
 
 
 
 
 
 Image generation with Gemini (aka Nano Banana üçå)  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Image generation with Gemini (aka Nano Banana üçå) 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini can generate and process images conversationally. You can prompt Gemini
with text, images, or a combination of both allowing you to create, edit, and
iterate on visuals with unprecedented control:

- Text-to-Image: Generate high-quality images from simple or complex text descriptions.

- Image + Text-to-Image (Editing): Provide an image and use text prompts to add, remove, or modify elements, change the style, or adjust the color grading.

- Multi-Image to Image (Composition & Style Transfer): Use multiple input images to compose a new scene or transfer the style from one image to another.

- Iterative Refinement: Engage in a conversation to progressively refine your image over multiple turns, making small adjustments until it's perfect.

- High-Fidelity Text Rendering: Accurately generate images that contain legible and well-placed text, ideal for logos, diagrams, and posters.

All generated images include a SynthID watermark .

## Image generation (text-to-image)

The following code demonstrates how to generate an image based on a descriptive
prompt.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

prompt = (
    "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"
)

response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
)

for part in response.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = part.as_image()
        image.save("generated_image.png")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      genai.Text("Create a picture of a nano banana dish in a " +
                 " fancy restaurant with a Gemini theme"),
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

 
 
 
 
 AI-generated image of a nano banana dish in a Gemini-themed restaurant 
 

## Image editing (text-and-image-to-image)

 Reminder : Make sure you have the necessary rights to any images you upload.
Don't generate content that infringe on others' rights, including videos or
images that deceive, harass, or harm. Your use of this generative AI service is
subject to our Prohibited Use Policy .

The following example demonstrates uploading base64 encoded images.
For multiple images, larger payloads, and supported MIME types, check the Image
understanding page.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

prompt = (
    "Create a picture of my cat eating a nano-banana in a "
    "fancy restaurant under the Gemini constellation",
)

image = Image.open("/path/to/cat_image.png")

response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt, image],
)

for part in response.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = part.as_image()
        image.save("generated_image.png")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "path/to/cat_image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    { text: "Create a picture of my cat eating a nano-banana in a" +
            "fancy restaurant under the Gemini constellation" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/cat_image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.5-flash-image",
     contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/cat_image.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

 
 
 
 
 AI-generated image of a cat eating a nano banana 
 

## Other image generation modes

Gemini supports other image interaction modes based on prompt structure and
context, including:

- Text to image(s) and text (interleaved): Outputs images with related text.

 Example prompt: "Generate an illustrated recipe for a paella."

 
- Image(s) and text to image(s) and text (interleaved) : Uses input images and text to create new related images and text.

 Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"

 
- Multi-turn image editing (chat): Keep generating and editing images conversationally.

 Example prompts: [upload an image of a blue car.] , "Turn this car into a convertible.", "Now change the color to yellow."

 

## Prompting guide and strategies

Mastering Gemini 2.5 Flash Image Generation starts with one fundamental
principle:

 

 Describe the scene, don't just list keywords. 
The model's core strength is its deep language understanding. A narrative,
descriptive paragraph will almost always produce a better, more coherent image
than a list of disconnected words.

 

### Prompts for generating images

The following strategies will help you create effective prompts to
generate exactly the images you're looking for.

#### 1. Photorealistic scenes

For realistic images, use photography terms. Mention camera angles, lens types,
lighting, and fine details to guide the model toward a photorealistic result.

 
 

### Template

 

```
A photorealistic [shot type] of [subject], [action or expression], set in
[environment]. The scene is illuminated by [lighting description], creating
a [mood] atmosphere. Captured with a [camera/lens details], emphasizing
[key textures and details]. The image should be in a [aspect ratio] format.
```

 
 

### Prompt

 

```
A photorealistic close-up portrait of an elderly Japanese ceramicist with
deep, sun-etched wrinkles and a warm, knowing smile. He is carefully
inspecting a freshly glazed tea bowl. The setting is his rustic,
sun-drenched workshop. The scene is illuminated by soft, golden hour light
streaming through a window, highlighting the fine texture of the clay.
Captured with an 85mm portrait lens, resulting in a soft, blurred background
(bokeh). The overall mood is serene and masterful. Vertical portrait
orientation.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('photorealistic_example.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("photorealistic_example.png", buffer);
      console.log("Image saved as photorealistic_example.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "photorealistic_example.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > photorealistic_example.png
```

 
 
 
 
 A photorealistic close-up portrait of an elderly Japanese ceramicist... 
 

#### 2. Stylized illustrations & stickers

To create stickers, icons, or assets, be explicit about the style and request a
transparent background.

 
 

### Template

 

```
A [style] sticker of a [subject], featuring [key characteristics] and a
[color palette]. The design should have [line style] and [shading style].
The background must be transparent.
```

 
 

### Prompt

 

```
A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's
munching on a green bamboo leaf. The design features bold, clean outlines,
simple cel-shading, and a vibrant color palette. The background must be white.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('red_panda_sticker.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("red_panda_sticker.png", buffer);
      console.log("Image saved as red_panda_sticker.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "red_panda_sticker.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It'"'"'s munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > red_panda_sticker.png
```

 
 
 
 
 A kawaii-style sticker of a happy red panda... 
 

#### 3. Accurate text in images

Gemini excels at rendering text. Be clear about the text, the font style
(descriptively), and the overall design.

 
 

### Template

 

```
Create a [image type] for [brand/concept] with the text "[text to render]"
in a [font style]. The design should be [style description], with a
[color scheme].
```

 
 

### Prompt

 

```
Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'.
The text should be in a clean, bold, sans-serif font. The design should
feature a simple, stylized icon of a a coffee bean seamlessly integrated
with the text. The color scheme is black and white.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('logo_example.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("logo_example.png", buffer);
      console.log("Image saved as logo_example.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "logo_example.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Create a modern, minimalist logo for a coffee shop called '"'"'The Daily Grind'"'"'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > logo_example.png
```

 
 
 
 
 Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'... 
 

#### 4. Product mockups & commercial photography

Perfect for creating clean, professional product shots for e-commerce,
advertising, or branding.

 
 

### Template

 

```
A high-resolution, studio-lit product photograph of a [product description]
on a [background surface/description]. The lighting is a [lighting setup,
e.g., three-point softbox setup] to [lighting purpose]. The camera angle is
a [angle type] to showcase [specific feature]. Ultra-realistic, with sharp
focus on [key detail]. [Aspect ratio].
```

 
 

### Prompt

 

```
A high-resolution, studio-lit product photograph of a minimalist ceramic
coffee mug in matte black, presented on a polished concrete surface. The
lighting is a three-point softbox setup designed to create soft, diffused
highlights and eliminate harsh shadows. The camera angle is a slightly
elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with
sharp focus on the steam rising from the coffee. Square image.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('product_mockup.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("product_mockup.png", buffer);
      console.log("Image saved as product_mockup.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "product_mockup.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > product_mockup.png
```

 
 
 
 
 A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug... 
 

#### 5. Minimalist & negative space design

Excellent for creating backgrounds for websites, presentations, or marketing
materials where text will be overlaid.

 
 

### Template

 

```
A minimalist composition featuring a single [subject] positioned in the
[bottom-right/top-left/etc.] of the frame. The background is a vast, empty
[color] canvas, creating significant negative space. Soft, subtle lighting.
[Aspect ratio].
```

 
 

### Prompt

 

```
A minimalist composition featuring a single, delicate red maple leaf
positioned in the bottom-right of the frame. The background is a vast, empty
off-white canvas, creating significant negative space for text. Soft,
diffused lighting from the top left. Square image.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('minimalist_design.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("minimalist_design.png", buffer);
      console.log("Image saved as minimalist_design.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "minimalist_design.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > minimalist_design.png
```

 
 
 
 
 A minimalist composition featuring a single, delicate red maple leaf... 
 

#### 6. Sequential art (Comic panel / Storyboard)

Builds on character consistency and scene description to create panels for
visual storytelling.

 
 

### Template

 

```
A single comic book panel in a [art style] style. In the foreground,
[character description and action]. In the background, [setting details].
The panel has a [dialogue/caption box] with the text "[Text]". The lighting
creates a [mood] mood. [Aspect ratio].
```

 
 

### Prompt

 

```
A single comic book panel in a gritty, noir art style with high-contrast
black and white inks. In the foreground, a detective in a trench coat stands
under a flickering streetlamp, rain soaking his shoulders. In the
background, the neon sign of a desolate bar reflects in a puddle. A caption
box at the top reads "The city was a tough place to keep secrets." The
lighting is harsh, creating a dramatic, somber mood. Landscape.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('comic_panel.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("comic_panel.png", buffer);
      console.log("Image saved as comic_panel.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "comic_panel.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > comic_panel.png
```

 
 
 
 
 A single comic book panel in a gritty, noir art style... 
 

### Prompts for editing images

These examples show how to provide images alongside your text prompts for
editing, composition, and style transfer.

#### 1. Adding and removing elements

Provide an image and describe your change. The model will match the original
image's style, lighting, and perspective.

 
 

### Template

 

```
Using the provided image of [subject], please [add/remove/modify] [element]
to/from the scene. Ensure the change is [description of how the change should
integrate].
```

 
 

### Prompt

 

```
"Using the provided image of my cat, please add a small, knitted wizard hat
on its head. Make it look like it's sitting comfortably and matches the soft
lighting of the photo."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A photorealistic picture of a fluffy ginger cat sitting on a wooden floor, looking directly at the camera. Soft, natural light from a window."
image_input = Image.open('/path/to/your/cat_photo.png')
text_input = """Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[text_input, image_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('cat_with_hat.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/cat_photo.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    { text: "Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off." },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("cat_with_hat.png", buffer);
      console.log("Image saved as cat_with_hat.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/cat_photo.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    genai.NewPartFromText("Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "cat_with_hat.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/cat_photo.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off.\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > cat_with_hat.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A photorealistic picture of a fluffy ginger cat... 
 
 
 
 
 
 Using the provided image of my cat, please add a small, knitted wizard hat... 
 
 
 
 

#### 2. Inpainting (Semantic masking)

Conversationally define a "mask" to edit a specific part of an image while
leaving the rest untouched.

 
 

### Template

 

```
Using the provided image, change only the [specific element] to [new
element/description]. Keep everything else in the image exactly the same,
preserving the original style, lighting, and composition.
```

 
 

### Prompt

 

```
"Using the provided image of a living room, change only the blue sofa to be
a vintage, brown leather chesterfield sofa. Keep the rest of the room,
including the pillows on the sofa and the lighting, unchanged."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A wide shot of a modern, well-lit living room with a prominent blue sofa in the center. A coffee table is in front of it and a large window is in the background."
living_room_image = Image.open('/path/to/your/living_room.png')
text_input = """Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[living_room_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('living_room_edited.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/living_room.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
    { text: "Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("living_room_edited.png", buffer);
      console.log("Image saved as living_room_edited.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/living_room.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "living_room_edited.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/living_room.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            },
            {\"text\": \"Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > living_room_edited.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A wide shot of a modern, well-lit living room... 
 
 
 
 
 
 Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa... 
 
 
 
 

#### 3. Style transfer

Provide an image and ask the model to recreate its content in a different
artistic style.

 
 

### Template

 

```
Transform the provided photograph of [subject] into the artistic style of [artist/art style]. Preserve the original composition but render it with [description of stylistic elements].
```

 
 

### Prompt

 

```
"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A photorealistic, high-resolution photograph of a busy city street in New York at night, with bright neon signs, yellow taxis, and tall skyscrapers."
city_image = Image.open('/path/to/your/city.png')
text_input = """Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[city_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('city_style_transfer.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/city.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
    { text: "Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("city_style_transfer.png", buffer);
      console.log("Image saved as city_style_transfer.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/city.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "city_style_transfer.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/city.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            },
            {\"text\": \"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > city_style_transfer.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A photorealistic, high-resolution photograph of a busy city street... 
 
 
 
 
 
 Transform the provided photograph of a modern city street at night... 
 
 
 
 

#### 4. Advanced composition: Combining multiple images

Provide multiple images as context to create a new, composite scene. This is
perfect for product mockups or creative collages.

 
 

### Template

 

```
Create a new image by combining the elements from the provided images. Take
the [element from image 1] and place it with/on the [element from image 2].
The final image should be a [description of the final scene].
```

 
 

### Prompt

 

```
"Create a professional e-commerce fashion photo. Take the blue floral dress
from the first image and let the woman from the second image wear it.
Generate a realistic, full-body shot of the woman wearing the dress, with
the lighting and shadows adjusted to match the outdoor environment."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompts:
# 1. Dress: "A professionally shot photo of a blue floral summer dress on a plain white background, ghost mannequin style."
# 2. Model: "Full-body shot of a woman with her hair in a bun, smiling, standing against a neutral grey studio background."
dress_image = Image.open('/path/to/your/dress.png')
model_image = Image.open('/path/to/your/model.png')

text_input = """Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[dress_image, model_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('fashion_ecommerce_shot.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath1 = "/path/to/your/dress.png";
  const imageData1 = fs.readFileSync(imagePath1);
  const base64Image1 = imageData1.toString("base64");
  const imagePath2 = "/path/to/your/model.png";
  const imageData2 = fs.readFileSync(imagePath2);
  const base64Image2 = imageData2.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image1,
      },
    },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image2,
      },
    },
    { text: "Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("fashion_ecommerce_shot.png", buffer);
      console.log("Image saved as fashion_ecommerce_shot.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imgData1, _ := os.ReadFile("/path/to/your/dress.png")
  imgData2, _ := os.ReadFile("/path/to/your/model.png")

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData1,
      },
    },
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData2,
      },
    },
    genai.NewPartFromText("Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "fashion_ecommerce_shot.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH1=/path/to/your/dress.png
IMG_PATH2=/path/to/your/model.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG1_BASE64\"
              }
            },
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG2_BASE64\"
              }
            },
            {\"text\": \"Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > fashion_ecommerce_shot.png
```

 
 
 
 
 

Input 1

 
 

Input 2

 
 

Output

 
 
 
 
 
 
 A professionally shot photo of a blue floral summer dress... 
 
 
 
 
 
 Full-body shot of a woman with her hair in a bun... 
 
 
 
 
 
 Create a professional e-commerce fashion photo... 
 
 
 
 

#### 5. High-fidelity detail preservation

To ensure critical details (like a face or logo) are preserved during an edit,
describe them in great detail along with your edit request.

 
 

### Template

 

```
Using the provided images, place [element from image 2] onto [element from
image 1]. Ensure that the features of [element from image 1] remain
completely unchanged. The added element should [description of how the
element should integrate].
```

 
 

### Prompt

 

```
"Take the first image of the woman with brown hair, blue eyes, and a neutral
expression. Add the logo from the second image onto her black t-shirt.
Ensure the woman's face and features remain completely unchanged. The logo
should look like it's naturally printed on the fabric, following the folds
of the shirt."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompts:
# 1. Woman: "A professional headshot of a woman with brown hair and blue eyes, wearing a plain black t-shirt, against a neutral studio background."
# 2. Logo: "A simple, modern logo with the letters 'G' and 'A' in a white circle."
woman_image = Image.open('/path/to/your/woman.png')
logo_image = Image.open('/path/to/your/logo.png')
text_input = """Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[woman_image, logo_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('woman_with_logo.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath1 = "/path/to/your/woman.png";
  const imageData1 = fs.readFileSync(imagePath1);
  const base64Image1 = imageData1.toString("base64");
  const imagePath2 = "/path/to/your/logo.png";
  const imageData2 = fs.readFileSync(imagePath2);
  const base64Image2 = imageData2.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image1,
      },
    },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image2,
      },
    },
    { text: "Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("woman_with_logo.png", buffer);
      console.log("Image saved as woman_with_logo.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imgData1, _ := os.ReadFile("/path/to/your/woman.png")
  imgData2, _ := os.ReadFile("/path/to/your/logo.png")

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData1,
      },
    },
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData2,
      },
    },
    genai.NewPartFromText("Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "woman_with_logo.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH1=/path/to/your/woman.png
IMG_PATH2=/path/to/your/logo.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG1_BASE64\"
              }
            },
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG2_BASE64\"
              }
            },
            {\"text\": \"Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > woman_with_logo.png
```

 
 
 
 
 

Input 1

 
 

Input 2

 
 

Output

 
 
 
 
 
 
 A professional headshot of a woman with brown hair and blue eyes... 
 
 
 
 
 
 A simple, modern logo with the letters 'G' and 'A'... 
 
 
 
 
 
 Take the first image of the woman with brown hair, blue eyes, and a neutral expression... 
 
 
 
 

### Best Practices

To elevate your results from good to great, incorporate these professional
strategies into your workflow.

- Be Hyper-Specific: The more detail you provide, the more control you
have. Instead of "fantasy armor," describe it: "ornate elven plate armor, etched
with silver leaf patterns, with a high collar and pauldrons shaped like falcon
wings."

- Provide Context and Intent: Explain the purpose of the image. The
model's understanding of context will influence the final output. For example,
"Create a logo for a high-end, minimalist skincare brand" will yield better
results than just "Create a logo."

- Iterate and Refine: Don't expect a perfect image on the first try. Use
the conversational nature of the model to make small changes. Follow up with
prompts like, "That's great, but can you make the lighting a bit warmer?" or
"Keep everything the same, but change the character's expression to be more
serious."

- Use Step-by-Step Instructions: For complex scenes with many elements,
break your prompt into steps. "First, create a background of a serene, misty
forest at dawn. Then, in the foreground, add a moss-covered ancient stone altar.
Finally, place a single, glowing sword on top of the altar."

- Use "Semantic Negative Prompts": Instead of saying "no cars," describe
the desired scene positively: "an empty, deserted street with no signs of
traffic."

- Control the Camera: Use photographic and cinematic language to control
the composition. Terms like `wide-angle shot`, `macro shot`, 

```
low-angle
perspective
```

.

## Limitations

- For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN,
hi-IN.

- Image generation does not support audio or video inputs.

- The model won't always follow the exact number of image outputs that the
user explicitly asks for.

- The model works best with up to 3 images as an input.

- When generating text for an image, Gemini works best if you first generate
the text and then ask for an image with the text.

- Uploading images of children is not currently supported in EEA, CH, and UK.

- All generated images include a SynthID watermark .

## Optional configurations

You can optionally configure the response modalities and aspect ratio of the
model's output in the `config` field of `generate_content` calls.

### Output types

The model defaults to returning text and image responses
(i.e. 

```
response_modalities=['Text', 'Image']
```

).
You can configure the response to return only images without text using
`response_modalities=['Image']`.

 
 

### Python

 

```
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
    config=types.GenerateContentConfig(
        response_modalities=['Image']
    )
)
```

 
 

### JavaScript

 

```
const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
    config: {
        responseModalities: ['Image']
    }
  });
```

 
 

### Go

 

```
result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash-image",
    genai.Text("Create a picture of a nano banana dish in a " +
                " fancy restaurant with a Gemini theme"),
    &genai.GenerateContentConfig{
        ResponseModalities: "Image",
    },
  )
```

 
 

### REST

 

```
-d '{
  "contents": [{
    "parts": [
      {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
    ]
  }],
  "generationConfig": {
    "responseModalities": ["Image"]
  }
}' \
```

 
 

### Aspect ratios

The model defaults to matching the output image size to that of your input
image, or otherwise generates 1:1 squares.
You can control the aspect ratio of the output image using the `aspect_ratio`
field under `image_config` in the response request, shown here:

 
 

### Python

 

```
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(
            aspect_ratio="16:9",
        )
    )
)
```

 
 

### JavaScript

 

```
const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
    config: {
      imageConfig: {
        aspectRatio: "16:9",
      },
    }
  });
```

 
 

### Go

 

```
result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash-image",
    genai.Text("Create a picture of a nano banana dish in a " +
                " fancy restaurant with a Gemini theme"),
    &genai.GenerateContentConfig{
        ImageConfig: &genai.ImageConfig{
          AspectRatio: "16:9",
        },
    }
  )
```

 
 

### REST

 

```
-d '{
  "contents": [{
    "parts": [
      {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
    ]
  }],
  "generationConfig": {
    "imageConfig": {
      "aspectRatio": "16:9"
    }
  }
}' \
```

 
 

The different ratios available and the size of the image generated are listed in
this table:

 
 
 
 Aspect ratio 
 Resolution 
 Tokens 
 
 

 
 
 1:1 
 1024x1024 
 1290 
 
 
 2:3 
 832x1248 
 1290 
 
 
 3:2 
 1248x832 
 1290 
 
 
 3:4 
 864x1184 
 1290 
 
 
 4:3 
 1184x864 
 1290 
 
 
 4:5 
 896x1152 
 1290 
 
 
 5:4 
 1152x896 
 1290 
 
 
 9:16 
 768x1344 
 1290 
 
 
 16:9 
 1344x768 
 1290 
 
 
 21:9 
 1536x672 
 1290 
 
 
 

## When to use Imagen

In addition to using Gemini's built-in image generation capabilities, you can
also access Imagen , our specialized image generation
model, through the Gemini API.

 
 
 
 
 
 
 
 
 Attribute 
 Imagen 
 Gemini Native Image 
 
 
 
 
 Strengths 
 Most capable image generation model to date. Recommended for photorealistic images, sharper clarity, improved spelling and typography. 
 Default recommendation. 
Unparalleled flexibility, contextual understanding, and simple, mask-free editing. Uniquely capable of multi-turn conversational editing. 
 
 
 Availability 
 Generally available 
 Preview (Production usage allowed) 
 
 
 Latency 
 Low . Optimized for near-real-time performance. 
 Higher. More computation is required for its advanced capabilities. 
 
 
 Cost 
 Cost-effective for specialized tasks. $0.02/image to $0.12/image 
 Token-based pricing. $30 per 1 million tokens for image output (image output tokenized at 1290 tokens per image flat, up to 1024x1024px) 
 
 
 Recommended tasks 
 
 

 - Image quality, photorealism, artistic detail, or specific styles (e.g., impressionism, anime) are top priorities.

 - Infusing branding, style, or generating logos and product designs.

 - Generating advanced spelling or typography.

 

 
 
 

 - Interleaved text and image generation to seamlessly blend text and images.

 - Combine creative elements from multiple images with a single prompt.

 - Make highly specific edits to images, modify individual elements with simple language commands, and iteratively work on an image.

 - Apply a specific design or texture from one image to another while preserving the original subject's form and details.

 

 
 
 
 

Imagen 4 should be your go-to model starting to generate images with
Imagen. Choose Imagen 4 Ultra for advanced use-cases or
when you need the best image quality (note that can only generate one image at
a time).

## What's next

- Find more examples and code samples in the cookbook guide .

- Check out the Veo guide to learn how to generate
videos with the Gemini API.

- To learn more about Gemini models, see Gemini models .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Generate videos with Veo 3.1 in Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/video#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Generate videos with Veo 3.1 in Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Generate videos with Veo 3.1 in Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 Veo 3.1 is Google's state-of-the-art
model for generating high-fidelity, 8-second 720p or 1080p videos featuring
stunning realism and natively generated audio. You can access
this model programmatically using the Gemini API. To learn more about the
available Veo model variants, see the Model Versions section.

Veo 3.1 excels at a wide range of visual and cinematic styles and introduces
several new capabilities:

- Video extension : Extend videos that were previously
generated using Veo.

- Frame-specific generation : Generate a video by
specifying the first and last frames.

- Image-based direction : Use up to three reference images to guide
the content of your generated video.

For more information about writing effective text prompts for video generation,
see the Veo prompt guide 

## Text to video generation

Choose an example to see how to generate a video with dialogue, cinematic
realism, or creative animation:

 
 
 
 
 

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"""

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("dialogue_example.mp4")
print("Generated video saved to dialogue_example.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.1-generate-preview",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### REST

 

```
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

 
 

 

## Image to video generation

The following code demonstrates generating an image using
 Gemini 2.5 Flash Image aka Nano Banana ,
then using that image as the
starting frame for generating a video with Veo 3.1.

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Nano Banana.
image = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=prompt,
    config={"response_modalities":['IMAGE']}
)

# Step 2: Generate video with Veo 3.1 using the image.
operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    image=image.parts[0].as_image(),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3_with_image_input.mp4")
print("Generated video saved to veo3_with_image_input.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Nano Banana.
const imageResponse = await ai.models.generateContent({
  model: "gemini-2.5-flash-image",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3.1 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: prompt,
  image: {
    imageBytes: imageResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Nano Banana.
    imageResponse, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3.1 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        prompt,
        imageResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### Using reference images

Veo 3.1 now accepts up to 3 reference images to guide your generated video's
content. Provide images of a person, character, or product to
preserve the subject's appearance in the output video.

For example, using these three images generated with
 Nano Banana as references with a
 well-written prompt creates the following video:

 
 
 
 ``dress_image`` 
 ``woman_image`` 
 ``glasses_image`` 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "The video opens with a medium, eye-level shot of a beautiful woman with dark hair and warm brown eyes. She wears a magnificent, high-fashion flamingo dress with layers of pink and fuchsia feathers, complemented by whimsical pink, heart-shaped sunglasses. She walks with serene confidence through the crystal-clear, shallow turquoise water of a sun-drenched lagoon. The camera slowly pulls back to a medium-wide shot, revealing the breathtaking scene as the dress's long train glides and floats gracefully on the water's surface behind her. The cinematic, dreamlike atmosphere is enhanced by the vibrant colors of the dress against the serene, minimalist landscape, capturing a moment of pure elegance and high-fashion fantasy."

dress_reference = types.VideoGenerationReferenceImage(
  image=dress_image, # Generated separately with Nano Banana
  reference_type="asset"
)

sunglasses_reference = types.VideoGenerationReferenceImage(
  image=glasses_image, # Generated separately with Nano Banana
  reference_type="asset"
)

woman_reference = types.VideoGenerationReferenceImage(
  image=woman_image, # Generated separately with Nano Banana
  reference_type="asset"
)

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    config=types.GenerateVideosConfig(
      reference_images=[dress_reference, glasses_reference, woman_reference],
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_with_reference_images.mp4")
print("Generated video saved to veo3.1_with_reference_images.mp4")
```

 
 

 

### Using first and last frames

Veo 3.1 lets you create videos using interpolation, or specifying the first and
last frames of the video. For information about writing effective text prompts
for video generation, see the Veo prompt guide .

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "A cinematic, haunting video. A ghostly woman with long white hair and a flowing dress swings gently on a rope swing beneath a massive, gnarled tree in a foggy, moonlit clearing. The fog thickens and swirls around her, and she slowly fades away, vanishing completely. The empty swing is left swaying rhythmically on its own in the eerie silence."

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt=prompt,
    image=first_image, # Generated separately with Nano Banana
    config=types.GenerateVideosConfig(
      last_frame=last_image # Generated separately with Nano Banana
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_with_interpolation.mp4")
print("Generated video saved to veo3.1_with_interpolation.mp4")
```

 
 
 
 
 
 ``first_image`` 
 ``last_image`` 
 veo3.1_with_interpolation.mp4 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

## Extending Veo videos

Use Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds
and up to 20 times.

Input video limitations:

- Veo-generated videos only up to 141 seconds long.

- Gemini API only supports video extensions for Veo-generated videos.

- The video should come from a previous generation, like `operation.response.generated_videos[0].video`

- Input videos are expected to have a certain length, aspect ratio, and dimensions:

 Aspect ratio: 9:16 or 16:9

- Resolution: 720p

- Video length: 141 seconds or less

 

The output of the extension is a single video combining the user input video and
the generated extended video for up to 148 seconds of video.

This example takes the a Veo-generated video, shown here with
its original prompt, and extends it using the `video` parameter and a new
prompt:

 
 
 
 Prompt 
 Output: `butterfly_video` 
 
 
 
 
 
 An origami butterfly flaps its wings and flies out of the french doors into the garden.
 
 
 
 
 
 
 
 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

prompt = "Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower."

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    video=operation.response.generated_videos[0].video, # This must be a video from a previous generation
    prompt=prompt,
    config=types.GenerateVideosConfig(
        number_of_videos=1,
        resolution="720p"
    ),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3.1_extension.mp4")
print("Generated video saved to veo3.1_extension.mp4")
```

 
 

 

For information about writing effective text prompts for video generation, see
the Veo prompt guide .

## Handling asynchronous operations

Video generation is a computationally intensive task. When you send a request
to the API, it starts a long-running job and immediately returns an `operation`
object. You must then poll until the video is ready, which is indicated by the
`done` status being true.

The core of this process is a polling loop, which periodically checks the job's
status.

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)

# Once done, the result is in operation.response.
# ... process and download your video ...
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

// After starting the job, you get an operation object.
let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
});

// Alternatively, you can use operation.name to get the operation.
// operation = types.GenerateVideosOperation(name=operation.name)

// This loop checks the job status every 10 seconds.
while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    // Refresh the operation object to get the latest status.
    operation = await ai.operations.getVideosOperation({ operation });
}

// Once done, the result is in operation.response.
// ... process and download your video ...
```

 
 

## Veo API parameters and specifications

These are the parameters you can set in your API request to control the video
generation process.

 
 
 
 Parameter 
 Description 
 Veo 3.1 & Veo 3.1 Fast 
 Veo 3 & Veo 3 Fast 
 Veo 2 
 
 
 
 
 `prompt` 
 The text description for the video. Supports audio cues. 
 `string` 
 `string` 
 `string` 
 
 
 `negativePrompt` 
 Text describing what not to include in the video. 
 `string` 
 `string` 
 `string` 
 
 
 `image` 
 An initial image to animate. 
 `Image` object 
 `Image` object 
 `Image` object 
 
 
 `lastFrame` 
 The final image for an interpolation video to transition. Must be used in combination with the `image` parameter. 
 `Image` object 
 `Image` object 
 `Image` object 
 
 
 `referenceImages` 
 Up to three images to be used as style and content references. 
 `VideoGenerationReferenceImage` object (Veo 3.1 only) 
 n/a 
 n/a 
 
 
 `video` 
 Video to be used for video extension. 
 `Video` object 
 n/a 
 n/a 
 
 
 `aspectRatio` 
 The video's aspect ratio. 
 `"16:9"` (default, 720p & 1080p),
`"9:16"`(720p & 1080p)

 
 `"16:9"` (default, 720p & 1080p),
`"9:16"` (720p & 1080p) 
 `"16:9"` (default, 720p),
`"9:16"` (720p) 
 
 
 `resolution` 
 The video's aspect ratio. 
 `"720p"` (default), 
`"1080p"` (only supports 8s duration)

 `"720p"` only for extension 
 `"720p"` (default), 
`"1080p"` (16:9 only) 
 Unsupported 
 
 
 `durationSeconds` 
 Length of the generated video. 
 `"4"`, `"6"`, `"8"`.

 Must be "8" when using extension or interpolation (supports both 16:9 and 9:16), and when using `referenceImages` (only supports 16:9) 
 `"4"`, `"6"`, `"8"` 
 `"5"`, `"6"`, `"8"` 
 
 
 `personGeneration` 
 Controls the generation of people.

 (See Limitations for region restrictions) 
 
 Text-to-video & Extension:
`"allow_all"` only

 Image-to-video, Interpolation, & Reference images:
`"allow_adult"` only
 
 
 Text-to-video:
`"allow_all"` only

 Image-to-video:
`"allow_adult"` only
 
 
 Text-to-video: 
`"allow_all"`, `"allow_adult"`, `"dont_allow"`
 
Image-to-video: 
`"allow_adult"`, and `"dont_allow"`
 
 
 
 

Note that the `seed` parameter is also available for Veo 3 models.
It doesn't guarantee determinism, but slightly improves it.

You can customize your video generation by setting parameters in your request.
For example you can specify `negativePrompt` to guide the model.

 
 

### Python

 

```
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-3.1-generate-preview",
    prompt="A cinematic shot of a majestic lion in the savannah.",
    config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let operation = await ai.models.generateVideos({
  model: "veo-3.1-generate-preview",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

 
 

### Go

 

```
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.1-generate-preview",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "parameters_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

 
 

### REST

 

```
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

 
 

## Veo prompt guide

This section contains examples of videos you can create using Veo, and shows you
how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that
generated videos and uploaded photos don't contain offensive content.
Prompts that violate our terms and guidelines are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get the most out of Veo, start with
identifying your core idea, refine your idea by adding keywords and modifiers,
and incorporate video-specific terminology into your prompts.

The following elements should be included in your prompt:

- Subject : The object, person, animal, or scenery that you want in your
video, such as cityscape , nature , vehicles , or puppies .

- Action : What the subject is doing (for example, walking , running , or
 turning their head ).

- Style : Specify creative direction using specific film
style keywords, such as sci-fi , horror film , film noir , or animated
styles like cartoon .

- Camera positioning and motion : [Optional] Control the camera's location
and movement using terms like aerial view , eye-level , top-down shot ,
 dolly shot , or worms eye .

- Composition : [Optional] How the shot is framed, such as wide shot ,
 close-up , single-shot or two-shot .

- Focus and lens effects : [Optional] Use terms like shallow focus ,
 deep focus , soft focus , macro lens , and wide-angle lens to achieve
specific visual effects.

- Ambiance : [Optional] How the color and light contribute to the scene,
such as blue tones , night , or warm tones .

#### More tips for writing prompts

- Use descriptive language : Use adjectives and adverbs to paint a clear
picture for Veo.

- Enhance the facial details : Specify
facial details as a focus of the photo like using the word portrait in
the prompt.

 For more comprehensive prompting strategies, visit Introduction to
prompt design . 

### Prompting for audio

With Veo 3, you can provide cues for sound effects, ambient noise, and dialogue.
The model captures the nuance of these cues to generate a synchronized
soundtrack.

- Dialogue: Use quotes for specific speech. (Example: "This must be the
key," he murmured.)

- Sound Effects (SFX): Explicitly describe sounds. (Example: tires
screeching loudly, engine roaring.)

- Ambient Noise: Describe the environment's soundscape. (Example: A faint,
eerie hum resonates in the background.)

These videos demonstrate prompting Veo 3's audio generation with increasing
levels of detail.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 More detail (Dialogue and ambience) 
A wide shot of a misty Pacific Northwest forest. Two exhausted hikers, a man and a woman, push through ferns when the man stops abruptly, staring at a tree. Close-up: Fresh, deep claw marks are gouged into the tree's bark. Man: (Hand on his hunting knife) "That's no ordinary bear." Woman: (Voice tight with fear, scanning the woods) "Then what is it?" A rough bark, snapping twigs, footsteps on the damp earth. A lone bird chirps. 
 
 
 
 
 
 Less detail (Dialogue) 
Paper Cut-Out Animation. New Librarian: "Where do you keep the forbidden books?" Old Curator: "We don't. They keep us." 
 
 
 
 
 
 

Try out these prompts yourself to hear the audio!

 Try Veo 3 

### Prompting with reference images

You can use one or more images as inputs to guide your generated videos, using
Veo's image-to-video 
capabilities. Veo uses the input image as the initial frame. Select an image
closest to what you envision as the first scene of your video to animate
everyday objects, bring drawings and paintings to life, and add movement and
sound to nature scenes.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Input image (Generated by Nano Banana) 
A hyperrealistic macro photo of tiny, miniature surfers riding ocean waves inside a rustic stone bathroom sink. A vintage brass faucet is running, creating the perpetual surf. Surreal, whimsical, bright natural lighting. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
A surreal, cinematic macro video. Tiny surfers ride perpetual, rolling waves inside a stone bathroom sink. A running vintage brass faucet generates the endless surf. The camera slowly pans across the whimsical, sunlit scene as the miniature figures expertly carve the turquoise water. 
 
 
 
 
 
 

Veo 3.1 lets you reference images or ingredients to direct your generated
video's content. Provide up to three asset images of a single person, character,
or product. Veo preserves the subject's appearance in the output video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Reference image (Generated by Nano Banana) 
A deep sea angler fish lurks in the deep dark water, teeth bared and bait glowing. 
 
 
 
 
 
 Reference image (Generated by Nano Banana) 
A pink child's princess costume complete with a wand and tiara, on a plain product background. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Create a silly cartoon version of the fish wearing the costume, swimming and waving the wand around. 
 
 
 
 
 
 

Using Veo 3.1, you can also generate videos by specifying the first and last
frames of the video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 First image (Generated by Nano Banana) 
A high quality photorealistic front image of a ginger cat driving a red convertible racing car on the French riviera coast. 
 
 
 
 
 
 Last image (Generated by Nano Banana) 
Show what happens when the car takes off from a cliff. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Optional 
 
 
 
 
 
 

This feature gives you precise control over your shot's composition by letting
you define the starting and ending frame. Upload an image or use a frame from a
previous video generation to make sure your scene begins and concludes exactly
as you envision it.

### Prompting for extension

To extend your Veo-generated video with Veo 3.1, use the video as an input along
with an optional text prompt. Extend finalizes the final second or 24 frames of
your video and continues the action.

Note that voice is not able to be effectively extended if it's not present in
the last 1 second of video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Input video (Generated by Veo 3.1) 
The paraglider takes off from the top of the mountain and starts gliding down the mountains overlooking the flower covered valleys below. 
 
 
 
 
 
 Output Video (Generated by Veo 3.1) 
Extend this video with the paraglider slowly descending. 
 
 
 
 
 
 

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can
elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of
 prompt writing basics in your prompt.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Close up shot (composition) of melting icicles (subject) on a frozen
 rock wall (context) with cool blue tones (ambiance), zoomed in
 (camera motion) maintaining close-up detail of water drips (action).
 
 
 
 
 
 
 

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly
specific details to get Veo to refine the output to your liking.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Less detail 
The camera dollies to show a close up of a desperate man in
 a green trench coat. He's making a call on a rotary-style wall phone with a
 green neon light. It looks like a movie scene. 
 
 
 
 
 
 More detail 
A close-up cinematic
 shot follows a desperate man in a weathered green trench coat as he dials a
 rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a
 green neon sign. The camera dollies in, revealing the tension in his jaw and
 the desperation etched on his face as he struggles to make the call. The
 shallow depth of field focuses on his furrowed brow and the black rotary
 phone, blurring the background into a sea of neon colors and indistinct
 shadows, creating a sense of urgency and isolation. 
 
 
 
 
 
 

#### Snow leopard

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Simple prompt: 
A cute creature with snow leopard-like fur is walking in winter
forest, 3D cartoon style render. 
 
 
 
 
 
 Detailed prompt: 
Create a short 3D animated scene in a joyful cartoon style. A cute
creature with snow leopard-like fur, large expressive eyes, and a friendly,
rounded form happily prances through a whimsical winter forest. The scene should
feature rounded, snow-covered trees, gentle falling snowflakes, and warm
sunlight filtering through the branches. The creature's bouncy movements and
wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with
bright, cheerful colors and playful animation. 
 
 
 
 
 
 

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject and context

Specify the main focus (subject) and the background or environment (context).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements 
 
 
 
 
 
 A satellite floating through outer space with the moon and some
stars in the background. 
 
 
 
 
 
 

#### Action

Specify what the subject is doing (e.g., walking, running, or turning their
head).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A wide shot of a woman walking along the beach, looking content and
relaxed towards the horizon at sunset. 
 
 
 
 
 
 

#### Style

Add keywords to steer the generation toward a specific aesthetic (e.g., surreal,
vintage, futuristic, film noir).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Film noir style, man and woman walk on the street, mystery,
cinematic, black and white. 
 
 
 
 
 
 

#### Camera motion and composition

Specify how the camera moves (POV shot, aerial view, tracking drone view) and
how the shot is framed (wide shot, close-up, low angle).

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A POV shot from a vintage car driving in the rain, Canada at night,
cinematic. 
 
 
 
 
 
 Extreme close-up of a an eye with city reflected in it. 
 
 
 
 
 
 

#### Ambiance

Color palettes and lighting influence the mood. Try terms like "muted orange
warm tones," "natural light," "sunrise," or "cool blue tones."

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 A close-up of a girl holding adorable golden retriever puppy in the park, sunlight. 
 
 
 
 
 
 Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood. 
 
 
 
 
 
 

### Negative prompts

Negative prompts specify elements you don't want in the video.

- ‚ùå Don't use instructive language like no or don't . (e.g., "No walls").

- ‚úÖ Do describe what you don't want to see. (e.g., "wall, frame").

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Without Negative Prompt: 
Generate a short, stylized animation of a large, solitary oak tree
with leaves blowing vigorously in a strong wind... [truncated] 
 
 
 
 
 
 With Negative Prompt: 
[Same prompt]

Negative prompt: urban background, man-made structures,
dark, stormy, or threatening atmosphere. 
 
 
 
 
 
 

### Aspect ratios

Veo lets you specify the aspect ratio for your video.

 
 
 
 Prompt 
 Generated output 
 
 
 
 
 Widescreen (16:9) 
Create a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows. 
 
 
 
 
 
 Portrait (9:16) 
Create a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest. 
 
 
 
 
 
 

## Limitations

- Request latency: Min: 11 seconds; Max: 6 minutes (during peak hours).

- Regional limitations: In EU, UK, CH, MENA locations, the following
are the allowed values for `personGeneration`:

 Veo 3: `allow_adult` only.

- Veo 2: `dont_allow` and `allow_adult`. Default is `dont_allow`.

 
- Video retention: Generated videos are stored on the server for 2 days,
after which they are removed. To save a local copy, you must download your
video within 2 days of generation. Extended videos are treated as newly
generated videos.

- Watermarking: Videos created by Veo are watermarked using SynthID , our tool for watermarking
and identifying AI-generated content. Videos can be verified using the
 SynthID verification platform.

- Safety: Generated videos are passed through safety filters and
memorization checking processes that help mitigate privacy, copyright and
bias risks.

- Audio error: Veo 3.1 will sometimes block a video from generating
because of safety filters or other processing issues with the audio. You
will not be charged if your video is blocked from generating.

## Model features

 
 
 
 Feature 
 Description 
 Veo 3.1 & Veo 3.1 Fast 
 Veo 3 & Veo 3 Fast 
 Veo 2 
 
 
 
 
 Audio 
 Natively generates audio with video. 
 Natively generates audio with video. 
 ‚úîÔ∏è Always on 
 ‚ùå Silent only 
 
 
 Input Modalities 
 The type of input used for generation. 
 Text-to-Video, Image-to-Video, Video-to-Video 
 Text-to-Video, Image-to-Video 
 Text-to-Video, Image-to-Video 
 
 
 Resolution 
 The output resolution of the video. 
 720p & 1080p (8s length only) 

720p only when using video extension. 
 720p & 1080p (16:9 only) 
 720p 
 
 
 Frame Rate 
 The output frame rate of the video. 
 24fps 
 24fps 
 24fps 
 
 
 Video Duration 
 Length of the generated video. 
 8 seconds, 6 seconds, 4 seconds 
8 seconds only when using reference images 
 8 seconds 
 5-8 seconds 
 
 
 Videos per Request 
 Number of videos generated per request. 
 1 
 1 
 1 or 2 
 
 
 Status & Details 
 Model availability and further details. 
 Preview 
 Stable 
 Stable 
 
 
 

## Model versions

Check out the Pricing and Rate limits pages for more Veo model-specific usage
details.

Veo Fast versions allow developers to create videos with sound while maintaining
high quality and optimizing for speed and business use cases. They're ideal for
backend services that programmatically generate ads, tools for rapid A/B testing
of creative concepts, or apps that need to quickly produce social media content.

 
 
 
 

### Veo 3.1 Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.1-generate-preview`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 
 
 

### Veo 3.1 Fast Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.1-fast-generate-preview`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 
 
 

### Veo 3

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.0-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 
 
 

### Veo 3 Fast

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-3.0-fast-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, Image

 
 
 

 Output 

 

Video with audio

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

1,024 tokens

 
 
 

 Output video 

 

1

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 
 
 

### Veo 2

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`veo-2.0-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text, image

 
 
 

 Output 

 

Video

 
 
 
 
 token_auto Limits 
 
 
 

 Text input 

 

N/A

 
 
 

 Image input 

 

Any image resolution and aspect ratio up to 20MB file size

 
 
 

 Output video 

 

Up to 2

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 
 
 

## What's next

- Get started with the Veo 3.1 API by experimenting in the Veo Quickstart Colab 
and the Veo 3.1 applet .

- Learn how to write even better prompts with our Introduction to prompt design .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-13 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-13 UTC."],[],[]]

---

### Image understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/image-understanding

- 
 
 
 
 
 
 
 
 
 
 
 Image understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Image understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

- Passing inline image data : Ideal for smaller files (total request
size less than 20MB, including prompts).

- Uploading images using the File API : Recommended for larger files or for
reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the
request to `generateContent`. You can provide image data as Base64 encoded
strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass
it to `generateContent` API for processing.

 
 

### Python

 

```
  from google import genai
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  client = genai.Client()
  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### Go

 

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

 
 

You can also fetch an image from a URL, convert it to bytes, and pass it to
`generateContent` as shown in the following examples.

 
 

### Python

 

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

 
 

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the
Files API. The following code uploads an image file and then uses the file in a
call to `generateContent`. See the Files API guide for
more information and examples.

 
 

### Python

 

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image
`Part` objects in the `contents` array. These can be a mix of inline data
(local files or URLs) and File API references.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an
image and get their bounding box coordinates. The coordinates, relative to image
dimensions, scale to [0, 1000]. You need to descale these coordinates based on
your original image size.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  )

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)
```

 
 

For more examples, check following notebooks in the Gemini Cookbook :

- 2D spatial understanding notebook 

- Experimental 3D pointing notebook 

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them
and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask.
Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with
normalized coordinates between 0 and 1000, a label ("`label`") that identifies
the object, and finally the segmentation mask inside the bounding box, as base64
encoded png that is a probability map with values between 0 and 255.
The mask needs to be resized to match the bounding box dimensions, then
binarized at your confidence threshold (127 for the midpoint).

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw
import io
import base64
import json
import numpy as np
import os

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
    if line == "```json":
      json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
      output = json_output.split("```")[0]  # Remove everything after the closing "```"
      break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  )

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")
```

 
 

Check the
 segmentation example 
in the cookbook guide for a more detailed example.

 
 
 An example segmentation output with objects and segmentation masks 
 

## Supported image formats

Gemini supports the following image format MIME types:

- PNG - `image/png`

- JPEG - `image/jpeg`

- WEBP - `image/webp`

- HEIC - `image/heic`

- HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of
image processing and computer vision tasks including but not limited to image captioning,
visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

- 

 Gemini 2.0 models are further trained to support enhanced object detection .

- 

 Gemini 2.5 models are further trained to support enhanced segmentation in addition to object detection .

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a
maximum of 3,600 image files per request.

### Token calculation

- Gemini 1.5 Flash and Gemini 1.5 Pro : 258 tokens if both dimensions
<= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized
to 768x768), with each tile costing 258 tokens.

- Gemini 2.0 Flash and Gemini 2.5 Flash/Pro : 258 tokens if both dimensions <= 384 pixels.
 Larger images are tiled into 768x768 pixel tiles, each costing 258
 tokens.

A rough formula for calculating the number of tiles is as follows:

- Calculate the crop unit size which is roughly: floor(min(width, height) / 1.5).

- Divide each dimension by the crop unit size and multiply together to get the
number of tiles.

For example, for an image of dimensions 960x540 would have a crop unit size
of 360. Divide each dimension by 360 and the number of tile is 3 * 2 = 6.

### Media resolution

Gemini 3 introduces granular control over multimodal vision processing with the
`media_resolution` parameter. The `media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to
read fine text or identify small details, but increase token usage and latency.

For more details about the parameter and how it can impact token calculations,
see the media resolution guide.

## Tips and best practices

- Verify that images are correctly rotated.

- Use clear, non-blurry images.

- When using a single image with text, place the text prompt after the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image
inputs. To learn more, see the following resources:

- Files API : Learn more about uploading and managing files for use with Gemini.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- Safety guidance : Sometimes generative
AI models produce unexpected outputs, such as outputs that are inaccurate,
biased, or offensive. Post-processing and human evaluation are essential to
limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Music generation using Lyria RealTime &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/music-generation#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Music generation using Lyria RealTime  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Music generation using Lyria RealTime 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API, using
 Lyria RealTime ,
provides access to a state-of-the-art, real-time, streaming music
generation model. It allows developers to build applications where users
can interactively create, continuously steer, and perform instrumental
music.

To experience what can be built using Lyria RealTime, try it on AI Studio
using the Prompt DJ or the
 MIDI DJ apps!

## How music generation works

Lyria RealTime music generation uses a persistent, bidirectional,
low-latency streaming connection using
 WebSocket .

## Generate and control music

Lyria RealTime works a bit like the Live API 
in the sense that it is using websockets to keep a real-time communication with
the model. It's still not exactly the same as you can't talk to the model and
you have to use a specific format to prompt it.

The following code demonstrates how to generate music:

 
 

### Python

This example initializes the Lyria RealTime session using
`client.aio.live.music.connect()`, then sends an
initial prompt with `session.set_weighted_prompts()` along with an initial
configuration using `session.set_music_generation_config`, starts the music
generation using `session.play()` and sets up
`receive_audio()` to process the audio chunks it receives.

 

```
  import asyncio
  from google import genai
  from google.genai import types

  client = genai.Client(http_options={'api_version': 'v1alpha'})

  async def main():
      async def receive_audio(session):
        """Example background task to process incoming audio."""
        while True:
          async for message in session.receive():
            audio_data = message.server_content.audio_chunks[0].data
            # Process audio...
            await asyncio.sleep(10**-12)

      async with (
        client.aio.live.music.connect(model='models/lyria-realtime-exp') as session,
        asyncio.TaskGroup() as tg,
      ):
        # Set up task to receive server messages.
        tg.create_task(receive_audio(session))

        # Send initial prompts and config
        await session.set_weighted_prompts(
          prompts=[
            types.WeightedPrompt(text='minimal techno', weight=1.0),
          ]
        )
        await session.set_music_generation_config(
          config=types.LiveMusicGenerationConfig(bpm=90, temperature=1.0)
        )

        # Start streaming music
        await session.play()
  if __name__ == "__main__":
      asyncio.run(main())
```

 
 
 
 
 

### JavaScript

This example initializes the Lyria RealTime session using
`client.live.music.connect()`, then sends an
initial prompt with `session.setWeightedPrompts()` along with an initial
configuration using `session.setMusicGenerationConfig`, starts the music
generation using `session.play()` and sets up an
`onMessage` callback to process the audio chunks it receives.

 

```
import { GoogleGenAI } from "@google/genai";
import Speaker from "speaker";
import { Buffer } from "buffer";

const client = new GoogleGenAI({
  apiKey: GEMINI_API_KEY,
    apiVersion: "v1alpha" ,
});

async function main() {
  const speaker = new Speaker({
    channels: 2,       // stereo
    bitDepth: 16,      // 16-bit PCM
    sampleRate: 44100, // 44.1 kHz
  });

  const session = await client.live.music.connect({
    model: "models/lyria-realtime-exp",
    callbacks: {
      onmessage: (message) => {
        if (message.serverContent?.audioChunks) {
          for (const chunk of message.serverContent.audioChunks) {
            const audioBuffer = Buffer.from(chunk.data, "base64");
            speaker.write(audioBuffer);
          }
        }
      },
      onerror: (error) => console.error("music session error:", error),
      onclose: () => console.log("Lyria RealTime stream closed."),
    },
  });

  await session.setWeightedPrompts({
    weightedPrompts: [
      { text: "Minimal techno with deep bass, sparse percussion, and atmospheric synths", weight: 1.0 },
    ],
  });

  await session.setMusicGenerationConfig({
    musicGenerationConfig: {
      bpm: 90,
      temperature: 1.0,
      audioFormat: "pcm16",  // important so we know format
      sampleRateHz: 44100,
    },
  });

  await session.play();
}

main().catch(console.error);
```

 
 

 
 

You can then use `session.play()`, `session.pause()`, `session.stop()` and
`session.reset_context()` to start, pause, stop or reset the session.

## Steer music in real-time

### Prompt Lyria RealTime

While the stream is active, you can send new `WeightedPrompt` messages at any
time to alter the generated music. The model will smoothly transition based
on the new input.

The prompts need to follow the right format with a `text` (the
actual prompt), and a `weight`. The `weight` can take any value except `0`. `1.0`
is usually a good starting point.

 
 

### Python

 

```
  from google.genai import types

  await session.set_weighted_prompts(
    prompts=[
      {"text": "Piano", "weight": 2.0},
      types.WeightedPrompt(text="Meditation", weight=0.5),
      types.WeightedPrompt(text="Live Performance", weight=1.0),
    ]
  )
```

 
 

### JavaScript

 

```
  await session.setMusicGenerationConfig({
    weightedPrompts: [
      { text: 'Harmonica', weight: 0.3 },
      { text: 'Afrobeat', weight: 0.7 }
    ],
  });
```

 
 

Note that the model transitions can be a bit abrupt when drastically changing
the prompts so it's recommended to implement some kind of cross-fading by
sending intermediate weight values to the model.

### Update the configuration

You can also update the music generation parameters in real time. You can't just
update a parameter, you need to set the whole configuration otherwise the other
fields will be reset back to their default values.

Since updating the bpm or the scale is a drastic change for the model you'll
also need to tell it to reset its context using `reset_context()` to take the
new config into account. It won't stop the stream, but it will be a hard
transition. You don't need to do it for the other parameters.

 
 

### Python

 

```
  from google.genai import types

  await session.set_music_generation_config(
    config=types.LiveMusicGenerationConfig(
      bpm=128,
      scale=types.Scale.D_MAJOR_B_MINOR,
      music_generation_mode=types.MusicGenerationMode.QUALITY
    )
  )
  await session.reset_context();
```

 
 

### JavaScript

 

```
  await session.setMusicGenerationConfig({
    musicGenerationConfig: { 
      bpm: 120,
      density: 0.75,
      musicGenerationMode: MusicGenerationMode.QUALITY
    },
  });
  await session.reset_context();
```

 
 

## Prompt guide for Lyria RealTime

Here's a non-exhaustive list of prompts you can use to prompt Lyria RealTime:

- Instruments: 

```
303 Acid Bass, 808 Hip Hop Beat, Accordion, Alto Saxophone,
Bagpipes, Balalaika Ensemble, Banjo, Bass Clarinet, Bongos, Boomy Bass,
Bouzouki, Buchla Synths, Cello, Charango, Clavichord, Conga Drums,
Didgeridoo, Dirty Synths, Djembe, Drumline, Dulcimer, Fiddle, Flamenco
Guitar, Funk Drums, Glockenspiel, Guitar, Hang Drum, Harmonica, Harp,
Harpsichord, Hurdy-gurdy, Kalimba, Koto, Lyre, Mandolin, Maracas, Marimba,
Mbira, Mellotron, Metallic Twang, Moog Oscillations, Ocarina, Persian Tar,
Pipa, Precision Bass, Ragtime Piano, Rhodes Piano, Shamisen, Shredding
Guitar, Sitar, Slide Guitar, Smooth Pianos, Spacey Synths, Steel Drum, Synth
Pads, Tabla, TR-909 Drum Machine, Trumpet, Tuba, Vibraphone, Viola Ensemble,
Warm Acoustic Guitar, Woodwinds, ...
```



- Music Genre: 

```
Acid Jazz, Afrobeat, Alternative Country, Baroque, Bengal Baul,
Bhangra, Bluegrass, Blues Rock, Bossa Nova, Breakbeat, Celtic Folk, Chillout,
Chiptune, Classic Rock, Contemporary R&B, Cumbia, Deep House, Disco Funk,
Drum & Bass, Dubstep, EDM, Electro Swing, Funk Metal, G-funk, Garage Rock,
Glitch Hop, Grime, Hyperpop, Indian Classical, Indie Electronic, Indie Folk,
Indie Pop, Irish Folk, Jam Band, Jamaican Dub, Jazz Fusion, Latin Jazz, Lo-Fi
Hip Hop, Marching Band, Merengue, New Jack Swing, Minimal Techno, Moombahton,
Neo-Soul, Orchestral Score, Piano Ballad, Polka, Post-Punk, 60s Psychedelic
Rock, Psytrance, R&B, Reggae, Reggaeton, Renaissance Music, Salsa, Shoegaze,
Ska, Surf Rock, Synthpop, Techno, Trance, Trap Beat, Trip Hop, Vaporwave,
Witch house, ...
```



- Mood/Description: 

```
Acoustic Instruments, Ambient, Bright Tones, Chill,
Crunchy Distortion, Danceable, Dreamy, Echo, Emotional, Ethereal Ambience,
Experimental, Fat Beats, Funky, Glitchy Effects, Huge Drop, Live Performance,
Lo-fi, Ominous Drone, Psychedelic, Rich Orchestration, Saturated Tones,
Subdued Melody, Sustained Chords, Swirling Phasers, Tight Groove,
Unsettling, Upbeat, Virtuoso, Weird Noises, ...
```



These are just some examples, Lyria RealTime can do much more. Experiment
with your own prompts!

## Best practices

- Client applications must implement robust audio buffering to ensure smooth
playback. This helps account for network jitter and slight variations in
generation latency.

- Effective prompting:

 Be descriptive. Use adjectives describing mood, genre, and instrumentation.

- Iterate and steer gradually. Rather than completely changing the prompt,
try adding or modifying elements to morph the music more smoothly.

- Experiment with weight on `WeightedPrompt` to influence how strongly a new
prompt affects the ongoing generation.

 

## Technical details

This section describes the specifics of how to use Lyria RealTime music
generation.

### Specifications

- Output format: Raw 16-bit PCM Audio

- Sample rate: 48kHz

- Channels: 2 (stereo)

### Controls

Music generation can be influenced in real time by sending messages containing:

- `WeightedPrompt`: A text string describing a musical idea, genre, instrument,
mood, or characteristic. Multiple prompts can potentially be supplied to blend
influences. See above for more details on how to best prompt
Lyria RealTime.

- `MusicGenerationConfig`: Configuration for the music generation process,
influencing the characteristics of the output audio.). Parameters
include:

 `guidance`: (float) Range: `[0.0, 6.0]`. Default: `4.0`.
Controls how strictly the model follows the prompts. Higher guidance
improves adherence to the prompt, but makes transitions more abrupt.

- `bpm`: (int) Range: `[60, 200]`.
Sets the Beats Per Minute you want for the generated music. You need to
stop/play or reset the context for the model it take into account the new
bpm.

- `density`: (float) Range: `[0.0, 1.0]`.
Controls the density of musical notes/sounds. Lower values produce sparser
music; higher values produce "busier" music.

- `brightness`: (float) Range: `[0.0, 1.0]`.
Adjusts the tonal quality. Higher values produce "brighter" sounding
audio, generally emphasizing higher frequencies.

- `scale`: (Enum)
Sets the musical scale (Key and Mode) for the generation. Use the
 `Scale` enum values provided by the SDK. You need to
stop/play or reset the context for the model it take into account the new
scale.

- `mute_bass`: (bool) Default: `False`.
Controls whether the model reduces the outputs' bass.

- `mute_drums`: (bool) Default: `False`.
Controls whether the model outputs reduces the outputs' drums.

- `only_bass_and_drums`: (bool) Default: `False`.
Steer the model to try to only output bass and drums.

- `music_generation_mode`: (Enum)
Indicates to the model if it should focus on `QUALITY` (default value) or
`DIVERSITY` of music. It can also be set to `VOCALIZATION` to let the
model generate vocalizations as another instrument (add them as new
pompts).

 
- `PlaybackControl`: Commands to control playback aspects, such as play, pause,
stop or reset the context.

For `bpm`, `density`, `brightness` and `scale`, if no value is provided, the
model will decide what's best according to your initial prompts.

More classical parameters like `temperature` (0.0 to 3.0, default 1.1), `top_k`
(1 to 1000, default 40), and `seed` (0 to 2 147 483 647, randomly selected by
default) are also customizable in the `MusicGenerationConfig`.

#### Scale Enum Values

Here are all the scale values that the model can accept:

 
 
 
 Enum Value 
 Scale / Key 
 
 

 
 
 `C_MAJOR_A_MINOR` 
 C major / A minor 
 
 
 `D_FLAT_MAJOR_B_FLAT_MINOR` 
 D‚ô≠ major / B‚ô≠ minor 
 
 
 `D_MAJOR_B_MINOR` 
 D major / B minor 
 
 
 `E_FLAT_MAJOR_C_MINOR` 
 E‚ô≠ major / C minor 
 
 
 `E_MAJOR_D_FLAT_MINOR` 
 E major / C‚ôØ/D‚ô≠ minor 
 
 
 `F_MAJOR_D_MINOR` 
 F major / D minor 
 
 
 `G_FLAT_MAJOR_E_FLAT_MINOR` 
 G‚ô≠ major / E‚ô≠ minor 
 
 
 `G_MAJOR_E_MINOR` 
 G major / E minor 
 
 
 `A_FLAT_MAJOR_F_MINOR` 
 A‚ô≠ major / F minor 
 
 
 `A_MAJOR_G_FLAT_MINOR` 
 A major / F‚ôØ/G‚ô≠ minor 
 
 
 `B_FLAT_MAJOR_G_MINOR` 
 B‚ô≠ major / G minor 
 
 
 `B_MAJOR_A_FLAT_MINOR` 
 B major / G‚ôØ/A‚ô≠ minor 
 
 
 `SCALE_UNSPECIFIED` 
 Default / The model decides 
 
 
 

The model is capable of guiding the notes that are played, but does
not distinguish between relative keys. Thus each enum corresponds both to the
relative major and minor. For example, `C_MAJOR_A_MINOR` would correspond to all
the white keys of a piano, and `F_MAJOR_D_MINOR` would be all the white keys
except B flat.

### Limitations

- Instrumental only: The model generates instrumental music only.

- Safety: Prompts are checked by safety filters. Prompts triggering the filters
will be ignored in which case an explanation will be written in the output's
`filtered_prompt` field.

- Watermarking: Output audio is always watermarked for identification following
our Responsible AI principles.

## What's next

- Instead of music, learn how to generate multi-speakers conversation using 
the TTS models ,

- Discover how to generate images or videos ,

- Instead of generation music or audio, find out how to Gemini can
 understand Audio files ,

- Have a real-time conversation with Gemini using the
 Live API .

Explore the Cookbook for more
code examples and tutorials.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-26 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-26 UTC."],[],[]]

---

### Video understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/video-understanding

- 
 
 
 
 
 
 
 
 
 
 
 Video understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Video understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models can process videos, enabling many frontier developer use cases
that would have historically required domain specific models.
Some of Gemini's vision capabilities include the ability to:

- Describe, segment, and extract information from videos

- Answer questions about video content

- Refer to specific timestamps within a video

Gemini was built to be multimodal from the ground up and we continue to push the
frontier of what is possible. This guide shows how to use the Gemini API to
generate text responses based on video inputs.

## Video input

You can provide videos as input to Gemini in the following ways:

- Upload a video file using the File API before making a
request to `generateContent`. Use this method for files larger than 20MB, videos
longer than approximately 1 minute, or when you want to reuse the file across
multiple requests.

- Pass inline video data with the request to `generateContent`. Use this method for smaller files (<20MB) and shorter durations.

- Pass YouTube URLs as part of your `generateContent` request.

### Upload a video file

You can use the Files API to upload a video file.
Always use the Files API when the total request size (including the file, text
prompt, system instructions, etc.) is larger than 20 MB, the video duration is
significant, or if you intend to use the same video in multiple prompts.
The File API accepts video file formats directly.

The following code downloads the sample video, uploads it using the File API,
waits for it to be processed, and then uses the file reference in
a `generateContent` request.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp4")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=[myfile, "Summarize this video. Then create a quiz with an answer key based on the information in this video."]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp4",
    config: { mimeType: "video/mp4" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.mp4", nil)

parts := []*genai.Part{
    genai.NewPartFromText("Summarize this video. Then create a quiz with an answer key based on the information in this video."),
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
VIDEO_PATH="path/to/sample.mp4"
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
DISPLAY_NAME=VIDEO

tmp_header_file=upload-header.tmp

echo "Starting file upload..."
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D ${tmp_header_file} \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

echo "Uploading video data..."
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

echo "File uploaded successfully. File URI: ${file_uri}"

# --- 3. Generate content using the uploaded video file ---
echo "Generating content from video..."
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Summarize this video. Then create a quiz with an answer key based on the information in this video."}]
        }]
      }' 2> /dev/null > response.json

jq -r ".candidates[].content.parts[].text" response.json
```

 
 

To learn more about working with media files, see
 Files API .

### Pass video data inline

Instead of uploading a video file using the File API, you can pass smaller
videos directly in the request to `generateContent`. This is suitable for
shorter videos under 20MB total request size.

Here's an example of providing inline video data:

 
 

### Python

 

```
from google import genai
from google.genai import types

# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "video/mp4",
      data: base64VideoFile,
    },
  },
  { text: "Please summarize the video in 3 sentences." }
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### REST

 

```
VIDEO_PATH=/path/to/your/video.mp4

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"video/mp4",
                "data": "'$(base64 $B64FLAGS $VIDEO_PATH)'"
              }
            },
            {"text": "Please summarize the video in 3 sentences."}
        ]
      }]
    }' 2> /dev/null
```

 
 

### Pass YouTube URLs

You can pass YouTube URLs directly to Gemini API as part of your `generateContent`request as follows:

 
 

### Python

 

```
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
const result = await model.generateContent([
  "Please summarize the video in 3 sentences.",
  {
    fileData: {
      fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
    },
  },
]);
console.log(result.response.text());
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  parts := []*genai.Part{
      genai.NewPartFromText("Please summarize the video in 3 sentences."),
      genai.NewPartFromURI("https://www.youtube.com/watch?v=9hE5-98ZeCg","video/mp4"),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Please summarize the video in 3 sentences."},
            {
              "file_data": {
                "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
              }
            }
        ]
      }]
    }' 2> /dev/null
```

 
 

 Limitations: 

- For the free tier, you can't upload more than 8 hours of YouTube video per day.

- For the paid tier, there is no limit based on video length.

- For models prior to Gemini 2.5, you can upload only 1 video per request. For Gemini 2.5 and later models, you can upload a maximum of 10 videos per request.

- You can only upload public videos (not private or unlisted videos).

## Refer to timestamps in the content

You can ask questions about specific points in time within the video using
timestamps of the form `MM:SS`.

 
 

### Python

 

```
prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?" # Adjusted timestamps for the NASA video
```

 
 

### JavaScript

 

```
const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";
```

 
 

### Go

 

```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
         // Adjusted timestamps for the NASA video
        genai.NewPartFromText("What are the examples given at 00:05 and " +
            "00:10 supposed to show us?"),
    }
```

 
 

### REST

 

```
PROMPT="What are the examples given at 00:05 and 00:10 supposed to show us?"
```

 
 

## Extract detailed insights from video

Gemini models offer powerful capabilities for understanding video content by
processing information from both the audio and visual streams. This lets you
extract a rich set of details, including generating descriptions of what is
happening in a video and answering questions about its content. For visual
descriptions, the model samples the video at a rate of 1 frame per second .
This sampling rate may affect the level of detail in the descriptions,
particularly for videos with rapidly changing visuals.

 
 

### Python

 

```
prompt = "Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments."
```

 
 

### JavaScript

 

```
const prompt = "Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments.";
```

 
 

### Go

 

```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
        genai.NewPartFromText("Describe the key events in this video, providing both audio and visual details. " +
      "Include timestamps for salient moments."),
    }
```

 
 

### REST

 

```
PROMPT="Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments."
```

 
 

## Customize video processing

You can customize video processing in the Gemini API by setting clipping
intervals or providing custom frame rate sampling.

### Set clipping intervals

You can clip video by specifying `videoMetadata` with start and end offsets.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),
                video_metadata=types.VideoMetadata(
                    start_offset='1250s',
                    end_offset='1570s'
                )
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash';

async function main() {
const contents = [
  {
    role: 'user',
    parts: [
      {
        fileData: {
          fileUri: 'https://www.youtube.com/watch?v=9hE5-98ZeCg',
          mimeType: 'video/*',
        },
        videoMetadata: {
          startOffset: '40s',
          endOffset: '80s',
        }
      },
      {
        text: 'Please summarize the video in 3 sentences.',
      },
    ],
  },
];

const response = await ai.models.generateContent({
  model,
  contents,
});

console.log(response.text)

}

await main();
```

 
 

### Set a custom frame rate

You can set custom frame rate sampling by passing an `fps` argument to
`videoMetadata`.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=video_bytes,
                    mime_type='video/mp4'),
                video_metadata=types.VideoMetadata(fps=5)
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

By default 1 frame per second (FPS) is sampled from the video. You might want to
set low FPS (< 1) for long videos. This is especially useful for mostly static
videos (e.g. lectures). If you want to capture more details in rapidly changing
visuals, consider setting a higher FPS value.

## Supported video formats

Gemini supports the following video format MIME types:

- `video/mp4`

- `video/mpeg`

- `video/mov`

- `video/avi`

- `video/x-flv`

- `video/mpg`

- `video/webm`

- `video/wmv`

- `video/3gpp`

## Technical details about videos

- Supported models & context : All Gemini 2.0 and 2.5 models can process video data.

 Models with a 2M context window can process videos up to 2 hours long at
default media resolution or 6 hours long at low media resolution, while
models with a 1M context window can process videos up to 1 hour long at
default media resolution or 3 hours long at low media resolution.

 
- File API processing : When using the File API, videos are stored at 1
frame per second (FPS) and audio is processed at 1Kbps (single channel).
Timestamps are added every second.

 These rates are subject to change in the future for improvements in inference.

- You can override the 1 FPS sampling rate by setting a custom frame rate .

 
- Token calculation : Each second of video is tokenized as follows:

 Individual frames (sampled at 1 FPS):

 If `mediaResolution` is set
to low, frames are tokenized at 66 tokens per frame.

- Otherwise, frames are tokenized at 258 tokens per frame.

 
- Audio: 32 tokens per second.

- Metadata is also included.

- Total: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.

 
- 

 Medial resolution : Gemini 3 introduces granular control over multimodal
vision processing with the `media_resolution` parameter. The
`media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to read fine text or identify
small details, but increase token usage and latency.

For more details about the parameter and how it can impact token
calculations, see the media resolution guide.

- 

 Timestamp format : When referring to specific moments in a video within your prompt, use the `MM:SS` format (e.g., `01:15` for 1 minute and 15 seconds).

- 

 Best practices :

 Use only one video per prompt request for optimal results.

- If combining text and a single video, place the text prompt after the video part in the `contents` array.

- Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

 

## What's next

This guide shows how to upload video files and generate text outputs from video
inputs. To learn more, see the following resources:

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- Files API : Learn more about uploading and managing
files for use with Gemini.

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- Safety guidance : Sometimes generative
 AI models produce unexpected outputs, such as outputs that are inaccurate,
 biased, or offensive. Post-processing and human evaluation are essential to
 limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Embeddings &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/embeddings#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Embeddings  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Embeddings 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API offers text embedding models to generate embeddings for words,
phrases, sentences, and code. These foundational embeddings power advanced NLP
tasks such as semantic search, classification, and clustering, providing more
accurate, context-aware results than keyword-based approaches.

Building Retrieval Augmented Generation (RAG) systems is a common use case for
embeddings. Embeddings play a key role in significantly enhancing model outputs
with improved factual accuracy, coherence, and contextual richness. They
efficiently retrieve relevant information from knowledge bases, represented by
embeddings, which are then passed as additional context in the input prompt to
language models, guiding it to generate more informed and accurate responses.

To learn more about the available embedding model variants, see the Model
versions section. For higher throughput serving at half the
price, try Batch API Embedding .

## Generating embeddings

Use the `embedContent` method to generate text embeddings:

 
 

### Python

 

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents="What is the meaning of life?")

print(result.embeddings)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: 'What is the meaning of life?',
    });

    console.log(response.embeddings);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-001",
     "content": {"parts":[{"text": "What is the meaning of life?"}]}
    }'
```

 
 

You can also generate embeddings for multiple chunks at once by passing them in
as a list of strings.

 
 

### Python

 

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents= [
            "What is the meaning of life?",
            "What is the purpose of existence?",
            "How do I bake a cake?"
        ])

for embedding in result.embeddings:
    print(embedding)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: [
            'What is the meaning of life?',
            'What is the purpose of existence?',
            'How do I bake a cake?'
        ],
    });

    console.log(response.embeddings);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?"),
        genai.NewContentFromText("How does photosynthesis work?"),
        genai.NewContentFromText("Tell me about the history of the internet."),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"requests": [{
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "What is the meaning of life?"}]}, },
    {
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "How much wood would a woodchuck chuck?"}]}, },
    {
    "model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
        "text": "How does the brain work?"}]}, }, ]}' 2> /dev/null | grep -C 5 values
    ```
```

 
 

## Specify task type to improve performance

You can use embeddings for a wide range of tasks from classification to document
search. Specifying the right task type helps optimize the embeddings for the
intended relationships, maximizing accuracy and efficiency. For a complete list
of supported task types, see the Supported task types 
table.

The following example shows how you can use
`SEMANTIC_SIMILARITY` to check how similar in meaning strings of texts are.

 
 

### Python

 

```
from google import genai
from google.genai import types
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

client = genai.Client()

texts = [
    "What is the meaning of life?",
    "What is the purpose of existence?",
    "How do I bake a cake?"]

result = [
    np.array(e.values) for e in client.models.embed_content(
        model="gemini-embedding-001",
        contents=texts,
        config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")).embeddings
]

# Calculate cosine similarity. Higher scores = greater semantic similarity.

embeddings_matrix = np.array(result)
similarity_matrix = cosine_similarity(embeddings_matrix)

for i, text1 in enumerate(texts):
    for j in range(i + 1, len(texts)):
        text2 = texts[j]
        similarity = similarity_matrix[i, j]
        print(f"Similarity between '{text1}' and '{text2}': {similarity:.4f}")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as cosineSimilarity from "compute-cosine-similarity";

async function main() {
    const ai = new GoogleGenAI({});

    const texts = [
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    ];

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: texts,
        taskType: 'SEMANTIC_SIMILARITY'
    });

    const embeddings = response.embeddings.map(e => e.values);

    for (let i = 0; i < texts.length; i++) {
        for (let j = i + 1; j < texts.length; j++) {
            const text1 = texts[i];
            const text2 = texts[j];
            const similarity = cosineSimilarity(embeddings[i], embeddings[j]);
            console.log(`Similarity between '${text1}' and '${text2}': ${similarity.toFixed(4)}`);
        }
    }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "math"

    "google.golang.org/genai"
)

// cosineSimilarity calculates the similarity between two vectors.
func cosineSimilarity(a, b []float32) (float64, error) {
    if len(a) != len(b) {
        return 0, fmt.Errorf("vectors must have the same length")
    }

    var dotProduct, aMagnitude, bMagnitude float64
    for i := 0; i < len(a); i++ {
        dotProduct += float64(a[i] * b[i])
        aMagnitude += float64(a[i] * a[i])
        bMagnitude += float64(b[i] * b[i])
    }

    if aMagnitude == 0 || bMagnitude == 0 {
        return 0, nil
    }

    return dotProduct / (math.Sqrt(aMagnitude) * math.Sqrt(bMagnitude)), nil
}

func main() {
    ctx := context.Background()
    client, _ := genai.NewClient(ctx, nil)
    defer client.Close()

    texts := []string{
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    }

    var contents []*genai.Content
    for _, text := range texts {
        contents = append(contents, genai.NewContentFromText(text, genai.RoleUser))
    }

    result, _ := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{TaskType: genai.TaskTypeSemanticSimilarity},
    )

    embeddings := result.Embeddings

    for i := 0; i < len(texts); i++ {
        for j := i + 1; j < len(texts); j++ {
            similarity, _ := cosineSimilarity(embeddings[i].Values, embeddings[j].Values)
            fmt.Printf("Similarity between '%s' and '%s': %.4f\n", texts[i], texts[j], similarity)
        }
    }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"task_type": "SEMANTIC_SIMILARITY",
    "content": {
    "parts":[{
    "text": "What is the meaning of life?"}, {"text": "How much wood would a woodchuck chuck?"}, {"text": "How does the brain work?"}]}
    }'
```

 
 

The following shows an example output from this code snippet:

 

```
Similarity between 'What is the meaning of life?' and 'What is the purpose of existence?': 0.9481

Similarity between 'What is the meaning of life?' and 'How do I bake a cake?': 0.7471

Similarity between 'What is the purpose of existence?' and 'How do I bake a cake?': 0.7371
```

 

### Supported task types

 

 Task type 
 Description 
 Examples 
 
 SEMANTIC_SIMILARITY 
 Embeddings optimized to assess text similarity. 
 Recommendation systems, duplicate detection 
 
 
 CLASSIFICATION 
 Embeddings optimized to classify texts according to preset labels. 
 Sentiment analysis, spam detection 
 
 
 CLUSTERING 
 Embeddings optimized to cluster texts based on their similarities. 
 Document organization, market research, anomaly detection 
 
 
 RETRIEVAL_DOCUMENT 
 Embeddings optimized for document search. 
 Indexing articles, books, or web pages for search. 
 
 
 RETRIEVAL_QUERY 
 
 Embeddings optimized for general search queries.
 Use `RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for documents to be retrieved.
 
 Custom search 
 
 
 CODE_RETRIEVAL_QUERY 
 
 Embeddings optimized for retrieval of code blocks based on natural language queries.
 Use `CODE_RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for code blocks to be retrieved.
 
 Code suggestions and search 
 
 
 QUESTION_ANSWERING 
 
 Embeddings for questions in a question-answering system, optimized for finding documents that answer the question.
 Use `QUESTION_ANSWERING` for questions; `RETRIEVAL_DOCUMENT` for documents to be retrieved.
 
 Chatbox 
 
 
 FACT_VERIFICATION 
 
 Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement.
 Use `FACT_VERIFICATION` for the target text; `RETRIEVAL_DOCUMENT` for documents to be retrieved
 
 Automated fact-checking systems 
 
 

## Controlling embedding size

The Gemini embedding model, `gemini-embedding-001`, is trained using the
Matryoshka Representation Learning (MRL) technique which teaches a model to
learn high-dimensional embeddings that have initial segments (or prefixes) which
are also useful, simpler versions of the same data.

Use the `output_dimensionality` parameter to control the size of
the output embedding vector. Selecting a smaller output dimensionality can save
storage space and increase computational efficiency for downstream applications,
while sacrificing little in terms of quality. By default, it outputs a
3072-dimensional embedding, but you can truncate it to a smaller size without
losing quality to save storage space. We recommend using 768, 1536, or 3072
output dimensions.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

result = client.models.embed_content(
    model="gemini-embedding-001",
    contents="What is the meaning of life?",
    config=types.EmbedContentConfig(output_dimensionality=768)
)

[embedding_obj] = result.embeddings
embedding_length = len(embedding_obj.values)

print(f"Length of embedding: {embedding_length}")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {
    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        content: 'What is the meaning of life?',
        outputDimensionality: 768,
    });

    const embeddingLength = response.embedding.values.length;
    console.log(`Length of embedding: ${embeddingLength}`);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client uses Application Default Credentials.
    // Authenticate with 'gcloud auth application-default login'.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }
    defer client.Close()

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }

    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{OutputDimensionality: 768},
    )
    if err != nil {
        log.Fatal(err)
    }

    embedding := result.Embeddings[0]
    embeddingLength := len(embedding.Values)
    fmt.Printf("Length of embedding: %d\n", embeddingLength)
}
```

 
 

### REST

 

```
curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
        "content": {"parts":[{ "text": "What is the meaning of life?"}]},
        "output_dimensionality": 768
    }'
```

 
 

Example output from the code snippet:

 

```
Length of embedding: 768
```

 

## Ensuring quality for smaller dimensions

The 3072 dimension embedding is normalized. Normalized embeddings produce more
accurate semantic similarity by comparing vector direction, not magnitude. For
other dimensions, including 768 and 1536, you need to normalize the embeddings
as follows:

 
 

### Python

 

```
import numpy as np
from numpy.linalg import norm

embedding_values_np = np.array(embedding_obj.values)
normed_embedding = embedding_values_np / np.linalg.norm(embedding_values_np)

print(f"Normed embedding length: {len(normed_embedding)}")
print(f"Norm of normed embedding: {np.linalg.norm(normed_embedding):.6f}") # Should be very close to 1
```

 
 

Example output from this code snippet:

 

```
Normed embedding length: 768
Norm of normed embedding: 1.000000
```

 

The following table shows the MTEB scores, a commonly used benchmark for
embeddings, for different dimensions. Notably, the result shows that performance
is not strictly tied to the size of the embedding dimension, with lower
dimensions achieving scores comparable to their higher dimension counterparts.

 
 MRL Dimension 
 MTEB Score 
 
 2048

 
 68.16

 
 
 
 1536

 
 68.17

 
 
 
 768

 
 67.99

 
 
 
 512

 
 67.55

 
 
 
 256

 
 66.19

 
 
 
 128

 
 63.31

 
 
 

## Use cases

Text embeddings are crucial for a variety of common AI use cases, such as:

- Retrieval-Augmented Generation (RAG): Embeddings enhance the quality
of generated text by retrieving and incorporating relevant information into
the context of a model.

- 

 Information retrieval: Search for the most semantically similar text or
documents given a piece of input text.

 
 Document search tutorial task 
 

- 

 Search reranking : Prioritize the most relevant items by semantically
scoring initial results against the query.

 
 Search reranking tutorial task 
 

- 

 Anomaly detection: Comparing groups of embeddings can help identify
hidden trends or outliers.

 
 Anomaly detection tutorial bubble_chart 
 

- 

 Classification: Automatically categorize text based on its content, such
as sentiment analysis or spam detection

 
 Classification tutorial token 
 

- 

 Clustering: Effectively grasp complex relationships by creating clusters
and visualizations of your embeddings.

 
 Clustering visualization tutorial bubble_chart 
 

## Storing embeddings

As you take embeddings to production, it is common to
use vector databases to efficiently store, index, and retrieve
high-dimensional embeddings. Google Cloud offers managed data services that
can be used for this purpose including
 BigQuery ,
 AlloyDB , and
 Cloud SQL .

The following tutorials show how to use other third party vector databases
with Gemini Embedding.

- 
 ChromaDB tutorials bolt 
 

- 
 QDrant tutorials bolt 
 

- 
 Weaviate tutorials bolt 
 

- 
 Pinecone tutorials bolt 
 

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`gemini-embedding-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Text embeddings

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

2,048

 
 
 

 Output dimension size 

 

Flexible, supports: 128 - 3072, Recommended: 768, 1536, 3072

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-embedding-001`

 - Experimental: `gemini-embedding-exp-03-07` (deprecating in Oct of 2025)

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 

## Batch embeddings

If latency is not a concern, try using the Gemini Embeddings model with
 Batch API . This
allows for much higher throughput at 50% of interactive Embedding pricing.
Find examples on how to get started in the Batch API cookbook .

## Responsible use notice

Unlike generative AI models that create new content, the Gemini Embedding model
is only intended to transform the format of your input data into a numerical
representation. While Google is responsible for providing an embedding model
that transforms the format of your input data to the numerical-format requested,
users retain full responsibility for the data they input and the resulting
embeddings. By using the Gemini Embedding model you confirm that you have the
necessary rights to any content that you upload. Do not generate content that
infringes on others' intellectual property or privacy rights. Your use of this
service is subject to our Prohibited Use
Policy and
 Google's Terms of Service .

## Start building with embeddings

Check out the embeddings quickstart
notebook 
to explore the model capabilities and learn how to customize and visualize your
embeddings.

## Deprecation notice for legacy models

The following models will be deprecated in October, 2025:
 - `embedding-001`
 - `embedding-gecko-001`
 - `gemini-embedding-exp-03-07` (`gemini-embedding-exp`)

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### Document understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/document-processing

- 
 
 
 
 
 
 
 
 
 
 
 Document understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Document understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models can process documents in PDF format, using native
vision to understand entire document contexts. This goes beyond
simple text extraction, allowing Gemini to:

- Analyze and interpret content, including text, images, diagrams,
charts, and tables, even in long documents up to 1000 pages.

- Extract information into structured output formats.

- Summarize and answer questions based on both the visual and textual elements
in a document.

- Transcribe document content (e.g. to HTML), preserving layouts and
formatting, for use in downstream applications.

## Passing inline PDF data

You can pass inline PDF data in the request to `generateContent`.
For PDF payloads under 20MB, you can choose between uploading base64
encoded documents or directly uploading locally stored files.

The following example shows you how to fetch a PDF from a URL and convert it to
bytes for processing:

 
 

### Python

 

```
from google import genai
from google.genai import types
import httpx

client = genai.Client()

doc_url = "https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf"

# Retrieve and encode the PDF byte
doc_data = httpx.get(doc_url).content

prompt = "Summarize this document"
response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[
      types.Part.from_bytes(
        data=doc_data,
        mime_type='application/pdf',
      ),
      prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const pdfResp = await fetch('https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf')
        .then((response) => response.arrayBuffer());

    const contents = [
        { text: "Summarize this document" },
        {
            inlineData: {
                mimeType: 'application/pdf',
                data: Buffer.from(pdfResp).toString("base64")
            }
        }
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: contents
    });
    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "io"
    "net/http"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    pdfResp, _ := http.Get("https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf")
    var pdfBytes []byte
    if pdfResp != nil && pdfResp.Body != nil {
        pdfBytes, _ = io.ReadAll(pdfResp.Body)
        pdfResp.Body.Close()
    }

    parts := []*genai.Part{
        &genai.Part{
            InlineData: &genai.Blob{
                MIMEType: "application/pdf",
                Data:     pdfBytes,
            },
        },
        genai.NewPartFromText("Summarize this document"),
    }

    contents := []*genai.Content{
        genai.NewContentFromParts(parts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
DOC_URL="https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf"
PROMPT="Summarize this document"
DISPLAY_NAME="base64_pdf"

# Download the PDF
wget -O "${DISPLAY_NAME}.pdf" "${DOC_URL}"

# Check for FreeBSD base64 and set flags accordingly
if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

# Base64 encode the PDF
ENCODED_PDF=$(base64 $B64FLAGS "${DISPLAY_NAME}.pdf")

# Generate content using the base64 encoded PDF
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"inline_data": {"mime_type": "application/pdf", "data": "'"$ENCODED_PDF"'"}},
          {"text": "'$PROMPT'"}
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json

# Clean up the downloaded PDF
rm "${DISPLAY_NAME}.pdf"
```

 
 

You can also read a PDF from a local file for processing:

 
 

### Python

 

```
from google import genai
from google.genai import types
import pathlib

client = genai.Client()

# Retrieve and encode the PDF byte
filepath = pathlib.Path('file.pdf')

prompt = "Summarize this document"
response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[
      types.Part.from_bytes(
        data=filepath.read_bytes(),
        mime_type='application/pdf',
      ),
      prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from 'fs';

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const contents = [
        { text: "Summarize this document" },
        {
            inlineData: {
                mimeType: 'application/pdf',
                data: Buffer.from(fs.readFileSync("content/343019_3_art_0_py4t4l_convrt.pdf")).toString("base64")
            }
        }
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: contents
    });
    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    pdfBytes, _ := os.ReadFile("path/to/your/file.pdf")

    parts := []*genai.Part{
        &genai.Part{
            InlineData: &genai.Blob{
                MIMEType: "application/pdf",
                Data:     pdfBytes,
            },
        },
        genai.NewPartFromText("Summarize this document"),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(parts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

## Uploading PDFs using the File API

You can use the File API to upload larger documents. Always use the File
API when the total request size (including the files, text prompt, system
instructions, etc.) is larger than 20MB.

Call `media.upload` to upload a file using the
File API. The following code uploads a document file and then uses the file in a
call to
 `models.generateContent` .

### Large PDFs from URLs

Use the File API to simplify uploading and processing large PDF files from URLs:

 
 

### Python

 

```
from google import genai
from google.genai import types
import io
import httpx

client = genai.Client()

long_context_pdf_path = "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"

# Retrieve and upload the PDF using the File API
doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)

sample_doc = client.files.upload(
  # You can pass a path or a file-like object here
  file=doc_io,
  config=dict(
    mime_type='application/pdf')
)

prompt = "Summarize this document"

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_doc, prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {

    const pdfBuffer = await fetch("https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf")
        .then((response) => response.arrayBuffer());

    const fileBlob = new Blob([pdfBuffer], { type: 'application/pdf' });

    const file = await ai.files.upload({
        file: fileBlob,
        config: {
            displayName: 'A17_FlightPlan.pdf',
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    // Add the file to the contents.
    const content = [
        'Summarize this document',
    ];

    if (file.uri && file.mimeType) {
        const fileContent = createPartFromUri(file.uri, file.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);

}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "io"
  "net/http"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, _ := genai.NewClient(ctx, &genai.ClientConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Backend: genai.BackendGeminiAPI,
  })

  pdfURL := "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"
  localPdfPath := "A17_FlightPlan_downloaded.pdf"

  respHttp, _ := http.Get(pdfURL)
  defer respHttp.Body.Close()

  outFile, _ := os.Create(localPdfPath)
  defer outFile.Close()

  _, _ = io.Copy(outFile, respHttp.Body)

  uploadConfig := &genai.UploadFileConfig{MIMEType: "application/pdf"}
  uploadedFile, _ := client.Files.UploadFromPath(ctx, localPdfPath, uploadConfig)

  promptParts := []*genai.Part{
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
    genai.NewPartFromText("Summarize this document"),
  }
  contents := []*genai.Content{
    genai.NewContentFromParts(promptParts, genai.RoleUser), // Specify role
  }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
PDF_PATH="https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"
DISPLAY_NAME="A17_FlightPlan"
PROMPT="Summarize this document"

# Download the PDF from the provided URL
wget -O "${DISPLAY_NAME}.pdf" "${PDF_PATH}"

MIME_TYPE=$(file -b --mime-type "${DISPLAY_NAME}.pdf")
NUM_BYTES=$(wc -c < "${DISPLAY_NAME}.pdf")

echo "MIME_TYPE: ${MIME_TYPE}"
echo "NUM_BYTES: ${NUM_BYTES}"

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${DISPLAY_NAME}.pdf" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo "file_uri: ${file_uri}"

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "'$PROMPT'"},
          {"file_data":{"mime_type": "application/pdf", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json

# Clean up the downloaded PDF
rm "${DISPLAY_NAME}.pdf"
```

 
 

### Large PDFs stored locally

 
 

### Python

 

```
from google import genai
from google.genai import types
import pathlib
import httpx

client = genai.Client()

# Retrieve and encode the PDF byte
file_path = pathlib.Path('large_file.pdf')

# Upload the PDF using the File API
sample_file = client.files.upload(
  file=file_path,
)

prompt="Summarize this document"

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_file, "Summarize this document"])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const file = await ai.files.upload({
        file: 'path-to-localfile.pdf'
        config: {
            displayName: 'A17_FlightPlan.pdf',
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    // Add the file to the contents.
    const content = [
        'Summarize this document',
    ];

    if (file.uri && file.mimeType) {
        const fileContent = createPartFromUri(file.uri, file.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);

}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })
    localPdfPath := "/path/to/file.pdf"

    uploadConfig := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile, _ := client.Files.UploadFromPath(ctx, localPdfPath, uploadConfig)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
        genai.NewPartFromText("Give me a summary of this pdf file."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
NUM_BYTES=$(wc -c < "${PDF_PATH}")
DISPLAY_NAME=TEXT
tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GEMINI_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: application/pdf" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${PDF_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Can you add a few more lines to this poem?"},
          {"file_data":{"mime_type": "application/pdf", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

You can verify the API successfully stored the uploaded file and get its
metadata by calling `files.get` . Only the `name`
(and by extension, the `uri`) are unique.

 
 

### Python

 

```
from google import genai
import pathlib

client = genai.Client()

fpath = pathlib.Path('example.txt')
fpath.write_text('hello')

file = client.files.upload(file='example.txt')

file_info = client.files.get(name=file.name)
print(file_info.model_dump_json(indent=4))
```

 
 

### REST

 

```
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

 
 

## Passing multiple PDFs

The Gemini API is capable of processing multiple PDF documents (up to 1000 pages)
in a single request, as long as the combined size of the documents and the text
prompt stays within the model's context window.

 
 

### Python

 

```
from google import genai
import io
import httpx

client = genai.Client()

doc_url_1 = "https://arxiv.org/pdf/2312.11805"
doc_url_2 = "https://arxiv.org/pdf/2403.05530"

# Retrieve and upload both PDFs using the File API
doc_data_1 = io.BytesIO(httpx.get(doc_url_1).content)
doc_data_2 = io.BytesIO(httpx.get(doc_url_2).content)

sample_pdf_1 = client.files.upload(
  file=doc_data_1,
  config=dict(mime_type='application/pdf')
)
sample_pdf_2 = client.files.upload(
  file=doc_data_2,
  config=dict(mime_type='application/pdf')
)

prompt = "What is the difference between each of the main benchmarks between these two papers? Output these in a table."

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_pdf_1, sample_pdf_2, prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function uploadRemotePDF(url, displayName) {
    const pdfBuffer = await fetch(url)
        .then((response) => response.arrayBuffer());

    const fileBlob = new Blob([pdfBuffer], { type: 'application/pdf' });

    const file = await ai.files.upload({
        file: fileBlob,
        config: {
            displayName: displayName,
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    return file;
}

async function main() {
    const content = [
        'What is the difference between each of the main benchmarks between these two papers? Output these in a table.',
    ];

    let file1 = await uploadRemotePDF("https://arxiv.org/pdf/2312.11805", "PDF 1")
    if (file1.uri && file1.mimeType) {
        const fileContent = createPartFromUri(file1.uri, file1.mimeType);
        content.push(fileContent);
    }
    let file2 = await uploadRemotePDF("https://arxiv.org/pdf/2403.05530", "PDF 2")
    if (file2.uri && file2.mimeType) {
        const fileContent = createPartFromUri(file2.uri, file2.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "io"
    "net/http"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    docUrl1 := "https://arxiv.org/pdf/2312.11805"
    docUrl2 := "https://arxiv.org/pdf/2403.05530"
    localPath1 := "doc1_downloaded.pdf"
    localPath2 := "doc2_downloaded.pdf"

    respHttp1, _ := http.Get(docUrl1)
    defer respHttp1.Body.Close()

    outFile1, _ := os.Create(localPath1)
    _, _ = io.Copy(outFile1, respHttp1.Body)
    outFile1.Close()

    respHttp2, _ := http.Get(docUrl2)
    defer respHttp2.Body.Close()

    outFile2, _ := os.Create(localPath2)
    _, _ = io.Copy(outFile2, respHttp2.Body)
    outFile2.Close()

    uploadConfig1 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile1, _ := client.Files.UploadFromPath(ctx, localPath1, uploadConfig1)

    uploadConfig2 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile2, _ := client.Files.UploadFromPath(ctx, localPath2, uploadConfig2)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile1.URI, uploadedFile1.MIMEType),
        genai.NewPartFromURI(uploadedFile2.URI, uploadedFile2.MIMEType),
        genai.NewPartFromText("What is the difference between each of the " +
                              "main benchmarks between these two papers? " +
                              "Output these in a table."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    modelName := "gemini-2.5-flash"
    result, _ := client.Models.GenerateContent(
        ctx,
        modelName,
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
DOC_URL_1="https://arxiv.org/pdf/2312.11805"
DOC_URL_2="https://arxiv.org/pdf/2403.05530"
DISPLAY_NAME_1="Gemini_paper"
DISPLAY_NAME_2="Gemini_1.5_paper"
PROMPT="What is the difference between each of the main benchmarks between these two papers? Output these in a table."

# Function to download and upload a PDF
upload_pdf() {
  local doc_url="$1"
  local display_name="$2"

  # Download the PDF
  wget -O "${display_name}.pdf" "${doc_url}"

  local MIME_TYPE=$(file -b --mime-type "${display_name}.pdf")
  local NUM_BYTES=$(wc -c < "${display_name}.pdf")

  echo "MIME_TYPE: ${MIME_TYPE}"
  echo "NUM_BYTES: ${NUM_BYTES}"

  local tmp_header_file=upload-header.tmp

  # Initial resumable request
  curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
    -D "${tmp_header_file}" \
    -H "X-Goog-Upload-Protocol: resumable" \
    -H "X-Goog-Upload-Command: start" \
    -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
    -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
    -H "Content-Type: application/json" \
    -d "{'file': {'display_name': '${display_name}'}}" 2> /dev/null

  local upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
  rm "${tmp_header_file}"

  # Upload the PDF
  curl "${upload_url}" \
    -H "Content-Length: ${NUM_BYTES}" \
    -H "X-Goog-Upload-Offset: 0" \
    -H "X-Goog-Upload-Command: upload, finalize" \
    --data-binary "@${display_name}.pdf" 2> /dev/null > "file_info_${display_name}.json"

  local file_uri=$(jq ".file.uri" "file_info_${display_name}.json")
  echo "file_uri for ${display_name}: ${file_uri}"

  # Clean up the downloaded PDF
  rm "${display_name}.pdf"

  echo "${file_uri}"
}

# Upload the first PDF
file_uri_1=$(upload_pdf "${DOC_URL_1}" "${DISPLAY_NAME_1}")

# Upload the second PDF
file_uri_2=$(upload_pdf "${DOC_URL_2}" "${DISPLAY_NAME_2}")

# Now generate content using both files
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data": {"mime_type": "application/pdf", "file_uri": '$file_uri_1'}},
          {"file_data": {"mime_type": "application/pdf", "file_uri": '$file_uri_2'}},
          {"text": "'$PROMPT'"}
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Technical details

Gemini supports a maximum of 1,000 document pages.
Each document page is equivalent to 258 tokens.

While there are no specific limits to the number of pixels in a document besides
the model's context window , larger pages are
scaled down to a maximum resolution of 3072x3072 while preserving their original
aspect ratio, while smaller pages are scaled up to 768x768 pixels. There is no
cost reduction for pages at lower sizes, other than bandwidth, or performance
improvement for pages at higher resolution.

### Gemini 3 models

Gemini 3 introduces granular control over multimodal vision processing with the
`media_resolution` parameter. You can now set the resolution to low, medium, or
high per individual media part. With this addition, the processing of PDF
documents has been updated:

- Native text inclusion: Text natively embedded in the PDF is extracted
and provided to the model.

- Billing & token reporting: 

 You are not charged for tokens originating from the extracted
 native text in PDFs.

- In the `usage_metadata` section of the API response, tokens generated
from processing PDF pages (as images) are now counted under the `IMAGE`
modality, not a separate `DOCUMENT` modality as in some earlier
versions.

 

For more details about the media resolution parameter, see the
 Media resolution guide.

### Document types

Technically, you can pass other MIME types for document understanding, like
TXT, Markdown, HTML, XML, etc. However, document vision only meaningfully
understands PDFs . Other types will be extracted as pure text, and the model
won't be able to interpret what we see in the rendering of those files. Any
file-type specifics like charts, diagrams, HTML tags, Markdown formatting, etc.,
will be lost.

### Best practices

For best results:

- Rotate pages to the correct orientation before uploading.

- Avoid blurry pages.

- If using a single page, place the text prompt after the page.

## What's next

To learn more, see the following resources:

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini Robotics-ER 1.5 &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/robotics-overview#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Robotics-ER 1.5  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini Robotics-ER 1.5 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini Robotics-ER 1.5 is a vision-language model (VLM) that brings Gemini's
agentic capabilities to robotics. It's designed for advanced reasoning in the
physical world, allowing robots to interpret complex visual data, perform
spatial reasoning, and plan actions from natural language commands.

Key features and benefits:

- Enhanced autonomy: Robots can reason, adapt, and respond to changes in
open-ended environments.

- Natural language interaction: Makes robots easier to use by enabling
complex task assignments using natural language.

- Task orchestration: Deconstructs natural language commands into subtasks
and integrates with existing robot controllers and behaviors to complete
long-horizon tasks.

- Versatile capabilities: Locates and identifies objects, understands
object relationships, plans grasps and trajectories, and interprets dynamic
scenes.

This document describes what the model does and takes you
through several examples that highlight the model's
agentic capabilities.

If you want to jump right in, you can try out the model in Google AI Studio.

 Try in Google AI Studio 

## Safety

While Gemini Robotics-ER 1.5 was built with safety in mind, it is your
responsibility to maintain a safe environment around the robot. Generative AI
models can make mistakes, and physical robots can cause damage. Safety is a
priority, and making generative AI models safe when used with real-world
robotics is an active and critical area of our research. To learn more, visit
the Google DeepMind robotics safety page .

## Getting started: Finding objects in a scene

The following example demonstrates a common robotics use case. It shows how to
pass an image and a text prompt to the model using the
 `generateContent` 
method to get a list of identified objects with their corresponding 2D points.
The model returns points for items it identified in an image, returning
their normalized 2D coordinates and labels.

You can use this output with a robotics API or call a vision-language-action
(VLA) model or any other third-party user-defined functions to generate actions
for a robot to perform.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
PROMPT = """
          Point to no more than 10 items in the image. The label returned
          should be an identifying name for the object detected.
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format
          normalized to 0-1000.
        """
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image
with open("my-image.png", 'rb') as f:
    image_bytes = f.read()

image_response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/png',
        ),
        PROMPT
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        thinking_config=types.ThinkingConfig(thinking_budget=0)
    )
)

print(image_response.text)
```

 
 

### REST

 

```
# First, ensure you have the image file locally.
# Encode the image to base64
IMAGE_BASE64=$(base64 -w 0 my-image.png)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-robotics-er-1.5-preview:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "inlineData": {
              "mimeType": "image/png",
              "data": "'"${IMAGE_BASE64}"'"
            }
          },
          {
            "text": "Point to no more than 10 items in the image. The label returned should be an identifying name for the object detected. The answer should follow the json format: [{\"point\": [y, x], \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000."
          }
        ]
      }
    ],
    "generationConfig": {
      "temperature": 0.5,
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

The output will be a JSON array containing objects, each with a `point`
(normalized `[y, x]` coordinates) and a `label` identifying the object.

 
 

### JSON

 

```
[
  {"point": [376, 508], "label": "small banana"},
  {"point": [287, 609], "label": "larger banana"},
  {"point": [223, 303], "label": "pink starfruit"},
  {"point": [435, 172], "label": "paper bag"},
  {"point": [270, 786], "label": "green plastic bowl"},
  {"point": [488, 775], "label": "metal measuring cup"},
  {"point": [673, 580], "label": "dark blue bowl"},
  {"point": [471, 353], "label": "light blue bowl"},
  {"point": [492, 497], "label": "bread"},
  {"point": [525, 429], "label": "lime"}
]
```

 
 

The following image is an example of how these points can be displayed:

 

## How it works

Gemini Robotics-ER 1.5 allows your robots to contextualize and work in the
physical world using spatial understanding. It takes image/video/audio input and
natural language prompts to:

- Understand objects and scene context : Identifies objects, and reasons
about their relationship to the scene, including their affordances.

- Understand task instructions : Interprets tasks given in natural
language, like "find the banana".

- Reason spatially and temporally : Understand sequences of actions and how
objects interact with a scene over time.

- Provide structured output : Returns coordinates (points or bounding
boxes) representing object locations.

This enables robots to "see" and "understand" their environment
programmatically.

Gemini Robotics-ER 1.5 is also agentic, which means it can break down complex
tasks (like "put the apple in the bowl") into sub-tasks to orchestrate long
horizon tasks:

- Sequencing subtasks : Decomposes commands into a logical sequence of
steps.

- Function calls/Code execution : Executes steps by calling your existing
robot functions/tools or executing generated code.

Read more about how function calling with Gemini works on the Function Calling
page .

### Using the thinking budget with Gemini Robotics-ER 1.5

Gemini Robotics-ER 1.5 has a flexible thinking budget that gives you control
over latency versus accuracy tradeoffs. For spatial understanding tasks like
object detection, the model can achieve high performance with a small thinking
budget. More complex reasoning tasks like counting and weight estimation benefit
from a larger thinking budget. This lets you balance the need for
low-latency responses with high-accuracy results for more challenging tasks.

To learn more about thinking budgets, see the
 Thinking 
core capabilities page.

## Agentic capabilities for robotics

This section walks through various capabilities of Gemini
Robotics-ER 1.5, demonstrating how to use the model for robotic perception,
reasoning, and planning applications.

The examples in this section demonstrate capabilities from pointing and finding
objects in an image to planning trajectories and orchestrating long horizon
tasks. For simplicity, the code snippets have been reduced to show the prompt
and the call to `generate_content` API. The full runnable code as well as
additional examples can be found in the
 Robotics cookbook .

### Pointing to objects

Pointing and finding objects in images or video frames is a common use case for
vision-and-language models (VLMs) in robotics. The following example asks the
model to find specific objects within an image and return their coordinates in
an image.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

queries = [
    "bread",
    "starfruit",
    "banana",
]

prompt = f"""
    Get all points matching the following objects: {', '.join(queries)}. The
    label returned should be an identifying name for the object detected.
    The answer should follow the json format:

    [{{"point": , "label": }}, ...]. The points are in

    [y, x] format normalized to 0-1000.
    """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The output would be similar to the getting started example, a JSON containing
the coordinates of the objects found and their labels.

 

```
[
  {"point": [671, 317], "label": "bread"},
  {"point": [738, 307], "label": "bread"},
  {"point": [702, 237], "label": "bread"},
  {"point": [629, 307], "label": "bread"},
  {"point": [833, 800], "label": "bread"},
  {"point": [609, 663], "label": "banana"},
  {"point": [770, 483], "label": "starfruit"}
]
```

 

 

Use the following prompt to request the model to interpret abstract categories
like "fruit" instead of specific objects and locate all instances in the image.

 
 

### Python

 

```
prompt = f"""
        Get all points for fruit. The label returned should be an identifying
        name for the object detected.
        """ + """The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...]. The points are in
        [y, x] format normalized to 0-1000."""
```

 
 

Visit the image understanding page for
other image processing techniques.

### Tracking objects in a video

Gemini Robotics-ER 1.5 can also analyze video frames to track objects
over time. See Video inputs 
for a list of supported video formats.

The following is the base prompt used to find specific objects in
each frame that the model analyzes:

 
 

### Python

 

```
# Define the objects to find
queries = [
    "pen (on desk)",
    "pen (in robot hand)",
    "laptop (opened)",
    "laptop (closed)",
]

base_prompt = f"""
  Point to the following objects in the provided image: {', '.join(queries)}.
  The answer should follow the json format:

  [{{"point": , "label": }}, ...].

  The points are in [y, x] format normalized to 0-1000.
  If no objects are found, return an empty JSON list [].
  """
```

 
 

The output shows a pen and laptop being tracked across the video frames.

 

For the full runnable code, see the
 Robotics cookbook .

### Object detection and bounding boxes

Beyond single points, the model can also return 2D bounding boxes, providing a
rectangular region enclosing an object.

This example requests 2D bounding boxes for identifiable objects on a table. The
model is instructed to limit the output to 25 objects and to name multiple
instances uniquely.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
      Return bounding boxes as a JSON array with labels. Never return masks
      or code fencing. Limit to 25 objects. Include as many objects as you
      can identify on the table.
      If an object is present multiple times, name them according to their
      unique characteristic (colors, size, position, unique characteristics, etc..).
      The format should be as follows: [{"box_2d": [ymin, xmin, ymax, xmax],
      "label": <label for the object>}] normalized to 0-1000. The values in
      box_2d must only be integers
      """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The following displays the boxes returned from the model.

 

For the full runnable code, see the Robotics
cookbook .
The Image understanding page also has
additional examples of visual tasks like segmentation and object detection.

Additional bounding box examples can be found in the
 Image understanding page.

### Trajectories

Gemini Robotics-ER 1.5 can generate sequences of points that define a
trajectory, useful for guiding robot movement.

This example requests a trajectory to move a red pen to an organizer, including
the starting point and a series of intermediate points.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

points_data = []
prompt = """
        Place a point on the red pen, then 15 points for the trajectory of
        moving the red pen to the top of the organizer on the left.
        The points should be labeled by order of the trajectory, from '0'
        (start point at left hand) to <n> (final point)
        The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...].
        The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
  )
)

print(image_response.text)
```

 
 

The response is a set of coordinates that describe the trajectory of the path
that the red pen should follow to complete the task of moving it on top of the
organizer:

 

```
[
  {"point": [550, 610], "label": "0"},
  {"point": [500, 600], "label": "1"},
  {"point": [450, 590], "label": "2"},
  {"point": [400, 580], "label": "3"},
  {"point": [350, 550], "label": "4"},
  {"point": [300, 520], "label": "5"},
  {"point": [250, 490], "label": "6"},
  {"point": [200, 460], "label": "7"},
  {"point": [180, 430], "label": "8"},
  {"point": [160, 400], "label": "9"},
  {"point": [140, 370], "label": "10"},
  {"point": [120, 340], "label": "11"},
  {"point": [110, 320], "label": "12"},
  {"point": [105, 310], "label": "13"},
  {"point": [100, 305], "label": "14"},
  {"point": [100, 300], "label": "15"}
]
```

 

 

### Orchestration

Gemini Robotics-ER 1.5 can perform higher-level spatial reasoning, inferring
actions or identifying optimal locations based on contextual understanding.

#### Making room for a laptop

This example shows how Gemini Robotics-ER can reason about a space. The prompt
asks the model to identify which object needs to be moved to create
space for another item.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Point to the object that I need to remove to make room for my laptop
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The response contains a 2D coordinate of the object that answers the user's
question, in this case, the object that should move to make room for a laptop.

 

```
[
  {"point": [672, 301], "label": "The object that I need to remove to make room for my laptop"}
]
```

 

 

#### Packing a Lunch

The model can also provide instructions for multi-step tasks and point to
relevant objects for each step. This example shows how the model plans a series
of steps to pack a lunch bag.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-of-lunch.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Explain how to pack the lunch box and lunch bag. Point to each
          object that you refer to. Each point should be in the format:
          [{"point": [y, x], "label": }], where the coordinates are
          normalized between 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

 
 

The response of this prompt is a set of step by step instructions on how to pack
a lunch bag from the image input.

 Input image 

 

 Model output 

 

```
Based on the image, here is a plan to pack the lunch box and lunch bag:

1.  **Pack the fruit into the lunch box.** Place the [apple](apple), [banana](banana), [red grapes](red grapes), and [green grapes](green grapes) into the [blue lunch box](blue lunch box).
2.  **Add the spoon to the lunch box.** Put the [blue spoon](blue spoon) inside the lunch box as well.
3.  **Close the lunch box.** Secure the lid on the [blue lunch box](blue lunch box).
4.  **Place the lunch box inside the lunch bag.** Put the closed [blue lunch box](blue lunch box) into the [brown lunch bag](brown lunch bag).
5.  **Pack the remaining items into the lunch bag.** Place the [blue snack bar](blue snack bar) and the [brown snack bar](brown snack bar) into the [brown lunch bag](brown lunch bag).

Here is the list of objects and their locations:
*   [{"point": [899, 440], "label": "apple"}]
*   [{"point": [814, 363], "label": "banana"}]
*   [{"point": [727, 470], "label": "red grapes"}]
*   [{"point": [675, 608], "label": "green grapes"}]
*   [{"point": [706, 529], "label": "blue lunch box"}]
*   [{"point": [864, 517], "label": "blue spoon"}]
*   [{"point": [499, 401], "label": "blue snack bar"}]
*   [{"point": [614, 705], "label": "brown snack bar"}]
*   [{"point": [448, 501], "label": "brown lunch bag"}]
```

 

#### Calling a custom robot API

This example demonstrates task orchestration with a custom robot API. It
introduces a mock API designed for a pick-and-place operation. The task is to
pick up a blue block and place it in an orange bowl:

 

Similar to the other examples on this page, the full runnable code is available
in the Robotics cookbook .

First step is to locate both of the items with the following prompt:

 
 

### Python

 

```
prompt = """
            Locate and point to the blue block and the orange bowl. The label
            returned should be an identifying name for the object detected.
            The answer should follow the json format: [{"point": <point>, "label": <label1>}, ...].
            The points are in [y, x] format normalized to 0-1000.
          """
```

 
 

The model response includes the normalized coordinates of the block and the bowl:

 

```
[
  {"point": [389, 252], "label": "orange bowl"},
  {"point": [727, 659], "label": "blue block"}
]
```

 

This example uses the following mock robot API: 

 
 

### Python

 

```
def move(x, y, high):
  print(f"moving to coordinates: {x}, {y}, {15 if high else 5}")

def setGripperState(opened):
  print("Opening gripper" if opened else "Closing gripper")

def returnToOrigin():
  print("Returning to origin pose")
```

 
 

The next step is calling a sequence of API functions with the necessary logic to
execute the action. The following prompt includes a description of the robot
API that the model should use when orchestrating this task.

 
 

### Python

 

```
prompt = f"""
    You are a robotic arm with six degrees-of-freedom. You have the
    following functions available to you:

    def move(x, y, high):
      # moves the arm to the given coordinates. The boolean value 'high' set
      to True means the robot arm should be lifted above the scene for
      avoiding obstacles during motion. 'high' set to False means the robot
      arm should have the gripper placed on the surface for interacting with
      objects.

    def setGripperState(opened):
      # Opens the gripper if opened set to true, otherwise closes the gripper

    def returnToOrigin():
      # Returns the robot to an initial state. Should be called as a cleanup
      operation.

    The origin point for calculating the moves is at normalized point
    y={robot_origin_y}, x={robot_origin_x}. Use this as the new (0,0) for
    calculating moves, allowing x and y to be negative.
    Perform a pick and place operation where you pick up the blue block at
    normalized coordinates ({block_x}, {block_y}) (relative coordinates:
    {block_relative_x}, {block_relative_y}) and place it into the orange
    bowl at normalized coordinates ({bowl_x}, {bowl_y})
    (relative coordinates: {bowl_relative_x}, {bowl_relative_y}).
    Provide the sequence of function calls as a JSON list of objects, where
    each object has a "function" key (the function name) and an "args" key
    (a list of arguments for the function).
    Also, include your reasoning before the JSON output.
    For example:
    Reasoning: To pick up the block, I will first move the arm to a high
    position above the block, open the gripper, move down to the block,
    close the gripper, lift the arm, move to a high position above the bowl,
    move down to the bowl, open the gripper, and then lift the arm back to
    a high position.
"""
```

 
 

The following shows a possible output of the model based on the prompt and
the mock robot API. The output includes the model thinking process and the
tasks that it planned out as a result. It also shows the output of the robot
function calls that the model sequenced together.

 

```
Reasoning: To perform the pick and place operation, I will first move the
robotic arm to a high position above the blue block to ensure clearance. Then, I
will open the gripper in preparation for grasping. Next, I will lower the arm to
the surface level to position the gripper around the block and close the gripper
to secure it. After picking up the block, I will lift the arm to clear the
surface and move towards the orange bowl, maintaining a high position to avoid
collisions. Once positioned above the bowl, I will lower the arm to place the
block inside. I will then open the gripper to release the block. Finally, I will
lift the arm out of the bowl and call `returnToOrigin` to reset the robot's
state.

[
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      false
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      true
    ]
  },
  {
    "function": "returnToOrigin",
    "args": []
  }
]


Executing Function Calls:
moving to coordinates: 163, 427, 15
Opening gripper
moving to coordinates: 163, 427, 5
Closing gripper
moving to coordinates: 163, 427, 15
moving to coordinates: -247, 90, 15
moving to coordinates: -247, 90, 5
Opening gripper
moving to coordinates: -247, 90, 15
Returning to origin pose
```

 

### Code execution

Gemini Robotics-ER 1.5 can suggest and execute Python code to perform tasks that
require dynamic actions, such as zooming into an image region for better detail.

This example demonstrates how the model can suggest using the
 code execution tool to "zoom in" on a specific
area of an image, which it then carries out to answer the user's question.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client(api_key=YOUR_API_KEY)

MODEL_ID = "gemini-robotics-er-1.5-preview"

# Load your image and set up your prompt
with open('path/to/image-of-object.jpg', 'rb') as f:
    image_bytes = f.read()
prompt = """
          What is the air quality reading? Using the code execution feature,
          zoom in on the image to take a closer look.
        """

response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/jpeg',
        ),
        prompt
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

The following shows a possible model response for this task. The output shows
the model generating code for zooming into the image to read the screen better.
It also shows the thought process around the adjustments it made to the
cropping. For the complete sample output, see the Robotics
cookbook .

 

```
# Detect relevant objects and output in json format
# [{"box_2d": [379, 407, 481, 553], "label": "display"}]
from PIL import Image

# Open the image
img = Image.open("input_file_0.jpeg")

# Define the coordinates for cropping (normalized to 0-1000)
# Based on visual inspection, the display is roughly centered around [430, 430] to [500, 500] in a 1000x1000 scale.
# Let's try to zoom in on the display area.
# Bounding box for display: [ymin, xmin, ymax, xmax] in pixels.
# From visual inspection of the original image (size 90489 bytes, resolution
unknown but let's assume a standard aspect ratio and roughly 1000x1000 for
relative positioning),
# the display is located at approximately y=380-480, x=410-540.
# Let's define a larger area around it for context.
# Let's say we want to crop from y=350 to y=550 and x=350 to x=650.

...

thought
The crop was too tight and only captured part of the display. Let's adjust the bounding box.
The previous crop was `img.crop((400, 350, 600, 500))`. This means xmin=400, ymin=350, xmax=600, ymax=500.
The display is centered around x=480, y=430.
The previous crop captured the left side of the display.
Let's expand the x range to the right and y range to cover the whole circle.
New bounding box estimate: ymin=370, xmin=400, ymax=500, xmax=600.
Let's try again with a wider crop.
# Detect relevant objects and output in json format
# [{"box_2d": [379, 407, 481, 553], "label": "display"}]
from PIL import Image

# Open the image
img = Image.open("input_file_0.jpeg")

# Define the coordinates for cropping (normalized to 0-1000)
‚Ä¶

The reading of 14 falls into the "Moderate" category.
The prompt asks for the "air quality reading", which can be interpreted as
either the numerical value or the categorical description. Given the numerical
value is explicitly displayed, it's the primary reading.

Let's provide the numerical value as the reading.
The reading is 014.
Based on the display on the air purifier, the air quality reading is **014**.
```

 

## Best practices

To optimize the performance and accuracy of your robotics applications, it's
crucial to understand how to interact with the Gemini model effectively. This
section outlines best practices and key strategies for crafting prompts,
handling visual data, and structuring tasks to achieve the most reliable
results.

- 

Use clear and simple language.

 

 Embrace natural language : The Gemini model is designed to comprehend
natural, conversational language. Structure your prompts in a way that is
semantically clear and mirrors how a person would naturally give
instructions.

- 

 Use everyday terminology : Opt for common, everyday language over
technical or specialized jargon. If the model is not responding as
expected to a particular term, try rephrasing it with a more common
synonym.

 
- 

Optimize the visual input.

 

 Zoom in for detail : When dealing with objects that are small or
difficult to discern in a wider shot, use a bounding box function to
isolate the object of interest. You can then crop the image to this
selection and send the new, focused image to the model for a more
detailed analysis.

- 

 Experiment with lighting and color : The model's perception can be
affected by challenging lighting conditions and poor color contrast.

 
- 

Break down complex problems into smaller steps. By addressing each smaller
step individually, you can guide the model to a more precise and successful
outcome.

- 

Improve accuracy through consensus. For tasks that require a high degree of
precision, you can query the model multiple times with the same prompt. By
averaging the returned results, you can arrive at a "consensus" that is
often more accurate and reliable.

## Limitations

Consider the following limitations when developing with Gemini Robotics-ER 1.5:

- Preview status: The model is currently in Preview . APIs and
capabilities may change, and it may not be suitable for production-critical
applications without thorough testing.

- Latency: Complex queries, high-resolution inputs, or extensive
`thinking_budget` can lead to increased processing times.

- Hallucinations: Like all large language models, Gemini Robotics-ER 1.5
can occasionally "hallucinate" or provide incorrect information, especially
for ambiguous prompts or out-of-distribution inputs.

- Dependence on prompt quality: The quality of the model's output is
highly dependent on the clarity and specificity of the input prompt. Vague
or poorly structured prompts can lead to suboptimal results.

- Computational cost: Running the model, especially with video inputs or
high `thinking_budget`, consumes computational resources and incurs costs.
See the Thinking page for more details.

- Input types: See the following topics for details on limitations for each mode.

 Image inputs 

- Video inputs 

- Audio inputs 

 

## Privacy Notice

You acknowledge that the models referenced in this document (the "Robotics
Models") leverage video and audio data in order to operate and move your
hardware in accordance with your instructions. You therefore may operate the
Robotics Models such that data from identifiable persons, such as voice,
imagery, and likeness data ("Personal Data"), will be collected by the Robotics
Models. If you elect to operate the Robotics Models in a manner that collects
Personal Data, you agree that you will not permit any identifiable persons to
interact with, or be present in the area surrounding, the Robotics Models,
unless and until such identifiable persons have been sufficiently notified of
and consented to the fact that their Personal Data may be provided to and used
by Google as outlined in the Gemini API Additional Terms of Service found at
 https://ai.google.dev/gemini-api/terms 
(the "Terms"), including in accordance
with the section entitled "How Google Uses Your Data". You will ensure that such
notice permits the collection and use of Personal Data as outlined in the Terms,
and you will use commercially reasonable efforts to minimize the collection and
distribution of Personal Data by using techniques such as face blurring and
operating the Robotics Models in areas not containing identifiable persons to
the extent practicable.

## Pricing

For detailed information on pricing and available regions, refer to the
 pricing page.

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-robotics-er-1.5-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-robotics-er-1.5-preview`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 

## Next steps

- Explore other capabilities and continue experimenting with different prompts
and inputs to discover more applications for Gemini Robotics-ER 1.5.
See the Robotics cookbook 
for more examples.

- Learn about how Gemini Robotics models were built with safety in mind, visit
the Google DeepMind robotics safety
page .

- Read about the latest updates on Gemini Robotics models on the
 Gemini Robotics landing page .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### Speech generation (text-to-speech) &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/speech-generation

- 
 
 
 
 
 
 
 
 
 
 
 Speech generation (text-to-speech)  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ÔøΩÔøΩÔøΩ ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Speech generation (text-to-speech) 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can transform text input into single speaker or multi-speaker
audio using native text-to-speech (TTS) generation capabilities.
Text-to-speech (TTS) generation is controllable ,
meaning you can use natural language to structure interactions and guide the
 style , accent , pace , and tone of the audio.

The TTS capability differs from speech generation provided through the
 Live API , which is designed for interactive,
unstructured audio, and multimodal inputs and outputs. While the Live API excels
in dynamic conversational contexts, TTS through the Gemini API
is tailored for scenarios that require exact text recitation with fine-grained
control over style and sound, such as podcast or audiobook generation.

This guide shows you how to generate single-speaker and multi-speaker audio from
text.

## Before you begin

Ensure you use a Gemini 2.5 model variant with native text-to-speech (TTS) capabilities, as listed in the Supported models section. For optimal results, consider which model best fits your specific use case. 

You may find it useful to test the Gemini 2.5 TTS models in AI Studio before you start building.

## Single-speaker text-to-speech

To convert text to single-speaker audio, set the response modality to "audio",
and pass a `SpeechConfig` object with `VoiceConfig` set.
You'll need to choose a voice name from the prebuilt output voices .

This example saves the output audio from the model in a wave file:

 
 

### Python

 

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents="Say cheerfully: Have a wonderful day!",
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
               voice_name='Kore',
            )
         )
      ),
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

 
 
 
 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Kore' },
               },
            },
      },
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}
await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "contents": [{
          "parts":[{
            "text": "Say cheerfully: Have a wonderful day!"
          }]
        }],
        "generationConfig": {
          "responseModalities": ["AUDIO"],
          "speechConfig": {
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }
        },
        "model": "gemini-2.5-flash-preview-tts",
    }' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
          base64 --decode >out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

 
 

## Multi-speaker text-to-speech

For multi-speaker audio, you'll need a `MultiSpeakerVoiceConfig` object with
each speaker (up to 2) configured as a `SpeakerVoiceConfig`.
You'll need to define each `speaker` with the same names used in the
 prompt :

 
 

### Python

 

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

prompt = """TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?"""

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=prompt,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Joe',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Jane',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const prompt = `TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?`;

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: prompt }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               multiSpeakerVoiceConfig: {
                  speakerVoiceConfigs: [
                        {
                           speaker: 'Joe',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Kore' }
                           }
                        },
                        {
                           speaker: 'Jane',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Puck' }
                           }
                        }
                  ]
               }
            }
      }
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
  "contents": [{
    "parts":[{
      "text": "TTS the following conversation between Joe and Jane:
                Joe: Hows it going today Jane?
                Jane: Not too bad, how about you?"
    }]
  }],
  "generationConfig": {
    "responseModalities": ["AUDIO"],
    "speechConfig": {
      "multiSpeakerVoiceConfig": {
        "speakerVoiceConfigs": [{
            "speaker": "Joe",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }, {
            "speaker": "Jane",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Puck"
              }
            }
          }]
      }
    }
  },
  "model": "gemini-2.5-flash-preview-tts",
}' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
    base64 --decode > out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

 
 

## Controlling speech style with prompts

You can control style, tone, accent, and pace using natural language prompts
for both single- and multi-speaker TTS.
For example, in a single-speaker prompt, you can say:

 

```
Say in an spooky whisper:
"By the pricking of my thumbs...
Something wicked this way comes"
```

 

In a multi-speaker prompt, provide the model with each speaker's name and
corresponding transcript. You can also provide guidance for each speaker
individually:

 

```
Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:

Speaker1: So... what's on the agenda today?
Speaker2: You're never going to guess!
```

 

Try using a voice option that corresponds to the style or emotion you
want to convey, to emphasize it even more. In the previous prompt, for example,
 Enceladus 's breathiness might emphasize "tired" and "bored", while
 Puck 's upbeat tone could complement "excited" and "happy".

## Generating a prompt to convert to audio

The TTS models only output audio, but you can use
 other models to generate a transcript first,
then pass that transcript to the TTS model to read aloud.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

transcript = client.models.generate_content(
   model="gemini-2.0-flash",
   contents="""Generate a short transcript around 100 words that reads
            like it was clipped from a podcast by excited herpetologists.
            The hosts names are Dr. Anya and Liam.""").text

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=transcript,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Dr. Anya',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Liam',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

# ...Code to stream or save the output
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {

const transcript = await ai.models.generateContent({
   model: "gemini-2.0-flash",
   contents: "Generate a short transcript around 100 words that reads like it was clipped from a podcast by excited herpetologists. The hosts names are Dr. Anya and Liam.",
   })

const response = await ai.models.generateContent({
   model: "gemini-2.5-flash-preview-tts",
   contents: transcript,
   config: {
      responseModalities: ['AUDIO'],
      speechConfig: {
         multiSpeakerVoiceConfig: {
            speakerVoiceConfigs: [
                   {
                     speaker: "Dr. Anya",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Kore"},
                     }
                  },
                  {
                     speaker: "Liam",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Puck"},
                    }
                  }
                ]
              }
            }
      }
  });
}
// ..JavaScript code for exporting .wav file for output audio

await main();
```

 
 

## Voice options

TTS models support the following 30 voice options in the `voice_name` field:

 
 
 
 
 
 
 
 
 
 Zephyr -- Bright 
 Puck -- Upbeat 
 Charon -- Informative 
 
 
 Kore -- Firm 
 Fenrir -- Excitable 
 Leda -- Youthful 
 
 
 Orus -- Firm 
 Aoede -- Breezy 
 Callirrhoe -- Easy-going 
 
 
 Autonoe -- Bright 
 Enceladus -- Breathy 
 Iapetus -- Clear 
 
 
 Umbriel -- Easy-going 
 Algieba -- Smooth 
 Despina -- Smooth 
 
 
 Erinome -- Clear 
 Algenib -- Gravelly 
 Rasalgethi -- Informative 
 
 
 Laomedeia -- Upbeat 
 Achernar -- Soft 
 Alnilam -- Firm 
 
 
 Schedar -- Even 
 Gacrux -- Mature 
 Pulcherrima -- Forward 
 
 
 Achird -- Friendly 
 Zubenelgenubi -- Casual 
 Vindemiatrix -- Gentle 
 
 
 Sadachbia -- Lively 
 Sadaltager -- Knowledgeable 
 Sulafat -- Warm 
 
 
 
 

You can hear all the voice options in
 AI Studio .

## Supported languages

The TTS models detect the input language automatically. They support the
following 24 languages:

 
 
 
 
 
 
 
 
 
 Language 
 BCP-47 Code 
 Language 
 BCP-47 Code 
 
 
 
 
 Arabic (Egyptian) 
 `ar-EG` 
 German (Germany) 
 `de-DE` 
 
 
 English (US) 
 `en-US` 
 Spanish (US) 
 `es-US` 
 
 
 French (France) 
 `fr-FR` 
 Hindi (India) 
 `hi-IN` 
 
 
 Indonesian (Indonesia) 
 `id-ID` 
 Italian (Italy) 
 `it-IT` 
 
 
 Japanese (Japan) 
 `ja-JP` 
 Korean (Korea) 
 `ko-KR` 
 
 
 Portuguese (Brazil) 
 `pt-BR` 
 Russian (Russia) 
 `ru-RU` 
 
 
 Dutch (Netherlands) 
 `nl-NL` 
 Polish (Poland) 
 `pl-PL` 
 
 
 Thai (Thailand) 
 `th-TH` 
 Turkish (Turkey) 
 `tr-TR` 
 
 
 Vietnamese (Vietnam) 
 `vi-VN` 
 Romanian (Romania) 
 `ro-RO` 
 
 
 Ukrainian (Ukraine) 
 `uk-UA` 
 Bengali (Bangladesh) 
 `bn-BD` 
 
 
 English (India) 
 `en-IN` & `hi-IN` bundle 
 Marathi (India) 
 `mr-IN` 
 
 
 Tamil (India) 
 `ta-IN` 
 Telugu (India) 
 `te-IN` 
 
 
 

## Supported models

 
 
 
 Model 
 Single speaker 
 Multispeaker 
 
 

 
 
 Gemini 2.5 Flash Preview TTS 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Pro Preview TTS 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 

## Limitations

- TTS models can only receive text inputs and generate audio outputs. 

- A TTS session has a context window limit of 32k
tokens.

- Review Languages section for language support.

## What's next

- Try the audio generation cookbook .

- Gemini's Live API offers interactive audio
generation options you can interleave with other modalities.

- For working with audio inputs , visit the Audio understanding guide.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini Developer API pricing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/pricing#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Developer API pricing  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 

# 
 Gemini Developer API pricing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Start building free of charge with generous limits, then scale up with pay-as-you-go
pricing for your production ready applications.

 
 
 
 

### Free

 

For developers and small projects getting started with the Gemini API.

 

 - check_circle Limited access to certain models

 - check_circle Free input & output tokens

 - check_circle Google AI Studio access

 - check_circle Content used to improve our products * 

 

 Get started for Free 
 
 
 

### Paid

 

For production applications that require higher volumes and advanced features.

 

 - check_circle Higher rate limits for production deployments

 - check_circle Access to Context caching

 - check_circle Batch API (50% cost reduction)

 - check_circle Access to Google's most advanced models

 - check_circle Content *not* used to improve our products * 

 

 Upgrade to Paid 
 
 
 

### Enterprise

 

For large-scale deployments with custom needs for security, support, and compliance, powered by Vertex AI .

 

 - check_circle All features in Paid, plus optional access to:

 - check_circle Dedicated support channels

 - check_circle Advanced security & compliance

 - check_circle Provisioned throughput

 - check_circle Volume-based discounts (based on usage)

 - check_circle ML ops, model garden and more

 

 Contact Sales 
 
 
 

 
 
 

## Gemini 3 Pro Preview

 `gemini-3-pro-preview` 
 
 

 Try it in Google AI Studio 
 

 

The best model in the world for multimodal understanding, and our most powerful
agentic and vibe-coding model yet.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $2.00, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $12.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.20, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then (Coming soon) $14 / 1,000 search queries 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.00, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $6.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.20, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then (Coming soon) $14 / 1,000 search queries 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Pro

 `gemini-2.5-pro` 
 
 

 Try it in Google AI Studio 
 

 

Our state-of-the-art multipurpose model, which excels at coding and complex
reasoning tasks.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $1.25, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $10.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.125, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 10,000 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.625, prompts 200k tokens 
 
 
 Output price (including thinking tokens) 
 Not available 
 $5.00, prompts 200k 
 
 
 Context caching price 
 Not available 
 $0.125, prompts 200k
$4.50 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash

 `gemini-2.5-flash` 
 
 

 Try it in Google AI Studio 
 

 

Our first hybrid reasoning model which supports a 1M token context window and
has thinking budgets.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image / video)
$0.50 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $1.25 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash Preview

 `gemini-2.5-flash-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

The latest model based on the 2.5 Flash model. 2.5 Flash Preview is best for
large scale processing, low-latency,
high volume tasks that require thinking, and agentic use cases.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image / video)
$0.50 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $1.25 
 
 
 Context caching price 
 Not available 
 $0.03 (text / image / video)
$0.1 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash-Lite

 `gemini-2.5-flash-lite` 
 
 

 Try it in Google AI Studio 
 

 

Our smallest and most cost effective model, built for at scale usage.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Free of charge 
 $0.10 (text / image / video)
$0.30 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash RPD) 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Not available 
 $0.05 (text / image / video)
$0.15 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash-Lite Preview

 `gemini-2.5-flash-lite-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

The latest model based on Gemini 2.5 Flash lite optimized for cost-efficiency,
high throughput and high quality.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Free of charge 
 $0.10 (text / image / video)
$0.30 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash RPD) 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price (text, image, video) 
 Not available 
 $0.05 (text / image / video)
$0.15 (audio) 
 
 
 Output price (including thinking tokens) 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.01 (text / image / video)
$0.03 (audio)
$1.00 / 1,000,000 tokens per hour (storage price) 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free, limit shared with Flash RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Flash Native Audio (Live API)

 `gemini-2.5-flash-native-audio-preview-09-2025` 
 
 

 Try it in Google AI Studio 
 

 

Our Live API native audio models optimized for higher
quality audio outputs with better pacing, voice naturalness, verbosity, and
mood.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.50 (text)
$3.00 (audio / video) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.00 (text)
$12.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

The Live API also includes half-cascade audio generation models:

- `gemini-live-2.5-flash-preview`: Same price as the native audio model.

- `gemini-2.0-flash-live-001`: Input $0.35 (text), $2.10 (audio / image / video),
Output: $1.50 (text), $8.50 (audio)

These models will be deprecated soon.

 
 
 

## Gemini 2.5 Flash Image üçå

 `gemini-2.5-flash-image` 
 
 

 Try it in Google AI Studio 
 

 

Our native image generation model, optimized for speed, flexibility, and
contextual understanding. Text input and output is priced the same as
 2.5 Flash .

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.30 (text / image) 
 
 
 Output price 
 Not available 
 $0.039 per image* 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.15 (text / image) 
 
 
 Output price 
 Not available 
 $0.0195 per image* 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

[*] Image output is priced at $30 per 1,000,000 tokens. Output images up to
1024x1024px consume 1290 tokens and are equivalent to $0.039 per image.

 
 
 

## Gemini 2.5 Flash Preview TTS

 `gemini-2.5-flash-preview-tts` 
 
 

 Try it in Google AI Studio 
 

 

Our 2.5 Flash text-to-speech audio model optimized for price-performant,
low-latency, controllable speech generation.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.50 (text) 
 
 
 Output price 
 Free of charge 
 $10.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.25 (text) 
 
 
 Output price 
 Not available 
 $5.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Pro Preview TTS

 `gemini-2.5-pro-preview-tts` 
 
 

 Try it in Google AI Studio 
 

 

Our 2.5 Pro text-to-speech audio model optimized for powerful, low-latency
speech generation for more natural outputs and easier to steer prompts.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.00 (text) 
 
 
 Output price 
 Not available 
 $20.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.50 (text) 
 
 
 Output price 
 Not available 
 $10.00 (audio) 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.0 Flash

 `gemini-2.0-flash` 
 
 

 Try it in Google AI Studio 
 

 

Our most balanced multimodal model with great performance across all tasks, with
a 1 million token context window, and built for the era of Agents.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.10 (text / image / video)
$0.70 (audio) 
 
 
 Output price 
 Free of charge 
 $0.40 
 
 
 Context caching price 
 Free of charge 
 $0.025 / 1,000,000 tokens (text/image/video)
$0.175 / 1,000,000 tokens (audio) 
 
 
 Context caching (storage) 
 Not available 
 $1.00 / 1,000,000 tokens per hour 
 
 
 Image generation pricing 
 Free of charge 
 $0.039 per image* 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 500 RPD 
 1,500 RPD (free), then $25 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.05 (text / image / video)
$0.35 (audio) 
 
 
 Output price 
 Not available 
 $0.20 
 
 
 Context caching price 
 Not available 
 $0.025 / 1,000,000 tokens (text/image/video)
$0.175 / 1,000,000 tokens (audio) 
 
 
 Context caching (storage) 
 Not available 
 $1.00 / 1,000,000 tokens per hour 
 
 
 Image generation pricing 
 Not available 
 $0.0195 per image* 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 1,500 RPD (free), then $35 / 1,000 grounded prompts 
 
 
 Grounding with Google Maps 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

[*] Image output is priced at $30 per 1,000,000 tokens. Output images up to
1024x1024px consume 1290 tokens and are equivalent to $0.039 per image.

 
 
 

## Gemini 2.0 Flash-Lite

 `gemini-2.0-flash-lite` 
 
 

 Try it in Google AI Studio 
 

 

Our smallest and most cost effective model, built for at scale usage.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.075 
 
 
 Output price 
 Free of charge 
 $0.30 
 
 
 Context caching price 
 Not available 
 Not available 
 
 
 Context caching (storage) 
 Not available 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.0375 
 
 
 Output price 
 Not available 
 $0.15 
 
 
 Context caching price 
 Not available 
 Not available 
 
 
 Context caching (storage) 
 Not available 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Imagen 4

 `imagen-4.0-generate-001`, `imagen-4.0-ultra-generate-001`, `imagen-4.0-fast-generate-001` 
 
 

 Try it in Google AI Studio 
 

 

Our latest image generation model, with significantly better text rendering and
better overall image quality.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per Image in USD 
 
 
 
 
 Imagen 4 Fast image price 
 Not available 
 $0.02 
 
 
 Imagen 4 Standard image price 
 Not available 
 $0.04 
 
 
 Imagen 4 Ultra image price 
 Not available 
 $0.06 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Imagen 3

 `imagen-3.0-generate-002` 
 
 

 Try it in Google AI Studio 
 

 

Our state-of-the-art image generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per Image in USD 
 
 
 
 
 Image price 
 Not available 
 $0.03 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 3.1

 `veo-3.1-generate-preview`, `veo-3.1-fast-generate-preview` 
 
 

 Try Veo 3.1 
 

 

Our latest video generation model, available to developers on the
paid tier of the Gemini API.

Preview models may change before becoming stable and have more restrictive rate
limits.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Veo 3.1 Standard video with audio price (default) 
 Not available 
 $0.40 
 
 
 Veo 3.1 Fast video with audio price (default) 
 Not available 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 3

 `veo-3.0-generate-001`, `veo-3.0-fast-generate-001` 
 
 

 Try Veo 3 
 

 

Our stable video generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Veo 3 Standard video with audio price (default) 
 Not available 
 $0.40 
 
 
 Veo 3 Fast video with audio price (default) 
 Not available 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Veo 2

 `veo-2.0-generate-001` 
 
 

 Try the API 
 

 

Our state-of-the-art video generation model, available to developers on the
paid tier of the Gemini API.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per second in USD 
 
 
 
 
 Video price 
 Not available 
 $0.35 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 
 

## Gemini Embedding

 `gemini-embedding-001` 
 
 

 Try the API 
 

 

Our newest embeddings model, more stable and with higher rate limits than
previous versions, available to developers on the free and paid tiers of the Gemini API.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.15 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $0.075 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini Robotics-ER 1.5 Preview

 `gemini-robotics-er-1.5-preview` 
 
 

 Try it in Google AI Studio 
 

 

Gemini Robotics-ER, short for Gemini Robotics-Embodied Reasoning, is a thinking
model that enhances robots' abilities to understand and interact with the
physical world.

 
 

### Standard

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 $0.30 (text / image / video)
$1.00 (audio) 
 
 
 Output price (including thinking tokens) 
 Free of charge 
 $2.50 
 
 
 Grounding with Google Search 
 Free of charge, up to 500 RPD (limit shared with Flash-Lite RPD) 
 1,500 RPD (free, limit shared with Flash-Lite RPD), then $35 / 1,000 grounded prompts 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 

### Batch

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 Not available 
 
 
 Output price (including thinking tokens) 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 
 
 
 
 

## Gemini 2.5 Computer Use Preview

 `gemini-2.5-computer-use-preview-10-2025` 
 
 

Our Computer Use model optimized for building browser control agents that
automate tasks.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Not available 
 $1.25, prompts 200k token 
 
 
 Output price 
 Not available 
 $10.00, prompts 200k 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 

## Gemma 3

 

 Try Gemma 3 
 

 

Our lightweight, state-of the art, open model built from the same technology
that powers our Gemini models.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 Not available 
 
 
 Output price 
 Free of charge 
 Not available 
 
 
 Context caching price 
 Free of charge 
 Not available 
 
 
 Context caching (storage) 
 Free of charge 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

 
 

## Gemma 3n

 

 Try Gemma 3n 
 

 

Our open model built for efficient performance on everyday devices like mobile
phones, laptops, and tablets.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Input price 
 Free of charge 
 Not available 
 
 
 Output price 
 Free of charge 
 Not available 
 
 
 Context caching price 
 Free of charge 
 Not available 
 
 
 Context caching (storage) 
 Free of charge 
 Not available 
 
 
 Tuning price 
 Not available 
 Not available 
 
 
 Grounding with Google Search 
 Not available 
 Not available 
 
 
 Used to improve our products 
 Yes 
 No 
 
 
 

## Pricing for Tools

Tools are priced at their own rates, applied to the model using them.
Check the Models page for which tools are available
to each model.

 
 
 
 
 
 
 
 
 
 Free Tier 
 Paid Tier, per 1M tokens in USD 
 
 
 
 
 Google Search 
 500 RPD free (limit shared for Flash and Flash-Lite).
Not available for Pro. 
 1,500 RPD free (limit shared for Flash and Flash-Lite).
Then $35 / 1,000 grounded prompts
 
 
 
 Google Maps 
 500 RPD
Not available for Pro. 
 1,500 RPD free (limit shared for Flash and Flash-Lite)
10,000 RPD free for Pro.
Then $25 / 1,000 grounded prompts 
 
 
 Code execution 
 Free of charge 
 Free of charge 
 
 
 URL context 
 Free of charge 
 Charged as input tokens per model pricing. 
 
 
 Computer use 
 Not available 
 See Gemini 2.5 Computer Use Preview pricing table. 
 
 
 File search 
 Free of charge 
 Charged for embeddings at $0.15 / 1M tokens.
Retrieved document tokens charged as regular tokens per model pricing. 
 
 
 

[*] Google AI Studio usage is free of charge in all available regions .
See Billing FAQs for details.

[**] Prices may differ from the prices listed here and the prices offered on
Vertex AI. For Vertex prices, see the Vertex AI pricing page .

[***] If you are using dynamic retrieval to
optimize costs, only requests that contain at least one grounding support URL
from the web in their response are charged for Grounding with Google Search.
Costs for Gemini always apply. Rate limits are subject to change.

 
 

 
 

 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Audio understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/audio

- 
 
 
 
 
 
 
 
 
 
 
 Audio understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Audio understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini can analyze and understand audio input, enabling use cases like the
following:

- Describe, summarize, or answer questions about audio content.

- Provide a transcription of the audio.

- Analyze specific segments of the audio.

This guide shows you how to use the Gemini API to generate a text response to
audio input.

### Before you begin

Before calling the Gemini API, ensure you have your SDK of choice 
installed, and a Gemini API key configured and ready to use.

## Input audio

You can provide audio data to Gemini in the following ways:

- Upload an audio file before making a request to
`generateContent`.

- Pass inline audio data with the request to
`generateContent`.

### Upload an audio file

You can use the Files API to upload an audio file.
Always use the Files API when the total request size (including the files, text
prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads an audio file and then uses the file in a call to
`generateContent`.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

To learn more about working with media files, see
 Files API .

### Pass audio data inline

Instead of uploading an audio file, you can pass inline audio data in the
request to `generateContent`:

 
 

### Python

 

```
from google import genai
from google.genai import types

with open('path/to/small-sample.mp3', 'rb') as f:
    audio_bytes = f.read()

client = genai.Client()
response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[
    'Describe this audio clip',
    types.Part.from_bytes(
      data=audio_bytes,
      mime_type='audio/mp3',
    )
  ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  audioBytes, _ := os.ReadFile("/path/to/small-sample.mp3")

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "audio/mp3",
        Data:     audioBytes,
      },
    },
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

A few things to keep in mind about inline audio data:

- The maximum request size is 20 MB, which includes text prompts,
system instructions, and files provided inline. If your file's
size will make the total request size exceed 20 MB, then
use the Files API to upload an audio file for use in
the request.

- If you're using an audio sample multiple times, it's more efficient
to upload an audio file .

## Get a transcript

To get a transcript of audio data, just ask for it in the prompt:

 
 

### Python

 

```
from google import genai

client = genai.Client()
myfile = client.files.upload(file='path/to/sample.mp3')
prompt = 'Generate a transcript of the speech.'

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[prompt, myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Generate a transcript of the speech."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

## Refer to timestamps

You can refer to specific sections of an audio file using timestamps of the form
`MM:SS`. For example, the following prompt requests a transcript that

- Starts at 2 minutes 30 seconds from the beginning of the file.

- 

Ends at 3 minutes 29 seconds from the beginning of the file.

 
 

### Python

 

```
# Create a prompt containing timestamps.
prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

 
 

### JavaScript

 

```
// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Provide a transcript of the speech " +
                            "between the timestamps 02:30 and 03:29."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

## Count tokens

Call the `countTokens` method to get a count of the number of tokens in an
audio file. For example:

 
 

### Python

 

```
from google import genai

client = genai.Client()
response = client.models.count_tokens(
  model='gemini-2.5-flash',
  contents=[myfile]
)

print(response)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  tokens, _ := client.Models.CountTokens(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
}
```

 
 

## Supported audio formats

Gemini supports the following audio format MIME types:

- WAV - `audio/wav`

- MP3 - `audio/mp3`

- AIFF - `audio/aiff`

- AAC - `audio/aac`

- OGG Vorbis - `audio/ogg`

- FLAC - `audio/flac`

## Technical details about audio

- Gemini represents each second of audio as 32 tokens; for example,
one minute of audio is represented as 1,920 tokens.

- Gemini can "understand" non-speech components, such as birdsong or sirens.

- The maximum supported length of audio data in a single prompt is 9.5 hours.
Gemini doesn't limit the number of audio files in a single prompt; however,
the total combined length of all audio files in a single prompt can't exceed
9.5 hours.

- Gemini downsamples audio files to a 16 Kbps data resolution.

- If the audio source contains multiple channels, Gemini combines those channels
into a single channel.

## What's next

This guide shows how to generate text in response to audio data. To learn more,
see the following resources:

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- Safety guidance : Sometimes generative AI
models produce unexpected outputs, such as outputs that are inaccurate,
biased, or offensive. Post-processing and human evaluation are essential to
limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-26 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-26 UTC."],[],[]]

---

### Rate limits &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/rate-limits#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Rate limits  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Rate limits 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Rate limits regulate the number of requests you can make to the Gemini API
within a given timeframe. These limits help maintain fair usage, protect against
abuse, and help maintain system performance for all users.

 View your active rate limits in AI Studio 

## How rate limits work

Rate limits are usually measured across three dimensions:

- Requests per minute ( RPM )

- Tokens per minute (input) ( TPM )

- Requests per day ( RPD )

Your usage is evaluated against each limit, and exceeding any of them will
trigger a rate limit error. For example, if your RPM limit is 20, making 21
requests within a minute will result in an error, even if you haven't exceeded
your TPM or other limits.

Rate limits are applied per project, not per API key.

Requests per day ( RPD ) quotas reset at midnight Pacific time.

Limits vary depending on the specific model being used, and some limits only
apply to specific models. For example, Images per minute, or IPM, is only
calculated for models capable of generating images (Imagen 3), but is
conceptually similar to TPM. Other models might have a token per day limit (TPD).

Rate limits are more restricted for experimental and preview models.

## Usage tiers

Rate limits are tied to the project's usage tier. As your API usage and spending
increase, you'll have an option to upgrade to a higher tier with increased rate
limits.

The qualifications for Tiers 2 and 3 are based on the total cumulative spending
on Google Cloud services (including, but not limited to, the Gemini API) for the
billing account linked to your project.

 
 
 
 Tier 
 Qualifications 
 
 

 
 
 Free 
 Users in eligible countries 
 
 
 Tier 1 
 Billing account linked to the project 
 
 
 Tier 2 
 Total spend: > $250 and at least 30 days since successful payment 
 
 
 Tier 3 
 Total spend: > $1,000 and at least 30 days since successful payment 
 
 
 

When you request an upgrade, our automated abuse protection system performs
additional checks. While meeting the stated qualification criteria is generally
sufficient for approval, in rare cases an upgrade request may be denied based on
other factors identified during the review process.

This system helps maintain the security and integrity of the Gemini API platform
for all users.

## Standard API rate limits

The following table lists the rate limits for all standard Gemini API calls.

 
 
 
 

### Free Tier

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 
 
 
 
 Text-out models 
 
 
 Gemini 2.5 Pro 
 2 
 125,000 
 50 
 
 
 Gemini 2.5 Flash 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash Preview 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash-Lite 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.0 Flash 
 15 
 1,000,000 
 200 
 
 
 Gemini 2.0 Flash-Lite 
 30 
 1,000,000 
 200 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 500,000 
 * 
 
 
 Gemini 2.0 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 3 
 10,000 
 15 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 10 
 200,000 
 100 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 
 
 Gemini Embedding 
 100 
 30,000 
 1,000 
 
 
 Gemini Robotics-ER 1.5 Preview 
 10 
 250,000 
 250 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 15 
 250,000 
 50 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 15 
 250,000 
 50 
 
 
 
 
 
 

### Tier 1

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 50 
 1,000,000 
 1,000 
 50,000,000 
 
 
 Gemini 2.5 Pro 
 150 
 2,000,000 
 10,000 
 5,000,000 
 
 
 Gemini 2.5 Flash 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash Preview 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash 
 2,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 10 
 10,000 
 100 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 10 
 10,000 
 50 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 500 
 500,000 
 2,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 1,000 
 1,000,000 
 10,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 10 
 * 
 70 
 * 
 
 
 Imagen 4 Ultra 
 5 
 * 
 30 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 2 
 * 
 10 
 * 
 
 
 Veo 3.1 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 3 
 2 
 * 
 10 
 * 
 
 
 Veo 3 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 3,000 
 1,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 300 
 1,000,000 
 10,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 150 
 2,000,000 
 10,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 2

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Pro 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Flash 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash Preview 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.0 Flash 
 10,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 20,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 100,000 
 10,000 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 25,000 
 1,000 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 2,000 
 1,500,000 
 50,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 2,000 
 3,000,000 
 100,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 15 
 * 
 1000 
 * 
 
 
 Imagen 4 Ultra 
 10 
 * 
 400 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 4 
 * 
 50 
 * 
 
 
 Veo 3.1 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 3 
 4 
 * 
 50 
 * 
 
 
 Veo 3 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 400 
 3,000,000 
 100,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 1,000 
 5,000,000 
 50,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 3

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Pro 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash Preview 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Imagen 4 Standard/Fast 
 20 
 * 
 15,000 
 * 
 
 
 Imagen 4 Ultra 
 15 
 * 
 5,000 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 10 
 * 
 500 
 * 
 
 
 Veo 3.1 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 3 
 10 
 * 
 500 
 * 
 
 
 Veo 3 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 10,000 
 10,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 600 
 8,000,000 
 * 
 *1,000,000,000* 
 
 
 Gemini 2.5 Computer Use Preview 
 2,000 
 8,000,000 
 * 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

Specified rate limits are not guaranteed and actual capacity may vary.

## Batch API rate limits

 Batch API requests are subject to their own rate
limits, separate from the non-batch API calls.

- Concurrent batch requests: 100

- Input file size limit: 2GB

- File storage limit: 20GB

- Enqueued tokens per model: The Batch Enqueued Tokens column in the
rate limits table lists the maximum number of tokens that can be enqueued
for batch processing across all your active batch jobs for a given model.
See in the standard API rate limits table .

## How to upgrade to the next tier

The Gemini API uses Cloud Billing for all billing services. To transition from
the Free tier to a paid tier, you must first enable Cloud Billing for your
Google Cloud project.

Once your project meets the specified criteria, it becomes eligible for an
upgrade to the next tier. To request an upgrade, follow these steps:

- Navigate to the API keys page in AI Studio.

- Locate the project you want to upgrade and click "Upgrade". The "Upgrade" option
will only show up for projects that meet next tier qualifications .

After a quick validation, the project will be upgraded to the next tier.

## Request a rate limit increase

Each model variation has an associated rate limit (requests per minute, RPM).
For details on those rate limits, see Gemini models .

 Request paid tier rate limit increase 

We offer no guarantees about increasing your rate limit, but we'll do our best
to review your request.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Gemini thinking &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/thinking

- 
 
 
 
 
 
 
 
 
 
 
 Gemini thinking  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini thinking 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

The Gemini 3 and 2.5 series models use an internal
"thinking process" that significantly improves their reasoning and multi-step
planning abilities, making them highly effective for complex tasks such as
coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the
Gemini API.

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content
generation request. The key difference lies in specifying one of the
 models with thinking support in the `model` field, as
demonstrated in the following text generation example:

 
 

### Python

 

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

 
 

## Thought summaries

Thought summaries are synthesized versions of the model's raw thoughts and offer
insights into the model's internal reasoning process. Note that
thinking levels and budgets apply to the model's raw thoughts and not to thought
summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your
request configuration. You can then access the summary by iterating through the
`response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries
without streaming, which returns a single, final thought summary with the
response:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

 
 

 

And here is an example using thinking with streaming, which returns rolling,
incremental summaries during generation:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Answer:")
      print(part.text)
      answer += part.text
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

 
 

## Controlling thinking

Gemini models engage in dynamic thinking by default, automatically adjusting the
amount of reasoning effort based on the complexity of the user's request.
However, if you have specific latency constraints or require the model to engage
in deeper reasoning than usual, you can optionally use parameters to control
thinking behavior.

### Thinking levels (Gemini 3 Pro)

The `thinkingLevel` parameter, recommended for Gemini 3 models and onwards,
lets you control reasoning behavior.
You can set thinking level to `"low"` or `"high"`.
If you don't specify a thinking level, Gemini will use the model's default
dynamic thinking level, `"high"`, for Gemini 3 Pro Preview.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_level="low")
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingLevel: "low",
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingLevelVal := "low"

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-3-pro-preview"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingLevel: &thinkingLevelVal,
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingLevel": "low"
    }
  }
}'
```

 
 

You cannot disable thinking for Gemini 3 Pro.
Gemini 2.5 series models don't support `thinkingLevel`; use `thinkingBudget`
instead.

### Thinking budgets

The `thinkingBudget` parameter, introduced with the Gemini 2.5 series, guides
the model on the specific number of thinking tokens to use for reasoning.

The following are `thinkingBudget` configuration details for each model type.
You can disable thinking by setting `thinkingBudget` to 0.
Setting the `thinkingBudget` to -1 turns
on dynamic thinking , meaning the model will adjust the budget based on the
complexity of the request.

 
 
 
 
 
 
 
 
 
 
 Model 
 Default setting
(Thinking budget is not set) 
 Range 
 Disable thinking 
 Turn on dynamic thinking 
 
 
 
 
 2.5 Pro 
 Dynamic thinking: Model decides when and how much to think 
 `128` to `32768` 
 N/A: Cannot disable thinking 
 `thinkingBudget = -1` 
 
 
 2.5 Flash 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite Preview 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 Robotics-ER 1.5 Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Live Native Audio Preview (09-2025) 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 
 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
    }
  }
}'
```

 
 

Depending on the prompt, the model might overflow or underflow the token budget.

## Thought signatures

The Gemini API is stateless, so the model treats every API request independently
and doesn't have access to thought context from previous turns in multi-turn
interactions.

In order to enable maintaining thought context across multi-turn interactions,
Gemini returns thought signatures, which are encrypted representations of the
model's internal thought process.

- Gemini 2.5 models return thought signatures when thinking is enabled and
the request includes function calling ,
specifically function declarations .

- Gemini 3 models may return thought signatures for all types of parts .
We recommend you always pass all signatures back as received, but it's
 required for function calling signatures. Read the
 Thought Signatures page to
learn more.

The Google GenAI SDK automatically handles the
return of thought signatures for you. You only need to
 manage thought signatures manually 
if you're modifying conversation history or using the REST API.

Other usage limitations to consider with function calling include:

- Signatures are returned from the model within other parts in the response,
for example function calling or text parts.
 Return the entire response 
with all parts back to the model in subsequent turns.

- Don't concatenate parts with signatures together.

- Don't merge one part with a signature with another part without a signature.

## Pricing

When thinking is turned on, response pricing is the sum of output
tokens and thinking tokens. You can get the total number of generated thinking
tokens from the `thoughtsTokenCount` field.

 
 

### Python

 

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

 
 

### JavaScript

 

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

 
 

### Go

 

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

 
 

Thinking models generate full thoughts to improve the quality of the final
response, and then output summaries to provide insight into the
thought process. So, pricing is based on the full thought tokens the
model needs to generate to create a summary, despite only the summary being
output from the API.

You can learn more about tokens in the Token counting 
guide.

## Best practices

This section includes some guidance for using thinking models efficiently.
As always, following our prompting guidance and best practices will get you the best results.

### Debugging and steering

- 

 Review reasoning : When you're not getting your expected response from the
thinking models, it can help to carefully analyze Gemini's thought summaries.
You can see how it broke down the task and arrived at its conclusion, and use
that information to correct towards the right results.

- 

 Provide Guidance in Reasoning : If you're hoping for a particularly lengthy
output, you may want to provide guidance in your prompt to constrain the
 amount of thinking the model uses. This lets you reserve more
of the token output for your response.

### Task complexity

- Easy Tasks (Thinking could be OFF): For straightforward requests where
complex reasoning isn't required, such as fact retrieval or
classification, thinking is not required. Examples include:

 "Where was DeepMind founded?"

- "Is this email asking for a meeting or just providing information?"

 
- Medium Tasks (Default/Some Thinking): Many common requests benefit from a
degree of step-by-step processing or deeper understanding. Gemini can flexibly
use thinking capability for tasks like:

 Analogize photosynthesis and growing up.

- Compare and contrast electric cars and hybrid cars.

 
- Hard Tasks (Maximum Thinking Capability): For truly complex challenges,
such as solving complex math problems or coding tasks, we recommend setting
a high thinking budget. These types of tasks require the model to engage
its full reasoning and planning capabilities, often
involving many internal steps before providing an answer. Examples include:

 Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for
which 17 b is a divisor of 97 b .

- Write Python code for a web application that visualizes real-time stock
market data, including user authentication. Make it as efficient as
possible.

 

 

## Supported models, tools, and capabilities

Thinking features are supported on all 3 and 2.5 series models.
You can find all model capabilities on the
 model overview page.

Thinking models work with all of Gemini's tools and capabilities. This allows
the models to interact with external systems, execute code, or access real-time
information, incorporating the results into their reasoning and final response.

You can try examples of using tools with thinking models in the
 Thinking cookbook .

## What's next?

- Thinking coverage is available in our OpenAI Compatibility guide.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Text generation &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/text-generation#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Text generation  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Text generation 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can generate text output from various inputs, including text,
images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithTextInput {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

 

## Thinking with Gemini 2.5

2.5 Flash and Pro models have "thinking" enabled by default to enhance quality, which may take longer to run and increase token usage. 

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero. 

For more details, see the thinking guide .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.ThinkingConfig;

public class GenerateContentWithThinkingConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            // Disables thinking
            .thinkingConfig(ThinkingConfig.builder().thinkingBudget(0))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so,
pass a `GenerateContentConfig` 
object.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithSystemInstruction {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            .systemInstruction(
                Content.fromParts(Part.fromText("You are a cat. Your name is Neko.")))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Hello there", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

The `GenerateContentConfig` 
object also lets you override default generation parameters, such as
 temperature .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config = GenerateContentConfig.builder().temperature(0.1f).build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Explain how AI works", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Refer to the `GenerateContentConfig` 
in our API reference for a complete list of configurable parameters and their
descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with
media files. The following example demonstrates providing an image:

 
 

### Python

 

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.Content;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithMultiModalInputs {
  public static void main(String[] args) {

    Client client = new Client();

    Content content =
      Content.fromParts(
          Part.fromText("Tell me about this instrument"),
          Part.fromUri("/path/to/organ.jpg", "image/jpeg"));

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", content, null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

 
 

For alternative methods of providing images and more advanced image processing,
see our image understanding guide .
The API also supports document , video , and audio 
inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation 
process is complete.

For more fluid interactions, use streaming to receive `GenerateContentResponse` instances incrementally
as they're generated.

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentStream {
  public static void main(String[] args) {

    Client client = new Client();

    ResponseStream<GenerateContentResponse> responseStream =
      client.models.generateContentStream(
          "gemini-2.5-flash", "Write a story about a magic backpack.", null);

    for (GenerateContentResponse res : responseStream) {
      System.out.print(res.text());
    }

    // To save resources and avoid connection leaks, it is recommended to close the response
    // stream after consumption (or using try block to get the response stream).
    responseStream.close();
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Multi-turn conversations (chat)

Our SDKs provide functionality to collect multiple rounds of prompts and
responses into a chat, giving you an easy way to keep track of the conversation
history.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversation {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    GenerateContentResponse response =
        chatSession.sendMessage("I have 2 dogs in my house.");
    System.out.println("First response: " + response.text());

    response = chatSession.sendMessage("How many paws are in my house?");
    System.out.println("Second response: " + response.text());

    // Get the history of the chat session.
    // Passing 'true' to getHistory() returns the curated history, which excludes
    // empty or invalid parts.
    // Passing 'false' here would return the comprehensive history, including
    // empty or invalid parts.
    ImmutableList<Content> history = chatSession.getHistory(true);
    System.out.println("History: " + history);
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Streaming can also be used for multi-turn conversations.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversationWithStreaming {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    ResponseStream<GenerateContentResponse> responseStream =
        chatSession.sendMessageStream("I have 2 dogs in my house.", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    responseStream = chatSession.sendMessageStream("How many paws are in my house?", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    // Get the history of the chat session. History is added after the stream
    // is consumed and includes the aggregated response from the stream.
    System.out.println("History: " + chatSession.getHistory(false));
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Supported models

All models in the Gemini family support text generation. To learn more
about the models and their capabilities, visit the
 Models page.

## Best practices

### Prompting tips

For basic text generation, a zero-shot 
prompt often suffices without needing examples, system instructions or specific
formatting.

For more tailored outputs:

- Use System instructions to guide the model.

- Provide few example inputs and outputs to guide the model. This is often referred to as few-shot prompting.

Consult our prompt engineering guide for
more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our
 structured output guide to learn how.

## What's next

- Try the Gemini API getting started Colab .

- Explore Gemini's image ,
 video , audio 
and document understanding capabilities.

- Learn about multimodal
 file prompting strategies .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Image generation with Gemini (aka Nano Banana üçå) &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/image-generation#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Image generation with Gemini (aka Nano Banana üçå)  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Image generation with Gemini (aka Nano Banana üçå) 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini can generate and process images conversationally. You can prompt Gemini
with text, images, or a combination of both allowing you to create, edit, and
iterate on visuals with unprecedented control:

- Text-to-Image: Generate high-quality images from simple or complex text descriptions.

- Image + Text-to-Image (Editing): Provide an image and use text prompts to add, remove, or modify elements, change the style, or adjust the color grading.

- Multi-Image to Image (Composition & Style Transfer): Use multiple input images to compose a new scene or transfer the style from one image to another.

- Iterative Refinement: Engage in a conversation to progressively refine your image over multiple turns, making small adjustments until it's perfect.

- High-Fidelity Text Rendering: Accurately generate images that contain legible and well-placed text, ideal for logos, diagrams, and posters.

All generated images include a SynthID watermark .

## Image generation (text-to-image)

The following code demonstrates how to generate an image based on a descriptive
prompt.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

prompt = (
    "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"
)

response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
)

for part in response.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = part.as_image()
        image.save("generated_image.png")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      genai.Text("Create a picture of a nano banana dish in a " +
                 " fancy restaurant with a Gemini theme"),
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

 
 
 
 
 AI-generated image of a nano banana dish in a Gemini-themed restaurant 
 

## Image editing (text-and-image-to-image)

 Reminder : Make sure you have the necessary rights to any images you upload.
Don't generate content that infringe on others' rights, including videos or
images that deceive, harass, or harm. Your use of this generative AI service is
subject to our Prohibited Use Policy .

The following example demonstrates uploading base64 encoded images.
For multiple images, larger payloads, and supported MIME types, check the Image
understanding page.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

prompt = (
    "Create a picture of my cat eating a nano-banana in a "
    "fancy restaurant under the Gemini constellation",
)

image = Image.open("/path/to/cat_image.png")

response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt, image],
)

for part in response.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = part.as_image()
        image.save("generated_image.png")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "path/to/cat_image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    { text: "Create a picture of my cat eating a nano-banana in a" +
            "fancy restaurant under the Gemini constellation" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/cat_image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.5-flash-image",
     contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/cat_image.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

 
 
 
 
 AI-generated image of a cat eating a nano banana 
 

## Other image generation modes

Gemini supports other image interaction modes based on prompt structure and
context, including:

- Text to image(s) and text (interleaved): Outputs images with related text.

 Example prompt: "Generate an illustrated recipe for a paella."

 
- Image(s) and text to image(s) and text (interleaved) : Uses input images and text to create new related images and text.

 Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"

 
- Multi-turn image editing (chat): Keep generating and editing images conversationally.

 Example prompts: [upload an image of a blue car.] , "Turn this car into a convertible.", "Now change the color to yellow."

 

## Prompting guide and strategies

Mastering Gemini 2.5 Flash Image Generation starts with one fundamental
principle:

 

 Describe the scene, don't just list keywords. 
The model's core strength is its deep language understanding. A narrative,
descriptive paragraph will almost always produce a better, more coherent image
than a list of disconnected words.

 

### Prompts for generating images

The following strategies will help you create effective prompts to
generate exactly the images you're looking for.

#### 1. Photorealistic scenes

For realistic images, use photography terms. Mention camera angles, lens types,
lighting, and fine details to guide the model toward a photorealistic result.

 
 

### Template

 

```
A photorealistic [shot type] of [subject], [action or expression], set in
[environment]. The scene is illuminated by [lighting description], creating
a [mood] atmosphere. Captured with a [camera/lens details], emphasizing
[key textures and details]. The image should be in a [aspect ratio] format.
```

 
 

### Prompt

 

```
A photorealistic close-up portrait of an elderly Japanese ceramicist with
deep, sun-etched wrinkles and a warm, knowing smile. He is carefully
inspecting a freshly glazed tea bowl. The setting is his rustic,
sun-drenched workshop. The scene is illuminated by soft, golden hour light
streaming through a window, highlighting the fine texture of the clay.
Captured with an 85mm portrait lens, resulting in a soft, blurred background
(bokeh). The overall mood is serene and masterful. Vertical portrait
orientation.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('photorealistic_example.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("photorealistic_example.png", buffer);
      console.log("Image saved as photorealistic_example.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "photorealistic_example.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > photorealistic_example.png
```

 
 
 
 
 A photorealistic close-up portrait of an elderly Japanese ceramicist... 
 

#### 2. Stylized illustrations & stickers

To create stickers, icons, or assets, be explicit about the style and request a
transparent background.

 
 

### Template

 

```
A [style] sticker of a [subject], featuring [key characteristics] and a
[color palette]. The design should have [line style] and [shading style].
The background must be transparent.
```

 
 

### Prompt

 

```
A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's
munching on a green bamboo leaf. The design features bold, clean outlines,
simple cel-shading, and a vibrant color palette. The background must be white.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('red_panda_sticker.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("red_panda_sticker.png", buffer);
      console.log("Image saved as red_panda_sticker.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "red_panda_sticker.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It'"'"'s munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > red_panda_sticker.png
```

 
 
 
 
 A kawaii-style sticker of a happy red panda... 
 

#### 3. Accurate text in images

Gemini excels at rendering text. Be clear about the text, the font style
(descriptively), and the overall design.

 
 

### Template

 

```
Create a [image type] for [brand/concept] with the text "[text to render]"
in a [font style]. The design should be [style description], with a
[color scheme].
```

 
 

### Prompt

 

```
Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'.
The text should be in a clean, bold, sans-serif font. The design should
feature a simple, stylized icon of a a coffee bean seamlessly integrated
with the text. The color scheme is black and white.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('logo_example.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("logo_example.png", buffer);
      console.log("Image saved as logo_example.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "logo_example.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Create a modern, minimalist logo for a coffee shop called '"'"'The Daily Grind'"'"'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > logo_example.png
```

 
 
 
 
 Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'... 
 

#### 4. Product mockups & commercial photography

Perfect for creating clean, professional product shots for e-commerce,
advertising, or branding.

 
 

### Template

 

```
A high-resolution, studio-lit product photograph of a [product description]
on a [background surface/description]. The lighting is a [lighting setup,
e.g., three-point softbox setup] to [lighting purpose]. The camera angle is
a [angle type] to showcase [specific feature]. Ultra-realistic, with sharp
focus on [key detail]. [Aspect ratio].
```

 
 

### Prompt

 

```
A high-resolution, studio-lit product photograph of a minimalist ceramic
coffee mug in matte black, presented on a polished concrete surface. The
lighting is a three-point softbox setup designed to create soft, diffused
highlights and eliminate harsh shadows. The camera angle is a slightly
elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with
sharp focus on the steam rising from the coffee. Square image.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('product_mockup.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("product_mockup.png", buffer);
      console.log("Image saved as product_mockup.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "product_mockup.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > product_mockup.png
```

 
 
 
 
 A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug... 
 

#### 5. Minimalist & negative space design

Excellent for creating backgrounds for websites, presentations, or marketing
materials where text will be overlaid.

 
 

### Template

 

```
A minimalist composition featuring a single [subject] positioned in the
[bottom-right/top-left/etc.] of the frame. The background is a vast, empty
[color] canvas, creating significant negative space. Soft, subtle lighting.
[Aspect ratio].
```

 
 

### Prompt

 

```
A minimalist composition featuring a single, delicate red maple leaf
positioned in the bottom-right of the frame. The background is a vast, empty
off-white canvas, creating significant negative space for text. Soft,
diffused lighting from the top left. Square image.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('minimalist_design.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("minimalist_design.png", buffer);
      console.log("Image saved as minimalist_design.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "minimalist_design.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > minimalist_design.png
```

 
 
 
 
 A minimalist composition featuring a single, delicate red maple leaf... 
 

#### 6. Sequential art (Comic panel / Storyboard)

Builds on character consistency and scene description to create panels for
visual storytelling.

 
 

### Template

 

```
A single comic book panel in a [art style] style. In the foreground,
[character description and action]. In the background, [setting details].
The panel has a [dialogue/caption box] with the text "[Text]". The lighting
creates a [mood] mood. [Aspect ratio].
```

 
 

### Prompt

 

```
A single comic book panel in a gritty, noir art style with high-contrast
black and white inks. In the foreground, a detective in a trench coat stands
under a flickering streetlamp, rain soaking his shoulders. In the
background, the neon sign of a desolate bar reflects in a puddle. A caption
box at the top reads "The city was a tough place to keep secrets." The
lighting is harsh, creating a dramatic, somber mood. Landscape.
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.",
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('comic_panel.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("comic_panel.png", buffer);
      console.log("Image saved as comic_panel.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "comic_panel.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

 
 

### REST

 

```
curl -s -X POST
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > comic_panel.png
```

 
 
 
 
 A single comic book panel in a gritty, noir art style... 
 

### Prompts for editing images

These examples show how to provide images alongside your text prompts for
editing, composition, and style transfer.

#### 1. Adding and removing elements

Provide an image and describe your change. The model will match the original
image's style, lighting, and perspective.

 
 

### Template

 

```
Using the provided image of [subject], please [add/remove/modify] [element]
to/from the scene. Ensure the change is [description of how the change should
integrate].
```

 
 

### Prompt

 

```
"Using the provided image of my cat, please add a small, knitted wizard hat
on its head. Make it look like it's sitting comfortably and matches the soft
lighting of the photo."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A photorealistic picture of a fluffy ginger cat sitting on a wooden floor, looking directly at the camera. Soft, natural light from a window."
image_input = Image.open('/path/to/your/cat_photo.png')
text_input = """Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[text_input, image_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('cat_with_hat.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/cat_photo.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    { text: "Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off." },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("cat_with_hat.png", buffer);
      console.log("Image saved as cat_with_hat.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/cat_photo.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    genai.NewPartFromText("Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "cat_with_hat.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/cat_photo.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off.\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > cat_with_hat.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A photorealistic picture of a fluffy ginger cat... 
 
 
 
 
 
 Using the provided image of my cat, please add a small, knitted wizard hat... 
 
 
 
 

#### 2. Inpainting (Semantic masking)

Conversationally define a "mask" to edit a specific part of an image while
leaving the rest untouched.

 
 

### Template

 

```
Using the provided image, change only the [specific element] to [new
element/description]. Keep everything else in the image exactly the same,
preserving the original style, lighting, and composition.
```

 
 

### Prompt

 

```
"Using the provided image of a living room, change only the blue sofa to be
a vintage, brown leather chesterfield sofa. Keep the rest of the room,
including the pillows on the sofa and the lighting, unchanged."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A wide shot of a modern, well-lit living room with a prominent blue sofa in the center. A coffee table is in front of it and a large window is in the background."
living_room_image = Image.open('/path/to/your/living_room.png')
text_input = """Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[living_room_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('living_room_edited.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/living_room.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
    { text: "Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("living_room_edited.png", buffer);
      console.log("Image saved as living_room_edited.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/living_room.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "living_room_edited.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/living_room.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            },
            {\"text\": \"Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > living_room_edited.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A wide shot of a modern, well-lit living room... 
 
 
 
 
 
 Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa... 
 
 
 
 

#### 3. Style transfer

Provide an image and ask the model to recreate its content in a different
artistic style.

 
 

### Template

 

```
Transform the provided photograph of [subject] into the artistic style of [artist/art style]. Preserve the original composition but render it with [description of stylistic elements].
```

 
 

### Prompt

 

```
"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompt: "A photorealistic, high-resolution photograph of a busy city street in New York at night, with bright neon signs, yellow taxis, and tall skyscrapers."
city_image = Image.open('/path/to/your/city.png')
text_input = """Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[city_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('city_style_transfer.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/city.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
    { text: "Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("city_style_transfer.png", buffer);
      console.log("Image saved as city_style_transfer.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/city.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "city_style_transfer.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH=/path/to/your/city.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            },
            {\"text\": \"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > city_style_transfer.png
```

 
 
 
 
 

Input

 
 

Output

 
 
 
 
 
 
 A photorealistic, high-resolution photograph of a busy city street... 
 
 
 
 
 
 Transform the provided photograph of a modern city street at night... 
 
 
 
 

#### 4. Advanced composition: Combining multiple images

Provide multiple images as context to create a new, composite scene. This is
perfect for product mockups or creative collages.

 
 

### Template

 

```
Create a new image by combining the elements from the provided images. Take
the [element from image 1] and place it with/on the [element from image 2].
The final image should be a [description of the final scene].
```

 
 

### Prompt

 

```
"Create a professional e-commerce fashion photo. Take the blue floral dress
from the first image and let the woman from the second image wear it.
Generate a realistic, full-body shot of the woman wearing the dress, with
the lighting and shadows adjusted to match the outdoor environment."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompts:
# 1. Dress: "A professionally shot photo of a blue floral summer dress on a plain white background, ghost mannequin style."
# 2. Model: "Full-body shot of a woman with her hair in a bun, smiling, standing against a neutral grey studio background."
dress_image = Image.open('/path/to/your/dress.png')
model_image = Image.open('/path/to/your/model.png')

text_input = """Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[dress_image, model_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('fashion_ecommerce_shot.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath1 = "/path/to/your/dress.png";
  const imageData1 = fs.readFileSync(imagePath1);
  const base64Image1 = imageData1.toString("base64");
  const imagePath2 = "/path/to/your/model.png";
  const imageData2 = fs.readFileSync(imagePath2);
  const base64Image2 = imageData2.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image1,
      },
    },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image2,
      },
    },
    { text: "Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("fashion_ecommerce_shot.png", buffer);
      console.log("Image saved as fashion_ecommerce_shot.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imgData1, _ := os.ReadFile("/path/to/your/dress.png")
  imgData2, _ := os.ReadFile("/path/to/your/model.png")

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData1,
      },
    },
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData2,
      },
    },
    genai.NewPartFromText("Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "fashion_ecommerce_shot.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH1=/path/to/your/dress.png
IMG_PATH2=/path/to/your/model.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG1_BASE64\"
              }
            },
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG2_BASE64\"
              }
            },
            {\"text\": \"Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > fashion_ecommerce_shot.png
```

 
 
 
 
 

Input 1

 
 

Input 2

 
 

Output

 
 
 
 
 
 
 A professionally shot photo of a blue floral summer dress... 
 
 
 
 
 
 Full-body shot of a woman with her hair in a bun... 
 
 
 
 
 
 Create a professional e-commerce fashion photo... 
 
 
 
 

#### 5. High-fidelity detail preservation

To ensure critical details (like a face or logo) are preserved during an edit,
describe them in great detail along with your edit request.

 
 

### Template

 

```
Using the provided images, place [element from image 2] onto [element from
image 1]. Ensure that the features of [element from image 1] remain
completely unchanged. The added element should [description of how the
element should integrate].
```

 
 

### Prompt

 

```
"Take the first image of the woman with brown hair, blue eyes, and a neutral
expression. Add the logo from the second image onto her black t-shirt.
Ensure the woman's face and features remain completely unchanged. The logo
should look like it's naturally printed on the fabric, following the folds
of the shirt."
```

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image

client = genai.Client()

# Base image prompts:
# 1. Woman: "A professional headshot of a woman with brown hair and blue eyes, wearing a plain black t-shirt, against a neutral studio background."
# 2. Logo: "A simple, modern logo with the letters 'G' and 'A' in a white circle."
woman_image = Image.open('/path/to/your/woman.png')
logo_image = Image.open('/path/to/your/logo.png')
text_input = """Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[woman_image, logo_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.parts
    if part.inline_data
]

if image_parts:
    image = part.as_image()
    image.save('woman_with_logo.png')
    image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath1 = "/path/to/your/woman.png";
  const imageData1 = fs.readFileSync(imagePath1);
  const base64Image1 = imageData1.toString("base64");
  const imagePath2 = "/path/to/your/logo.png";
  const imageData2 = fs.readFileSync(imagePath2);
  const base64Image2 = imageData2.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image1,
      },
    },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image2,
      },
    },
    { text: "Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("woman_with_logo.png", buffer);
      console.log("Image saved as woman_with_logo.png");
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imgData1, _ := os.ReadFile("/path/to/your/woman.png")
  imgData2, _ := os.ReadFile("/path/to/your/logo.png")

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData1,
      },
    },
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData2,
      },
    },
    genai.NewPartFromText("Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "woman_with_logo.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

 
 

### REST

 

```
IMG_PATH1=/path/to/your/woman.png
IMG_PATH2=/path/to/your/logo.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG1_BASE64\"
              }
            },
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG2_BASE64\"
              }
            },
            {\"text\": \"Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > woman_with_logo.png
```

 
 
 
 
 

Input 1

 
 

Input 2

 
 

Output

 
 
 
 
 
 
 A professional headshot of a woman with brown hair and blue eyes... 
 
 
 
 
 
 A simple, modern logo with the letters 'G' and 'A'... 
 
 
 
 
 
 Take the first image of the woman with brown hair, blue eyes, and a neutral expression... 
 
 
 
 

### Best Practices

To elevate your results from good to great, incorporate these professional
strategies into your workflow.

- Be Hyper-Specific: The more detail you provide, the more control you
have. Instead of "fantasy armor," describe it: "ornate elven plate armor, etched
with silver leaf patterns, with a high collar and pauldrons shaped like falcon
wings."

- Provide Context and Intent: Explain the purpose of the image. The
model's understanding of context will influence the final output. For example,
"Create a logo for a high-end, minimalist skincare brand" will yield better
results than just "Create a logo."

- Iterate and Refine: Don't expect a perfect image on the first try. Use
the conversational nature of the model to make small changes. Follow up with
prompts like, "That's great, but can you make the lighting a bit warmer?" or
"Keep everything the same, but change the character's expression to be more
serious."

- Use Step-by-Step Instructions: For complex scenes with many elements,
break your prompt into steps. "First, create a background of a serene, misty
forest at dawn. Then, in the foreground, add a moss-covered ancient stone altar.
Finally, place a single, glowing sword on top of the altar."

- Use "Semantic Negative Prompts": Instead of saying "no cars," describe
the desired scene positively: "an empty, deserted street with no signs of
traffic."

- Control the Camera: Use photographic and cinematic language to control
the composition. Terms like `wide-angle shot`, `macro shot`, 

```
low-angle
perspective
```

.

## Limitations

- For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN,
hi-IN.

- Image generation does not support audio or video inputs.

- The model won't always follow the exact number of image outputs that the
user explicitly asks for.

- The model works best with up to 3 images as an input.

- When generating text for an image, Gemini works best if you first generate
the text and then ask for an image with the text.

- Uploading images of children is not currently supported in EEA, CH, and UK.

- All generated images include a SynthID watermark .

## Optional configurations

You can optionally configure the response modalities and aspect ratio of the
model's output in the `config` field of `generate_content` calls.

### Output types

The model defaults to returning text and image responses
(i.e. 

```
response_modalities=['Text', 'Image']
```

).
You can configure the response to return only images without text using
`response_modalities=['Image']`.

 
 

### Python

 

```
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
    config=types.GenerateContentConfig(
        response_modalities=['Image']
    )
)
```

 
 

### JavaScript

 

```
const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
    config: {
        responseModalities: ['Image']
    }
  });
```

 
 

### Go

 

```
result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash-image",
    genai.Text("Create a picture of a nano banana dish in a " +
                " fancy restaurant with a Gemini theme"),
    &genai.GenerateContentConfig{
        ResponseModalities: "Image",
    },
  )
```

 
 

### REST

 

```
-d '{
  "contents": [{
    "parts": [
      {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
    ]
  }],
  "generationConfig": {
    "responseModalities": ["Image"]
  }
}' \
```

 
 

### Aspect ratios

The model defaults to matching the output image size to that of your input
image, or otherwise generates 1:1 squares.
You can control the aspect ratio of the output image using the `aspect_ratio`
field under `image_config` in the response request, shown here:

 
 

### Python

 

```
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[prompt],
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(
            aspect_ratio="16:9",
        )
    )
)
```

 
 

### JavaScript

 

```
const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
    config: {
      imageConfig: {
        aspectRatio: "16:9",
      },
    }
  });
```

 
 

### Go

 

```
result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash-image",
    genai.Text("Create a picture of a nano banana dish in a " +
                " fancy restaurant with a Gemini theme"),
    &genai.GenerateContentConfig{
        ImageConfig: &genai.ImageConfig{
          AspectRatio: "16:9",
        },
    }
  )
```

 
 

### REST

 

```
-d '{
  "contents": [{
    "parts": [
      {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
    ]
  }],
  "generationConfig": {
    "imageConfig": {
      "aspectRatio": "16:9"
    }
  }
}' \
```

 
 

The different ratios available and the size of the image generated are listed in
this table:

 
 
 
 Aspect ratio 
 Resolution 
 Tokens 
 
 

 
 
 1:1 
 1024x1024 
 1290 
 
 
 2:3 
 832x1248 
 1290 
 
 
 3:2 
 1248x832 
 1290 
 
 
 3:4 
 864x1184 
 1290 
 
 
 4:3 
 1184x864 
 1290 
 
 
 4:5 
 896x1152 
 1290 
 
 
 5:4 
 1152x896 
 1290 
 
 
 9:16 
 768x1344 
 1290 
 
 
 16:9 
 1344x768 
 1290 
 
 
 21:9 
 1536x672 
 1290 
 
 
 

## When to use Imagen

In addition to using Gemini's built-in image generation capabilities, you can
also access Imagen , our specialized image generation
model, through the Gemini API.

 
 
 
 
 
 
 
 
 Attribute 
 Imagen 
 Gemini Native Image 
 
 
 
 
 Strengths 
 Most capable image generation model to date. Recommended for photorealistic images, sharper clarity, improved spelling and typography. 
 Default recommendation. 
Unparalleled flexibility, contextual understanding, and simple, mask-free editing. Uniquely capable of multi-turn conversational editing. 
 
 
 Availability 
 Generally available 
 Preview (Production usage allowed) 
 
 
 Latency 
 Low . Optimized for near-real-time performance. 
 Higher. More computation is required for its advanced capabilities. 
 
 
 Cost 
 Cost-effective for specialized tasks. $0.02/image to $0.12/image 
 Token-based pricing. $30 per 1 million tokens for image output (image output tokenized at 1290 tokens per image flat, up to 1024x1024px) 
 
 
 Recommended tasks 
 
 

 - Image quality, photorealism, artistic detail, or specific styles (e.g., impressionism, anime) are top priorities.

 - Infusing branding, style, or generating logos and product designs.

 - Generating advanced spelling or typography.

 

 
 
 

 - Interleaved text and image generation to seamlessly blend text and images.

 - Combine creative elements from multiple images with a single prompt.

 - Make highly specific edits to images, modify individual elements with simple language commands, and iteratively work on an image.

 - Apply a specific design or texture from one image to another while preserving the original subject's form and details.

 

 
 
 
 

Imagen 4 should be your go-to model starting to generate images with
Imagen. Choose Imagen 4 Ultra for advanced use-cases or
when you need the best image quality (note that can only generate one image at
a time).

## What's next

- Find more examples and code samples in the cookbook guide .

- Check out the Veo guide to learn how to generate
videos with the Gemini API.

- To learn more about Gemini models, see Gemini models .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### Thought Signatures &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/thought-signatures

- 
 
 
 
 
 
 
 
 
 
 
 Thought Signatures  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Thought Signatures 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

Thought signatures are encrypted representations of the model's internal thought
process and are used to preserve reasoning context across multi-turn
interactions.
When using thinking models (such as the Gemini 3 and 2.5 series), the API may
return a `thoughtSignature` field within the content parts 
of the response (e.g., `text` or `functionCall` parts).

As a general rule, if you receive a thought signature in a model response,
you should pass it back exactly as received when sending the conversation
history in the next turn.
 When using Gemini 3 Pro, you must pass back thought signatures during function
calling, otherwise you will get a validation error (4xx status code).

## How it works

The graphic below visualizes the meaning of "turn" and "step" as they pertain to
 function calling in the Gemini API. A "turn"
is a single, complete exchange in a conversation between a user and a model. A
"step" is a finer-grained action or operation performed by the model, often as
part of a larger process to complete a turn.

 

 This document focuses on handling function calling for Gemini 3 Pro. Refer to
the model behavior section for discrepancies with 2.5. 

Gemini 3 Pro returns thought signatures for all model responses (responses from
the API) with a function call. Thought signatures show up in the following
cases:

- When there are parallel function 
calls, the first function call part returned by the model response will have a
thought signature.

- When there are sequential function calls (multi-step), each function call will
have a signature and you must pass all signatures back.

- Model responses without a function call will return a thought signature inside
the last part returned by the model.

The following table provides a visualization for multi-step function calls,
combining the definitions of turns and steps with the concept of signatures
introduced above:

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1 = user_prompt` 
 `FC1 + signature` 
 `FR1` 
 
 
 

1

 
 

2

 
 `request2 = request1 + (FC1 + signature) + FR1` 
 `FC2 + signature` 
 `FR2` 
 
 
 

1

 
 

3

 
 `request3 = request2 + (FC2 + signature) + FR2` 
 `text_output(no FCs)` 
 

None

 
 
 

## Signatures in function calling parts

When Gemini generates a `functionCall`, it relies on the `thought_signature`
to process the tool's output correctly in the next turn.

- Behavior :

 Single Function Call : The `functionCall` part will contain a `thought_signature`.

- Parallel Function Calls : If the model generates parallel function calls
in a response, the `thought_signature` is attached only to the first 
`functionCall` part. Subsequent `functionCall` parts in the same response will
 not contain a signature.

 
- Requirement : You must return this signature in the exact part where it
was received when sending the conversation history back.

- Validation : Strict validation is enforced for all function calls within
the current turn . (Only current turn is required; we don't validate on
previous turns)

 The API goes back in the history (newest to oldest) to find the most recent
 User message that contains standard content (e.g., `text`) ( which would
be the start of the current turn). This will not be a `functionResponse`.

- All model `functionCall` turns occurring after that specific use
message are considered part of the turn.

- The first `functionCall` part in each step of the current turn
 must include its `thought_signature`.

- If you omit a `thought_signature` for the first `functionCall` part in any
step of the current turn, the request will fail with a 400 error.

 
- If proper signatures are not returned, here is how you will error out 

 `gemini-3-pro-preview`: Failure to include signatures will result in a 400
error. The verbiage will be of the form :

 Function call ` ` in the ` `
content block is missing a `thought_signature`. For example, Function
call `FC1` in the `1.` content block is missing a `thought_signature`. 

 

 

### Sequential function calling example

This section shows an example of multiple function calls where the user asks a
complex question requiring multiple tasks.

Let's walk through a multiple-turn function calling example where the user asks
a complex question requiring multiple tasks: 

```
"Check flight status for AA100 and
book a taxi if delayed"
```

.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 

```
request1="Check flight status for AA100 and book a taxi 2 hours before if delayed."
```

 
 `FC1 ("check_flight") + signature` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request2 = request1 + FC1 ("check_flight") + signature + FR1
```

 
 `FC2("book_taxi") + signature` 
 `FR2` 
 
 
 

1

 
 

3

 
 

```
request3 = request2 + FC2 ("book_taxi") + signature + FR2
```

 
 `text_output(no FCs)` 
 `None` 
 
 

The following code illustrates the sequence in the above table.

 Turn 1, Step 1 (User request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "check_flight",
          "description": "Gets the current status of a flight",
          "parameters": {
            "type": "object",
            "properties": {
              "flight": {
                "type": "string",
                "description": "The flight number to check"
              }
            },
            "required": [
              "flight"
            ]
          }
        },
        {
          "name": "book_taxi",
          "description": "Book a taxi",
          "parameters": {
            "type": "object",
            "properties": {
              "time": {
                "type": "string",
                "description": "time to book the taxi"
              }
            },
            "required": [
              "time"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model response) 

 

```
{
"content": {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>"
          }
        ]
  }
}
```

 

 Turn 1, Step 2 (User response - Sending tool outputs) Since this user turn
only contains a `functionResponse` (no fresh text), we are still in Turn 1. We
must preserve ` `.

 

```
{
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    },
    {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "check_flight",
              "response": {
                "status": "delayed",
                "departure_time": "12 PM"
                }
              }
            }
        ]
}
```

 

 Turn 1, Step 2 (Model) The model now decides to book a taxi based on the
previous tool output.

 

```
{
      "content": {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "book_taxi",
              "args": {
                "time": "10 AM"
              }
            },
            "thoughtSignature": "<Signature B>"
          }
        ]
      }
}
```

 

 Turn 1, Step 3 (User - Sending tool output) To send the taxi booking
confirmation, we must include signatures for ALL function calls in this loop
(` ` + ` `).

 

```
{
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    },
    {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "check_flight",
              "response": {
                "status": "delayed",
                "departure_time": "12 PM"
              }
              }
            }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "book_taxi",
              "args": {
                "time": "10 AM"
              }
            },
            "thoughtSignature": "<Signature B>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "book_taxi",
              "response": {
                "booking_status": "success"
              }
              }
            }
        ]
    }
}
```

 

### Parallel function calling example

Let's walk through a parallel function calling example where the users asks
`"Check weather in Paris and London"` to see where the model does validation.

 
 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 
 

1

 
 

1

 
 

request1="Check the weather in Paris and London"

 
 

FC1 ("Paris") + signature

FC2 ("London")

 
 

FR1

 
 
 
 

1

 
 

2

 
 

request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")

 
 

text_output

(no FCs)

 
 

None

 
 
 

The following code illustrates the sequence in the above table.

 Turn 1, Step 1 (User request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check the weather in Paris and London."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_temperature",
          "description": "Gets the current temperature for a given location.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco"
              }
            },
            "required": [
              "location"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model response) 

 

```
{
  "content": {
    "parts": [
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "location": "Paris"
          }
        },
        "thoughtSignature": "<Signature_A>"// INCLUDED on First FC
      },
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "location": "London"
          }// NO signature on subsequent parallel FCs
        }
      }
    ]
  }
}
```

 

 Turn 1, Step 2 (User response - Sending tool outputs) We must preserve
` ` on the first part exactly as received.

 

```
[
  {
    "role": "user",
    "parts": [
      {
        "text": "Check the weather in Paris and London."
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "city": "Paris"
          }
        },
        "thought_signature": "<Signature_A>" // MUST BE INCLUDED
      },
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "city": "London"
          }
        }
      } // NO SIGNATURE FIELD
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "name": "get_current_temperature",
          "response": {
            "temp": "15C"
          }
        }
      },
      {
        "functionResponse": {
          "name": "get_current_temperature",
          "response": {
            "temp": "12C"
          }
        }
      }
    ]
  }
]
```

 

## Signatures in non `functionCall` parts

Gemini may also return `thought_signatures` in the final part of the response
in non-function-call parts.

- Behavior : The final content part (`text, inlineData‚Ä¶`) returned by the
model may contain a `thought_signature`.

- Recommendation : Returning these signatures is recommended to ensure
the model maintains high-quality reasoning, especially for complex instruction
following or simulated agentic workflows.

- Validation : The API does not strictly enforce validation. You won't
receive a blocking error if you omit them, though performance may degrade.

### Text/In-context reasoning (No validation)

 Turn 1, Step 1 (Model response) 

 

```
{
  "role": "model",
  "parts": [
    {
      "text": "I need to calculate the risk. Let me think step-by-step...",
      "thought_signature": "<Signature_C>" // OPTIONAL (Recommended)
    }
  ]
}
```

 

 Turn 2, Step 1 (User) 

 

```
[
  { "role": "user", "parts": [{ "text": "What is the risk?" }] },
  {
    "role": "model", 
    "parts": [
      {
        "text": "I need to calculate the risk. Let me think step-by-step...",
        // If you omit <Signature_C> here, no error will occur.
      }
    ]
  },
  { "role": "user", "parts": [{ "text": "Summarize it." }] }
]
```

 

## Signatures for OpenAI compatibility

The following examples shows how to handle thought signatures for a chat
completion API using OpenAI compatibility .

### Sequential function calling example

This is an example of multiple function calling where the user asks a complex
question requiring multiple tasks.

Let's walk through a multiple-turn function calling example where the user asks


```
Check flight status for AA100 and book a taxi if delayed
```

 and you can see what
happens when the user asks a complex question requiring multiple tasks.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1="Check the weather in Paris and London"` 
 `FC1 ("Paris") + signatureFC2 ("London")` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")
```

 
 `text_output(no FCs)` 
 `None` 
 
 

The following code walks through the given sequence.

 Turn 1, Step 1 (User Request) 

 

```
{
  "model": "google/gemini-3-pro-preview",
  "messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "check_flight",
        "description": "Gets the current status of a flight",
        "parameters": {
          "type": "object",
          "properties": {
            "flight": {
              "type": "string",
              "description": "The flight number to check."
            }
          },
          "required": [
            "flight"
          ]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "book_taxi",
        "description": "Book a taxi",
        "parameters": {
          "type": "object",
          "properties": {
            "time": {
              "type": "string",
              "description": "time to book the taxi"
            }
          },
          "required": [
            "time"
          ]
        }
      }
    }
  ]
}
```

 

 Turn 1, Step 1 (Model Response) 

 

```
{
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>"
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    }
```

 

 Turn 1, Step 2 (User Response - Sending Tool Outputs) 

Since this user turn only contains a `functionResponse` (no fresh text), we are
still in Turn 1 and must preserve ` `.

 

```
"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"                 
    }
  ]
```

 

 Turn 1, Step 2 (Model) 

The model now decides to book a taxi based on the previous tool output.

 

```
{
"role": "model",
"tool_calls": [
{
"extra_content": {
"google": {
"thought_signature": "<Signature B>"
}
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-2",
            "type": "function"
          }
       ]
}
```

 

 Turn 1, Step 3 (User - Sending Tool Output) 

To send the taxi booking confirmation, we must include signatures for ALL
function calls in this loop (` ` + ` `).

 

```
"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"                 
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature B>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "book_taxi",
      "tool_call_id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
      "content": "{\"booking_status\":\"success\"}"
    }
  ]
```

 

### Parallel function calling example

Let's walk through a parallel function calling example where the users asks
`"Check weather in Paris and London"` and you can see where the model does
validation.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1="Check the weather in Paris and London"` 
 `FC1 ("Paris") + signatureFC2 ("London")` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")
```

 
 `text_output(no FCs)` 
 `None` 
 
 

Here's the code to walk through the given sequence.

 Turn 1, Step 1 (User Request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check the weather in Paris and London."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_temperature",
          "description": "Gets the current temperature for a given location.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco"
              }
            },
            "required": [
              "location"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model Response) 

 

```
{
"role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Signature returned
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": {
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function" // No signature on Parallel FC
          }
        ]
}
```

 

 Turn 1, Step 2 (User Response - Sending Tool Outputs) 

You must preserve ` ` on the first part exactly as received.

 

```
"messages": [
    {
      "role": "user",
      "content": "Check the weather in Paris and London."
    },
    {
      "role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": { //No Signature
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function"
          }
        ]
    },
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
      "content": "{\"temp\":\"15C\"}"
    },    
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
      "content": "{\"temp\":\"12C\"}"
    }
  ]
```

 

## FAQs

- 

 How do I transfer history from a different model to Gemini 3 Pro with a
function call part in the current turn and step? I need to provide function call
parts that were not generated by the API and therefore don't have an associated
thought signature? 

While injecting custom function call blocks into the request is strongly
discouraged, in cases where it can't be avoided, e.g. providing information
to the model on function calls and responses that were executed
deterministically by the client, or transferring a trace from a different
model that does not include thought signatures, you can set the following
dummy signatures of either `"context_engineering_is_the_way_to_go"` or
`"skip_thought_signature_validator"` in the thought signature field to skip
validation.

- 

 I am sending back interleaved parallel function calls and responses and the
API is returning a 400. Why? 

When the API returns parallel function calls "FC1 + signature, FC2", the
user response expected is "FC1+ signature, FC2, FR1, FR2". If you have them
interleaved as "FC1 + signature, FR1, FC2, FR2" the API will return a 400
error.

- 

 When streaming and the model is not returning a function call I can't find
the thought signature 

During a model response not containing a FC with a streaming request, the
model may return the thought signature in a part with an empty text content
part. It is advisable to parse the entire request until the `finish_reason`
is returned by the model.

## Thought signature behavior by model series

Gemini 3 Pro and Gemini 2.5 models behave differently with thought signatures
in function calls:

- If there are function calls in a response,

 Gemini 3 Pro will always have the signature on the first function call part.
It is mandatory to return that part.

- Gemini 2.5 will have the signature in the first part (regardless of
type). It is optional to return that part.

 
- If there are no function calls in a response,

 Gemini 3 Pro will have the signature on the last part if the model generates
a thought.

- Gemini 2.5 won't have a signature in any part.

 

For Gemini 2.5 models thought signature behavior, refer to the
 Thinking page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Structured Outputs &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/structured-output

- 
 
 
 
 
 
 
 
 
 
 
 Structured Outputs  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Structured Outputs 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

You can configure Gemini models to generate responses that adhere to a provided
JSON Schema. This capability guarantees predictable and parsable results, ensures format
and type-safety, enables the programmatic detection of refusals, and
simplifies prompting.

Using structured outputs is ideal for a wide range of applications:

- Data extraction: Pull specific information from unstructured text, like extracting names, dates, and amounts from an invoice.

- Structured classification: Classify text into predefined categories and assign structured labels, such as categorizing customer feedback by sentiment and topic.

- Agentic workflows: Generate structured data that can be used to call other tools or APIs, like creating a character sheet for a game or filling out a form.

In addition to supporting JSON Schema in the REST API, the Google GenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod , respectively. The example below demonstrates how to extract information from unstructured text that conforms to a schema defined in code.

 
 
 
 
 

This example demonstrates how to extract structured data from text using basic JSON Schema types like `object`, `array`, `string`, and `integer`.

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import List, Optional

class Ingredient(BaseModel):
    name: str = Field(description="Name of the ingredient.")
    quantity: str = Field(description="Quantity of the ingredient, including units.")

class Recipe(BaseModel):
    recipe_name: str = Field(description="The name of the recipe.")
    prep_time_minutes: Optional[int] = Field(description="Optional time in minutes to prepare the recipe.")
    ingredients: List[Ingredient]
    instructions: List[str]

client = genai.Client()

prompt = """
Please extract the recipe from the following text.
The user wants to make delicious chocolate chip cookies.
They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
For the best part, they'll need 2 cups of semisweet chocolate chips.
First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
onto ungreased baking sheets and bake for 9 to 11 minutes.
"""

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=prompt,
    config={
        "response_mime_type": "application/json",
        "response_json_schema": Recipe.model_json_schema(),
    },
)

recipe = Recipe.model_validate_json(response.text)
print(recipe)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ingredientSchema = z.object({
  name: z.string().describe("Name of the ingredient."),
  quantity: z.string().describe("Quantity of the ingredient, including units."),
});

const recipeSchema = z.object({
  recipe_name: z.string().describe("The name of the recipe."),
  prep_time_minutes: z.number().optional().describe("Optional time in minutes to prepare the recipe."),
  ingredients: z.array(ingredientSchema),
  instructions: z.array(z.string()),
});

const ai = new GoogleGenAI({});

const prompt = `
Please extract the recipe from the following text.
The user wants to make delicious chocolate chip cookies.
They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
For the best part, they'll need 2 cups of semisweet chocolate chips.
First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
onto ungreased baking sheets and bake for 9 to 11 minutes.
`;

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: prompt,
  config: {
    responseMimeType: "application/json",
    responseJsonSchema: zodToJsonSchema(recipeSchema),
  },
});

const recipe = recipeSchema.parse(JSON.parse(response.text));
console.log(recipe);
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `
  Please extract the recipe from the following text.
  The user wants to make delicious chocolate chip cookies.
  They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
  1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
  3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
  For the best part, they'll need 2 cups of semisweet chocolate chips.
  First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
  baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
  until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
  ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
  onto ungreased baking sheets and bake for 9 to 11 minutes.
  `
    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseJsonSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "recipe_name": map[string]any{
                    "type":        "string",
                    "description": "The name of the recipe.",
                },
                "prep_time_minutes": map[string]any{
                    "type":        "integer",
                    "description": "Optional time in minutes to prepare the recipe.",
                },
                "ingredients": map[string]any{
                    "type": "array",
                    "items": map[string]any{
                        "type": "object",
                        "properties": map[string]any{
                            "name": map[string]any{
                                "type":        "string",
                                "description": "Name of the ingredient.",
                            },
                            "quantity": map[string]any{
                                "type":        "string",
                                "description": "Quantity of the ingredient, including units.",
                            },
                        },
                        "required": []string{"name", "quantity"},
                    },
                },
                "instructions": map[string]any{
                    "type":  "array",
                    "items": map[string]any{"type": "string"},
                },
            },
            "required": []string{"recipe_name", "ingredients", "instructions"},
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text(prompt),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          { "text": "Please extract the recipe from the following text.\nThe user wants to make delicious chocolate chip cookies.\nThey need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,\n1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,\n3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.\nFor the best part, they will need 2 cups of semisweet chocolate chips.\nFirst, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,\nbaking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar\nuntil light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry\ningredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons\nonto ungreased baking sheets and bake for 9 to 11 minutes." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
          "type": "object",
          "properties": {
            "recipe_name": {
              "type": "string",
              "description": "The name of the recipe."
            },
            "prep_time_minutes": {
                "type": "integer",
                "description": "Optional time in minutes to prepare the recipe."
            },
            "ingredients": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "name": { "type": "string", "description": "Name of the ingredient."},
                  "quantity": { "type": "string", "description": "Quantity of the ingredient, including units."}
                },
                "required": ["name", "quantity"]
              }
            },
            "instructions": {
              "type": "array",
              "items": { "type": "string" }
            }
          },
          "required": ["recipe_name", "ingredients", "instructions"]
        }
      }
    }'
```

 
 

 Example Response: 

 

```
{
  "recipe_name": "Delicious Chocolate Chip Cookies",
  "ingredients": [
    {
      "name": "all-purpose flour",
      "quantity": "2 and 1/4 cups"
    },
    {
      "name": "baking soda",
      "quantity": "1 teaspoon"
    },
    {
      "name": "salt",
      "quantity": "1 teaspoon"
    },
    {
      "name": "unsalted butter (softened)",
      "quantity": "1 cup"
    },
    {
      "name": "granulated sugar",
      "quantity": "3/4 cup"
    },
    {
      "name": "packed brown sugar",
      "quantity": "3/4 cup"
    },
    {
      "name": "vanilla extract",
      "quantity": "1 teaspoon"
    },
    {
      "name": "large eggs",
      "quantity": "2"
    },
    {
      "name": "semisweet chocolate chips",
      "quantity": "2 cups"
    }
  ],
  "instructions": [
    "Preheat the oven to 375¬∞F (190¬∞C).",
    "In a small bowl, whisk together the flour, baking soda, and salt.",
    "In a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.",
    "Beat in the vanilla and eggs, one at a time.",
    "Gradually beat in the dry ingredients until just combined.",
    "Stir in the chocolate chips.",
    "Drop by rounded tablespoons onto ungreased baking sheets and bake for 9 to 11 minutes."
  ]
}
```

 

## Streaming

You can stream structured outputs, which allows you to start processing the response as it's being generated, without having to wait for the entire output to be complete. This can improve the perceived performance of your application.

The streamed chunks will be valid partial JSON strings, which can be concatenated to form the final, complete JSON object.

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import Literal

class Feedback(BaseModel):
    sentiment: Literal["positive", "neutral", "negative"]
    summary: str

client = genai.Client()
prompt = "The new UI is incredibly intuitive and visually appealing. Great job. Add a very long summary to test streaming!"

response_stream = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=prompt,
    config={
        "response_mime_type": "application/json",
        "response_json_schema": Feedback.model_json_schema(),
    },
)

for chunk in response_stream:
    print(chunk.candidates[0].content.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});
const prompt = "The new UI is incredibly intuitive and visually appealing. Great job! Add a very long summary to test streaming!";

const feedbackSchema = z.object({
  sentiment: z.enum(["positive", "neutral", "negative"]),
  summary: z.string(),
});

const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: prompt,
  config: {
    responseMimeType: "application/json",
    responseJsonSchema: zodToJsonSchema(feedbackSchema),
  },
});

for await (const chunk of stream) {
  console.log(chunk.candidates[0].content.parts[0].text)
}
```

 
 

## Structured outputs with tools

Gemini 3 lets you combine Structured Outputs with built-in tools, including Grounding with Google Search , URL Context , and Code Execution .

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import List

class MatchResult(BaseModel):
    winner: str = Field(description="The name of the winner.")
    final_match_score: str = Field(description="The final match score.")
    scorers: List[str] = Field(description="The name of the scorer.")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Search for all details for the latest Euro.",
    config={
        "tools": [
            {"google_search": {}},
            {"url_context": {}}
        ],
        "response_mime_type": "application/json",
        "response_json_schema": MatchResult.model_json_schema(),
    },  
)

result = MatchResult.model_validate_json(response.text)
print(result)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});

const matchSchema = z.object({
  winner: z.string().describe("The name of the winner."),
  final_match_score: z.string().describe("The final score."),
  scorers: z.array(z.string()).describe("The name of the scorer.")
});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Search for all details for the latest Euro.",
    config: {
      tools: [
        { googleSearch: {} },
        { urlContext: {} }
      ],
      responseMimeType: "application/json",
      responseJsonSchema: zodToJsonSchema(matchSchema),
    },
  });

  const match = matchSchema.parse(JSON.parse(response.text));
  console.log(match);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Search for all details for the latest Euro."}]
    }],
    "tools": [
      {"googleSearch": {}},
      {"urlContext": {}}
    ],
    "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
            "type": "object",
            "properties": {
                "winner": {"type": "string", "description": "The name of the winner."},
                "final_match_score": {"type": "string", "description": "The final score."},
                "scorers": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "The name of the scorer."
                }
            },
            "required": ["winner", "final_match_score", "scorers"]
        }
    }
  }'
```

 
 

## JSON schema support

To generate a JSON object, set the `response_mime_type` in the generation configuration to `application/json` and provide a `response_json_schema`. The schema must be a valid JSON Schema that describes the desired output format.

The model will then generate a response that is a syntactically valid JSON string matching the provided schema. When using structured outputs, the model will produce outputs in the same order as the keys in the schema.

Gemini's structured output mode supports a subset of the JSON Schema specification.

The following values of `type` are supported:

- `string` : For text.

- `number` : For floating-point numbers.

- `integer` : For whole numbers.

- `boolean` : For true/false values.

- `object` : For structured data with key-value pairs.

- `array` : For lists of items.

- `null` : To allow a property to be null, include `"null"` in the type array (e.g., 

```
{"type": ["string", "null"]}
```

).

These descriptive properties help guide the model:

- `title` : A short description of a property.

- `description` : A longer and more detailed description of a property.

### Type-specific properties

 For `object` values: 

- `properties` : An object where each key is a property name and each value is a schema for that property.

- `required` : An array of strings, listing which properties are mandatory.

- `additionalProperties` : Controls whether properties not listed in `properties` are allowed. Can be a boolean or a schema.

 For `string` values: 

- `enum` : Lists a specific set of possible strings for classification tasks.

- `format` : Specifies a syntax for the string, such as `date-time`, `date`, `time`.

 For `number` and `integer` values: 

- `enum` : Lists a specific set of possible numeric values.

- `minimum` : The minimum inclusive value.

- `maximum` : The maximum inclusive value.

 For `array` values: 

- `items` : Defines the schema for all items in the array.

- `prefixItems` : Defines a list of schemas for the first N items, allowing for tuple-like structures.

- `minItems` : The minimum number of items in the array.

- `maxItems` : The maximum number of items in the array.

## Model support

The following models support structured output:

 
 
 
 Model 
 Structured Outputs 
 
 

 
 
 Gemini 3 Pro Preview 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è* 
 
 
 Gemini 2.0 Flash-Lite 
 ‚úîÔ∏è* 
 
 
 

 * Note that Gemini 2.0 requires an explicit `propertyOrdering` list within the JSON input to define the preferred structure. You can find an example in this cookbook . 

## Structured outputs vs. function calling

Both structured outputs and function calling use JSON schemas, but they serve different purposes:

 
 
 
 Feature 
 Primary Use Case 
 
 

 
 
 Structured Outputs 
 Formatting the final response to the user. Use this when you want the model's answer to be in a specific format (e.g., extracting data from a document to save to a database). 
 
 
 Function Calling 
 Taking action during the conversation. Use this when the model needs to ask you to perform a task (e.g., "get current weather") before it can provide a final answer. 
 
 
 

## Best practices

- Clear descriptions: Use the `description` field in your schema to provide clear instructions to the model about what each property represents. This is crucial for guiding the model's output.

- Strong typing: Use specific types (`integer`, `string`, `enum`) whenever possible. If a parameter has a limited set of valid values, use an `enum`.

- Prompt engineering: Clearly state in your prompt what you want the model to do. For example, "Extract the following information from the text..." or "Classify this feedback according to the provided schema...".

- Validation: While structured output guarantees syntactically correct JSON, it does not guarantee the values are semantically correct. Always validate the final output in your application code before using it.

- Error handling: Implement robust error handling in your application to gracefully manage cases where the model's output, while schema-compliant, may not meet your business logic requirements.

## Limitations

- Schema subset: Not all features of the JSON Schema specification are supported. The model ignores unsupported properties.

- Schema complexity: The API may reject very large or deeply nested schemas. If you encounter errors, try simplifying your schema by shortening property names, reducing nesting, or limiting the number of constraints.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Image understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/image-understanding#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Image understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Image understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

- Passing inline image data : Ideal for smaller files (total request
size less than 20MB, including prompts).

- Uploading images using the File API : Recommended for larger files or for
reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the
request to `generateContent`. You can provide image data as Base64 encoded
strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass
it to `generateContent` API for processing.

 
 

### Python

 

```
  from google import genai
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  client = genai.Client()
  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### Go

 

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

 
 

You can also fetch an image from a URL, convert it to bytes, and pass it to
`generateContent` as shown in the following examples.

 
 

### Python

 

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

 
 

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the
Files API. The following code uploads an image file and then uses the file in a
call to `generateContent`. See the Files API guide for
more information and examples.

 
 

### Python

 

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image
`Part` objects in the `contents` array. These can be a mix of inline data
(local files or URLs) and File API references.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an
image and get their bounding box coordinates. The coordinates, relative to image
dimensions, scale to [0, 1000]. You need to descale these coordinates based on
your original image size.

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  )

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)
```

 
 

For more examples, check following notebooks in the Gemini Cookbook :

- 2D spatial understanding notebook 

- Experimental 3D pointing notebook 

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them
and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask.
Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with
normalized coordinates between 0 and 1000, a label ("`label`") that identifies
the object, and finally the segmentation mask inside the bounding box, as base64
encoded png that is a probability map with values between 0 and 255.
The mask needs to be resized to match the bounding box dimensions, then
binarized at your confidence threshold (127 for the midpoint).

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw
import io
import base64
import json
import numpy as np
import os

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
    if line == "```json":
      json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
      output = json_output.split("```")[0]  # Remove everything after the closing "```"
      break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  )

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")
```

 
 

Check the
 segmentation example 
in the cookbook guide for a more detailed example.

 
 
 An example segmentation output with objects and segmentation masks 
 

## Supported image formats

Gemini supports the following image format MIME types:

- PNG - `image/png`

- JPEG - `image/jpeg`

- WEBP - `image/webp`

- HEIC - `image/heic`

- HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of
image processing and computer vision tasks including but not limited to image captioning,
visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

- 

 Gemini 2.0 models are further trained to support enhanced object detection .

- 

 Gemini 2.5 models are further trained to support enhanced segmentation in addition to object detection .

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a
maximum of 3,600 image files per request.

### Token calculation

- Gemini 1.5 Flash and Gemini 1.5 Pro : 258 tokens if both dimensions
<= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized
to 768x768), with each tile costing 258 tokens.

- Gemini 2.0 Flash and Gemini 2.5 Flash/Pro : 258 tokens if both dimensions <= 384 pixels.
 Larger images are tiled into 768x768 pixel tiles, each costing 258
 tokens.

A rough formula for calculating the number of tiles is as follows:

- Calculate the crop unit size which is roughly: floor(min(width, height) / 1.5).

- Divide each dimension by the crop unit size and multiply together to get the
number of tiles.

For example, for an image of dimensions 960x540 would have a crop unit size
of 360. Divide each dimension by 360 and the number of tile is 3 * 2 = 6.

### Media resolution

Gemini 3 introduces granular control over multimodal vision processing with the
`media_resolution` parameter. The `media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to
read fine text or identify small details, but increase token usage and latency.

For more details about the parameter and how it can impact token calculations,
see the media resolution guide.

## Tips and best practices

- Verify that images are correctly rotated.

- Use clear, non-blurry images.

- When using a single image with text, place the text prompt after the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image
inputs. To learn more, see the following resources:

- Files API : Learn more about uploading and managing files for use with Gemini.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- Safety guidance : Sometimes generative
AI models produce unexpected outputs, such as outputs that are inaccurate,
biased, or offensive. Post-processing and human evaluation are essential to
limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Function calling with the Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/function-calling

- 
 
 
 
 
 
 
 
 
 
 
 Function calling with the Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Function calling with the Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Function calling lets you connect models to external tools and APIs.
Instead of generating text responses, the model determines when to call specific
functions and provides the necessary parameters to execute real-world actions.
This allows the model to act as a bridge between natural language and real-world
actions and data. Function calling has 3 primary use cases:

- Augment Knowledge: Access information from external sources like
databases, APIs, and knowledge bases.

- Extend Capabilities: Use external tools to perform computations and
extend the limitations of the model, such as using a calculator or creating
charts.

- Take Actions: Interact with external systems using APIs, such as
scheduling appointments, creating invoices, sending emails, or controlling
smart home devices.

 
 
 
 
 

 
 

### Python

 

```
from google import genai
from google.genai import types

# Define the function declaration for the model
schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[schedule_meeting_function])
config = types.GenerateContentConfig(tools=[tools])

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.",
    config=config,
)

# Check for a function call
if response.candidates[0].content.parts[0].function_call:
    function_call = response.candidates[0].content.parts[0].function_call
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.args}")
    #  In a real app, you would call your function here:
    #  result = schedule_meeting(**function_call.args)
else:
    print("No function call found in the response.")
    print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Type } from '@google/genai';

// Configure the client
const ai = new GoogleGenAI({});

// Define the function declaration for the model
const scheduleMeetingFunctionDeclaration = {
  name: 'schedule_meeting',
  description: 'Schedules a meeting with specified attendees at a given time and date.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      attendees: {
        type: Type.ARRAY,
        items: { type: Type.STRING },
        description: 'List of people attending the meeting.',
      },
      date: {
        type: Type.STRING,
        description: 'Date of the meeting (e.g., "2024-07-29")',
      },
      time: {
        type: Type.STRING,
        description: 'Time of the meeting (e.g., "15:00")',
      },
      topic: {
        type: Type.STRING,
        description: 'The subject or topic of the meeting.',
      },
    },
    required: ['attendees', 'date', 'time', 'topic'],
  },
};

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: 'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',
  config: {
    tools: [{
      functionDeclarations: [scheduleMeetingFunctionDeclaration]
    }],
  },
});

// Check for function calls in the response
if (response.functionCalls && response.functionCalls.length > 0) {
  const functionCall = response.functionCalls[0]; // Assuming one function call
  console.log(`Function to call: ${functionCall.name}`);
  console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);
  // In a real app, you would call your actual function here:
  // const result = await scheduleMeeting(functionCall.args);
} else {
  console.log("No function call found in the response.");
  console.log(response.text);
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning."
          }
        ]
      }
    ],
    "tools": [
      {
        "functionDeclarations": [
          {
            "name": "schedule_meeting",
            "description": "Schedules a meeting with specified attendees at a given time and date.",
            "parameters": {
              "type": "object",
              "properties": {
                "attendees": {
                  "type": "array",
                  "items": {"type": "string"},
                  "description": "List of people attending the meeting."
                },
                "date": {
                  "type": "string",
                  "description": "Date of the meeting (e.g., '2024-07-29')"
                },
                "time": {
                  "type": "string",
                  "description": "Time of the meeting (e.g., '15:00')"
                },
                "topic": {
                  "type": "string",
                  "description": "The subject or topic of the meeting."
                }
              },
              "required": ["attendees", "date", "time", "topic"]
            }
          }
        ]
      }
    ]
  }'
```

 
 

## How function calling works

 

Function calling involves a structured interaction between your application, the
model, and external functions. Here's a breakdown of the process:

- Define Function Declaration: Define the function declaration in your
application code. Function Declarations describe the function's name,
parameters, and purpose to the model.

- Call LLM with function declarations: Send user prompt along with the
function declaration(s) to the model. It analyzes the request and determines
if a function call would be helpful. If so, it responds with a structured
JSON object.

- Execute Function Code (Your Responsibility): The Model does not 
execute the function itself. It's your application's responsibility to
process the response and check for Function Call, if

 Yes : Extract the name and args of the function and execute the
corresponding function in your application.

- No: The model has provided a direct text response to the prompt
(this flow is less emphasized in the example but is a possible outcome).

 
- Create User friendly response: If a function was executed, capture the
result and send it back to the model in a subsequent turn of the
conversation. It will use the result to generate a final, user-friendly
response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex
interactions and workflows. The model also supports calling multiple functions
in a single turn ( parallel function
calling ) and in
sequence ( compositional function
calling ).

### Step 1: Define a function declaration

Define a function and its declaration within your application code that allows
users to set light values and make an API request. This function could call
external services or APIs.

 
 

### Python

 

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

 
 

### JavaScript

 

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

 
 

### Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to
use them. It analyzes the prompt and function declarations and decides whether
to respond directly or to call a function. If a function is called, the response
object will contain a function call suggestion.

 
 

### Python

 

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

 
 

The model then returns a `functionCall` object in an OpenAPI compatible
schema specifying how to call one or more of the declared functions in order to
respond to the user's question.

 
 

### Python

 

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

 
 

### JavaScript

 

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

 
 

### Step 3: Execute set_light_values function code

Extract the function call details from the model's response, parse the arguments
, and execute the `set_light_values` function.

 
 

### Python

 

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

 
 

### JavaScript

 

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

 
 

### Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can
incorporate this information into its final response to the user.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

client = genai.Client()
final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

 
 

### JavaScript

 

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

 
 

This completes the function calling flow. The model successfully used the
`set_light_values` function to perform the request action of the user.

## Function declarations

When you implement function calling in a prompt, you create a `tools` object,
which contains one or more `function declarations`. You define functions using
JSON, specifically with a select subset 
of the OpenAPI schema format. A
single function declaration can include the following parameters:

- `name` (string): A unique name for the function (`get_weather_forecast`,
`send_email`). Use descriptive names without spaces or special characters
(use underscores or camelCase).

- `description` (string): A clear and detailed explanation of the function's
purpose and capabilities. This is crucial for the model to understand when
to use the function. Be specific and provide examples if helpful ("Finds
theaters based on location and optionally movie title which is currently
playing in theaters.").

- `parameters` (object): Defines the input parameters the function
expects.

 `type` (string): Specifies the overall data type, such as `object`.

- `properties` (object): Lists individual parameters, each with:

 `type` (string): The data type of the parameter, such as `string`,
`integer`, `boolean, array`.

- `description` (string): A description of the parameter's purpose and
format. Provide examples and constraints ("The city and state,
e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").

- `enum` (array, optional): If the parameter values are from a fixed
set, use "enum" to list the allowed values instead of just describing
them in the description. This improves accuracy ("enum":
["daylight", "cool", "warm"]).

 
- `required` (array): An array of strings listing the parameter names that
are mandatory for the function to operate.

 

You can also construct `FunctionDeclarations` from Python functions directly using


```
types.FunctionDeclaration.from_callable(client=client, callable=your_function)
```

.

## Function calling with thinking models

Gemini 3 and 2.5 series models use an internal "thinking" process to reason through requests. This
significantly improves function calling performance,
allowing the model to better determine when to call a function and which
parameters to use. Because the Gemini API is stateless, models use
 thought signatures to maintain context
across multi-turn conversations.

This section covers advanced management of thought signatures and is only
necessary if you're manually constructing API requests (e.g., via REST) or
manipulating conversation history.

 If you're using the Google GenAI SDKs (our
official libraries), you don't need to manage this process . The SDKs
automatically handle the necessary steps, as shown in the earlier
 example .

### Managing conversation history manually

If you modify the conversation history manually, instead of sending the
 complete previous response you
must correctly handle the `thought_signature` included in the model's turn.

Follow these rules to ensure the model's context is preserved:

- Always send the `thought_signature` back to the model inside its original
 `Part` .

- Don't merge a `Part` containing a signature with one that does not. This
breaks the positional context of the thought.

- Don't combine two `Parts` that both contain signatures, as the signature
strings cannot be merged.

#### Gemini 3 thought signatures

In Gemini 3, any `Part` of a model response
may contain a thought signature.
While we generally recommend returning signatures from all `Part` types,
passing back thought signatures is mandatory for function calling. Unless you
are manipulating conversation history manually, the Google GenAI SDK will
handle thought signatures automatically.

If you are manipulating conversation history manually, refer to the
 Thoughts Signatures page for complete
guidance and details on handling thought signatures for Gemini 3.

### Inspecting thought signatures

While not necessary for implementation, you can inspect the response to see the
`thought_signature` for debugging or educational purposes.

 
 

### Python

 

```
import base64
# After receiving a response from a model with thinking enabled
# response = client.models.generate_content(...)

# The signature is attached to the response part containing the function call
part = response.candidates[0].content.parts[0]
if part.thought_signature:
  print(base64.b64encode(part.thought_signature).decode("utf-8"))
```

 
 

### JavaScript

 

```
// After receiving a response from a model with thinking enabled
// const response = await ai.models.generateContent(...)

// The signature is attached to the response part containing the function call
const part = response.candidates[0].content.parts[0];
if (part.thoughtSignature) {
  console.log(part.thoughtSignature);
}
```

 
 

Learn more about limitations and usage of thought signatures, and about thinking
models in general, on the Thinking page.

## Parallel function calling

In addition to single turn function calling, you can also call multiple
functions at once. Parallel function calling lets you execute multiple functions
at once and is used when the functions are not dependent on each other. This is
useful in scenarios like gathering data from multiple independent sources, such
as retrieving customer details from different databases or checking inventory
levels across various warehouses or performing multiple actions such as
converting your apartment into a disco.

 
 

### Python

 

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

 
 

### JavaScript

 

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

 
 

Configure the function calling mode to allow using all of the specified tools.
To learn more, you can read about
 configuring function calling .

 
 

### Python

 

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

 
 

Each of the printed results reflects a single function call that the model has
requested. To send the results back, include the responses in the same order as
they were requested.

The Python SDK supports automatic function calling ,
which automatically converts Python functions to declarations, handles the
function call execution and response cycle for you. Following is an example for
the disco use case.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

 
 

## Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple
function calls together to fulfill a complex request. For example, to answer
"Get the temperature in my current location", the Gemini API might first invoke
a `get_current_location()` function followed by a `get_weather()` function that
takes the location as a parameter.

The following example demonstrates how to implement compositional function
calling using the Python SDK and automatic function calling.

 
 

### Python

This example uses the automatic function calling feature of the
`google-genai` Python SDK. The SDK automatically converts the Python
functions to the required schema, executes the function calls when requested
by the model, and sends the results back to the model to complete the task.

 

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20¬∞C in London, set the thermostat to 20¬∞C, otherwise set it to 18¬∞C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

 

 Expected Output 

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20¬∞C.
```

 
 

### JavaScript

This example shows how to use JavaScript/TypeScript SDK to do comopositional
function calling using a manual execution loop.

 

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20¬∞C in London, set the thermostat to 20¬∞C, otherwise set it to 18¬∞C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

 

 Expected Output 

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25¬∞C in London, so I've set the thermostat to 20¬∞C.
```

 
 

Compositional function calling is a native Live
API feature. This means Live API
can handle the function calling similar to the Python SDK.

 
 

### Python

 

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

 
 

### JavaScript

 

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

 
 

## Function calling modes

The Gemini API lets you control how the model uses the provided tools
(function declarations). Specifically, you can set the mode within
the.`function_calling_config`.

- `AUTO (Default)`: The model decides whether to generate a natural language
response or suggest a function call based on the prompt and context. This is the
most flexible mode and recommended for most scenarios.

- `ANY`: The model is constrained to always predict a function call and
guarantees function schema adherence. If `allowed_function_names` is not
specified, the model can choose from any of the provided function declarations.
If `allowed_function_names` is provided as a list, the model can only choose
from the functions in that list. Use this mode when you require a function
call response to every prompt (if applicable).

- `NONE`: The model is prohibited from making function calls. This is
equivalent to sending a request without any function declarations. Use this to
temporarily disable function calling without removing your tool definitions.

- 

`VALIDATED` (Preview): The model is constrained to predict either function
calls or natural language, and ensures function schema adherence. If
`allowed_function_names` is not provided, the model picks from all of the
available function declarations. If `allowed_function_names` is provided, the
model picks from the set of allowed functions.

 
 

### Python

 

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

 
 

### JavaScript

 

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

 
 

## Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools.
The SDK converts these functions into declarations, manages the function call
execution, and handles the response cycle for you. Define your function with
type hints and a docstring. For optimal results, it is recommended to use
 Google-style docstrings. 
The SDK will then automatically:

- Detect function call responses from the model.

- Call the corresponding Python function in your code.

- Send the function's response back to the model.

- Return the model's final text response.

The SDK currently does not parse argument descriptions into the property
description slots of the generated function declaration. Instead, it sends the
entire docstring as the top-level function description.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco, CA

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

 
 

You can disable automatic function calling with:

 
 

### Python

 

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

 
 

### Automatic function schema declaration

The API is able to describe any of the following types. `Pydantic` types are
allowed, as long as the fields defined on them are also composed of allowed
types. Dict types (like `dict[str: int]`) are not well supported here, don't
use them.

 
 

### Python

 

```
AllowedType = (
  int | float | bool | str | list['AllowedType'] | pydantic.BaseModel)
```

 
 

To see what the inferred schema looks like, you can convert it using
 `from_callable` :

 
 

### Python

 

```
from google import genai
from google.genai import types

def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

client = genai.Client()
fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

 
 

## Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with
function calling at the same time. Here's an example that enables two tools,
 Grounding with Google Search and
 code execution , in a request using the
 Live API .

 
 

### Python

 

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

 
 

### JavaScript

 

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

 
 

Python developers can try this out in the Live API Tool Use
notebook .

## Model context protocol (MCP)

 Model Context Protocol (MCP) is
an open standard for connecting AI applications with external tools and data.
MCP provides a common protocol for models to access context, such as functions
(tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and
offering
 automatic tool calling 
for MCP tools. When the model generates an MCP tool call, the Python and
JavaScript client SDK can automatically execute the MCP tool and send the
response back to the model in a subsequent request, continuing this loop until
no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and
`mcp` SDK.

 
 

### Python

Make sure the latest version of the
 `mcp` SDK is installed on
your platform of choice.

 

```
pip install mcp
```

 

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

 
 

### JavaScript

Make sure the latest version of the `mcp` SDK is installed on your platform
of choice.

 

```
npm install @modelcontextprotocol/sdk
```

 

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

 
 

### Limitations with built-in MCP support

Built-in MCP support is a experimental 
feature in our SDKs and has the following limitations:

- Only tools are supported, not resources nor prompts

- It is available for the Python and JavaScript/TypeScript SDK.

- Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're
building.

## Supported models

This section lists models and their function calling capabilities. Experimental
models are not included. You can find a comprehensive capabilities overview on
the model overview page.

 
 
 
 Model 
 Function Calling 
 Parallel Function Calling 
 Compositional Function Calling 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash-Lite 
 X 
 X 
 X 
 
 
 

## Best practices

- Function and Parameter Descriptions: Be extremely clear and specific in
your descriptions. The model relies on these to choose the correct function
and provide appropriate arguments.

- Naming: Use descriptive function names (without spaces, periods, or
dashes).

- Strong Typing: Use specific types (integer, string, enum) for parameters
to reduce errors. If a parameter has a limited set of valid values, use an
enum.

- Tool Selection: While the model can use an arbitrary number of tools,
providing too many can increase the risk of selecting an incorrect or
suboptimal tool. For best results, aim to provide only the relevant tools
for the context or task, ideally keeping the active set to a maximum of
10-20. Consider dynamic tool selection based on conversation context if you
have a large total number of tools.

- Prompt Engineering: 

 Provide context: Tell the model its role (e.g., "You are a helpful
weather assistant.").

- Give instructions: Specify how and when to use functions (e.g., "Don't
guess dates; always use a future date for forecasts.").

- Encourage clarification: Instruct the model to ask clarifying questions
if needed.

 
- 

 Temperature: Use a low temperature (e.g., 0) for more deterministic and
reliable function calls.

- 

 Validation: If a function call has significant consequences (e.g.,
placing an order), validate the call with the user before executing it.

- 

 Check Finish Reason: Always check the `finishReason` 
in the model's response to handle cases where the model failed to generate a
valid function call.

- 

 Error Handling : Implement robust error handling in your functions to
gracefully handle unexpected inputs or API failures. Return informative
error messages that the model can use to generate helpful responses to the
user.

- 

 Security: Be mindful of security when calling external APIs. Use
appropriate authentication and authorization mechanisms. Avoid exposing
sensitive data in function calls.

- 

 Token Limits: Function descriptions and parameters count towards your
input token limit. If you're hitting token limits, consider limiting the
number of functions or the length of the descriptions, break down complex
tasks into smaller, more focused function sets.

## Notes and limitations

- Only a subset of the OpenAPI
schema is supported.

- Supported parameter types in Python are limited.

- Automatic function calling is a Python SDK feature only.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Video understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/video-understanding#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Video understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Video understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models can process videos, enabling many frontier developer use cases
that would have historically required domain specific models.
Some of Gemini's vision capabilities include the ability to:

- Describe, segment, and extract information from videos

- Answer questions about video content

- Refer to specific timestamps within a video

Gemini was built to be multimodal from the ground up and we continue to push the
frontier of what is possible. This guide shows how to use the Gemini API to
generate text responses based on video inputs.

## Video input

You can provide videos as input to Gemini in the following ways:

- Upload a video file using the File API before making a
request to `generateContent`. Use this method for files larger than 20MB, videos
longer than approximately 1 minute, or when you want to reuse the file across
multiple requests.

- Pass inline video data with the request to `generateContent`. Use this method for smaller files (<20MB) and shorter durations.

- Pass YouTube URLs as part of your `generateContent` request.

### Upload a video file

You can use the Files API to upload a video file.
Always use the Files API when the total request size (including the file, text
prompt, system instructions, etc.) is larger than 20 MB, the video duration is
significant, or if you intend to use the same video in multiple prompts.
The File API accepts video file formats directly.

The following code downloads the sample video, uploads it using the File API,
waits for it to be processed, and then uses the file reference in
a `generateContent` request.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp4")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=[myfile, "Summarize this video. Then create a quiz with an answer key based on the information in this video."]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp4",
    config: { mimeType: "video/mp4" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.mp4", nil)

parts := []*genai.Part{
    genai.NewPartFromText("Summarize this video. Then create a quiz with an answer key based on the information in this video."),
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
)

fmt.Println(result.Text())
```

 
 

### REST

 

```
VIDEO_PATH="path/to/sample.mp4"
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
DISPLAY_NAME=VIDEO

tmp_header_file=upload-header.tmp

echo "Starting file upload..."
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D ${tmp_header_file} \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

echo "Uploading video data..."
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

echo "File uploaded successfully. File URI: ${file_uri}"

# --- 3. Generate content using the uploaded video file ---
echo "Generating content from video..."
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Summarize this video. Then create a quiz with an answer key based on the information in this video."}]
        }]
      }' 2> /dev/null > response.json

jq -r ".candidates[].content.parts[].text" response.json
```

 
 

To learn more about working with media files, see
 Files API .

### Pass video data inline

Instead of uploading a video file using the File API, you can pass smaller
videos directly in the request to `generateContent`. This is suitable for
shorter videos under 20MB total request size.

Here's an example of providing inline video data:

 
 

### Python

 

```
from google import genai
from google.genai import types

# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "video/mp4",
      data: base64VideoFile,
    },
  },
  { text: "Please summarize the video in 3 sentences." }
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### REST

 

```
VIDEO_PATH=/path/to/your/video.mp4

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"video/mp4",
                "data": "'$(base64 $B64FLAGS $VIDEO_PATH)'"
              }
            },
            {"text": "Please summarize the video in 3 sentences."}
        ]
      }]
    }' 2> /dev/null
```

 
 

### Pass YouTube URLs

You can pass YouTube URLs directly to Gemini API as part of your `generateContent`request as follows:

 
 

### Python

 

```
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
const result = await model.generateContent([
  "Please summarize the video in 3 sentences.",
  {
    fileData: {
      fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
    },
  },
]);
console.log(result.response.text());
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  parts := []*genai.Part{
      genai.NewPartFromText("Please summarize the video in 3 sentences."),
      genai.NewPartFromURI("https://www.youtube.com/watch?v=9hE5-98ZeCg","video/mp4"),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Please summarize the video in 3 sentences."},
            {
              "file_data": {
                "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
              }
            }
        ]
      }]
    }' 2> /dev/null
```

 
 

 Limitations: 

- For the free tier, you can't upload more than 8 hours of YouTube video per day.

- For the paid tier, there is no limit based on video length.

- For models prior to Gemini 2.5, you can upload only 1 video per request. For Gemini 2.5 and later models, you can upload a maximum of 10 videos per request.

- You can only upload public videos (not private or unlisted videos).

## Refer to timestamps in the content

You can ask questions about specific points in time within the video using
timestamps of the form `MM:SS`.

 
 

### Python

 

```
prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?" # Adjusted timestamps for the NASA video
```

 
 

### JavaScript

 

```
const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";
```

 
 

### Go

 

```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
         // Adjusted timestamps for the NASA video
        genai.NewPartFromText("What are the examples given at 00:05 and " +
            "00:10 supposed to show us?"),
    }
```

 
 

### REST

 

```
PROMPT="What are the examples given at 00:05 and 00:10 supposed to show us?"
```

 
 

## Extract detailed insights from video

Gemini models offer powerful capabilities for understanding video content by
processing information from both the audio and visual streams. This lets you
extract a rich set of details, including generating descriptions of what is
happening in a video and answering questions about its content. For visual
descriptions, the model samples the video at a rate of 1 frame per second .
This sampling rate may affect the level of detail in the descriptions,
particularly for videos with rapidly changing visuals.

 
 

### Python

 

```
prompt = "Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments."
```

 
 

### JavaScript

 

```
const prompt = "Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments.";
```

 
 

### Go

 

```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
        genai.NewPartFromText("Describe the key events in this video, providing both audio and visual details. " +
      "Include timestamps for salient moments."),
    }
```

 
 

### REST

 

```
PROMPT="Describe the key events in this video, providing both audio and visual details. Include timestamps for salient moments."
```

 
 

## Customize video processing

You can customize video processing in the Gemini API by setting clipping
intervals or providing custom frame rate sampling.

### Set clipping intervals

You can clip video by specifying `videoMetadata` with start and end offsets.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),
                video_metadata=types.VideoMetadata(
                    start_offset='1250s',
                    end_offset='1570s'
                )
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash';

async function main() {
const contents = [
  {
    role: 'user',
    parts: [
      {
        fileData: {
          fileUri: 'https://www.youtube.com/watch?v=9hE5-98ZeCg',
          mimeType: 'video/*',
        },
        videoMetadata: {
          startOffset: '40s',
          endOffset: '80s',
        }
      },
      {
        text: 'Please summarize the video in 3 sentences.',
      },
    ],
  },
];

const response = await ai.models.generateContent({
  model,
  contents,
});

console.log(response.text)

}

await main();
```

 
 

### Set a custom frame rate

You can set custom frame rate sampling by passing an `fps` argument to
`videoMetadata`.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

client = genai.Client()
response = client.models.generate_content(
    model='models/gemini-2.5-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=video_bytes,
                    mime_type='video/mp4'),
                video_metadata=types.VideoMetadata(fps=5)
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

 
 

By default 1 frame per second (FPS) is sampled from the video. You might want to
set low FPS (< 1) for long videos. This is especially useful for mostly static
videos (e.g. lectures). If you want to capture more details in rapidly changing
visuals, consider setting a higher FPS value.

## Supported video formats

Gemini supports the following video format MIME types:

- `video/mp4`

- `video/mpeg`

- `video/mov`

- `video/avi`

- `video/x-flv`

- `video/mpg`

- `video/webm`

- `video/wmv`

- `video/3gpp`

## Technical details about videos

- Supported models & context : All Gemini 2.0 and 2.5 models can process video data.

 Models with a 2M context window can process videos up to 2 hours long at
default media resolution or 6 hours long at low media resolution, while
models with a 1M context window can process videos up to 1 hour long at
default media resolution or 3 hours long at low media resolution.

 
- File API processing : When using the File API, videos are stored at 1
frame per second (FPS) and audio is processed at 1Kbps (single channel).
Timestamps are added every second.

 These rates are subject to change in the future for improvements in inference.

- You can override the 1 FPS sampling rate by setting a custom frame rate .

 
- Token calculation : Each second of video is tokenized as follows:

 Individual frames (sampled at 1 FPS):

 If `mediaResolution` is set
to low, frames are tokenized at 66 tokens per frame.

- Otherwise, frames are tokenized at 258 tokens per frame.

 
- Audio: 32 tokens per second.

- Metadata is also included.

- Total: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.

 
- 

 Medial resolution : Gemini 3 introduces granular control over multimodal
vision processing with the `media_resolution` parameter. The
`media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to read fine text or identify
small details, but increase token usage and latency.

For more details about the parameter and how it can impact token
calculations, see the media resolution guide.

- 

 Timestamp format : When referring to specific moments in a video within your prompt, use the `MM:SS` format (e.g., `01:15` for 1 minute and 15 seconds).

- 

 Best practices :

 Use only one video per prompt request for optimal results.

- If combining text and a single video, place the text prompt after the video part in the `contents` array.

- Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

 

## What's next

This guide shows how to upload video files and generate text outputs from video
inputs. To learn more, see the following resources:

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- Files API : Learn more about uploading and managing
files for use with Gemini.

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- Safety guidance : Sometimes generative
 AI models produce unexpected outputs, such as outputs that are inaccurate,
 biased, or offensive. Post-processing and human evaluation are essential to
 limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Long context &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/long-context

- 
 
 
 
 
 
 
 
 
 
 
 Long context  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Long context 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Many Gemini models come with large context windows of 1 million or more tokens.
Historically, large language models (LLMs) were significantly limited by
the amount of text (or tokens) that could be passed to the model at one time.
The Gemini long context window unlocks many new use cases and developer
paradigms.

The code you already use for cases like text
generation or multimodal
inputs will work without any changes with long context.

This document gives you an overview of what you can achieve using models with
context windows of 1M and more tokens. The page gives a brief overview of
a context window, and explores how developers should think about long context,
various real world use cases for long context, and ways to optimize the usage
of long context.

For the context window sizes of specific models, see the
 Models page.

## What is a context window?

The basic way you use the Gemini models is by passing information (context)
to the model, which will subsequently generate a response. An analogy for the
context window is short term memory. There is a limited amount of information
that can be stored in someone's short term memory, and the same is true for
generative models.

You can read more about how models work under the hood in our generative models
guide .

## Getting started with long context

Earlier versions of generative models were only able to process 8,000
tokens at a time. Newer models pushed this further by accepting 32,000 or even
128,000 tokens. Gemini is the first model capable of accepting 1 million tokens.

In practice, 1 million tokens would look like:

- 50,000 lines of code (with the standard 80 characters per line)

- All the text messages you have sent in the last 5 years

- 8 average length English novels

- Transcripts of over 200 average length podcast episodes

The more limited context windows common in many other models often require
strategies like arbitrarily dropping old messages, summarizing content, using
RAG with vector databases, or filtering prompts to save tokens.

While these techniques remain valuable in specific scenarios, Gemini's extensive
context window invites a more direct approach: providing all relevant
information upfront. Because Gemini models were purpose-built with massive
context capabilities, they demonstrate powerful in-context learning. For
example, using only in-context instructional materials (a 500-page reference
grammar, a dictionary, and ‚âà400 parallel sentences), Gemini
 learned to translate 
from English to Kalamang‚Äîa Papuan language with
fewer than 200 speakers‚Äîwith quality similar to a human learner using the same
materials. This illustrates the paradigm shift enabled by Gemini's long context,
empowering new possibilities through robust in-context learning.

## Long context use cases

While the standard use case for most generative models is still text input, the
Gemini model family enables a new paradigm of multimodal use cases. These
models can natively understand text, video, audio, and images. They are
accompanied by the Gemini API that takes in multimodal file
types for
convenience.

### Long form text

Text has proved to be the layer of intelligence underpinning much of the
momentum around LLMs. As mentioned earlier, much of the practical limitation of
LLMs was because of not having a large enough context window to do certain
tasks. This led to the rapid adoption of retrieval augmented generation (RAG)
and other techniques which dynamically provide the model with relevant
contextual information. Now, with larger and larger context windows, there are
new techniques becoming available which unlock new use cases.

Some emerging and standard use cases for text based long context include:

- Summarizing large corpuses of text

 Previous summarization options with smaller context models would require
a sliding window or another technique to keep state of previous sections
as new tokens are passed to the model

 
- Question and answering

 Historically this was only possible with RAG given the limited amount of
context and models' factual recall being low

 
- Agentic workflows

 Text is the underpinning of how agents keep state of what they have done
and what they need to do; not having enough information about the world
and the agent's goal is a limitation on the reliability of agents

 

 Many-shot in-context learning is one of the
most unique capabilities unlocked by long context models. Research has shown
that taking the common "single shot" or "multi-shot" example paradigm, where the
model is presented with one or a few examples of a task, and scaling that up to
hundreds, thousands, or even hundreds of thousands of examples, can lead to
novel model capabilities. This many-shot approach has also been shown to perform
similarly to models which were fine-tuned for a specific task. For use cases
where a Gemini model's performance is not yet sufficient for a production
rollout, you can try the many-shot approach. As you might explore later in the
long context optimization section, context caching makes this type of high input
token workload much more economically feasible and even lower latency in some
cases.

### Long form video

Video content's utility has long been constrained by the lack of accessibility
of the medium itself. It was hard to skim the content, transcripts often failed
to capture the nuance of a video, and most tools don't process image, text, and
audio together. With Gemini, the long-context text capabilities translate to
the ability to reason and answer questions about multimodal inputs with
sustained performance.

Some emerging and standard use cases for video long context include:

- Video question and answering

- Video memory, as shown with Google's Project Astra 

- Video captioning

- Video recommendation systems, by enriching existing metadata with new
multimodal understanding

- Video customization, by looking at a corpus of data and associated video
metadata and then removing parts of videos that are not relevant to the
viewer

- Video content moderation

- Real-time video processing

When working with videos, it is important to consider how the videos are
processed into tokens , which affects
billing and usage limits. You can learn more about prompting with video files in
the Prompting
guide .

### Long form audio

The Gemini models were the first natively multimodal large language models
that could understand audio. Historically, the typical developer workflow would
involve stringing together multiple domain specific models, like a
speech-to-text model and a text-to-text model, in order to process audio. This
led to additional latency required by performing multiple round-trip requests
and decreased performance usually attributed to disconnected architectures of
the multiple model setup.

Some emerging and standard use cases for audio context include:

- Real-time transcription and translation

- Podcast / video question and answering

- Meeting transcription and summarization

- Voice assistants

You can learn more about prompting with audio files in the Prompting
guide .

## Long context optimizations

The primary optimization when working with long context and the Gemini
models is to use context
caching . Beyond the previous
impossibility of processing lots of tokens in a single request, the other main
constraint was the cost. If you have a "chat with your data" app where a user
uploads 10 PDFs, a video, and some work documents, you would historically have
to work with a more complex retrieval augmented generation (RAG) tool /
framework in order to process these requests and pay a significant amount for
tokens moved into the context window. Now, you can cache the files the user
uploads and pay to store them on a per hour basis. The input / output cost per
request with Gemini Flash for example is ~4x less than the standard
input / output cost, so if
the user chats with their data enough, it becomes a huge cost saving for you as
the developer.

## Long context limitations

In various sections of this guide, we talked about how Gemini models achieve
high performance across various needle-in-a-haystack retrieval evals. These
tests consider the most basic setup, where you have a single needle you are
looking for. In cases where you might have multiple "needles" or specific pieces
of information you are looking for, the model does not perform with the same
accuracy. Performance can vary to a wide degree depending on the context. This
is important to consider as there is an inherent tradeoff between getting the
right information retrieved and cost. You can get ~99% on a single query, but
you have to pay the input token cost every time you send that query. So for 100
pieces of information to be retrieved, if you needed 99% performance, you would
likely need to send 100 requests. This is a good example of where context
caching can significantly reduce the cost associated with using Gemini models
while keeping the performance high.

## FAQs

### Where is the best place to put my query in the context window?

In most cases, especially if the total context is long, the model's
performance will be better if you put your query / question at the end of the
prompt (after all the other context).

### Do I lose model performance when I add more tokens to a query?

Generally, if you don't need tokens to be passed to the model, it is best to
avoid passing them. However, if you have a large chunk of tokens with some
information and want to ask questions about that information, the model is
highly capable of extracting that information (up to 99% accuracy in many
cases).

### How can I lower my cost with long-context queries?

If you have a similar set of tokens / context that you want to re-use many
times, context caching can help reduce the costs
associated with asking questions about that information.

### Does the context length affect the model latency?

There is some fixed amount of latency in any given request, regardless of the
size, but generally longer queries will have higher latency (time to first
token).

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Document understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/document-processing#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Document understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Document understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini models can process documents in PDF format, using native
vision to understand entire document contexts. This goes beyond
simple text extraction, allowing Gemini to:

- Analyze and interpret content, including text, images, diagrams,
charts, and tables, even in long documents up to 1000 pages.

- Extract information into structured output formats.

- Summarize and answer questions based on both the visual and textual elements
in a document.

- Transcribe document content (e.g. to HTML), preserving layouts and
formatting, for use in downstream applications.

## Passing inline PDF data

You can pass inline PDF data in the request to `generateContent`.
For PDF payloads under 20MB, you can choose between uploading base64
encoded documents or directly uploading locally stored files.

The following example shows you how to fetch a PDF from a URL and convert it to
bytes for processing:

 
 

### Python

 

```
from google import genai
from google.genai import types
import httpx

client = genai.Client()

doc_url = "https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf"

# Retrieve and encode the PDF byte
doc_data = httpx.get(doc_url).content

prompt = "Summarize this document"
response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[
      types.Part.from_bytes(
        data=doc_data,
        mime_type='application/pdf',
      ),
      prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const pdfResp = await fetch('https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf')
        .then((response) => response.arrayBuffer());

    const contents = [
        { text: "Summarize this document" },
        {
            inlineData: {
                mimeType: 'application/pdf',
                data: Buffer.from(pdfResp).toString("base64")
            }
        }
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: contents
    });
    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "io"
    "net/http"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    pdfResp, _ := http.Get("https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf")
    var pdfBytes []byte
    if pdfResp != nil && pdfResp.Body != nil {
        pdfBytes, _ = io.ReadAll(pdfResp.Body)
        pdfResp.Body.Close()
    }

    parts := []*genai.Part{
        &genai.Part{
            InlineData: &genai.Blob{
                MIMEType: "application/pdf",
                Data:     pdfBytes,
            },
        },
        genai.NewPartFromText("Summarize this document"),
    }

    contents := []*genai.Content{
        genai.NewContentFromParts(parts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
DOC_URL="https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf"
PROMPT="Summarize this document"
DISPLAY_NAME="base64_pdf"

# Download the PDF
wget -O "${DISPLAY_NAME}.pdf" "${DOC_URL}"

# Check for FreeBSD base64 and set flags accordingly
if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

# Base64 encode the PDF
ENCODED_PDF=$(base64 $B64FLAGS "${DISPLAY_NAME}.pdf")

# Generate content using the base64 encoded PDF
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"inline_data": {"mime_type": "application/pdf", "data": "'"$ENCODED_PDF"'"}},
          {"text": "'$PROMPT'"}
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json

# Clean up the downloaded PDF
rm "${DISPLAY_NAME}.pdf"
```

 
 

You can also read a PDF from a local file for processing:

 
 

### Python

 

```
from google import genai
from google.genai import types
import pathlib

client = genai.Client()

# Retrieve and encode the PDF byte
filepath = pathlib.Path('file.pdf')

prompt = "Summarize this document"
response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[
      types.Part.from_bytes(
        data=filepath.read_bytes(),
        mime_type='application/pdf',
      ),
      prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from 'fs';

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const contents = [
        { text: "Summarize this document" },
        {
            inlineData: {
                mimeType: 'application/pdf',
                data: Buffer.from(fs.readFileSync("content/343019_3_art_0_py4t4l_convrt.pdf")).toString("base64")
            }
        }
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: contents
    });
    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    pdfBytes, _ := os.ReadFile("path/to/your/file.pdf")

    parts := []*genai.Part{
        &genai.Part{
            InlineData: &genai.Blob{
                MIMEType: "application/pdf",
                Data:     pdfBytes,
            },
        },
        genai.NewPartFromText("Summarize this document"),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(parts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

## Uploading PDFs using the File API

You can use the File API to upload larger documents. Always use the File
API when the total request size (including the files, text prompt, system
instructions, etc.) is larger than 20MB.

Call `media.upload` to upload a file using the
File API. The following code uploads a document file and then uses the file in a
call to
 `models.generateContent` .

### Large PDFs from URLs

Use the File API to simplify uploading and processing large PDF files from URLs:

 
 

### Python

 

```
from google import genai
from google.genai import types
import io
import httpx

client = genai.Client()

long_context_pdf_path = "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"

# Retrieve and upload the PDF using the File API
doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)

sample_doc = client.files.upload(
  # You can pass a path or a file-like object here
  file=doc_io,
  config=dict(
    mime_type='application/pdf')
)

prompt = "Summarize this document"

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_doc, prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {

    const pdfBuffer = await fetch("https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf")
        .then((response) => response.arrayBuffer());

    const fileBlob = new Blob([pdfBuffer], { type: 'application/pdf' });

    const file = await ai.files.upload({
        file: fileBlob,
        config: {
            displayName: 'A17_FlightPlan.pdf',
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    // Add the file to the contents.
    const content = [
        'Summarize this document',
    ];

    if (file.uri && file.mimeType) {
        const fileContent = createPartFromUri(file.uri, file.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);

}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "io"
  "net/http"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, _ := genai.NewClient(ctx, &genai.ClientConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Backend: genai.BackendGeminiAPI,
  })

  pdfURL := "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"
  localPdfPath := "A17_FlightPlan_downloaded.pdf"

  respHttp, _ := http.Get(pdfURL)
  defer respHttp.Body.Close()

  outFile, _ := os.Create(localPdfPath)
  defer outFile.Close()

  _, _ = io.Copy(outFile, respHttp.Body)

  uploadConfig := &genai.UploadFileConfig{MIMEType: "application/pdf"}
  uploadedFile, _ := client.Files.UploadFromPath(ctx, localPdfPath, uploadConfig)

  promptParts := []*genai.Part{
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
    genai.NewPartFromText("Summarize this document"),
  }
  contents := []*genai.Content{
    genai.NewContentFromParts(promptParts, genai.RoleUser), // Specify role
  }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
PDF_PATH="https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"
DISPLAY_NAME="A17_FlightPlan"
PROMPT="Summarize this document"

# Download the PDF from the provided URL
wget -O "${DISPLAY_NAME}.pdf" "${PDF_PATH}"

MIME_TYPE=$(file -b --mime-type "${DISPLAY_NAME}.pdf")
NUM_BYTES=$(wc -c < "${DISPLAY_NAME}.pdf")

echo "MIME_TYPE: ${MIME_TYPE}"
echo "NUM_BYTES: ${NUM_BYTES}"

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${DISPLAY_NAME}.pdf" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo "file_uri: ${file_uri}"

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "'$PROMPT'"},
          {"file_data":{"mime_type": "application/pdf", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json

# Clean up the downloaded PDF
rm "${DISPLAY_NAME}.pdf"
```

 
 

### Large PDFs stored locally

 
 

### Python

 

```
from google import genai
from google.genai import types
import pathlib
import httpx

client = genai.Client()

# Retrieve and encode the PDF byte
file_path = pathlib.Path('large_file.pdf')

# Upload the PDF using the File API
sample_file = client.files.upload(
  file=file_path,
)

prompt="Summarize this document"

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_file, "Summarize this document"])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
    const file = await ai.files.upload({
        file: 'path-to-localfile.pdf'
        config: {
            displayName: 'A17_FlightPlan.pdf',
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    // Add the file to the contents.
    const content = [
        'Summarize this document',
    ];

    if (file.uri && file.mimeType) {
        const fileContent = createPartFromUri(file.uri, file.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);

}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })
    localPdfPath := "/path/to/file.pdf"

    uploadConfig := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile, _ := client.Files.UploadFromPath(ctx, localPdfPath, uploadConfig)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
        genai.NewPartFromText("Give me a summary of this pdf file."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
NUM_BYTES=$(wc -c < "${PDF_PATH}")
DISPLAY_NAME=TEXT
tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GEMINI_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: application/pdf" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${PDF_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Can you add a few more lines to this poem?"},
          {"file_data":{"mime_type": "application/pdf", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

You can verify the API successfully stored the uploaded file and get its
metadata by calling `files.get` . Only the `name`
(and by extension, the `uri`) are unique.

 
 

### Python

 

```
from google import genai
import pathlib

client = genai.Client()

fpath = pathlib.Path('example.txt')
fpath.write_text('hello')

file = client.files.upload(file='example.txt')

file_info = client.files.get(name=file.name)
print(file_info.model_dump_json(indent=4))
```

 
 

### REST

 

```
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

 
 

## Passing multiple PDFs

The Gemini API is capable of processing multiple PDF documents (up to 1000 pages)
in a single request, as long as the combined size of the documents and the text
prompt stays within the model's context window.

 
 

### Python

 

```
from google import genai
import io
import httpx

client = genai.Client()

doc_url_1 = "https://arxiv.org/pdf/2312.11805"
doc_url_2 = "https://arxiv.org/pdf/2403.05530"

# Retrieve and upload both PDFs using the File API
doc_data_1 = io.BytesIO(httpx.get(doc_url_1).content)
doc_data_2 = io.BytesIO(httpx.get(doc_url_2).content)

sample_pdf_1 = client.files.upload(
  file=doc_data_1,
  config=dict(mime_type='application/pdf')
)
sample_pdf_2 = client.files.upload(
  file=doc_data_2,
  config=dict(mime_type='application/pdf')
)

prompt = "What is the difference between each of the main benchmarks between these two papers? Output these in a table."

response = client.models.generate_content(
  model="gemini-2.5-flash",
  contents=[sample_pdf_1, sample_pdf_2, prompt])
print(response.text)
```

 
 

### JavaScript

 

```
import { createPartFromUri, GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function uploadRemotePDF(url, displayName) {
    const pdfBuffer = await fetch(url)
        .then((response) => response.arrayBuffer());

    const fileBlob = new Blob([pdfBuffer], { type: 'application/pdf' });

    const file = await ai.files.upload({
        file: fileBlob,
        config: {
            displayName: displayName,
        },
    });

    // Wait for the file to be processed.
    let getFile = await ai.files.get({ name: file.name });
    while (getFile.state === 'PROCESSING') {
        getFile = await ai.files.get({ name: file.name });
        console.log(`current file status: ${getFile.state}`);
        console.log('File is still processing, retrying in 5 seconds');

        await new Promise((resolve) => {
            setTimeout(resolve, 5000);
        });
    }
    if (file.state === 'FAILED') {
        throw new Error('File processing failed.');
    }

    return file;
}

async function main() {
    const content = [
        'What is the difference between each of the main benchmarks between these two papers? Output these in a table.',
    ];

    let file1 = await uploadRemotePDF("https://arxiv.org/pdf/2312.11805", "PDF 1")
    if (file1.uri && file1.mimeType) {
        const fileContent = createPartFromUri(file1.uri, file1.mimeType);
        content.push(fileContent);
    }
    let file2 = await uploadRemotePDF("https://arxiv.org/pdf/2403.05530", "PDF 2")
    if (file2.uri && file2.mimeType) {
        const fileContent = createPartFromUri(file2.uri, file2.mimeType);
        content.push(fileContent);
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: content,
    });

    console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "io"
    "net/http"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    docUrl1 := "https://arxiv.org/pdf/2312.11805"
    docUrl2 := "https://arxiv.org/pdf/2403.05530"
    localPath1 := "doc1_downloaded.pdf"
    localPath2 := "doc2_downloaded.pdf"

    respHttp1, _ := http.Get(docUrl1)
    defer respHttp1.Body.Close()

    outFile1, _ := os.Create(localPath1)
    _, _ = io.Copy(outFile1, respHttp1.Body)
    outFile1.Close()

    respHttp2, _ := http.Get(docUrl2)
    defer respHttp2.Body.Close()

    outFile2, _ := os.Create(localPath2)
    _, _ = io.Copy(outFile2, respHttp2.Body)
    outFile2.Close()

    uploadConfig1 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile1, _ := client.Files.UploadFromPath(ctx, localPath1, uploadConfig1)

    uploadConfig2 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile2, _ := client.Files.UploadFromPath(ctx, localPath2, uploadConfig2)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile1.URI, uploadedFile1.MIMEType),
        genai.NewPartFromURI(uploadedFile2.URI, uploadedFile2.MIMEType),
        genai.NewPartFromText("What is the difference between each of the " +
                              "main benchmarks between these two papers? " +
                              "Output these in a table."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    modelName := "gemini-2.5-flash"
    result, _ := client.Models.GenerateContent(
        ctx,
        modelName,
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
DOC_URL_1="https://arxiv.org/pdf/2312.11805"
DOC_URL_2="https://arxiv.org/pdf/2403.05530"
DISPLAY_NAME_1="Gemini_paper"
DISPLAY_NAME_2="Gemini_1.5_paper"
PROMPT="What is the difference between each of the main benchmarks between these two papers? Output these in a table."

# Function to download and upload a PDF
upload_pdf() {
  local doc_url="$1"
  local display_name="$2"

  # Download the PDF
  wget -O "${display_name}.pdf" "${doc_url}"

  local MIME_TYPE=$(file -b --mime-type "${display_name}.pdf")
  local NUM_BYTES=$(wc -c < "${display_name}.pdf")

  echo "MIME_TYPE: ${MIME_TYPE}"
  echo "NUM_BYTES: ${NUM_BYTES}"

  local tmp_header_file=upload-header.tmp

  # Initial resumable request
  curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
    -D "${tmp_header_file}" \
    -H "X-Goog-Upload-Protocol: resumable" \
    -H "X-Goog-Upload-Command: start" \
    -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
    -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
    -H "Content-Type: application/json" \
    -d "{'file': {'display_name': '${display_name}'}}" 2> /dev/null

  local upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
  rm "${tmp_header_file}"

  # Upload the PDF
  curl "${upload_url}" \
    -H "Content-Length: ${NUM_BYTES}" \
    -H "X-Goog-Upload-Offset: 0" \
    -H "X-Goog-Upload-Command: upload, finalize" \
    --data-binary "@${display_name}.pdf" 2> /dev/null > "file_info_${display_name}.json"

  local file_uri=$(jq ".file.uri" "file_info_${display_name}.json")
  echo "file_uri for ${display_name}: ${file_uri}"

  # Clean up the downloaded PDF
  rm "${display_name}.pdf"

  echo "${file_uri}"
}

# Upload the first PDF
file_uri_1=$(upload_pdf "${DOC_URL_1}" "${DISPLAY_NAME_1}")

# Upload the second PDF
file_uri_2=$(upload_pdf "${DOC_URL_2}" "${DISPLAY_NAME_2}")

# Now generate content using both files
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data": {"mime_type": "application/pdf", "file_uri": '$file_uri_1'}},
          {"file_data": {"mime_type": "application/pdf", "file_uri": '$file_uri_2'}},
          {"text": "'$PROMPT'"}
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Technical details

Gemini supports a maximum of 1,000 document pages.
Each document page is equivalent to 258 tokens.

While there are no specific limits to the number of pixels in a document besides
the model's context window , larger pages are
scaled down to a maximum resolution of 3072x3072 while preserving their original
aspect ratio, while smaller pages are scaled up to 768x768 pixels. There is no
cost reduction for pages at lower sizes, other than bandwidth, or performance
improvement for pages at higher resolution.

### Gemini 3 models

Gemini 3 introduces granular control over multimodal vision processing with the
`media_resolution` parameter. You can now set the resolution to low, medium, or
high per individual media part. With this addition, the processing of PDF
documents has been updated:

- Native text inclusion: Text natively embedded in the PDF is extracted
and provided to the model.

- Billing & token reporting: 

 You are not charged for tokens originating from the extracted
 native text in PDFs.

- In the `usage_metadata` section of the API response, tokens generated
from processing PDF pages (as images) are now counted under the `IMAGE`
modality, not a separate `DOCUMENT` modality as in some earlier
versions.

 

For more details about the media resolution parameter, see the
 Media resolution guide.

### Document types

Technically, you can pass other MIME types for document understanding, like
TXT, Markdown, HTML, XML, etc. However, document vision only meaningfully
understands PDFs . Other types will be extracted as pure text, and the model
won't be able to interpret what we see in the rendering of those files. Any
file-type specifics like charts, diagrams, HTML tags, Markdown formatting, etc.,
will be lost.

### Best practices

For best results:

- Rotate pages to the correct orientation before uploading.

- Avoid blurry pages.

- If using a single page, place the text prompt after the page.

## What's next

To learn more, see the following resources:

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Using tools with Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/tools

- 
 
 
 
 
 
 
 
 
 
 
 Using tools with Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Using tools with Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Tools extend the capabilities of Gemini models, enabling them to take action in the world, access real-time information, and perform complex computational tasks. Models can use tools in both standard request-response interactions and real-time streaming sessions via the Live API .

The Gemini API provides a suite of fully managed, built-in tools optimized for Gemini models or you can define custom tools using Function Calling .

## Available built-in tools

 
 
 
 Tool 
 Description 
 Use Cases 
 
 

 
 
 Google Search 
 Ground responses in current events and facts from the web to reduce hallucinations. 
 - Answering questions about recent events 
 - Verifying facts with diverse sources 
 
 
 Google Maps 
 Build location-aware assistants that can find places, get directions, and provide rich local context. 
 - Planning travel itineraries with multiple stops 
 - Finding local businesses based on user criteria 
 
 
 Code Execution 
 Allow the model to write and run Python code to solve math problems or process data accurately. 
 - Solving complex mathematical equations 
 - Processing and analyzing text data precisely 
 
 
 URL Context 
 Direct the model to read and analyze content from specific web pages or documents. 
 - Answering questions based on specific URLs or documents 
 - Retrieving information across different web pages 
 
 
 Computer Use (Preview) 
 Enable Gemini to view a screen and generate actions to interact with web browser UIs (Client-side execution). 
 - Automating repetitive web-based workflows 
 - Testing web application user interfaces 
 
 
 File Search 
 Index and search your own documents to enable Retrieval Augmented Generation (RAG). 
 - Searching technical manuals 
 - Question answering over proprietary data 
 
 
 

See the Pricing page for details on costs associated with specific tools.

## How tools execution works

Tools allow the model to request actions during a conversation. The flow differs depending on whether the tool is built-in (managed by Google) or custom (managed by you).

### Built-in tool flow

For built-in tools like Google Search or Code Execution, the entire process happens within one API call:

- You send a prompt: "What is the square root of the latest stock price of GOOG?" 

- Gemini decides it needs tools and executes them on Google's servers (e.g., searches for the stock price, then runs Python code to calculate the square root). 

- Gemini sends back the final answer grounded in the tool results.

### Custom tool flow (Function Calling)

For custom tools and Computer Use, your application handles the execution:

- You send a prompt along with functions (tools) declarations. 

- Gemini might send back a structured JSON to call a specific function (for example, 

```
{"name": "get_order_status", "args": {"order_id": "123"}}
```

). 

- You execute the function in your application or environment. 

- You send the function results back to Gemini. 

- Gemini uses the results to generate a final response or another tool call.

Learn more in the Function calling guide .

## Structured outputs vs. function Calling

Gemini offers two methods for generating structured outputs. Use Function calling when the model needs to perform an intermediate step by connecting to your own tools or data systems. Use Structured Outputs when you strictly need the model's final response to adhere to a specific schema, such as for rendering a custom UI.

## Structured outputs with tools

You can combine Structured Outputs with built-in tools to ensure that model responses grounded in external data or computation still adhere to a strict schema. 

See Structured outputs with tools for code examples.

## Building agents

Agents are systems that use models and tools to complete multi-step tasks. While Gemini provides the reasoning capabilities (the "brain") and the essential tools (the "hands"), you often need an orchestration framework to manage the agent's memory, plan loops, and perform complex tool chaining.

Gemini integrates with leading open-source agent frameworks:

- LangChain / LangGraph : Build stateful, complex application flows and multi-agent systems using graph structures. 

- LlamaIndex : Connect Gemini agents to your private data for RAG-enhanced workflows. 

- CrewAI : Orchestrate collaborative, role-playing autonomous AI agents.

- Vercel AI SDK : Build AI-powered user interfaces and agents in JavaScript/TypeScript. 

- Google ADK : An open-source framework for building and orchestrating interoperable AI agents.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Speech generation (text-to-speech) &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/speech-generation#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Speech generation (text-to-speech)  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Speech generation (text-to-speech) 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can transform text input into single speaker or multi-speaker
audio using native text-to-speech (TTS) generation capabilities.
Text-to-speech (TTS) generation is controllable ,
meaning you can use natural language to structure interactions and guide the
 style , accent , pace , and tone of the audio.

The TTS capability differs from speech generation provided through the
 Live API , which is designed for interactive,
unstructured audio, and multimodal inputs and outputs. While the Live API excels
in dynamic conversational contexts, TTS through the Gemini API
is tailored for scenarios that require exact text recitation with fine-grained
control over style and sound, such as podcast or audiobook generation.

This guide shows you how to generate single-speaker and multi-speaker audio from
text.

## Before you begin

Ensure you use a Gemini 2.5 model variant with native text-to-speech (TTS) capabilities, as listed in the Supported models section. For optimal results, consider which model best fits your specific use case. 

You may find it useful to test the Gemini 2.5 TTS models in AI Studio before you start building.

## Single-speaker text-to-speech

To convert text to single-speaker audio, set the response modality to "audio",
and pass a `SpeechConfig` object with `VoiceConfig` set.
You'll need to choose a voice name from the prebuilt output voices .

This example saves the output audio from the model in a wave file:

 
 

### Python

 

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents="Say cheerfully: Have a wonderful day!",
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
               voice_name='Kore',
            )
         )
      ),
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

 
 
 
 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Kore' },
               },
            },
      },
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}
await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "contents": [{
          "parts":[{
            "text": "Say cheerfully: Have a wonderful day!"
          }]
        }],
        "generationConfig": {
          "responseModalities": ["AUDIO"],
          "speechConfig": {
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }
        },
        "model": "gemini-2.5-flash-preview-tts",
    }' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
          base64 --decode >out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

 
 

## Multi-speaker text-to-speech

For multi-speaker audio, you'll need a `MultiSpeakerVoiceConfig` object with
each speaker (up to 2) configured as a `SpeakerVoiceConfig`.
You'll need to define each `speaker` with the same names used in the
 prompt :

 
 

### Python

 

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

prompt = """TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?"""

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=prompt,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Joe',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Jane',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const prompt = `TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?`;

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: prompt }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               multiSpeakerVoiceConfig: {
                  speakerVoiceConfigs: [
                        {
                           speaker: 'Joe',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Kore' }
                           }
                        },
                        {
                           speaker: 'Jane',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Puck' }
                           }
                        }
                  ]
               }
            }
      }
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
  "contents": [{
    "parts":[{
      "text": "TTS the following conversation between Joe and Jane:
                Joe: Hows it going today Jane?
                Jane: Not too bad, how about you?"
    }]
  }],
  "generationConfig": {
    "responseModalities": ["AUDIO"],
    "speechConfig": {
      "multiSpeakerVoiceConfig": {
        "speakerVoiceConfigs": [{
            "speaker": "Joe",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }, {
            "speaker": "Jane",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Puck"
              }
            }
          }]
      }
    }
  },
  "model": "gemini-2.5-flash-preview-tts",
}' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
    base64 --decode > out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

 
 

## Controlling speech style with prompts

You can control style, tone, accent, and pace using natural language prompts
for both single- and multi-speaker TTS.
For example, in a single-speaker prompt, you can say:

 

```
Say in an spooky whisper:
"By the pricking of my thumbs...
Something wicked this way comes"
```

 

In a multi-speaker prompt, provide the model with each speaker's name and
corresponding transcript. You can also provide guidance for each speaker
individually:

 

```
Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:

Speaker1: So... what's on the agenda today?
Speaker2: You're never going to guess!
```

 

Try using a voice option that corresponds to the style or emotion you
want to convey, to emphasize it even more. In the previous prompt, for example,
 Enceladus 's breathiness might emphasize "tired" and "bored", while
 Puck 's upbeat tone could complement "excited" and "happy".

## Generating a prompt to convert to audio

The TTS models only output audio, but you can use
 other models to generate a transcript first,
then pass that transcript to the TTS model to read aloud.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

transcript = client.models.generate_content(
   model="gemini-2.0-flash",
   contents="""Generate a short transcript around 100 words that reads
            like it was clipped from a podcast by excited herpetologists.
            The hosts names are Dr. Anya and Liam.""").text

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=transcript,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Dr. Anya',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Liam',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

# ...Code to stream or save the output
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {

const transcript = await ai.models.generateContent({
   model: "gemini-2.0-flash",
   contents: "Generate a short transcript around 100 words that reads like it was clipped from a podcast by excited herpetologists. The hosts names are Dr. Anya and Liam.",
   })

const response = await ai.models.generateContent({
   model: "gemini-2.5-flash-preview-tts",
   contents: transcript,
   config: {
      responseModalities: ['AUDIO'],
      speechConfig: {
         multiSpeakerVoiceConfig: {
            speakerVoiceConfigs: [
                   {
                     speaker: "Dr. Anya",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Kore"},
                     }
                  },
                  {
                     speaker: "Liam",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Puck"},
                    }
                  }
                ]
              }
            }
      }
  });
}
// ..JavaScript code for exporting .wav file for output audio

await main();
```

 
 

## Voice options

TTS models support the following 30 voice options in the `voice_name` field:

 
 
 
 
 
 
 
 
 
 Zephyr -- Bright 
 Puck -- Upbeat 
 Charon -- Informative 
 
 
 Kore -- Firm 
 Fenrir -- Excitable 
 Leda -- Youthful 
 
 
 Orus -- Firm 
 Aoede -- Breezy 
 Callirrhoe -- Easy-going 
 
 
 Autonoe -- Bright 
 Enceladus -- Breathy 
 Iapetus -- Clear 
 
 
 Umbriel -- Easy-going 
 Algieba -- Smooth 
 Despina -- Smooth 
 
 
 Erinome -- Clear 
 Algenib -- Gravelly 
 Rasalgethi -- Informative 
 
 
 Laomedeia -- Upbeat 
 Achernar -- Soft 
 Alnilam -- Firm 
 
 
 Schedar -- Even 
 Gacrux -- Mature 
 Pulcherrima -- Forward 
 
 
 Achird -- Friendly 
 Zubenelgenubi -- Casual 
 Vindemiatrix -- Gentle 
 
 
 Sadachbia -- Lively 
 Sadaltager -- Knowledgeable 
 Sulafat -- Warm 
 
 
 
 

You can hear all the voice options in
 AI Studio .

## Supported languages

The TTS models detect the input language automatically. They support the
following 24 languages:

 
 
 
 
 
 
 
 
 
 Language 
 BCP-47 Code 
 Language 
 BCP-47 Code 
 
 
 
 
 Arabic (Egyptian) 
 `ar-EG` 
 German (Germany) 
 `de-DE` 
 
 
 English (US) 
 `en-US` 
 Spanish (US) 
 `es-US` 
 
 
 French (France) 
 `fr-FR` 
 Hindi (India) 
 `hi-IN` 
 
 
 Indonesian (Indonesia) 
 `id-ID` 
 Italian (Italy) 
 `it-IT` 
 
 
 Japanese (Japan) 
 `ja-JP` 
 Korean (Korea) 
 `ko-KR` 
 
 
 Portuguese (Brazil) 
 `pt-BR` 
 Russian (Russia) 
 `ru-RU` 
 
 
 Dutch (Netherlands) 
 `nl-NL` 
 Polish (Poland) 
 `pl-PL` 
 
 
 Thai (Thailand) 
 `th-TH` 
 Turkish (Turkey) 
 `tr-TR` 
 
 
 Vietnamese (Vietnam) 
 `vi-VN` 
 Romanian (Romania) 
 `ro-RO` 
 
 
 Ukrainian (Ukraine) 
 `uk-UA` 
 Bengali (Bangladesh) 
 `bn-BD` 
 
 
 English (India) 
 `en-IN` & `hi-IN` bundle 
 Marathi (India) 
 `mr-IN` 
 
 
 Tamil (India) 
 `ta-IN` 
 Telugu (India) 
 `te-IN` 
 
 
 

## Supported models

 
 
 
 Model 
 Single speaker 
 Multispeaker 
 
 

 
 
 Gemini 2.5 Flash Preview TTS 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Pro Preview TTS 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 

## Limitations

- TTS models can only receive text inputs and generate audio outputs. 

- A TTS session has a context window limit of 32k
tokens.

- Review Languages section for language support.

## What's next

- Try the audio generation cookbook .

- Gemini's Live API offers interactive audio
generation options you can interleave with other modalities.

- For working with audio inputs , visit the Audio understanding guide.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Grounding with Google Search &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/google-search

- 
 
 
 
 
 
 
 
 
 
 
 Grounding with Google Search  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Grounding with Google Search 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

Grounding with Google Search connects the Gemini model to real-time web content
and works with all available languages. This allows
Gemini to provide more accurate answers and cite verifiable sources beyond its
knowledge cutoff.

Grounding helps you build applications that can:

- Increase factual accuracy: Reduce model hallucinations by basing
responses on real-world information.

- Access real-time information: Answer questions about recent events and
topics.

- 

 Provide citations: Build user trust by showing the sources for the
model's claims.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const groundingTool = {
  googleSearch: {},
};

const config = {
  tools: [groundingTool],
};

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

 
 

You can learn more by trying the Search tool
notebook .

## How grounding with Google Search works

When you enable the `google_search` tool, the model handles the entire workflow
of searching, processing, and citing information automatically.

 

- User Prompt: Your application sends a user's prompt to the Gemini API
with the `google_search` tool enabled.

- Prompt Analysis: The model analyzes the prompt and determines if a
Google Search can improve the answer.

- Google Search: If needed, the model automatically generates one or
multiple search queries and executes them.

- Search Results Processing: The model processes the search results,
synthesizes the information, and formulates a response.

- Grounded Response: The API returns a final, user-friendly response that
is grounded in the search results. This response includes the model's text
answer and `groundingMetadata` with the search queries, web results, and
citations.

## Understanding the grounding response

When a response is successfully grounded, the response includes a
`groundingMetadata` field. This structured data is essential for verifying
claims and building a rich citation experience in your application.

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

 

The Gemini API returns the following information with the `groundingMetadata`:

- `webSearchQueries` : Array of the search queries used. This is useful for
debugging and understanding the model's reasoning process.

- `searchEntryPoint` : Contains the HTML and CSS to render the required Search
Suggestions. Full usage requirements are detailed in the Terms of
Service .

- `groundingChunks` : Array of objects containing the web sources (`uri` and
`title`).

- `groundingSupports` : Array of chunks to connect model response `text` to
the sources in `groundingChunks`. Each chunk links a text `segment` (defined
by `startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This
is the key to building inline citations.

Grounding with Google Search can also be used in combination with the URL
context tool to ground responses in both public
web data and the specific URLs you provide.

## Attributing sources with inline citations

The API returns structured citation data, giving you complete control over how
you display sources in your user interface. You can use the `groundingSupports`
and `groundingChunks` fields to link the model's statements directly to their
sources. Here is a common pattern for processing the metadata to create a
response with inline, clickable citations.

 
 

### Python

 

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

 
 

### JavaScript

 

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

 
 

The new response with inline citations will look like this:

 

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

 

## Pricing

When you use Grounding with Google Search, your project is billed for each
search query that the model decides to execute. If the model decides to execute
multiple search queries to answer a single prompt (for example, searching for
`"UEFA Euro 2024 winner"` and `"Spain vs England Euro 2024 final score"` within
the same API call), this counts as two billable uses of the tool for that
request.

For detailed pricing information, see the Gemini API pricing
page .

## Supported models

Experimental and Preview models are not included. You can find their
capabilities on the model
overview page.

 
 
 
 Model 
 Grounding with Google Search 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Flash 
 ‚úîÔ∏è 
 
 
 

## Supported tools combinations

You can use Grounding with Google Search with other tools like
 code execution and
 URL context to power more complex use cases.

## Grounding with Gemini 1.5 Models (Legacy)

While the `google_search` tool is recommended for Gemini 2.0 and later, Gemini
1.5 supports a legacy tool named `google_search_retrieval`. This tool provides a
`dynamic` mode that allows the model to decide whether to perform a search based
on its confidence that the prompt requires fresh information. If the model's
confidence is above a `dynamic_threshold` you set (a value between 0.0 and 1.0),
it will perform a search.

 
 

### Python

 

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

 
 

### JavaScript

 

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

 
 

## What's next

- Try the Grounding with Google Search in the Gemini API
Cookbook .

- Learn about other available tools, like Function Calling .

- Learn how to augment prompts with specific URLs using the URL context
tool .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Audio understanding &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/audio#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Audio understanding  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Audio understanding 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini can analyze and understand audio input, enabling use cases like the
following:

- Describe, summarize, or answer questions about audio content.

- Provide a transcription of the audio.

- Analyze specific segments of the audio.

This guide shows you how to use the Gemini API to generate a text response to
audio input.

### Before you begin

Before calling the Gemini API, ensure you have your SDK of choice 
installed, and a Gemini API key configured and ready to use.

## Input audio

You can provide audio data to Gemini in the following ways:

- Upload an audio file before making a request to
`generateContent`.

- Pass inline audio data with the request to
`generateContent`.

### Upload an audio file

You can use the Files API to upload an audio file.
Always use the Files API when the total request size (including the files, text
prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads an audio file and then uses the file in a call to
`generateContent`.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### REST

 

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

To learn more about working with media files, see
 Files API .

### Pass audio data inline

Instead of uploading an audio file, you can pass inline audio data in the
request to `generateContent`:

 
 

### Python

 

```
from google import genai
from google.genai import types

with open('path/to/small-sample.mp3', 'rb') as f:
    audio_bytes = f.read()

client = genai.Client()
response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[
    'Describe this audio clip',
    types.Part.from_bytes(
      data=audio_bytes,
      mime_type='audio/mp3',
    )
  ]
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  audioBytes, _ := os.ReadFile("/path/to/small-sample.mp3")

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "audio/mp3",
        Data:     audioBytes,
      },
    },
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

A few things to keep in mind about inline audio data:

- The maximum request size is 20 MB, which includes text prompts,
system instructions, and files provided inline. If your file's
size will make the total request size exceed 20 MB, then
use the Files API to upload an audio file for use in
the request.

- If you're using an audio sample multiple times, it's more efficient
to upload an audio file .

## Get a transcript

To get a transcript of audio data, just ask for it in the prompt:

 
 

### Python

 

```
from google import genai

client = genai.Client()
myfile = client.files.upload(file='path/to/sample.mp3')
prompt = 'Generate a transcript of the speech.'

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[prompt, myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Generate a transcript of the speech."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

## Refer to timestamps

You can refer to specific sections of an audio file using timestamps of the form
`MM:SS`. For example, the following prompt requests a transcript that

- Starts at 2 minutes 30 seconds from the beginning of the file.

- 

Ends at 3 minutes 29 seconds from the beginning of the file.

 
 

### Python

 

```
# Create a prompt containing timestamps.
prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

 
 

### JavaScript

 

```
// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Provide a transcript of the speech " +
                            "between the timestamps 02:30 and 03:29."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

## Count tokens

Call the `countTokens` method to get a count of the number of tokens in an
audio file. For example:

 
 

### Python

 

```
from google import genai

client = genai.Client()
response = client.models.count_tokens(
  model='gemini-2.5-flash',
  contents=[myfile]
)

print(response)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  tokens, _ := client.Models.CountTokens(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
}
```

 
 

## Supported audio formats

Gemini supports the following audio format MIME types:

- WAV - `audio/wav`

- MP3 - `audio/mp3`

- AIFF - `audio/aiff`

- AAC - `audio/aac`

- OGG Vorbis - `audio/ogg`

- FLAC - `audio/flac`

## Technical details about audio

- Gemini represents each second of audio as 32 tokens; for example,
one minute of audio is represented as 1,920 tokens.

- Gemini can "understand" non-speech components, such as birdsong or sirens.

- The maximum supported length of audio data in a single prompt is 9.5 hours.
Gemini doesn't limit the number of audio files in a single prompt; however,
the total combined length of all audio files in a single prompt can't exceed
9.5 hours.

- Gemini downsamples audio files to a 16 Kbps data resolution.

- If the audio source contains multiple channels, Gemini combines those channels
into a single channel.

## What's next

This guide shows how to generate text in response to audio data. To learn more,
see the following resources:

- File prompting strategies : The
Gemini API supports prompting with text, image, audio, and video data, also
known as multimodal prompting.

- System instructions :
System instructions let you steer the behavior of the model based on your
specific needs and use cases.

- Safety guidance : Sometimes generative AI
models produce unexpected outputs, such as outputs that are inaccurate,
biased, or offensive. Post-processing and human evaluation are essential to
limit the risk of harm from such outputs.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-26 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-26 UTC."],[],[]]

---

### Grounding with Google Maps &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/maps-grounding

- 
 
 
 
 
 
 
 
 
 
 
 Grounding with Google Maps  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Grounding with Google Maps 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Grounding with Google Maps connects the generative capabilities of Gemini with
the rich, factual, and up-to-date data of Google Maps. This feature enables
developers to easily incorporate location-aware functionality into their
applications. When a user query has a context related to Maps data, the Gemini
model leverages Google Maps to provide factually accurate and fresh answers that
are relevant to the user's specified location or general area.

- Accurate, location-aware responses: Leverage Google Maps' extensive and
current data for geographically specific queries.

- Enhanced personalization: Tailor recommendations and information based
on user-provided locations.

- Contextual information and widgets: Context tokens to render interactive
Google Maps widgets alongside generated content.

## Get started

This example demonstrates how to integrate Grounding with Google Maps into your
application to provide accurate, location-aware responses to user queries. The
prompt asks for local recommendations with an optional user location, enabling
the Gemini model to leverage Google Maps data.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "What are the best Italian restaurants within a 15-minute walk from here?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
        # Turn on grounding with Google Maps
        tools=[types.Tool(google_maps=types.GoogleMaps())],
        # Optionally provide the relevant location context (this is in Los Angeles)
        tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
            lat_lng=types.LatLng(
                latitude=34.050481, longitude=-118.248526))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in grounding.grounding_chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/gnai";

const ai = new GoogleGenAI({});

async function generateContentWithMapsGrounding() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What are the best Italian restaurants within a 15-minute walk from here?",
    config: {
      // Turn on grounding with Google Maps
      tools: [{ googleMaps: {} }],
      toolConfig: {
        retrievalConfig: {
          // Optionally provide the relevant location context (this is in Los Angeles)
          latLng: {
            latitude: 34.050481,
            longitude: -118.248526,
          },
        },
      },
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const grounding = response.candidates[0]?.groundingMetadata;
  if (grounding?.groundingChunks) {
    console.log("-".repeat(40));
    console.log("Sources:");
    for (const chunk of grounding.groundingChunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

generateContentWithMapsGrounding();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "What are the best Italian restaurants within a 15-minute walk from here?"
    }]
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 34.050481, "longitude": -118.248526}
    }
  }
}'
```

 
 

## How Grounding with Google Maps works

Grounding with Google Maps integrates the Gemini API with the Google Geo
ecosystem by using the Maps API as a grounding source. When a user's query
contains geographical context, the Gemini model can invoke the Grounding with
Google Maps tool. The model can then generate responses grounded in Google Maps
data relevant to the provided location.

The process typically involves:

- User query: A user submits a query to your application, potentially
including geographical context (e.g., "coffee shops near me," "museums in
San Francisco").

- Tool invocation: The Gemini model, recognizing the geographical intent,
invokes the Grounding with Google Maps tool. This tool can optionally be
provided with the user's `latitude` and `longitude`. The tool is a textual
search tool and behaves similarly to searching on Maps, in that local
queries ("near me") will use the coordinates, while specific or non-local
queries are unlikely to be influenced by the explicit location.

- Data retrieval: The Grounding with Google Maps service queries Google
Maps for relevant information (e.g., places, reviews, photos, addresses,
opening hours).

- Grounded generation: The retrieved Maps data is used to inform the
Gemini model's response, ensuring factual accuracy and relevance.

- Response & widget token: The model returns a text response, which
includes citations to Google Maps sources. Optionally, the API response may
also contain a `google_maps_widget_context_token`, allowing developers to
render a contextual Google Maps widget in their application for visual
interaction.

## Why and when to use Grounding with Google Maps

Grounding with Google Maps is ideal for applications that require accurate,
up-to-date, and location-specific information. It enhances the user experience
by providing relevant and personalized content backed by Google Maps' extensive
database of over 250 million places worldwide.

You should use Grounding with Google Maps when your application needs to:

- Provide complete and accurate responses to geo-specific questions.

- Build conversational trip planners and local guides.

- Recommend points of interest based on
location and user preferences like restaurants or shops.

- Create location-aware experiences for social, retail, or food delivery
services.

Grounding with Google Maps excels in use cases where proximity and current
factual data are critical, such as finding the "best coffee shop near me" or
getting directions.

## API methods and parameters

Grounding with Google Maps is exposed through the Gemini API as a tool within
the `generateContent` method. You enable and configure
Grounding with Google Maps by including a
 `googleMaps` object in the `tools` parameter of your
request.

 
 

### JSON

 

```
{
  "contents": [{
    "parts": [
      {"text": "Restaurants near Times Square."}
    ]
  }],
  "tools":  { "googleMaps": {} }
}
```

 
 

The `googleMaps` tool can additionally accept a boolean `enableWidget`
parameter, that is used to control whether the `googleMapsWidgetContextToken` 
field is returned in the response. This can be used to display a
 contextual Places widget .

 
 

### JSON

 

```
{
"contents": [{
    "parts": [
      {"text": "Restaurants near Times Square."}
    ]
  }],
  "tools":  { "googleMaps": { "enableWidget": true } }
}
```

 
 

Additionally, the tool supports passing the contextual location as `toolConfig`.

 
 

### JSON

 

```
{
  "contents": [{
    "parts": [
      {"text": "Restaurants near here."}
    ]
  }],
  "tools":  { "googleMaps": {} },
  "toolConfig":  {
    "retrievalConfig": {
      "latLng": {
        "latitude": 40.758896,
        "longitude": -73.985130
      }
    }
  }
}
```

 
 

### Understanding the grounding response

When a response is successfully grounded with Google Maps data, the response
includes a `groundingMetadata` field.
This structured data is essential for verifying claims and building a rich
citation experience in your application, as well as meeting the service usage
requirements.

 
 

### JSON

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "CanteenM is an American restaurant with..."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "groundingChunks": [
          {
            "maps": {
              "uri": "https://maps.google.com/?cid=13100894621228039586",
              "title": "Heaven on 7th Marketplace",
              "placeId": "places/ChIJ0-zA1vBZwokRon0fGj-6z7U"
            },
            // repeated ...
          }
        ],
        "groundingSupports": [
          {
            "segment": {
              "startIndex": 0,
              "endIndex": 79,
              "text": "CanteenM is an American restaurant with a 4.6-star rating and is open 24 hours."
            },
            "groundingChunkIndices": [0]
          },
          // repeated ...
        ],
        "webSearchQueries": [
          "restaurants near me"
        ],
        "googleMapsWidgetContextToken": "widgetcontent/..."
      }
    }
  ]
}
```

 
 

The Gemini API returns the following information with the
 `groundingMetadata` :

- `groundingChunks`: Array of objects containing the `maps` sources (`uri`,
`placeId` and `title`).

- `groundingSupports`: Array of chunks to connect model response text to the
sources in `groundingChunks`. Each chunk links a text span (defined by
`startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This is
the key to building inline citations.

- `googleMapsWidgetContextToken`: A text token that can be used to render a
 contextual Places
widget .

For a code snippet showing how to render inline citations in text, see the
example 
in the Grounding with Google Search docs.

### Display the Google Maps contextual widget

To use the returned `googleMapsWidgetContextToken`, you need to load the
Google Maps JavaScript
API .

## Use cases

Grounding with Google Maps supports a variety of location-aware use cases. The
following examples demonstrate how different prompts and parameters can leverage
Grounding with Google Maps. Information in the Google Maps Grounded Results may
differ from actual conditions.

### Handling place-specific questions

Ask detailed questions about a specific place to get answers based on Google
user reviews and other Maps data.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Is there a cafe near the corner of 1st and Main that has outdoor seating?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
        # Turn on the Maps tool
        tools=[types.Tool(google_maps=types.GoogleMaps())],

        # Provide the relevant location context (this is in Los Angeles)
        tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
            lat_lng=types.LatLng(
                latitude=34.050481, longitude=-118.248526))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if chunks := grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
  ```
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Is there a cafe near the corner of 1st and Main that has outdoor seating?";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      // Turn on the Maps tool
      tools: [{googleMaps: {}}],
      // Provide the relevant location context (this is in Los Angeles)
      toolConfig: {
        retrievalConfig: {
          latLng: {
            latitude: 34.050481,
            longitude: -118.248526
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const chunks = response.candidates[0].groundingMetadata?.groundingChunks;
  if (chunks) {
    console.log('-'.repeat(40));
    console.log("Sources:");
    for (const chunk of chunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Is there a cafe near the corner of 1st and Main that has outdoor seating?"
    }]
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 34.050481, "longitude": -118.248526}
    }
  }
}'
```

 
 

### Providing location-based personalization

Get recommendations tailored to a user's preferences and a specific geographical
area.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Which family-friendly restaurants near here have the best playground reviews?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
      tools=[types.Tool(google_maps=types.GoogleMaps())],
      tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
          # Provide the location as context; this is Austin, TX.
          lat_lng=types.LatLng(
              latitude=30.2672, longitude=-97.7431))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if chunks := grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Which family-friendly restaurants near here have the best playground reviews?";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      tools: [{googleMaps: {}}],
      toolConfig: {
        retrievalConfig: {
          // Provide the location as context; this is Austin, TX.
          latLng: {
            latitude: 30.2672,
            longitude: -97.7431
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const chunks = response.candidates[0].groundingMetadata?.groundingChunks;
  if (chunks) {
    console.log('-'.repeat(40));
    console.log("Sources:");
    for (const chunk of chunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Which family-friendly restaurants near here have the best playground reviews?"
    }],
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 30.2672, "longitude": -97.7431}
    }
  }
}'
```

 
 

### Assisting with itinerary planning

Generate multi-day plans with directions and information about various
locations, perfect for travel applications.

In this example, the `googleMapsWidgetContextToken` has been requested by
enabling the widget in the Google Maps tool. When enabled, the returned token
can be used to render a contextual Places widget using the ` component`
from the Google Maps JavaScript API.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner."

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
      tools=[types.Tool(google_maps=types.GoogleMaps(enable_widget=True))],
      tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
          # Provide the location as context, this is in San Francisco.
          lat_lng=types.LatLng(
              latitude=37.78193, longitude=-122.40476))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in grounding.grounding_chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')

  if widget_token := grounding.google_maps_widget_context_token:
    print('-' * 40)
    print(f'<gmp-place-contextual context-token="{widget_token}"></gmp-place-contextual>')
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner.";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      tools: [{googleMaps: {enableWidget: true}}],
      toolConfig: {
        retrievalConfig: {
          // Provide the location as context, this is in San Francisco.
          latLng: {
            latitude: 37.78193,
            longitude: -122.40476
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const groundingMetadata = response.candidates[0]?.groundingMetadata;
  if (groundingMetadata) {
    if (groundingMetadata.groundingChunks) {
      console.log('-'.repeat(40));
      console.log("Sources:");
      for (const chunk of groundingMetadata.groundingChunks) {
        if (chunk.maps) {
          console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
        }
      }
    }

    if (groundingMetadata.googleMapsWidgetContextToken) {
      console.log('-'.repeat(40));
      document.body.insertAdjacentHTML('beforeend', `<gmp-place-contextual context-token="${groundingMetadata.googleMapsWidgetContextToken}`"></gmp-place-contextual>`);
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner."
    }]
  }],
  "tools": [{"googleMaps": {"enableWidget":"true"}}],
  "toolConfig": {
    "retrievalConfig": {
    "latLng": {"latitude": 37.78193, "longitude": -122.40476}
  }
  }
}'
```

 
 

When the widget is rendered, it will look something like the following:

 

## Service usage requirements

This section describes the service usage requirements for Grounding with Google
Maps.

### Inform the user about the use of Google Maps sources

With each Google Maps Grounded Result, you'll receive sources in `groundingChunks`
that support each response. The following metadata is also returned:

- source uri

- title

- ID

When presenting results from Grounding with Google Maps, you must specify the
associated Google Maps sources, and inform your users of the following:

- The Google Maps sources must immediately follow the generated content that
the sources support. This generated content is also referred to as Google
Maps Grounded Result.

- The Google Maps sources must be viewable within one user interaction.

### Display Google Maps sources with Google Maps links

For each source in `groundingChunks` and in


```
grounding_chunks.maps.placeAnswerSources.reviewSnippets
```

, a link preview must be
generated following these requirements:

- Attribute each source to Google Maps following the Google Maps text
 attribution guidelines .

- Display the source title provided in the response.

- Link to the source using the `uri` or `googleMapsUri` from the response.

These images show the minimum requirements for displaying the sources and Google
Maps links.

 

You can collapse the view of the sources.

 

Optional: Enhance the link preview with additional content, such as:

- A Google Maps favicon 
is inserted before the Google Maps text attribution.

- A photo from the source URL (`og:image`).

For more information about some of our Google Maps data providers and their
license terms, see the Google Maps and Google Earth legal notices .

### Google Maps text attribution guidelines

When you attribute sources to Google Maps in text, follow these guidelines:

- Don't modify the text Google Maps in any way:

 Don't change the capitalization of Google Maps.

- Don't wrap Google Maps onto multiple lines.

- Don't localize Google Maps into another language.

- Prevent browsers from translating Google Maps by using the HTML
attribute translate="no".

 
- Style Google Maps text as described in the following table:

 
 
 
 Property 
 Style 
 
 
 
 
 `Font family` 
 Roboto. Loading the font is optional. 
 
 
 `Fallback font family` 
 Any sans serif body font already used in your product or "Sans-Serif" to invoke the default system font 
 
 
 `Font style` 
 Normal 
 
 
 `Font weight` 
 400 
 
 
 `Font color` 
 White, black (#1F1F1F), or gray (#5E5E5E). Maintain accessible (4.5:1) contrast against the background. 
 
 
 `Font size` 
 
 
- Minimum font size: 12sp

 - Maximum font size: 16sp

 - To learn about sp, see Font size units on the Material Design website .

 
 
 
 
 `Spacing` 
 Normal 
 
 
 

#### Example CSS

The following CSS renders Google Maps with the appropriate typographic style and
color on a white or light background.

 
 

### CSS

 

```
@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');

.GMP-attribution {

font-family: Roboto, Sans-Serif;
font-style: normal;
font-weight: 400;
font-size: 1rem;
letter-spacing: normal;
white-space: nowrap;
color: #5e5e5e;
}
```

 
 

### Context token, place ID, and review ID

The Google Maps data includes context token, place ID, and review ID. You might
cache, store, and export the following response data:

- `googleMapsWidgetContextToken`

- `placeId`

- `reviewId`

The restrictions against caching in the Grounding with Google Maps Terms don't
apply.

### Prohibited activity and territory

Grounding with Google Maps has additional restrictions for certain content and
activities to maintain a safe and reliable platform. In addition to the usage
restrictions in the Terms, you will not use Grounding with Google Maps
for high risk activities including emergency response services. You will
not distribute or market your application that offers Grounding with
Google Maps in a Prohibited Territory. The current Prohibited Territories are:

- China

- Crimea

- Cuba

- Donetsk People's Republic

- Iran

- Luhansk People's Republic

- North Korea

- Syria

- Vietnam

This list may be updated from time to time.

## Best practices

- Provide user location: For the most relevant and personalized responses,
always include the `user_location` (latitude and longitude) in your
`googleMapsGrounding` configuration when the user's location is known.

- Render the Google Maps contextual widget: The contextual widget is
rendered using the context token, `googleMapsWidgetContextToken`, which is
returned in the Gemini API response and can be used to render visual content
from Google Maps. For more information on the contextual widget, see
 Grounding with Google Maps
widget 
in the Google Developer Guide.

- Inform End-Users: Clearly inform your end-users that Google Maps
data is being used to answer their queries, especially when the tool is
enabled.

- Monitor Latency: For conversational applications, ensure that the P95
latency for grounded responses remains within acceptable thresholds to
maintain a smooth user experience.

- Toggle Off When Not Needed: Grounding with Google Maps is off by
default. Only enable it (`"tools": [{"googleMaps": {}}]`) when a query has a
clear geographical context, to optimize performance and cost.

## Limitations

- Geographical Scope: Currently, Grounding with Google Maps is globally
available

- Model Support: Only specific Gemini models support Grounding with Google
Maps: Gemini 2.5 Flash-Lite, Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini
2.0 Flash (but not 2.0 Flash Lite).

- Multimodal Inputs/Outputs: Grounding with Google Maps does not currently
support multimodal inputs or outputs beyond text and contextual map widgets.

- Default State: The Grounding with Google Maps tool is off by default.
You must explicitly enable it in your API requests.

## Pricing and rate limits

Grounding with Google Maps pricing is based on queries. The current rate is
 $25 / 1K grounded prompts . The free tier also has up to 500 requests per day
available. A request is only counted towards the quota when
a prompt successfully returns at least one Google Maps grounded result (i.e.,
results containing at least one Google Maps source). If multiple queries are
sent to Google Maps from a single request, it counts as one request towards the
rate limit.

For detailed pricing information, see the Gemini API pricing page .

## Supported models

You can find their capabilities on the model overview page.

 
 
 
 Model 
 Grounding with Google Maps 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 
 
 

## What's next

- Try the Grounding with Google Search in the Gemini API
Cookbook .

- Learn about other available tools, like
 Function calling .

- To learn more about responsible AI best practices and Gemini API's safety
filters, see the Safety settings guide .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Gemini thinking &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/thinking#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini thinking  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini thinking 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

The Gemini 3 and 2.5 series models use an internal
"thinking process" that significantly improves their reasoning and multi-step
planning abilities, making them highly effective for complex tasks such as
coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the
Gemini API.

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content
generation request. The key difference lies in specifying one of the
 models with thinking support in the `model` field, as
demonstrated in the following text generation example:

 
 

### Python

 

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

 
 

## Thought summaries

Thought summaries are synthesized versions of the model's raw thoughts and offer
insights into the model's internal reasoning process. Note that
thinking levels and budgets apply to the model's raw thoughts and not to thought
summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your
request configuration. You can then access the summary by iterating through the
`response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries
without streaming, which returns a single, final thought summary with the
response:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

 
 

 

And here is an example using thinking with streaming, which returns rolling,
incremental summaries during generation:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Answer:")
      print(part.text)
      answer += part.text
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

 
 

## Controlling thinking

Gemini models engage in dynamic thinking by default, automatically adjusting the
amount of reasoning effort based on the complexity of the user's request.
However, if you have specific latency constraints or require the model to engage
in deeper reasoning than usual, you can optionally use parameters to control
thinking behavior.

### Thinking levels (Gemini 3 Pro)

The `thinkingLevel` parameter, recommended for Gemini 3 models and onwards,
lets you control reasoning behavior.
You can set thinking level to `"low"` or `"high"`.
If you don't specify a thinking level, Gemini will use the model's default
dynamic thinking level, `"high"`, for Gemini 3 Pro Preview.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_level="low")
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingLevel: "low",
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingLevelVal := "low"

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-3-pro-preview"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingLevel: &thinkingLevelVal,
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingLevel": "low"
    }
  }
}'
```

 
 

You cannot disable thinking for Gemini 3 Pro.
Gemini 2.5 series models don't support `thinkingLevel`; use `thinkingBudget`
instead.

### Thinking budgets

The `thinkingBudget` parameter, introduced with the Gemini 2.5 series, guides
the model on the specific number of thinking tokens to use for reasoning.

The following are `thinkingBudget` configuration details for each model type.
You can disable thinking by setting `thinkingBudget` to 0.
Setting the `thinkingBudget` to -1 turns
on dynamic thinking , meaning the model will adjust the budget based on the
complexity of the request.

 
 
 
 
 
 
 
 
 
 
 Model 
 Default setting
(Thinking budget is not set) 
 Range 
 Disable thinking 
 Turn on dynamic thinking 
 
 
 
 
 2.5 Pro 
 Dynamic thinking: Model decides when and how much to think 
 `128` to `32768` 
 N/A: Cannot disable thinking 
 `thinkingBudget = -1` 
 
 
 2.5 Flash 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite Preview 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 Robotics-ER 1.5 Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Live Native Audio Preview (09-2025) 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 
 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
    }
  }
}'
```

 
 

Depending on the prompt, the model might overflow or underflow the token budget.

## Thought signatures

The Gemini API is stateless, so the model treats every API request independently
and doesn't have access to thought context from previous turns in multi-turn
interactions.

In order to enable maintaining thought context across multi-turn interactions,
Gemini returns thought signatures, which are encrypted representations of the
model's internal thought process.

- Gemini 2.5 models return thought signatures when thinking is enabled and
the request includes function calling ,
specifically function declarations .

- Gemini 3 models may return thought signatures for all types of parts .
We recommend you always pass all signatures back as received, but it's
 required for function calling signatures. Read the
 Thought Signatures page to
learn more.

The Google GenAI SDK automatically handles the
return of thought signatures for you. You only need to
 manage thought signatures manually 
if you're modifying conversation history or using the REST API.

Other usage limitations to consider with function calling include:

- Signatures are returned from the model within other parts in the response,
for example function calling or text parts.
 Return the entire response 
with all parts back to the model in subsequent turns.

- Don't concatenate parts with signatures together.

- Don't merge one part with a signature with another part without a signature.

## Pricing

When thinking is turned on, response pricing is the sum of output
tokens and thinking tokens. You can get the total number of generated thinking
tokens from the `thoughtsTokenCount` field.

 
 

### Python

 

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

 
 

### JavaScript

 

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

 
 

### Go

 

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

 
 

Thinking models generate full thoughts to improve the quality of the final
response, and then output summaries to provide insight into the
thought process. So, pricing is based on the full thought tokens the
model needs to generate to create a summary, despite only the summary being
output from the API.

You can learn more about tokens in the Token counting 
guide.

## Best practices

This section includes some guidance for using thinking models efficiently.
As always, following our prompting guidance and best practices will get you the best results.

### Debugging and steering

- 

 Review reasoning : When you're not getting your expected response from the
thinking models, it can help to carefully analyze Gemini's thought summaries.
You can see how it broke down the task and arrived at its conclusion, and use
that information to correct towards the right results.

- 

 Provide Guidance in Reasoning : If you're hoping for a particularly lengthy
output, you may want to provide guidance in your prompt to constrain the
 amount of thinking the model uses. This lets you reserve more
of the token output for your response.

### Task complexity

- Easy Tasks (Thinking could be OFF): For straightforward requests where
complex reasoning isn't required, such as fact retrieval or
classification, thinking is not required. Examples include:

 "Where was DeepMind founded?"

- "Is this email asking for a meeting or just providing information?"

 
- Medium Tasks (Default/Some Thinking): Many common requests benefit from a
degree of step-by-step processing or deeper understanding. Gemini can flexibly
use thinking capability for tasks like:

 Analogize photosynthesis and growing up.

- Compare and contrast electric cars and hybrid cars.

 
- Hard Tasks (Maximum Thinking Capability): For truly complex challenges,
such as solving complex math problems or coding tasks, we recommend setting
a high thinking budget. These types of tasks require the model to engage
its full reasoning and planning capabilities, often
involving many internal steps before providing an answer. Examples include:

 Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for
which 17 b is a divisor of 97 b .

- Write Python code for a web application that visualizes real-time stock
market data, including user authentication. Make it as efficient as
possible.

 

 

## Supported models, tools, and capabilities

Thinking features are supported on all 3 and 2.5 series models.
You can find all model capabilities on the
 model overview page.

Thinking models work with all of Gemini's tools and capabilities. This allows
the models to interact with external systems, execute code, or access real-time
information, incorporating the results into their reasoning and final response.

You can try examples of using tools with thinking models in the
 Thinking cookbook .

## What's next?

- Thinking coverage is available in our OpenAI Compatibility guide.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Code execution &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/code-execution

- 
 
 
 
 
 
 
 
 
 
 
 Code execution  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Code execution 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API provides a code execution tool that enables the model to
generate and run Python code. The model can then learn iteratively from the
code execution results until it arrives at a final output. You can use code
execution to build applications that benefit from code-based reasoning. For
example, you can use code execution to solve equations or process text. You can
also use the libraries included in the code execution
environment to perform more specialized tasks.

Gemini is only able to execute code in Python. You can still ask Gemini to
generate code in another language, but the model can't use the code execution
tool to run it.

## Enable code execution

To enable code execution, configure the code execution tool on the model. This
allows the model to generate and run code.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50.",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: [
    "What is the sum of the first 50 prime numbers? " +
      "Generate and run code for the calculation, and make sure you get all 50.",
  ],
  config: {
    tools: [{ codeExecution: {} }],
  },
});

const parts = response?.candidates?.[0]?.content?.parts || [];
parts.forEach((part) => {
  if (part.text) {
    console.log(part.text);
  }

  if (part.executableCode && part.executableCode.code) {
    console.log(part.executableCode.code);
  }

  if (part.codeExecutionResult && part.codeExecutionResult.output) {
    console.log(part.codeExecutionResult.output);
  }
});
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("What is the sum of the first 50 prime numbers? " +
                  "Generate and run code for the calculation, and make sure you get all 50."),
        config,
    )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d ' {"tools": [{"code_execution": {}}],
    "contents": {
      "parts":
        {
            "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
        }
    },
}'
```

 
 

The output might look something like the following, which has been formatted for
readability:

 

```
Okay, I need to calculate the sum of the first 50 prime numbers. Here's how I'll
approach this:

1.  **Generate Prime Numbers:** I'll use an iterative method to find prime
    numbers. I'll start with 2 and check if each subsequent number is divisible
    by any number between 2 and its square root. If not, it's a prime.
2.  **Store Primes:** I'll store the prime numbers in a list until I have 50 of
    them.
3.  **Calculate the Sum:**  Finally, I'll sum the prime numbers in the list.

Here's the Python code to do this:

def is_prime(n):
  """Efficiently checks if a number is prime."""
  if n <= 1:
    return False
  if n <= 3:
    return True
  if n % 2 == 0 or n % 3 == 0:
    return False
  i = 5
  while i * i <= n:
    if n % i == 0 or n % (i + 2) == 0:
      return False
    i += 6
  return True

primes = []
num = 2
while len(primes) < 50:
  if is_prime(num):
    primes.append(num)
  num += 1

sum_of_primes = sum(primes)
print(f'{primes=}')
print(f'{sum_of_primes=}')

primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,
71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,
157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]
sum_of_primes=5117

The sum of the first 50 prime numbers is 5117.
```

 

This output combines several content parts that the model returns when using
code execution:

- `text`: Inline text generated by the model

- `executableCode`: Code generated by the model that is meant to be executed

- `codeExecutionResult`: Result of the executable code

The naming conventions for these parts vary by programming language.

## Use code execution in chat

You can also use code execution as part of a chat.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

response = chat.send_message("I have a math question for you.")
print(response.text)

response = chat.send_message(
    "What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50."
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from "@google/genai";

const ai = new GoogleGenAI({});

const chat = ai.chats.create({
  model: "gemini-2.5-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "I have a math question for you:" }],
    },
    {
      role: "model",
      parts: [{ text: "Great! I'm ready for your math question. Please ask away." }],
    },
  ],
  config: {
    tools: [{codeExecution:{}}],
  }
});

const response = await chat.sendMessage({
  message: "What is the sum of the first 50 prime numbers? " +
            "Generate and run code for the calculation, and make sure you get all 50."
});
console.log("Chat response:", response.text);
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    chat, _ := client.Chats.Create(
        ctx,
        "gemini-2.5-flash",
        config,
        nil,
    )

    result, _ := chat.SendMessage(
                    ctx,
                    genai.Part{Text: "What is the sum of the first 50 prime numbers? " +
                                          "Generate and run code for the calculation, and " +
                                          "make sure you get all 50.",
                              },
                )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"tools": [{"code_execution": {}}],
    "contents": [
        {
            "role": "user",
            "parts": [{
                "text": "Can you print \"Hello world!\"?"
            }]
        },{
            "role": "model",
            "parts": [
              {
                "text": ""
              },
              {
                "executable_code": {
                  "language": "PYTHON",
                  "code": "\nprint(\"hello world!\")\n"
                }
              },
              {
                "code_execution_result": {
                  "outcome": "OUTCOME_OK",
                  "output": "hello world!\n"
                }
              },
              {
                "text": "I have printed \"hello world!\" using the provided python code block. \n"
              }
            ],
        },{
            "role": "user",
            "parts": [{
                "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
            }]
        }
    ]
}'
```

 
 

## Input/output (I/O)

Starting with
 Gemini 2.0 Flash , code
execution supports file input and graph output. Using these input and output
capabilities, you can upload CSV and text files, ask questions about the
files, and have Matplotlib graphs generated as part
of the response. The output files are returned as inline images in the response.

### I/O pricing

When using code execution I/O, you're charged for input tokens and output
tokens:

 Input tokens: 

- User prompt

 Output tokens: 

- Code generated by the model

- Code execution output in the code environment

- Thinking tokens

- Summary generated by the model

### I/O details

When you're working with code execution I/O, be aware of the following technical
details:

- The maximum runtime of the code environment is 30 seconds.

- If the code environment generates an error, the model may decide to
regenerate the code output. This can happen up to 5 times.

- The maximum file input size is limited by the model token window. In
AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million
tokens (roughly 2MB for text files of the supported input types). If you
upload a file that's too large, AI Studio won't let you send it.

- Code execution works best with text and CSV files.

- The input file can be passed in `part.inlineData` or `part.fileData` (uploaded
via the Files API ), and the output file is always
returned as `part.inlineData`.

 
 
 
 
 Single turn 
 Bidirectional (Multimodal Live API) 
 
 

 
 
 Models supported 
 All Gemini 2.0 and 2.5 models 
 Only Flash experimental models 
 
 
 File input types supported 
 .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts 
 .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts 
 
 
 Plotting libraries supported 
 Matplotlib, seaborn 
 Matplotlib, seaborn 
 
 
 Multi-tool use 
 Yes (code execution + grounding only) 
 Yes 
 
 
 

## Billing

There's no additional charge for enabling code execution from the Gemini API.
You'll be billed at the current rate of input and output tokens based on the
Gemini model you're using.

Here are a few other things to know about billing for code execution:

- You're only billed once for the input tokens you pass to the model, and you're
billed for the final output tokens returned to you by the model.

- Tokens representing generated code are counted as output tokens. Generated
code can include text and multimodal output like images.

- Code execution results are also counted as output tokens.

The billing model is shown in the following diagram:

 

- You're billed at the current rate of input and output tokens based on the
Gemini model you're using.

- If Gemini uses code execution when generating your response, the original
prompt, the generated code, and the result of the executed code are labeled
 intermediate tokens and are billed as input tokens .

- Gemini then generates a summary and returns the generated code, the result of
the executed code, and the final summary. These are billed as output tokens .

- The Gemini API includes an intermediate token count in the API response, so
you know why you're getting additional input tokens beyond your initial
prompt.

## Limitations

- The model can only generate and execute code. It can't return other artifacts
like media files.

- In some cases, enabling code execution can lead to regressions in other areas
of model output (for example, writing a story).

- There is some variation in the ability of the different models to use code
execution successfully.

## Supported tools combinations

Code execution tool can be combined with
 Grounding with Google Search to
power more complex use cases.

## Supported libraries

The code execution environment includes the following libraries:

- attrs

- chess

- contourpy

- fpdf

- geopandas

- imageio

- jinja2

- joblib

- jsonschema

- jsonschema-specifications

- lxml

- matplotlib

- mpmath

- numpy

- opencv-python

- openpyxl

- packaging

- pandas

- pillow

- protobuf

- pylatex

- pyparsing

- PyPDF2

- python-dateutil

- python-docx

- python-pptx

- reportlab

- scikit-learn

- scipy

- seaborn

- six

- striprtf

- sympy

- tabulate

- tensorflow

- toolz

- xlrd

You can't install your own libraries.

## What's next

- Try the
 code execution Colab .

- Learn about other Gemini API tools:

 Function calling 

- Grounding with Google Search 

 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-06 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-06 UTC."],[],[]]

---

### Thought Signatures &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/thought-signatures#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Thought Signatures  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Thought Signatures 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

Thought signatures are encrypted representations of the model's internal thought
process and are used to preserve reasoning context across multi-turn
interactions.
When using thinking models (such as the Gemini 3 and 2.5 series), the API may
return a `thoughtSignature` field within the content parts 
of the response (e.g., `text` or `functionCall` parts).

As a general rule, if you receive a thought signature in a model response,
you should pass it back exactly as received when sending the conversation
history in the next turn.
 When using Gemini 3 Pro, you must pass back thought signatures during function
calling, otherwise you will get a validation error (4xx status code).

## How it works

The graphic below visualizes the meaning of "turn" and "step" as they pertain to
 function calling in the Gemini API. A "turn"
is a single, complete exchange in a conversation between a user and a model. A
"step" is a finer-grained action or operation performed by the model, often as
part of a larger process to complete a turn.

 

 This document focuses on handling function calling for Gemini 3 Pro. Refer to
the model behavior section for discrepancies with 2.5. 

Gemini 3 Pro returns thought signatures for all model responses (responses from
the API) with a function call. Thought signatures show up in the following
cases:

- When there are parallel function 
calls, the first function call part returned by the model response will have a
thought signature.

- When there are sequential function calls (multi-step), each function call will
have a signature and you must pass all signatures back.

- Model responses without a function call will return a thought signature inside
the last part returned by the model.

The following table provides a visualization for multi-step function calls,
combining the definitions of turns and steps with the concept of signatures
introduced above:

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1 = user_prompt` 
 `FC1 + signature` 
 `FR1` 
 
 
 

1

 
 

2

 
 `request2 = request1 + (FC1 + signature) + FR1` 
 `FC2 + signature` 
 `FR2` 
 
 
 

1

 
 

3

 
 `request3 = request2 + (FC2 + signature) + FR2` 
 `text_output(no FCs)` 
 

None

 
 
 

## Signatures in function calling parts

When Gemini generates a `functionCall`, it relies on the `thought_signature`
to process the tool's output correctly in the next turn.

- Behavior :

 Single Function Call : The `functionCall` part will contain a `thought_signature`.

- Parallel Function Calls : If the model generates parallel function calls
in a response, the `thought_signature` is attached only to the first 
`functionCall` part. Subsequent `functionCall` parts in the same response will
 not contain a signature.

 
- Requirement : You must return this signature in the exact part where it
was received when sending the conversation history back.

- Validation : Strict validation is enforced for all function calls within
the current turn . (Only current turn is required; we don't validate on
previous turns)

 The API goes back in the history (newest to oldest) to find the most recent
 User message that contains standard content (e.g., `text`) ( which would
be the start of the current turn). This will not be a `functionResponse`.

- All model `functionCall` turns occurring after that specific use
message are considered part of the turn.

- The first `functionCall` part in each step of the current turn
 must include its `thought_signature`.

- If you omit a `thought_signature` for the first `functionCall` part in any
step of the current turn, the request will fail with a 400 error.

 
- If proper signatures are not returned, here is how you will error out 

 `gemini-3-pro-preview`: Failure to include signatures will result in a 400
error. The verbiage will be of the form :

 Function call ` ` in the ` `
content block is missing a `thought_signature`. For example, Function
call `FC1` in the `1.` content block is missing a `thought_signature`. 

 

 

### Sequential function calling example

This section shows an example of multiple function calls where the user asks a
complex question requiring multiple tasks.

Let's walk through a multiple-turn function calling example where the user asks
a complex question requiring multiple tasks: 

```
"Check flight status for AA100 and
book a taxi if delayed"
```

.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 

```
request1="Check flight status for AA100 and book a taxi 2 hours before if delayed."
```

 
 `FC1 ("check_flight") + signature` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request2 = request1 + FC1 ("check_flight") + signature + FR1
```

 
 `FC2("book_taxi") + signature` 
 `FR2` 
 
 
 

1

 
 

3

 
 

```
request3 = request2 + FC2 ("book_taxi") + signature + FR2
```

 
 `text_output(no FCs)` 
 `None` 
 
 

The following code illustrates the sequence in the above table.

 Turn 1, Step 1 (User request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "check_flight",
          "description": "Gets the current status of a flight",
          "parameters": {
            "type": "object",
            "properties": {
              "flight": {
                "type": "string",
                "description": "The flight number to check"
              }
            },
            "required": [
              "flight"
            ]
          }
        },
        {
          "name": "book_taxi",
          "description": "Book a taxi",
          "parameters": {
            "type": "object",
            "properties": {
              "time": {
                "type": "string",
                "description": "time to book the taxi"
              }
            },
            "required": [
              "time"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model response) 

 

```
{
"content": {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>"
          }
        ]
  }
}
```

 

 Turn 1, Step 2 (User response - Sending tool outputs) Since this user turn
only contains a `functionResponse` (no fresh text), we are still in Turn 1. We
must preserve ` `.

 

```
{
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    },
    {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "check_flight",
              "response": {
                "status": "delayed",
                "departure_time": "12 PM"
                }
              }
            }
        ]
}
```

 

 Turn 1, Step 2 (Model) The model now decides to book a taxi based on the
previous tool output.

 

```
{
      "content": {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "book_taxi",
              "args": {
                "time": "10 AM"
              }
            },
            "thoughtSignature": "<Signature B>"
          }
        ]
      }
}
```

 

 Turn 1, Step 3 (User - Sending tool output) To send the taxi booking
confirmation, we must include signatures for ALL function calls in this loop
(` ` + ` `).

 

```
{
      "role": "user",
      "parts": [
        {
          "text": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
        }
      ]
    },
    {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "check_flight",
              "args": {
                "flight": "AA100"
              }
            },
            "thoughtSignature": "<Signature A>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "check_flight",
              "response": {
                "status": "delayed",
                "departure_time": "12 PM"
              }
              }
            }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "book_taxi",
              "args": {
                "time": "10 AM"
              }
            },
            "thoughtSignature": "<Signature B>" //Required and Validated
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "functionResponse": {
              "name": "book_taxi",
              "response": {
                "booking_status": "success"
              }
              }
            }
        ]
    }
}
```

 

### Parallel function calling example

Let's walk through a parallel function calling example where the users asks
`"Check weather in Paris and London"` to see where the model does validation.

 
 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 
 

1

 
 

1

 
 

request1="Check the weather in Paris and London"

 
 

FC1 ("Paris") + signature

FC2 ("London")

 
 

FR1

 
 
 
 

1

 
 

2

 
 

request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")

 
 

text_output

(no FCs)

 
 

None

 
 
 

The following code illustrates the sequence in the above table.

 Turn 1, Step 1 (User request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check the weather in Paris and London."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_temperature",
          "description": "Gets the current temperature for a given location.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco"
              }
            },
            "required": [
              "location"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model response) 

 

```
{
  "content": {
    "parts": [
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "location": "Paris"
          }
        },
        "thoughtSignature": "<Signature_A>"// INCLUDED on First FC
      },
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "location": "London"
          }// NO signature on subsequent parallel FCs
        }
      }
    ]
  }
}
```

 

 Turn 1, Step 2 (User response - Sending tool outputs) We must preserve
` ` on the first part exactly as received.

 

```
[
  {
    "role": "user",
    "parts": [
      {
        "text": "Check the weather in Paris and London."
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "city": "Paris"
          }
        },
        "thought_signature": "<Signature_A>" // MUST BE INCLUDED
      },
      {
        "functionCall": {
          "name": "get_current_temperature",
          "args": {
            "city": "London"
          }
        }
      } // NO SIGNATURE FIELD
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "name": "get_current_temperature",
          "response": {
            "temp": "15C"
          }
        }
      },
      {
        "functionResponse": {
          "name": "get_current_temperature",
          "response": {
            "temp": "12C"
          }
        }
      }
    ]
  }
]
```

 

## Signatures in non `functionCall` parts

Gemini may also return `thought_signatures` in the final part of the response
in non-function-call parts.

- Behavior : The final content part (`text, inlineData‚Ä¶`) returned by the
model may contain a `thought_signature`.

- Recommendation : Returning these signatures is recommended to ensure
the model maintains high-quality reasoning, especially for complex instruction
following or simulated agentic workflows.

- Validation : The API does not strictly enforce validation. You won't
receive a blocking error if you omit them, though performance may degrade.

### Text/In-context reasoning (No validation)

 Turn 1, Step 1 (Model response) 

 

```
{
  "role": "model",
  "parts": [
    {
      "text": "I need to calculate the risk. Let me think step-by-step...",
      "thought_signature": "<Signature_C>" // OPTIONAL (Recommended)
    }
  ]
}
```

 

 Turn 2, Step 1 (User) 

 

```
[
  { "role": "user", "parts": [{ "text": "What is the risk?" }] },
  {
    "role": "model", 
    "parts": [
      {
        "text": "I need to calculate the risk. Let me think step-by-step...",
        // If you omit <Signature_C> here, no error will occur.
      }
    ]
  },
  { "role": "user", "parts": [{ "text": "Summarize it." }] }
]
```

 

## Signatures for OpenAI compatibility

The following examples shows how to handle thought signatures for a chat
completion API using OpenAI compatibility .

### Sequential function calling example

This is an example of multiple function calling where the user asks a complex
question requiring multiple tasks.

Let's walk through a multiple-turn function calling example where the user asks


```
Check flight status for AA100 and book a taxi if delayed
```

 and you can see what
happens when the user asks a complex question requiring multiple tasks.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1="Check the weather in Paris and London"` 
 `FC1 ("Paris") + signatureFC2 ("London")` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")
```

 
 `text_output(no FCs)` 
 `None` 
 
 

The following code walks through the given sequence.

 Turn 1, Step 1 (User Request) 

 

```
{
  "model": "google/gemini-3-pro-preview",
  "messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "check_flight",
        "description": "Gets the current status of a flight",
        "parameters": {
          "type": "object",
          "properties": {
            "flight": {
              "type": "string",
              "description": "The flight number to check."
            }
          },
          "required": [
            "flight"
          ]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "book_taxi",
        "description": "Book a taxi",
        "parameters": {
          "type": "object",
          "properties": {
            "time": {
              "type": "string",
              "description": "time to book the taxi"
            }
          },
          "required": [
            "time"
          ]
        }
      }
    }
  ]
}
```

 

 Turn 1, Step 1 (Model Response) 

 

```
{
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>"
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    }
```

 

 Turn 1, Step 2 (User Response - Sending Tool Outputs) 

Since this user turn only contains a `functionResponse` (no fresh text), we are
still in Turn 1 and must preserve ` `.

 

```
"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"                 
    }
  ]
```

 

 Turn 1, Step 2 (Model) 

The model now decides to book a taxi based on the previous tool output.

 

```
{
"role": "model",
"tool_calls": [
{
"extra_content": {
"google": {
"thought_signature": "<Signature B>"
}
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-2",
            "type": "function"
          }
       ]
}
```

 

 Turn 1, Step 3 (User - Sending Tool Output) 

To send the taxi booking confirmation, we must include signatures for ALL
function calls in this loop (` ` + ` `).

 

```
"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"                 
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature B>" //Required and Validated
              }
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "book_taxi",
      "tool_call_id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
      "content": "{\"booking_status\":\"success\"}"
    }
  ]
```

 

### Parallel function calling example

Let's walk through a parallel function calling example where the users asks
`"Check weather in Paris and London"` and you can see where the model does
validation.

 
 
 

 Turn 

 
 

 Step 

 
 

 User Request 

 
 

 Model Response 

 
 

 FunctionResponse 

 
 
 
 

1

 
 

1

 
 `request1="Check the weather in Paris and London"` 
 `FC1 ("Paris") + signatureFC2 ("London")` 
 `FR1` 
 
 
 

1

 
 

2

 
 

```
request 2 = request1 + FC1 ("Paris") + signature + FC2 ("London")
```

 
 `text_output(no FCs)` 
 `None` 
 
 

Here's the code to walk through the given sequence.

 Turn 1, Step 1 (User Request) 

 

```
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check the weather in Paris and London."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_temperature",
          "description": "Gets the current temperature for a given location.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco"
              }
            },
            "required": [
              "location"
            ]
          }
        }
      ]
    }
  ]
}
```

 

 Turn 1, Step 1 (Model Response) 

 

```
{
"role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Signature returned
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": {
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function" // No signature on Parallel FC
          }
        ]
}
```

 

 Turn 1, Step 2 (User Response - Sending Tool Outputs) 

You must preserve ` ` on the first part exactly as received.

 

```
"messages": [
    {
      "role": "user",
      "content": "Check the weather in Paris and London."
    },
    {
      "role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<Signature A>" //Required
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": { //No Signature
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function"
          }
        ]
    },
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
      "content": "{\"temp\":\"15C\"}"
    },    
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
      "content": "{\"temp\":\"12C\"}"
    }
  ]
```

 

## FAQs

- 

 How do I transfer history from a different model to Gemini 3 Pro with a
function call part in the current turn and step? I need to provide function call
parts that were not generated by the API and therefore don't have an associated
thought signature? 

While injecting custom function call blocks into the request is strongly
discouraged, in cases where it can't be avoided, e.g. providing information
to the model on function calls and responses that were executed
deterministically by the client, or transferring a trace from a different
model that does not include thought signatures, you can set the following
dummy signatures of either `"context_engineering_is_the_way_to_go"` or
`"skip_thought_signature_validator"` in the thought signature field to skip
validation.

- 

 I am sending back interleaved parallel function calls and responses and the
API is returning a 400. Why? 

When the API returns parallel function calls "FC1 + signature, FC2", the
user response expected is "FC1+ signature, FC2, FR1, FR2". If you have them
interleaved as "FC1 + signature, FR1, FC2, FR2" the API will return a 400
error.

- 

 When streaming and the model is not returning a function call I can't find
the thought signature 

During a model response not containing a FC with a streaming request, the
model may return the thought signature in a part with an empty text content
part. It is advisable to parse the entire request until the `finish_reason`
is returned by the model.

## Thought signature behavior by model series

Gemini 3 Pro and Gemini 2.5 models behave differently with thought signatures
in function calls:

- If there are function calls in a response,

 Gemini 3 Pro will always have the signature on the first function call part.
It is mandatory to return that part.

- Gemini 2.5 will have the signature in the first part (regardless of
type). It is optional to return that part.

 
- If there are no function calls in a response,

 Gemini 3 Pro will have the signature on the last part if the model generates
a thought.

- Gemini 2.5 won't have a signature in any part.

 

For Gemini 2.5 models thought signature behavior, refer to the
 Thinking page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Structured Outputs &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/structured-output#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Structured Outputs  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Structured Outputs 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

You can configure Gemini models to generate responses that adhere to a provided
JSON Schema. This capability guarantees predictable and parsable results, ensures format
and type-safety, enables the programmatic detection of refusals, and
simplifies prompting.

Using structured outputs is ideal for a wide range of applications:

- Data extraction: Pull specific information from unstructured text, like extracting names, dates, and amounts from an invoice.

- Structured classification: Classify text into predefined categories and assign structured labels, such as categorizing customer feedback by sentiment and topic.

- Agentic workflows: Generate structured data that can be used to call other tools or APIs, like creating a character sheet for a game or filling out a form.

In addition to supporting JSON Schema in the REST API, the Google GenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod , respectively. The example below demonstrates how to extract information from unstructured text that conforms to a schema defined in code.

 
 
 
 
 

This example demonstrates how to extract structured data from text using basic JSON Schema types like `object`, `array`, `string`, and `integer`.

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import List, Optional

class Ingredient(BaseModel):
    name: str = Field(description="Name of the ingredient.")
    quantity: str = Field(description="Quantity of the ingredient, including units.")

class Recipe(BaseModel):
    recipe_name: str = Field(description="The name of the recipe.")
    prep_time_minutes: Optional[int] = Field(description="Optional time in minutes to prepare the recipe.")
    ingredients: List[Ingredient]
    instructions: List[str]

client = genai.Client()

prompt = """
Please extract the recipe from the following text.
The user wants to make delicious chocolate chip cookies.
They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
For the best part, they'll need 2 cups of semisweet chocolate chips.
First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
onto ungreased baking sheets and bake for 9 to 11 minutes.
"""

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=prompt,
    config={
        "response_mime_type": "application/json",
        "response_json_schema": Recipe.model_json_schema(),
    },
)

recipe = Recipe.model_validate_json(response.text)
print(recipe)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ingredientSchema = z.object({
  name: z.string().describe("Name of the ingredient."),
  quantity: z.string().describe("Quantity of the ingredient, including units."),
});

const recipeSchema = z.object({
  recipe_name: z.string().describe("The name of the recipe."),
  prep_time_minutes: z.number().optional().describe("Optional time in minutes to prepare the recipe."),
  ingredients: z.array(ingredientSchema),
  instructions: z.array(z.string()),
});

const ai = new GoogleGenAI({});

const prompt = `
Please extract the recipe from the following text.
The user wants to make delicious chocolate chip cookies.
They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
For the best part, they'll need 2 cups of semisweet chocolate chips.
First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
onto ungreased baking sheets and bake for 9 to 11 minutes.
`;

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: prompt,
  config: {
    responseMimeType: "application/json",
    responseJsonSchema: zodToJsonSchema(recipeSchema),
  },
});

const recipe = recipeSchema.parse(JSON.parse(response.text));
console.log(recipe);
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `
  Please extract the recipe from the following text.
  The user wants to make delicious chocolate chip cookies.
  They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,
  1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,
  3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.
  For the best part, they'll need 2 cups of semisweet chocolate chips.
  First, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,
  baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar
  until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry
  ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons
  onto ungreased baking sheets and bake for 9 to 11 minutes.
  `
    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseJsonSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "recipe_name": map[string]any{
                    "type":        "string",
                    "description": "The name of the recipe.",
                },
                "prep_time_minutes": map[string]any{
                    "type":        "integer",
                    "description": "Optional time in minutes to prepare the recipe.",
                },
                "ingredients": map[string]any{
                    "type": "array",
                    "items": map[string]any{
                        "type": "object",
                        "properties": map[string]any{
                            "name": map[string]any{
                                "type":        "string",
                                "description": "Name of the ingredient.",
                            },
                            "quantity": map[string]any{
                                "type":        "string",
                                "description": "Quantity of the ingredient, including units.",
                            },
                        },
                        "required": []string{"name", "quantity"},
                    },
                },
                "instructions": map[string]any{
                    "type":  "array",
                    "items": map[string]any{"type": "string"},
                },
            },
            "required": []string{"recipe_name", "ingredients", "instructions"},
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text(prompt),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          { "text": "Please extract the recipe from the following text.\nThe user wants to make delicious chocolate chip cookies.\nThey need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,\n1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,\n3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.\nFor the best part, they will need 2 cups of semisweet chocolate chips.\nFirst, preheat the oven to 375¬∞F (190¬∞C). Then, in a small bowl, whisk together the flour,\nbaking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar\nuntil light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry\ningredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons\nonto ungreased baking sheets and bake for 9 to 11 minutes." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
          "type": "object",
          "properties": {
            "recipe_name": {
              "type": "string",
              "description": "The name of the recipe."
            },
            "prep_time_minutes": {
                "type": "integer",
                "description": "Optional time in minutes to prepare the recipe."
            },
            "ingredients": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "name": { "type": "string", "description": "Name of the ingredient."},
                  "quantity": { "type": "string", "description": "Quantity of the ingredient, including units."}
                },
                "required": ["name", "quantity"]
              }
            },
            "instructions": {
              "type": "array",
              "items": { "type": "string" }
            }
          },
          "required": ["recipe_name", "ingredients", "instructions"]
        }
      }
    }'
```

 
 

 Example Response: 

 

```
{
  "recipe_name": "Delicious Chocolate Chip Cookies",
  "ingredients": [
    {
      "name": "all-purpose flour",
      "quantity": "2 and 1/4 cups"
    },
    {
      "name": "baking soda",
      "quantity": "1 teaspoon"
    },
    {
      "name": "salt",
      "quantity": "1 teaspoon"
    },
    {
      "name": "unsalted butter (softened)",
      "quantity": "1 cup"
    },
    {
      "name": "granulated sugar",
      "quantity": "3/4 cup"
    },
    {
      "name": "packed brown sugar",
      "quantity": "3/4 cup"
    },
    {
      "name": "vanilla extract",
      "quantity": "1 teaspoon"
    },
    {
      "name": "large eggs",
      "quantity": "2"
    },
    {
      "name": "semisweet chocolate chips",
      "quantity": "2 cups"
    }
  ],
  "instructions": [
    "Preheat the oven to 375¬∞F (190¬∞C).",
    "In a small bowl, whisk together the flour, baking soda, and salt.",
    "In a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.",
    "Beat in the vanilla and eggs, one at a time.",
    "Gradually beat in the dry ingredients until just combined.",
    "Stir in the chocolate chips.",
    "Drop by rounded tablespoons onto ungreased baking sheets and bake for 9 to 11 minutes."
  ]
}
```

 

## Streaming

You can stream structured outputs, which allows you to start processing the response as it's being generated, without having to wait for the entire output to be complete. This can improve the perceived performance of your application.

The streamed chunks will be valid partial JSON strings, which can be concatenated to form the final, complete JSON object.

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import Literal

class Feedback(BaseModel):
    sentiment: Literal["positive", "neutral", "negative"]
    summary: str

client = genai.Client()
prompt = "The new UI is incredibly intuitive and visually appealing. Great job. Add a very long summary to test streaming!"

response_stream = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=prompt,
    config={
        "response_mime_type": "application/json",
        "response_json_schema": Feedback.model_json_schema(),
    },
)

for chunk in response_stream:
    print(chunk.candidates[0].content.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});
const prompt = "The new UI is incredibly intuitive and visually appealing. Great job! Add a very long summary to test streaming!";

const feedbackSchema = z.object({
  sentiment: z.enum(["positive", "neutral", "negative"]),
  summary: z.string(),
});

const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: prompt,
  config: {
    responseMimeType: "application/json",
    responseJsonSchema: zodToJsonSchema(feedbackSchema),
  },
});

for await (const chunk of stream) {
  console.log(chunk.candidates[0].content.parts[0].text)
}
```

 
 

## Structured outputs with tools

Gemini 3 lets you combine Structured Outputs with built-in tools, including Grounding with Google Search , URL Context , and Code Execution .

 
 

### Python

 

```
from google import genai
from pydantic import BaseModel, Field
from typing import List

class MatchResult(BaseModel):
    winner: str = Field(description="The name of the winner.")
    final_match_score: str = Field(description="The final match score.")
    scorers: List[str] = Field(description="The name of the scorer.")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Search for all details for the latest Euro.",
    config={
        "tools": [
            {"google_search": {}},
            {"url_context": {}}
        ],
        "response_mime_type": "application/json",
        "response_json_schema": MatchResult.model_json_schema(),
    },  
)

result = MatchResult.model_validate_json(response.text)
print(result)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const ai = new GoogleGenAI({});

const matchSchema = z.object({
  winner: z.string().describe("The name of the winner."),
  final_match_score: z.string().describe("The final score."),
  scorers: z.array(z.string()).describe("The name of the scorer.")
});

async function run() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Search for all details for the latest Euro.",
    config: {
      tools: [
        { googleSearch: {} },
        { urlContext: {} }
      ],
      responseMimeType: "application/json",
      responseJsonSchema: zodToJsonSchema(matchSchema),
    },
  });

  const match = matchSchema.parse(JSON.parse(response.text));
  console.log(match);
}

run();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [{
      "parts": [{"text": "Search for all details for the latest Euro."}]
    }],
    "tools": [
      {"googleSearch": {}},
      {"urlContext": {}}
    ],
    "generationConfig": {
        "responseMimeType": "application/json",
        "responseJsonSchema": {
            "type": "object",
            "properties": {
                "winner": {"type": "string", "description": "The name of the winner."},
                "final_match_score": {"type": "string", "description": "The final score."},
                "scorers": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "The name of the scorer."
                }
            },
            "required": ["winner", "final_match_score", "scorers"]
        }
    }
  }'
```

 
 

## JSON schema support

To generate a JSON object, set the `response_mime_type` in the generation configuration to `application/json` and provide a `response_json_schema`. The schema must be a valid JSON Schema that describes the desired output format.

The model will then generate a response that is a syntactically valid JSON string matching the provided schema. When using structured outputs, the model will produce outputs in the same order as the keys in the schema.

Gemini's structured output mode supports a subset of the JSON Schema specification.

The following values of `type` are supported:

- `string` : For text.

- `number` : For floating-point numbers.

- `integer` : For whole numbers.

- `boolean` : For true/false values.

- `object` : For structured data with key-value pairs.

- `array` : For lists of items.

- `null` : To allow a property to be null, include `"null"` in the type array (e.g., 

```
{"type": ["string", "null"]}
```

).

These descriptive properties help guide the model:

- `title` : A short description of a property.

- `description` : A longer and more detailed description of a property.

### Type-specific properties

 For `object` values: 

- `properties` : An object where each key is a property name and each value is a schema for that property.

- `required` : An array of strings, listing which properties are mandatory.

- `additionalProperties` : Controls whether properties not listed in `properties` are allowed. Can be a boolean or a schema.

 For `string` values: 

- `enum` : Lists a specific set of possible strings for classification tasks.

- `format` : Specifies a syntax for the string, such as `date-time`, `date`, `time`.

 For `number` and `integer` values: 

- `enum` : Lists a specific set of possible numeric values.

- `minimum` : The minimum inclusive value.

- `maximum` : The maximum inclusive value.

 For `array` values: 

- `items` : Defines the schema for all items in the array.

- `prefixItems` : Defines a list of schemas for the first N items, allowing for tuple-like structures.

- `minItems` : The minimum number of items in the array.

- `maxItems` : The maximum number of items in the array.

## Model support

The following models support structured output:

 
 
 
 Model 
 Structured Outputs 
 
 

 
 
 Gemini 3 Pro Preview 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è* 
 
 
 Gemini 2.0 Flash-Lite 
 ‚úîÔ∏è* 
 
 
 

 * Note that Gemini 2.0 requires an explicit `propertyOrdering` list within the JSON input to define the preferred structure. You can find an example in this cookbook . 

## Structured outputs vs. function calling

Both structured outputs and function calling use JSON schemas, but they serve different purposes:

 
 
 
 Feature 
 Primary Use Case 
 
 

 
 
 Structured Outputs 
 Formatting the final response to the user. Use this when you want the model's answer to be in a specific format (e.g., extracting data from a document to save to a database). 
 
 
 Function Calling 
 Taking action during the conversation. Use this when the model needs to ask you to perform a task (e.g., "get current weather") before it can provide a final answer. 
 
 
 

## Best practices

- Clear descriptions: Use the `description` field in your schema to provide clear instructions to the model about what each property represents. This is crucial for guiding the model's output.

- Strong typing: Use specific types (`integer`, `string`, `enum`) whenever possible. If a parameter has a limited set of valid values, use an `enum`.

- Prompt engineering: Clearly state in your prompt what you want the model to do. For example, "Extract the following information from the text..." or "Classify this feedback according to the provided schema...".

- Validation: While structured output guarantees syntactically correct JSON, it does not guarantee the values are semantically correct. Always validate the final output in your application code before using it.

- Error handling: Implement robust error handling in your application to gracefully manage cases where the model's output, while schema-compliant, may not meet your business logic requirements.

## Limitations

- Schema subset: Not all features of the JSON Schema specification are supported. The model ignores unsupported properties.

- Schema complexity: The API may reject very large or deeply nested schemas. If you encounter errors, try simplifying your schema by shortening property names, reducing nesting, or limiting the number of constraints.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### URL context &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/url-context

- 
 
 
 
 
 
 
 
 
 
 
 URL context  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÔøΩÔøΩÔøΩÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 URL context 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The URL context tool lets you provide additional context to the models in the
form of URLs. By including URLs in your request, the model will access
the content from those pages (as long as it's not a URL type listed in the
 limitations section ) to inform
and enhance its response.

The URL context tool is useful for tasks like the following:

- Extract Data : Pull specific info like prices, names, or key
findings from multiple URLs.

- Compare Documents : Analyze multiple reports, articles, or PDFs to
identify differences and track trends.

- Synthesize & Create Content : Combine information from several source
URLs to generate accurate summaries, blog posts, or reports.

- Analyze Code & Docs : Point to a GitHub repository or technical
documentation to explain code, generate setup instructions, or answer
questions.

The following example shows how to compare two recipes from different websites.

 
 

### Python

 

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = [
  {"url_context": {}},
]

url1 = "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592"
url2 = "https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/"

response = client.models.generate_content(
    model=model_id,
    contents=f"Compare the ingredients and cooking times from the recipes at {url1} and {url2}",
    config=GenerateContentConfig(
        tools=tools,
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)

# For verification, you can inspect the metadata to see which URLs the model retrieved
print(response.candidates[0].url_context_metadata)
```

 
 

### Javascript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Compare the ingredients and cooking times from the recipes at https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592 and https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/",
    ],
    config: {
      tools: [{urlContext: {}}],
    },
  });
  console.log(response.text);

  // For verification, you can inspect the metadata to see which URLs the model retrieved
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Compare the ingredients and cooking times from the recipes at https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592 and https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/"}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          }
      ]
  }' > result.json

cat result.json
```

 
 

## How it works

The URL Context tool uses a two-step retrieval process to
balance speed, cost, and access to fresh data. When you provide a URL, the tool
first attempts to fetch the content from an internal index cache. This acts as a
highly optimized cache. If a URL is not available in the index (for example, if
it's a very new page), the tool automatically falls back to do a live fetch.
This directly accesses the URL to retrieve its content in real-time.

## Combining with other tools

You can combine the URL context tool with other tools to create more powerful
workflows.

### Grounding with search

When both URL context and
 Grounding with Google Search are enabled,
the model can use its search capabilities to find
relevant information online and then use the URL context tool to get a more
in-depth understanding of the pages it finds. This approach is powerful for
prompts that require both broad searching and deep analysis of specific pages.

 
 

### Python

 

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, UrlContext

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = [
      {"url_context": {}},
      {"google_search": {}}
  ]

response = client.models.generate_content(
    model=model_id,
    contents="Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    config=GenerateContentConfig(
        tools=tools,
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)
```

 
 

### Javascript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    ],
    config: {
      tools: [
        {urlContext: {}},
        {googleSearch: {}}
        ],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute."}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          },
          {
              "google_search": {}
          }
      ]
  }' > result.json

cat result.json
```

 
 

## Understanding the response

When the model uses the URL context tool, the response includes a
`url_context_metadata` object. This object lists the URLs the model retrieved
content from and the status of each retrieval attempt, which is useful for
verification and debugging.

The following is an example of that part of the response
(parts of the response have been omitted for brevity):

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata": {
        "url_metadata": [
          {
            "retrieved_url": "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592",
            "url_retrieval_status": "URL_RETRIEVAL_STATUS_SUCCESS"
          },
          {
            "retrieved_url": "https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/",
            "url_retrieval_status": "URL_RETRIEVAL_STATUS_SUCCESS"
          }
        ]
      }
    }
}
```

 

For complete detail about this object , see the
 `UrlContextMetadata` API reference .

### Safety checks

The system performs a content moderation check on the URL to confirm
they meet safety standards. If the URL you provided fails this check, you will
get an `url_retrieval_status` of `URL_RETRIEVAL_STATUS_UNSAFE`.

### Token count

The content retrieved from the URLs you specify in your prompt is counted
as part of the input tokens. You can see the token count for your prompt and
tools usage in the `usage_metadata` 
object of the model output. The following is an example output:

 

```
'usage_metadata': {
  'candidates_token_count': 45,
  'prompt_token_count': 27,
  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 27}],
  'thoughts_token_count': 31,
  'tool_use_prompt_token_count': 10309,
  'tool_use_prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 10309}],
  'total_token_count': 10412
  }
```

 

Price per token depends on the model used, see the
 pricing page for details.

## Supported models

- gemini-2.5-pro 

- gemini-2.5-flash 

- gemini-2.5-flash-lite 

- gemini-live-2.5-flash-preview 

- gemini-2.0-flash-live-001 

## Best Practices

- Provide specific URLs : For the best results, provide direct URLs to the
content you want the model to analyze. The model will only retrieve content
from the URLs you provide, not any content from nested links.

- Check for accessibility : Verify that the URLs you provide don't lead to
pages that require a login or are behind a paywall.

- Use the complete URL : Provide the full URL, including the protocol
(e.g., https://www.google.com instead of just google.com).

## Limitations

- Pricing : Content retrieved from URLs counts as input tokens. Rate limit
and pricing is the based on the model used. See the
 rate limits and
 pricing pages for details.

- Request limit : The tool can process up to 20 URLs per request.

- URL content size : The maximum size for content retrieved from a single
URL is 34MB.

### Supported and unsupported content types

The tool can extract content from URLs with the following content types:

- Text (text/html, application/json, text/plain, text/xml, text/css,
text/javascript , text/csv, text/rtf)

- Image (image/png, image/jpeg, image/bmp, image/webp)

- PDF (application/pdf)

The following content types are not supported:

- Paywalled content

- YouTube videos (See
 video understanding to learn
how to process YouTube URLs)

- Google workspace files like Google docs or spreadsheets

- Video and audio files

## What's next

- Explore the URL context cookbook 
for more examples.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Computer Use &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/computer-use

- 
 
 
 
 
 
 
 
 
 
 
 Computer Use  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÔøΩÔøΩÔøΩÏñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Computer Use 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini 2.5 Computer Use Preview model and tool enable you to build browser
control agents that interact with and automate tasks. Using screenshots, the
Computer Use model can "see" a computer screen, and "act" by generating specific
UI actions like mouse clicks and keyboard inputs. Similar to function calling,
you need to write the client-side application code to receive and execute the
Computer Use actions.

With Computer Use, you can build agents that:

- Automate repetitive data entry or form filling on websites.

- Perform automated testing of web applications and user flows

- Conduct research across various websites (e.g., gathering product
information, prices, and reviews from ecommerce sites to inform a purchase)

The easiest way to test the Gemini Computer Use model is through the reference
implementation or
 Browserbase demo environment .

## How Computer Use works

To build a browser control agent with the Computer Use model, implement
an agent loop that does the following:

- 

 Send a request to the model 

 Add the Computer Use tool and optionally any custom user-defined
functions or excluded functions to your API request.

- Prompt the Computer Use model with the user's request.

 
- 

 Receive the model response 

 The Computer Use model analyzes the user request and screenshot, and
generates a response which includes a suggested `function_call`
representing a UI action (e.g., "click at coordinate (x,y)" or "type
'text'"). For a description of all UI actions supported by the Computer
Use model, see Supported actions .

- The API response may also include a `safety_decision` from an internal
safety system that checks the model's proposed action. This
`safety_decision` classifies the action as:

 Regular / allowed: The action is considered safe. This may also
be represented by no `safety_decision` being present.

- Requires confirmation (`require_confirmation`): The model is about to perform an action
that may be risky (e.g., clicking on an "accept cookie banner").

 

 
- 

 Execute the received action 

 Your client-side code receives the `function_call` and any accompanying
`safety_decision`.

 Regular / allowed: If the `safety_decision` indicates regular /
allowed (or if no `safety_decision` is present), your client-side
code can execute the specified `function_call` in your target
environment (e.g., a web browser).

- Requires confirmation: If the `safety_decision` indicates
requires confirmation, your application must prompt the end-user for
confirmation before executing the `function_call`. If the user
confirms, proceed to execute the action. If the user denies, don't
execute the action.

 

 
- 

 Capture the new environment state 

 If the action has been executed, your client captures a new screenshot
of the GUI and the current URL to send back to the Computer Use model as
part of a `function_response`.

- If an action was blocked by the safety system or denied confirmation by
the user, your application might send a different form of feedback to
the model or end the interaction.

 

This process repeats from step 2 with the Computer Use model using the new
screenshot and the ongoing goal to suggest the next action. The loop continues
until the task is completed, an error occurs, or the process is terminated
(e.g., due to a "block" safety response or user decision).

 

## How to implement Computer Use

Before building with the Computer Use model and tool you will need to set up the
following:

- Secure execution environment: For safety reasons, you should run your
Computer Use agent in a secure and controlled environment (e.g., a sandboxed
virtual machine, a container, or a dedicated browser profile with limited
permissions).

- Client-side action handler: You will need to implement client-side logic
to execute the actions generated by the model and
capture screenshots of the environment after each action.

The examples in this section use a browser as the execution environment
and Playwright as the client-side action handler. To
run these samples you must install the necessary dependencies and initialize a
Playwright browser instance.

 
 
 

#### Install Playwright

 

 

```
    pip install google-genai playwright
    playwright install chromium
```

 
 
 

 
 
 

#### Initialize Playwright browser instance

 

 

```
    from playwright.sync_api import sync_playwright

    # 1. Configure screen dimensions for the target environment
    SCREEN_WIDTH = 1440
    SCREEN_HEIGHT = 900

    # 2. Start the Playwright browser
    # In production, utilize a sandboxed environment.
    playwright = sync_playwright().start()
    # Set headless=False to see the actions performed on your screen
    browser = playwright.chromium.launch(headless=False)

    # 3. Create a context and page with the specified dimensions
    context = browser.new_context(
        viewport={"width": SCREEN_WIDTH, "height": SCREEN_HEIGHT}
    )
    page = context.new_page()

    # 4. Navigate to an initial page to start the task
    page.goto("https://www.google.com")

    # The 'page', 'SCREEN_WIDTH', and 'SCREEN_HEIGHT' variables
    # will be used in the steps below.
```

 
 
 

Sample code for extending to an Android
environment is included in the Using custom user-defined
functions section.

### 1. Send a request to the model

Add the Computer Use tool to your API request and send a prompt to the Computer
Use model that includes the user's goal.
You must use the Gemini Computer Use model,
`gemini-2.5-computer-use-preview-10-2025`. If you try to use the Computer Use
tool with a different model, you will get an error.

You can also optionally add the following parameters:

- Excluded actions: If there are any actions from the list of Supported
UI actions that you don't want the model to take,
specify these actions as `excluded_predefined_functions`.

- User-defined functions: In addition to the Computer Use tool, you may
want to include custom user-defined functions.

Note that there is no need to specify the display size when issuing a request;
the model predicts pixel coordinates scaled to the height and width of the
screen.

 
 

### Python

 

```
from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

# Specify predefined functions to exclude (optional)
excluded_functions = ["drag_and_drop"]

generate_content_config = genai.types.GenerateContentConfig(
    tools=[
        # 1. Computer Use tool with browser environment
        types.Tool(
            computer_use=types.ComputerUse(
                environment=types.Environment.ENVIRONMENT_BROWSER,
                # Optional: Exclude specific predefined functions
                excluded_predefined_functions=excluded_functions
                )
              ),
        # 2. Optional: Custom user-defined functions
        #types.Tool(
          # function_declarations=custom_functions
          #   )
          ],
  )

# Create the content with user message
contents=[
    Content(
        role="user",
        parts=[
            Part(text="Search for highly rated smart fridges with touchscreen, 2 doors, around 25 cu ft, priced below 4000 dollars on Google Shopping. Create a bulleted list of the 3 cheapest options in the format of name, description, price in an easy-to-read layout."),
        ],
    )
]

# Generate content with the configured settings
response = client.models.generate_content(
    model='gemini-2.5-computer-use-preview-10-2025',
    contents=contents,
    config=generate_content_config,
)

# Print the response output
print(response)
```

 
 

For an example with custom functions, see Using custom
user-defined functions .

### 2. Receive the model response

The Computer Use model will respond with one or more `FunctionCalls` if it
determines UI actions are needed to complete the task. Computer Use supports
parallel function calling, meaning the model can return multiple actions in a
single turn.

Here is an example model response.

 

```
{
  "content": {
    "parts": [
      {
        "text": "I will type the search query into the search bar. The search bar is in the center of the page."
      },
      {
        "function_call": {
          "name": "type_text_at",
          "args": {
            "x": 371,
            "y": 470,
            "text": "highly rated smart fridges with touchscreen, 2 doors, around 25 cu ft, priced below 4000 dollars on Google Shopping",
            "press_enter": true
          }
        }
      }
    ]
  }
}
```

 

### 3. Execute the received actions

Your application code needs to parse the model response, execute the actions,
and collect the results.

The example code below extracts function calls from the Computer Use model
response, and translates them into actions that can be executed with Playwright.
The model outputs normalized coordinates (0-999) regardless of the input image
dimensions, so part of the translation step is converting these normalized
coordinates back to actual pixel values.

The recommended screen size for use
with the Computer Use model is (1440, 900). The model will work with any
resolution, though the quality of the results may be impacted.

Note that this example only includes the implementation for the 3 most common
UI actions: `open_web_browser`, `click_at`, and `type_text_at`. For
production use cases, you will need to implement all other UI actions from the
 Supported actions list unless you explicitly add them as
`excluded_predefined_functions`.

 
 

### Python

 

```
from typing import Any, List, Tuple
import time

def denormalize_x(x: int, screen_width: int) -> int:
    """Convert normalized x coordinate (0-1000) to actual pixel coordinate."""
    return int(x / 1000 * screen_width)

def denormalize_y(y: int, screen_height: int) -> int:
    """Convert normalized y coordinate (0-1000) to actual pixel coordinate."""
    return int(y / 1000 * screen_height)

def execute_function_calls(candidate, page, screen_width, screen_height):
    results = []
    function_calls = []
    for part in candidate.content.parts:
        if part.function_call:
            function_calls.append(part.function_call)

    for function_call in function_calls:
        action_result = {}
        fname = function_call.name
        args = function_call.args
        print(f"  -> Executing: {fname}")

        try:
            if fname == "open_web_browser":
                pass # Already open
            elif fname == "click_at":
                actual_x = denormalize_x(args["x"], screen_width)
                actual_y = denormalize_y(args["y"], screen_height)
                page.mouse.click(actual_x, actual_y)
            elif fname == "type_text_at":
                actual_x = denormalize_x(args["x"], screen_width)
                actual_y = denormalize_y(args["y"], screen_height)
                text = args["text"]
                press_enter = args.get("press_enter", False)

                page.mouse.click(actual_x, actual_y)
                # Simple clear (Command+A, Backspace for Mac)
                page.keyboard.press("Meta+A")
                page.keyboard.press("Backspace")
                page.keyboard.type(text)
                if press_enter:
                    page.keyboard.press("Enter")
            else:
                print(f"Warning: Unimplemented or custom function {fname}")

            # Wait for potential navigations/renders
            page.wait_for_load_state(timeout=5000)
            time.sleep(1)

        except Exception as e:
            print(f"Error executing {fname}: {e}")
            action_result = {"error": str(e)}

        results.append((fname, action_result))

    return results
```

 
 

### 4. Capture the new environment state

After executing the actions, send the result of the function execution back to
the model so it can use this information to generate the next action. If
multiple actions (parallel calls) were executed, you must send a
`FunctionResponse` for each one in the subsequent user turn.

 
 

### Python

 

```
def get_function_responses(page, results):
    screenshot_bytes = page.screenshot(type="png")
    current_url = page.url
    function_responses = []
    for name, result in results:
        response_data = {"url": current_url}
        response_data.update(result)
        function_responses.append(
            types.FunctionResponse(
                name=name,
                response=response_data,
                parts=[types.FunctionResponsePart(
                        inline_data=types.FunctionResponseBlob(
                            mime_type="image/png",
                            data=screenshot_bytes))
                ]
            )
        )
    return function_responses
```

 
 

## Build an agent loop

To enable multi-step interactions, combine the four steps from the How to
implement Computer Use section into a loop.
Remember to manage the conversation history correctly by appending both model
responses and your function responses.

To run this code sample you need to:

- Install the necessary Playwright
dependencies .

- 

Define the helper functions from steps (3) Execute the received
actions and (4) Capture the new environment
state .

 
 

### Python

 

```
import time
from typing import Any, List, Tuple
from playwright.sync_api import sync_playwright

from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

# Constants for screen dimensions
SCREEN_WIDTH = 1440
SCREEN_HEIGHT = 900

# Setup Playwright
print("Initializing browser...")
playwright = sync_playwright().start()
browser = playwright.chromium.launch(headless=False)
context = browser.new_context(viewport={"width": SCREEN_WIDTH, "height": SCREEN_HEIGHT})
page = context.new_page()

# Define helper functions. Copy/paste from steps 3 and 4
# def denormalize_x(...)
# def denormalize_y(...)
# def execute_function_calls(...)
# def get_function_responses(...)

try:
    # Go to initial page
    page.goto("https://ai.google.dev/gemini-api/docs")

    # Configure the model (From Step 1)
    config = types.GenerateContentConfig(
        tools=[types.Tool(computer_use=types.ComputerUse(
            environment=types.Environment.ENVIRONMENT_BROWSER
        ))],
        thinking_config=types.ThinkingConfig(include_thoughts=True),
    )

    # Initialize history
    initial_screenshot = page.screenshot(type="png")
    USER_PROMPT = "Go to ai.google.dev/gemini-api/docs and search for pricing."
    print(f"Goal: {USER_PROMPT}")

    contents = [
        Content(role="user", parts=[
            Part(text=USER_PROMPT),
            Part.from_bytes(data=initial_screenshot, mime_type='image/png')
        ])
    ]

    # Agent Loop
    turn_limit = 5
    for i in range(turn_limit):
        print(f"\n--- Turn {i+1} ---")
        print("Thinking...")
        response = client.models.generate_content(
            model='gemini-2.5-computer-use-preview-10-2025',
            contents=contents,
            config=config,
        )

        candidate = response.candidates[0]
        contents.append(candidate.content)

        has_function_calls = any(part.function_call for part in candidate.content.parts)
        if not has_function_calls:
            text_response = " ".join([part.text for part in candidate.content.parts if part.text])
            print("Agent finished:", text_response)
            break

        print("Executing actions...")
        results = execute_function_calls(candidate, page, SCREEN_WIDTH, SCREEN_HEIGHT)

        print("Capturing state...")
        function_responses = get_function_responses(page, results)

        contents.append(
            Content(role="user", parts=[Part(function_response=fr) for fr in function_responses])
        )

finally:
    # Cleanup
    print("\nClosing browser...")
    browser.close()
    playwright.stop()
```

 
 

## Using custom user-defined functions

You can optionally include custom user-defined functions in your request to
extend the functionality of the model. The example below adapts the Computer Use
model and tool for mobile use cases by including custom user-defined actions
like `open_app`, `long_press_at`, and `go_home`, while excluding
browser-specific actions. The model can intelligently call these custom
functions alongside standard UI actions to complete tasks in non-browser
environments.

 
 

### Python

 

```
from typing import Optional, Dict, Any

from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

SYSTEM_PROMPT = """You are operating an Android phone. Today's date is October 15, 2023, so ignore any other date provided.
* To provide an answer to the user, *do not use any tools* and output your answer on a separate line. IMPORTANT: Do not add any formatting or additional punctuation/text, just output the answer by itself after two empty lines.
* Make sure you scroll down to see everything before deciding something isn't available.
* You can open an app from anywhere. The icon doesn't have to currently be on screen.
* Unless explicitly told otherwise, make sure to save any changes you make.
* If text is cut off or incomplete, scroll or click into the element to get the full text before providing an answer.
* IMPORTANT: Complete the given task EXACTLY as stated. DO NOT make any assumptions that completing a similar task is correct.  If you can't find what you're looking for, SCROLL to find it.
* If you want to edit some text, ONLY USE THE `type` tool. Do not use the onscreen keyboard.
* Quick settings shouldn't be used to change settings. Use the Settings app instead.
* The given task may already be completed. If so, there is no need to do anything.
"""

def open_app(app_name: str, intent: Optional[str] = None) -> Dict[str, Any]:
    """Opens an app by name.

    Args:
        app_name: Name of the app to open (any string).
        intent: Optional deep-link or action to pass when launching, if the app supports it.

    Returns:
        JSON payload acknowledging the request (app name and optional intent).
    """
    return {"status": "requested_open", "app_name": app_name, "intent": intent}

def long_press_at(x: int, y: int) -> Dict[str, int]:
    """Long-press at a specific screen coordinate.

    Args:
        x: X coordinate (absolute), scaled to the device screen width (pixels).
        y: Y coordinate (absolute), scaled to the device screen height (pixels).

    Returns:
        Object with the coordinates pressed and the duration used.
    """
    return {"x": x, "y": y}

def go_home() -> Dict[str, str]:
    """Navigates to the device home screen.

    Returns:
        A small acknowledgment payload.
    """
    return {"status": "home_requested"}

#  Build function declarations
CUSTOM_FUNCTION_DECLARATIONS = [
    types.FunctionDeclaration.from_callable(client=client, callable=open_app),
    types.FunctionDeclaration.from_callable(client=client, callable=long_press_at),
    types.FunctionDeclaration.from_callable(client=client, callable=go_home),
]

#Exclude browser functions
EXCLUDED_PREDEFINED_FUNCTIONS = [
    "open_web_browser",
    "search",
    "navigate",
    "hover_at",
    "scroll_document",
    "go_forward",
    "key_combination",
    "drag_and_drop",
]

#Utility function to construct a GenerateContentConfig
def make_generate_content_config() -> genai.types.GenerateContentConfig:
    """Return a fixed GenerateContentConfig with Computer Use + custom functions."""
    return genai.types.GenerateContentConfig(
        system_instruction=SYSTEM_PROMPT,
        tools=[
            types.Tool(
                computer_use=types.ComputerUse(
                    environment=types.Environment.ENVIRONMENT_BROWSER,
                    excluded_predefined_functions=EXCLUDED_PREDEFINED_FUNCTIONS,
                )
            ),
            types.Tool(function_declarations=CUSTOM_FUNCTION_DECLARATIONS),
        ],
    )

# Create the content with user message
contents: list[Content] = [
    Content(
        role="user",
        parts=[
            # text instruction
            Part(text="Open Chrome, then long-press at 200,400."),
        ],
    )
]

# Build your fixed config (from helper)
config = make_generate_content_config()

# Generate content with the configured settings
response = client.models.generate_content(
        model='gemini-2.5-computer-use-preview-10-2025',
        contents=contents,
        config=config,
    )

print(response)
```

 
 

## Supported UI actions

The Computer Use model can request the following UI actions via a
`FunctionCall`. Your client-side code must implement the execution logic for
these actions. See the reference
implementation for
examples.

 
 
 
 Command Name 
 Description 
 Arguments (in Function Call) 
 Example Function Call 
 
 

 
 
 open_web_browser 
 Opens the web browser. 
 None 
 

```
{"name": "open_web_browser", "args": {}}
```

 
 
 
 wait_5_seconds 
 Pauses execution for 5 seconds to allow dynamic content to load or animations to complete. 
 None 
 

```
{"name": "wait_5_seconds", "args": {}}
```

 
 
 
 go_back 
 Navigates to the previous page in the browser's history. 
 None 
 

```
{"name": "go_back", "args": {}}
```

 
 
 
 go_forward 
 Navigates to the next page in the browser's history. 
 None 
 

```
{"name": "go_forward", "args": {}}
```

 
 
 
 search 
 Navigates to the default search engine's homepage (e.g., Google). Useful for starting a new search task. 
 None 
 

```
{"name": "search", "args": {}}
```

 
 
 
 navigate 
 Navigates the browser directly to the specified URL. 
 `url`: str 
 

```
{"name": "navigate", "args": {"url": "https://www.wikipedia.org"}}
```

 
 
 
 click_at 
 Clicks at a specific coordinate on the webpage. The x and y values are based on a 1000x1000 grid and are scaled to the screen dimensions. 
 `y`: int (0-999), `x`: int (0-999) 
 

```
{"name": "click_at", "args": {"y": 300, "x": 500}}
```

 
 
 
 hover_at 
 Hovers the mouse at a specific coordinate on the webpage. Useful for revealing sub-menus. x and y are based on a 1000x1000 grid. 
 `y`: int (0-999) `x`: int (0-999) 
 

```
{"name": "hover_at", "args": {"y": 150, "x": 250}}
```

 
 
 
 type_text_at 
 Types text at a specific coordinate, defaults to clearing the field first and pressing ENTER after typing, but these can be disabled. x and y are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `text`: str, `press_enter`: bool (Optional, default True), `clear_before_typing`: bool (Optional, default True) 
 

```
{"name": "type_text_at", "args": {"y": 250, "x": 400, "text": "search query", "press_enter": false}}
```

 
 
 
 key_combination 
 Press keyboard keys or combinations, such as "Control+C" or "Enter". Useful for triggering actions (like submitting a form with "Enter") or clipboard operations. 
 `keys`: str (e.g. 'enter', 'control+c'). 
 

```
{"name": "key_combination", "args": {"keys": "Control+A"}}
```

 
 
 
 scroll_document 
 Scrolls the entire webpage "up", "down", "left", or "right". 
 `direction`: str ("up", "down", "left", or "right") 
 

```
{"name": "scroll_document", "args": {"direction": "down"}}
```

 
 
 
 scroll_at 
 Scrolls a specific element or area at coordinate (x, y) in the specified direction by a certain magnitude. Coordinates and magnitude (default 800) are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `direction`: str ("up", "down", "left", "right"), `magnitude`: int (0-999, Optional, default 800) 
 

```
{"name": "scroll_at", "args": {"y": 500, "x": 500, "direction": "down", "magnitude": 400}}
```

 
 
 
 drag_and_drop 
 Drags an element from a starting coordinate (x, y) and drops it at a destination coordinate (destination_x, destination_y). All coordinates are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `destination_y`: int (0-999), `destination_x`: int (0-999) 
 

```
{"name": "drag_and_drop", "args": {"y": 100, "x": 100, "destination_y": 500, "destination_x": 500}}
```

 
 
 
 

## Safety and security

### Acknowledge safety decision

Depending on the action, the model response might also include a
`safety_decision` from an internal safety system that checks the model's
proposed action.

 

```
{
  "content": {
    "parts": [
      {
        "text": "I have evaluated step 2. It seems Google detected unusual traffic and is asking me to verify I'm not a robot. I need to click the 'I'm not a robot' checkbox located near the top left (y=98, x=95).",
      },
      {
        "function_call": {
          "name": "click_at",
          "args": {
            "x": 60,
            "y": 100,
            "safety_decision": {
              "explanation": "I have encountered a CAPTCHA challenge that requires interaction. I need you to complete the challenge by clicking the 'I'm not a robot' checkbox and any subsequent verification steps.",
              "decision": "require_confirmation"
            }
          }
        }
      }
    ]
  }
}
```

 

If the `safety_decision` is `require_confirmation`, you must
ask the end user to confirm before proceeding with executing the action. Per the
 terms of service , you are not allowed
to bypass requests for human confirmation.

This code sample prompts the end-user for confirmation before executing the
action. If the user does not confirm the action, the loop terminates. If the
user confirms the action, the action is executed and the
`safety_acknowledgement` field is marked as `True`.

 
 

### Python

 

```
import termcolor

def get_safety_confirmation(safety_decision):
    """Prompt user for confirmation when safety check is triggered."""
    termcolor.cprint("Safety service requires explicit confirmation!", color="red")
    print(safety_decision["explanation"])

    decision = ""
    while decision.lower() not in ("y", "n", "ye", "yes", "no"):
        decision = input("Do you wish to proceed? [Y]es/[N]o\n")

    if decision.lower() in ("n", "no"):
        return "TERMINATE"
    return "CONTINUE"

def execute_function_calls(candidate, page, screen_width, screen_height):

    # ... Extract function calls from response ...

    for function_call in function_calls:
        extra_fr_fields = {}

        # Check for safety decision
        if 'safety_decision' in function_call.args:
            decision = get_safety_confirmation(function_call.args['safety_decision'])
            if decision == "TERMINATE":
                print("Terminating agent loop")
                break
            extra_fr_fields["safety_acknowledgement"] = "true" # Safety acknowledgement

        # ... Execute function call and append to results ...
```

 
 

If the user confirms, you must include the safety acknowledgement in
your `FunctionResponse`.

 
 

### Python

 

```
function_response_parts.append(
    FunctionResponse(
        name=name,
        response={"url": current_url,
                  **extra_fr_fields},  # Include safety acknowledgement
        parts=[
            types.FunctionResponsePart(
                inline_data=types.FunctionResponseBlob(
                    mime_type="image/png", data=screenshot
                )
             )
           ]
         )
       )
```

 
 

### Safety best practices

Computer Use API is a novel API and presents new risks that developers should be
mindful of:

- Untrusted content & scams: As the model tries to achieve the user's
goal, it may rely on untrustworthy sources of information and instructions
from the screen. For example, if the user's goal is to purchase a Pixel
phone and the model encounters a "Free-Pixel if you complete a survey" scam,
there is some chance that the model will complete the survey.

- Occasional unintended actions: The model can misinterpret a user's goal
or webpage content, causing it to take incorrect actions like clicking the
wrong button or filling the wrong form. This can lead to failed tasks or
data exfiltration.

- Policy violations: The API's capabilities could be directed, either
intentionally or unintentionally, toward activities that violate Google's
policies ( Gen AI Prohibited Use
Policy and the
 Gemini API Additional Terms of
Service . This includes actions that
could interfere with a system's integrity, compromise security, bypass
security measures,
control medical devices, etc.

To address these risks, you can implement the following safety measures and best
practices:

- 

 Human-in-the-Loop (HITL): 

 Implement user confirmation: When the safety response indicates
`require_confirmation`, you must implement user confirmation before
execution. See Acknowledge safety decision for
sample code.

- 

 Provide custom safety instructions: In addition to the built-in user
confirmation checks, developers may optionally add a custom system
instruction 
that enforces their own safety policies, either to block certain model
actions or require user confirmation before the model takes certain
high-stakes irreversible actions. Here is an example of a custom safety
system instruction you may include when interacting with the model.

 
 
 

#### Example safety instructions

 

Set your custom safety rules as a system instruction:

 

```
    ## **RULE 1: Seek User Confirmation (USER_CONFIRMATION)**

    This is your first and most important check. If the next required action falls
    into any of the following categories, you MUST stop immediately, and seek the
    user's explicit permission.

    **Procedure for Seeking Confirmation:**  * **For Consequential Actions:**
    Perform all preparatory steps (e.g., navigating, filling out forms, typing a
    message). You will ask for confirmation **AFTER** all necessary information is
    entered on the screen, but **BEFORE** you perform the final, irreversible action
    (e.g., before clicking "Send", "Submit", "Confirm Purchase", "Share").  * **For
    Prohibited Actions:** If the action is strictly forbidden (e.g., accepting legal
    terms, solving a CAPTCHA), you must first inform the user about the required
    action and ask for their confirmation to proceed.

    **USER_CONFIRMATION Categories:**

    *   **Consent and Agreements:** You are FORBIDDEN from accepting, selecting, or
        agreeing to any of the following on the user's behalf. You must ask the
        user to confirm before performing these actions.
        *   Terms of Service
        *   Privacy Policies
        *   Cookie consent banners
        *   End User License Agreements (EULAs)
        *   Any other legally significant contracts or agreements.
    *   **Robot Detection:** You MUST NEVER attempt to solve or bypass the
        following. You must ask the user to confirm before performing these actions.
    *   CAPTCHAs (of any kind)
        *   Any other anti-robot or human-verification mechanisms, even if you are
            capable.
    *   **Financial Transactions:**
        *   Completing any purchase.
        *   Managing or moving money (e.g., transfers, payments).
        *   Purchasing regulated goods or participating in gambling.
    *   **Sending Communications:**
        *   Sending emails.
        *   Sending messages on any platform (e.g., social media, chat apps).
        *   Posting content on social media or forums.
    *   **Accessing or Modifying Sensitive Information:**
        *   Health, financial, or government records (e.g., medical history, tax
            forms, passport status).
        *   Revealing or modifying sensitive personal identifiers (e.g., SSN, bank
            account number, credit card number).
    *   **User Data Management:**
        *   Accessing, downloading, or saving files from the web.
        *   Sharing or sending files/data to any third party.
        *   Transferring user data between systems.
    *   **Browser Data Usage:**
        *   Accessing or managing Chrome browsing history, bookmarks, autofill data,
            or saved passwords.
    *   **Security and Identity:**
        *   Logging into any user account.
        *   Any action that involves misrepresentation or impersonation (e.g.,
            creating a fan account, posting as someone else).
    *   **Insurmountable Obstacles:** If you are technically unable to interact with
        a user interface element or are stuck in a loop you cannot resolve, ask the
        user to take over.
    ---

    ## **RULE 2: Default Behavior (ACTUATE)**

    If an action does **NOT** fall under the conditions for `USER_CONFIRMATION`,
    your default behavior is to **Actuate**.

    **Actuation Means:**  You MUST proactively perform all necessary steps to move
    the user's request forward. Continue to actuate until you either complete the
    non-consequential task or encounter a condition defined in Rule 1.

    *   **Example 1:** If asked to send money, you will navigate to the payment
        portal, enter the recipient's details, and enter the amount. You will then
        **STOP** as per Rule 1 and ask for confirmation before clicking the final
        "Send" button.
    *   **Example 2:** If asked to post a message, you will navigate to the site,
        open the post composition window, and write the full message. You will then
        **STOP** as per Rule 1 and ask for confirmation before clicking the final
        "Post" button.

        After the user has confirmed, remember to get the user's latest screen
        before continuing to perform actions.

    # Final Response Guidelines:
    Write final response to the user in the following cases:
    - User confirmation
    - When the task is complete or you have enough information to respond to the user
```

 
 
 

 
- 

 Secure execution environment: Run your agent in a secure, sandboxed
environment to limit its potential impact (e.g., A sandboxed virtual machine
(VM), a container (e.g., Docker), or a dedicated browser profile with limited
permissions).

- 

 Input sanitization: Sanitize all user-generated text in prompts to
mitigate the risk of unintended instructions or prompt injection. This is a
helpful layer of security, but not a replacement for a secure execution
environment.

- 

 Content guardrails: Use guardrails and content safety
APIs to evaluate user inputs,
tool input and output, an agent's response for appropriateness, prompt
injection, and jailbreak detection.

- 

 Allowlists and blocklists: Implement filtering mechanisms to control
where the model can navigate and what it can do. A blocklist of prohibited
websites is a good starting point, while a more restrictive allowlist is
even more secure.

- 

 Observability and logging: Maintain detailed logs for debugging,
auditing, and incident response. Your client should log prompts,
screenshots, model-suggested actions (function_call), safety responses, and
all actions ultimately executed by the client.

- 

 Environment management: Ensure the GUI environment is consistent.
Unexpected pop-ups, notifications, or changes in layout can confuse the
model. Start from a known, clean state for each new task if possible.

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`gemini-2.5-computer-use-preview-10-2025`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Image, text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

128,000

 
 
 

 Output token limit 

 

64,000

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-computer-use-preview-10-2025`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 

## What's next

- Experiment with Computer Use in the Browserbase demo
environment .

- Check out the Reference
implementation for example
code.

- Learn about other Gemini API tools:

 Function calling 

- Grounding with Google Search 

 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### Function calling with the Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/function-calling#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Function calling with the Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Function calling with the Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Function calling lets you connect models to external tools and APIs.
Instead of generating text responses, the model determines when to call specific
functions and provides the necessary parameters to execute real-world actions.
This allows the model to act as a bridge between natural language and real-world
actions and data. Function calling has 3 primary use cases:

- Augment Knowledge: Access information from external sources like
databases, APIs, and knowledge bases.

- Extend Capabilities: Use external tools to perform computations and
extend the limitations of the model, such as using a calculator or creating
charts.

- Take Actions: Interact with external systems using APIs, such as
scheduling appointments, creating invoices, sending emails, or controlling
smart home devices.

 
 
 
 
 

 
 

### Python

 

```
from google import genai
from google.genai import types

# Define the function declaration for the model
schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[schedule_meeting_function])
config = types.GenerateContentConfig(tools=[tools])

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.",
    config=config,
)

# Check for a function call
if response.candidates[0].content.parts[0].function_call:
    function_call = response.candidates[0].content.parts[0].function_call
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.args}")
    #  In a real app, you would call your function here:
    #  result = schedule_meeting(**function_call.args)
else:
    print("No function call found in the response.")
    print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Type } from '@google/genai';

// Configure the client
const ai = new GoogleGenAI({});

// Define the function declaration for the model
const scheduleMeetingFunctionDeclaration = {
  name: 'schedule_meeting',
  description: 'Schedules a meeting with specified attendees at a given time and date.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      attendees: {
        type: Type.ARRAY,
        items: { type: Type.STRING },
        description: 'List of people attending the meeting.',
      },
      date: {
        type: Type.STRING,
        description: 'Date of the meeting (e.g., "2024-07-29")',
      },
      time: {
        type: Type.STRING,
        description: 'Time of the meeting (e.g., "15:00")',
      },
      topic: {
        type: Type.STRING,
        description: 'The subject or topic of the meeting.',
      },
    },
    required: ['attendees', 'date', 'time', 'topic'],
  },
};

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: 'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',
  config: {
    tools: [{
      functionDeclarations: [scheduleMeetingFunctionDeclaration]
    }],
  },
});

// Check for function calls in the response
if (response.functionCalls && response.functionCalls.length > 0) {
  const functionCall = response.functionCalls[0]; // Assuming one function call
  console.log(`Function to call: ${functionCall.name}`);
  console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);
  // In a real app, you would call your actual function here:
  // const result = await scheduleMeeting(functionCall.args);
} else {
  console.log("No function call found in the response.");
  console.log(response.text);
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning."
          }
        ]
      }
    ],
    "tools": [
      {
        "functionDeclarations": [
          {
            "name": "schedule_meeting",
            "description": "Schedules a meeting with specified attendees at a given time and date.",
            "parameters": {
              "type": "object",
              "properties": {
                "attendees": {
                  "type": "array",
                  "items": {"type": "string"},
                  "description": "List of people attending the meeting."
                },
                "date": {
                  "type": "string",
                  "description": "Date of the meeting (e.g., '2024-07-29')"
                },
                "time": {
                  "type": "string",
                  "description": "Time of the meeting (e.g., '15:00')"
                },
                "topic": {
                  "type": "string",
                  "description": "The subject or topic of the meeting."
                }
              },
              "required": ["attendees", "date", "time", "topic"]
            }
          }
        ]
      }
    ]
  }'
```

 
 

## How function calling works

 

Function calling involves a structured interaction between your application, the
model, and external functions. Here's a breakdown of the process:

- Define Function Declaration: Define the function declaration in your
application code. Function Declarations describe the function's name,
parameters, and purpose to the model.

- Call LLM with function declarations: Send user prompt along with the
function declaration(s) to the model. It analyzes the request and determines
if a function call would be helpful. If so, it responds with a structured
JSON object.

- Execute Function Code (Your Responsibility): The Model does not 
execute the function itself. It's your application's responsibility to
process the response and check for Function Call, if

 Yes : Extract the name and args of the function and execute the
corresponding function in your application.

- No: The model has provided a direct text response to the prompt
(this flow is less emphasized in the example but is a possible outcome).

 
- Create User friendly response: If a function was executed, capture the
result and send it back to the model in a subsequent turn of the
conversation. It will use the result to generate a final, user-friendly
response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex
interactions and workflows. The model also supports calling multiple functions
in a single turn ( parallel function
calling ) and in
sequence ( compositional function
calling ).

### Step 1: Define a function declaration

Define a function and its declaration within your application code that allows
users to set light values and make an API request. This function could call
external services or APIs.

 
 

### Python

 

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

 
 

### JavaScript

 

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

 
 

### Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to
use them. It analyzes the prompt and function declarations and decides whether
to respond directly or to call a function. If a function is called, the response
object will contain a function call suggestion.

 
 

### Python

 

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

 
 

The model then returns a `functionCall` object in an OpenAPI compatible
schema specifying how to call one or more of the declared functions in order to
respond to the user's question.

 
 

### Python

 

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

 
 

### JavaScript

 

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

 
 

### Step 3: Execute set_light_values function code

Extract the function call details from the model's response, parse the arguments
, and execute the `set_light_values` function.

 
 

### Python

 

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

 
 

### JavaScript

 

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

 
 

### Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can
incorporate this information into its final response to the user.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

client = genai.Client()
final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

 
 

### JavaScript

 

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

 
 

This completes the function calling flow. The model successfully used the
`set_light_values` function to perform the request action of the user.

## Function declarations

When you implement function calling in a prompt, you create a `tools` object,
which contains one or more `function declarations`. You define functions using
JSON, specifically with a select subset 
of the OpenAPI schema format. A
single function declaration can include the following parameters:

- `name` (string): A unique name for the function (`get_weather_forecast`,
`send_email`). Use descriptive names without spaces or special characters
(use underscores or camelCase).

- `description` (string): A clear and detailed explanation of the function's
purpose and capabilities. This is crucial for the model to understand when
to use the function. Be specific and provide examples if helpful ("Finds
theaters based on location and optionally movie title which is currently
playing in theaters.").

- `parameters` (object): Defines the input parameters the function
expects.

 `type` (string): Specifies the overall data type, such as `object`.

- `properties` (object): Lists individual parameters, each with:

 `type` (string): The data type of the parameter, such as `string`,
`integer`, `boolean, array`.

- `description` (string): A description of the parameter's purpose and
format. Provide examples and constraints ("The city and state,
e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").

- `enum` (array, optional): If the parameter values are from a fixed
set, use "enum" to list the allowed values instead of just describing
them in the description. This improves accuracy ("enum":
["daylight", "cool", "warm"]).

 
- `required` (array): An array of strings listing the parameter names that
are mandatory for the function to operate.

 

You can also construct `FunctionDeclarations` from Python functions directly using


```
types.FunctionDeclaration.from_callable(client=client, callable=your_function)
```

.

## Function calling with thinking models

Gemini 3 and 2.5 series models use an internal "thinking" process to reason through requests. This
significantly improves function calling performance,
allowing the model to better determine when to call a function and which
parameters to use. Because the Gemini API is stateless, models use
 thought signatures to maintain context
across multi-turn conversations.

This section covers advanced management of thought signatures and is only
necessary if you're manually constructing API requests (e.g., via REST) or
manipulating conversation history.

 If you're using the Google GenAI SDKs (our
official libraries), you don't need to manage this process . The SDKs
automatically handle the necessary steps, as shown in the earlier
 example .

### Managing conversation history manually

If you modify the conversation history manually, instead of sending the
 complete previous response you
must correctly handle the `thought_signature` included in the model's turn.

Follow these rules to ensure the model's context is preserved:

- Always send the `thought_signature` back to the model inside its original
 `Part` .

- Don't merge a `Part` containing a signature with one that does not. This
breaks the positional context of the thought.

- Don't combine two `Parts` that both contain signatures, as the signature
strings cannot be merged.

#### Gemini 3 thought signatures

In Gemini 3, any `Part` of a model response
may contain a thought signature.
While we generally recommend returning signatures from all `Part` types,
passing back thought signatures is mandatory for function calling. Unless you
are manipulating conversation history manually, the Google GenAI SDK will
handle thought signatures automatically.

If you are manipulating conversation history manually, refer to the
 Thoughts Signatures page for complete
guidance and details on handling thought signatures for Gemini 3.

### Inspecting thought signatures

While not necessary for implementation, you can inspect the response to see the
`thought_signature` for debugging or educational purposes.

 
 

### Python

 

```
import base64
# After receiving a response from a model with thinking enabled
# response = client.models.generate_content(...)

# The signature is attached to the response part containing the function call
part = response.candidates[0].content.parts[0]
if part.thought_signature:
  print(base64.b64encode(part.thought_signature).decode("utf-8"))
```

 
 

### JavaScript

 

```
// After receiving a response from a model with thinking enabled
// const response = await ai.models.generateContent(...)

// The signature is attached to the response part containing the function call
const part = response.candidates[0].content.parts[0];
if (part.thoughtSignature) {
  console.log(part.thoughtSignature);
}
```

 
 

Learn more about limitations and usage of thought signatures, and about thinking
models in general, on the Thinking page.

## Parallel function calling

In addition to single turn function calling, you can also call multiple
functions at once. Parallel function calling lets you execute multiple functions
at once and is used when the functions are not dependent on each other. This is
useful in scenarios like gathering data from multiple independent sources, such
as retrieving customer details from different databases or checking inventory
levels across various warehouses or performing multiple actions such as
converting your apartment into a disco.

 
 

### Python

 

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

 
 

### JavaScript

 

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

 
 

Configure the function calling mode to allow using all of the specified tools.
To learn more, you can read about
 configuring function calling .

 
 

### Python

 

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

 
 

Each of the printed results reflects a single function call that the model has
requested. To send the results back, include the responses in the same order as
they were requested.

The Python SDK supports automatic function calling ,
which automatically converts Python functions to declarations, handles the
function call execution and response cycle for you. Following is an example for
the disco use case.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

 
 

## Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple
function calls together to fulfill a complex request. For example, to answer
"Get the temperature in my current location", the Gemini API might first invoke
a `get_current_location()` function followed by a `get_weather()` function that
takes the location as a parameter.

The following example demonstrates how to implement compositional function
calling using the Python SDK and automatic function calling.

 
 

### Python

This example uses the automatic function calling feature of the
`google-genai` Python SDK. The SDK automatically converts the Python
functions to the required schema, executes the function calls when requested
by the model, and sends the results back to the model to complete the task.

 

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20¬∞C in London, set the thermostat to 20¬∞C, otherwise set it to 18¬∞C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

 

 Expected Output 

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20¬∞C.
```

 
 

### JavaScript

This example shows how to use JavaScript/TypeScript SDK to do comopositional
function calling using a manual execution loop.

 

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20¬∞C in London, set the thermostat to 20¬∞C, otherwise set it to 18¬∞C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

 

 Expected Output 

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25¬∞C in London, so I've set the thermostat to 20¬∞C.
```

 
 

Compositional function calling is a native Live
API feature. This means Live API
can handle the function calling similar to the Python SDK.

 
 

### Python

 

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

 
 

### JavaScript

 

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

 
 

## Function calling modes

The Gemini API lets you control how the model uses the provided tools
(function declarations). Specifically, you can set the mode within
the.`function_calling_config`.

- `AUTO (Default)`: The model decides whether to generate a natural language
response or suggest a function call based on the prompt and context. This is the
most flexible mode and recommended for most scenarios.

- `ANY`: The model is constrained to always predict a function call and
guarantees function schema adherence. If `allowed_function_names` is not
specified, the model can choose from any of the provided function declarations.
If `allowed_function_names` is provided as a list, the model can only choose
from the functions in that list. Use this mode when you require a function
call response to every prompt (if applicable).

- `NONE`: The model is prohibited from making function calls. This is
equivalent to sending a request without any function declarations. Use this to
temporarily disable function calling without removing your tool definitions.

- 

`VALIDATED` (Preview): The model is constrained to predict either function
calls or natural language, and ensures function schema adherence. If
`allowed_function_names` is not provided, the model picks from all of the
available function declarations. If `allowed_function_names` is provided, the
model picks from the set of allowed functions.

 
 

### Python

 

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

 
 

### JavaScript

 

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

 
 

## Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools.
The SDK converts these functions into declarations, manages the function call
execution, and handles the response cycle for you. Define your function with
type hints and a docstring. For optimal results, it is recommended to use
 Google-style docstrings. 
The SDK will then automatically:

- Detect function call responses from the model.

- Call the corresponding Python function in your code.

- Send the function's response back to the model.

- Return the model's final text response.

The SDK currently does not parse argument descriptions into the property
description slots of the generated function declaration. Instead, it sends the
entire docstring as the top-level function description.

 
 

### Python

 

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco, CA

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

 
 

You can disable automatic function calling with:

 
 

### Python

 

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

 
 

### Automatic function schema declaration

The API is able to describe any of the following types. `Pydantic` types are
allowed, as long as the fields defined on them are also composed of allowed
types. Dict types (like `dict[str: int]`) are not well supported here, don't
use them.

 
 

### Python

 

```
AllowedType = (
  int | float | bool | str | list['AllowedType'] | pydantic.BaseModel)
```

 
 

To see what the inferred schema looks like, you can convert it using
 `from_callable` :

 
 

### Python

 

```
from google import genai
from google.genai import types

def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

client = genai.Client()
fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

 
 

## Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with
function calling at the same time. Here's an example that enables two tools,
 Grounding with Google Search and
 code execution , in a request using the
 Live API .

 
 

### Python

 

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

 
 

### JavaScript

 

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

 
 

Python developers can try this out in the Live API Tool Use
notebook .

## Model context protocol (MCP)

 Model Context Protocol (MCP) is
an open standard for connecting AI applications with external tools and data.
MCP provides a common protocol for models to access context, such as functions
(tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and
offering
 automatic tool calling 
for MCP tools. When the model generates an MCP tool call, the Python and
JavaScript client SDK can automatically execute the MCP tool and send the
response back to the model in a subsequent request, continuing this loop until
no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and
`mcp` SDK.

 
 

### Python

Make sure the latest version of the
 `mcp` SDK is installed on
your platform of choice.

 

```
pip install mcp
```

 

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

 
 

### JavaScript

Make sure the latest version of the `mcp` SDK is installed on your platform
of choice.

 

```
npm install @modelcontextprotocol/sdk
```

 

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

 
 

### Limitations with built-in MCP support

Built-in MCP support is a experimental 
feature in our SDKs and has the following limitations:

- Only tools are supported, not resources nor prompts

- It is available for the Python and JavaScript/TypeScript SDK.

- Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're
building.

## Supported models

This section lists models and their function calling capabilities. Experimental
models are not included. You can find a comprehensive capabilities overview on
the model overview page.

 
 
 
 Model 
 Function Calling 
 Parallel Function Calling 
 Compositional Function Calling 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash-Lite 
 X 
 X 
 X 
 
 
 

## Best practices

- Function and Parameter Descriptions: Be extremely clear and specific in
your descriptions. The model relies on these to choose the correct function
and provide appropriate arguments.

- Naming: Use descriptive function names (without spaces, periods, or
dashes).

- Strong Typing: Use specific types (integer, string, enum) for parameters
to reduce errors. If a parameter has a limited set of valid values, use an
enum.

- Tool Selection: While the model can use an arbitrary number of tools,
providing too many can increase the risk of selecting an incorrect or
suboptimal tool. For best results, aim to provide only the relevant tools
for the context or task, ideally keeping the active set to a maximum of
10-20. Consider dynamic tool selection based on conversation context if you
have a large total number of tools.

- Prompt Engineering: 

 Provide context: Tell the model its role (e.g., "You are a helpful
weather assistant.").

- Give instructions: Specify how and when to use functions (e.g., "Don't
guess dates; always use a future date for forecasts.").

- Encourage clarification: Instruct the model to ask clarifying questions
if needed.

 
- 

 Temperature: Use a low temperature (e.g., 0) for more deterministic and
reliable function calls.

- 

 Validation: If a function call has significant consequences (e.g.,
placing an order), validate the call with the user before executing it.

- 

 Check Finish Reason: Always check the `finishReason` 
in the model's response to handle cases where the model failed to generate a
valid function call.

- 

 Error Handling : Implement robust error handling in your functions to
gracefully handle unexpected inputs or API failures. Return informative
error messages that the model can use to generate helpful responses to the
user.

- 

 Security: Be mindful of security when calling external APIs. Use
appropriate authentication and authorization mechanisms. Avoid exposing
sensitive data in function calls.

- 

 Token Limits: Function descriptions and parameters count towards your
input token limit. If you're hitting token limits, consider limiting the
number of functions or the length of the descriptions, break down complex
tasks into smaller, more focused function sets.

## Notes and limitations

- Only a subset of the OpenAPI
schema is supported.

- Supported parameter types in Python are limited.

- Automatic function calling is a Python SDK feature only.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### File Search &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/file-search

- 
 
 
 
 
 
 
 
 
 
 
 File Search  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 File Search 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API enables Retrieval Augmented Generation ("RAG") through the File
Search tool. File Search imports, chunks, and indexes your data to
enable fast retrieval of relevant information based on a user's prompt. This
information is then provided as context to the model, allowing the model to
provide more accurate and relevant answers.

You can use the `uploadToFileSearchStore` 
API to directly upload an existing file to your File Search store, or separately
upload and then `importFile` 
if you want to create the file at the same time.

## Directly upload to File Search store

This examples shows how to directly upload a file to a file store:

 
 

### Python

 

```
from google import genai
from google.genai import types
import time

client = genai.Client()

# Create the File Search store with an optional display name
file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

# Upload and import a file into the File Search store, supply a file name which will be visible in citations
operation = client.file_search_stores.upload_to_file_search_store(
  file='sample.txt',
  file_search_store_name=file_search_store.name,
  config={
      'display_name' : 'display-file-name',
  }
)

# Wait until import is complete
while not operation.done:
    time.sleep(5)
    operation = client.operations.get(operation)

# Ask a question about the file
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="""Can you tell me about Robert Graves""",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name]
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
const { GoogleGenAI } = require('@google/genai');

const ai = new GoogleGenAI({});

async function run() {
  // Create the File Search store with an optional display name
  const fileSearchStore = await ai.fileSearchStores.create({
    config: { displayName: 'your-fileSearchStore-name' }
  });

  // Upload and import a file into the File Search store, supply a file name which will be visible in citations
  let operation = await ai.fileSearchStores.uploadToFileSearchStore({
    file: 'file.txt',
    fileSearchStoreName: fileSearchStore.name,
    config: {
      displayName: 'file-name',
    }
  });

  // Wait until import is complete
  while (!operation.done) {
    await new Promise(resolve => setTimeout(resolve, 5000));
    operation = await ai.operations.get({ operation });
  }

  // Ask a question about the file
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Can you tell me about Robert Graves",
    config: {
      tools: [
        {
          fileSearch: {
            fileSearchStoreNames: [fileSearchStore.name]
          }
        }
      ]
    }
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \ "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" > /dev/null

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null

# Generate content using the FileSearchStore
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "What does the research say about ..."}]          
            }],
            "tools": [{
                "file_search": { "file_search_store_names":["'$STORE_NAME'"] }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Check the API reference for `uploadToFileSearchStore` for more information.

## Importing files

Alternatively, you can upload an existing file and import it to your file store:

 
 

### Python

 

```
from google import genai
from google.genai import types
import time

client = genai.Client()

# Upload the file using the Files API, supply a file name which will be visible in citations
sample_file = client.files.upload(file='sample.txt', config={'name': 'display_file_name'})

# Create the File Search store with an optional display name
file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

# Import the file into the File Search store
operation = client.file_search_stores.import_file(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name
)

# Wait until import is complete
while not operation.done:
    time.sleep(5)
    operation = client.operations.get(operation)

# Ask a question about the file
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="""Can you tell me about Robert Graves""",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name]
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
const { GoogleGenAI } = require('@google/genai');

const ai = new GoogleGenAI({});

async function run() {
  // Upload the file using the Files API, supply a file name which will be visible in citations
  const sampleFile = await ai.files.upload({
    file: 'sample.txt',
    config: { name: 'file-name' }
  });

  // Create the File Search store with an optional display name
  const fileSearchStore = await ai.fileSearchStores.create({
    config: { displayName: 'your-fileSearchStore-name' }
  });

  // Import the file into the File Search store
  let operation = await ai.fileSearchStores.importFile({
    fileSearchStoreName: fileSearchStore.name,
    fileName: sampleFile.name
  });

  // Wait until import is complete
  while (!operation.done) {
    await new Promise(resolve => setTimeout(resolve, 5000));
    operation = await ai.operations.get({ operation: operation });
  }

  // Ask a question about the file
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Can you tell me about Robert Graves",
    config: {
      tools: [
        {
          fileSearch: {
            fileSearchStoreNames: [fileSearchStore.name]
          }
        }
      ]
    }
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -X POST "https://generativelanguage.googleapis.com/upload/v1beta/files?key=${GEMINI_API_KEY}" \
  -D "${TMP_HEADER}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" 2> /dev/null

UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# Upload the actual bytes.
curl -s -X POST "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.name" file_info.json)

# Import files into the file search store
operation_name=$(curl "https://generativelanguage.googleapis.com/v1beta/${STORE_NAME}:importFile?key=${GEMINI_API_KEY}" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
        "file_name":'$file_uri'
    }' | jq -r .name)

# Wait for long running operation to complete
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "https://generativelanguage.googleapis.com/v1beta/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    break
  fi
  # Wait for 10 seconds before checking again.
  sleep 10
done

# Generate content using the FileSearchStore
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "What does the research say about ..."}]          
            }],
            "tools": [{
                "file_search": { "file_search_store_names":["'$STORE_NAME'"] }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Check the API reference for `importFile` for more information.

## Chunking configuration

When you import a file into a File Search store, it's automatically broken down
into chunks, embedded, indexed, and uploaded to your File Search store. If you
need more control over the chunking strategy, you can specify a
 `chunking_config` setting
to set a maximum number of tokens per chunk and maximum number of overlapping
tokens.

 
 

### Python

 

```
# Upload and import and upload the file into the File Search store with a custom chunking configuration
operation = client.file_search_stores.upload_to_file_search_store(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name,
    config={
        'chunking_config': {
          'white_space_config': {
            'max_tokens_per_chunk': 200,
            'max_overlap_tokens': 20
          }
        }
    }
)
```

 
 

### JavaScript

 

```
// Upload and import and upload the file into the File Search store with a custom chunking configuration
let operation = await ai.fileSearchStores.uploadToFileSearchStore({
  file: 'file.txt',
  fileSearchStoreName: fileSearchStore.name,
  config: {
    displayName: 'file-name',
    chunkingConfig: {
      whiteSpaceConfig: {
        maxTokensPerChunk: 200,
        maxOverlapTokens: 20
      }
    }
  }
});
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \ "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" > /dev/null
  -d '{
        "chunking_config": {
          "white_space_config": {
            "max_tokens_per_chunk": 200,
            "max_overlap_tokens": 20
          }
        }
    }'

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null
```

 
 

To use your File Search store, pass it as a tool to the `generateContent`
method, as shown in the Upload and Import examples.

## How it works

File Search uses a technique called semantic search to find information relevant
to the user prompt. Unlike traditional keyword-based search, semantic search
understands the meaning and context of your query.

When you import a file, it's converted into numerical representations called
 embeddings , which capture the semantic meaning of
the text. These embeddings are stored in a specialized File Search database.
When you make a query, it's also converted into an embedding. Then the system
performs a File Search to find the most similar and relevant document chunks
from the File Search store.

Here's a breakdown of the process for using the File Search
`uploadToFileSearchStore` API:

- 

 Create a File Search store : A File Search store contains the processed
data from your files. It's the persistent container for the embeddings that the
semantic search will operate on.

- 

 Upload a file and import into a File Search store : Simultaneously upload
a file and import the results into your File Search store. This creates a
temporary `File` object, which is a reference to your raw document. That data is
then chunked, converted into File Search embeddings, and indexed. The `File`
object gets deleted after 48 hours, while the data imported into the File Search
store will be stored indefinitely until you choose to delete it.

- 

 Query with File Search : Finally, you use the `FileSearch` tool in a
`generateContent` call. In the tool configuration, you specify a
`FileSearchRetrievalResource`, which points to the `FileSearchStore` you want to
search. This tells the model to perform a semantic search on that specific
File Search store to find relevant information to ground its response.

 
 
 The indexing and querying process of File Search 
 

In this diagram, the dotted line from from Documents to Embedding model 
(using `gemini-embedding-001` )
represents the `uploadToFileSearchStore` API (bypassing File storage ).
Otherwise, using the Files API to separately create
and then import files moves the indexing process from Documents to
 File storage and then to Embedding model .

## File Search stores

A File Search store is a container for your document embeddings. While raw files
uploaded through the File API are deleted after 48 hours, the data imported into
a File Search store is stored indefinitely until you manually delete it. You can
create multiple File Search stores to organize your documents. The
`FileSearchStore` API lets you create, list, get, and delete to manage your file
search stores. File Search store names are globally scoped.

Here are some examples of how to manage your File Search stores:

 
 

### Python

 

```
# Create a File Search store (including optional display_name for easier reference)
file_search_store = client.file_search_stores.create(config={'display_name': 'my-file_search-store-123'})

# List all your File Search stores
for file_search_store in client.file_search_stores.list():
    print(file_search_store)

# Get a specific File Search store by name
my_file_search_store = client.file_search_stores.get(name='fileSearchStores/my-file_search-store-123')

# Delete a File Search store
client.file_search_stores.delete(name='fileSearchStores/my-file_search-store-123', config={'force': True})
```

 
 

### JavaScript

 

```
// Create a File Search store (including optional display_name for easier reference)
const fileSearchStore = await ai.fileSearchStores.create({
  config: { displayName: 'my-file_search-store-123' }
});

// List all your File Search stores
const fileSearchStores = await ai.fileSearchStores.list();
for await (const store of fileSearchStores) {
  console.log(store);
}

// Get a specific File Search store by name
const myFileSearchStore = await ai.fileSearchStores.get({
  name: 'fileSearchStores/my-file_search-store-123'
});

// Delete a File Search store
await ai.fileSearchStores.delete({
  name: 'fileSearchStores/my-file_search-store-123',
  config: { force: true }
});
```

 
 

### REST

 

```
# Create a File Search store (including optional display_name for easier reference)
curl -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" 
    -d '{ "displayName": "My Store" }'

# List all your File Search stores
curl "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \

# Get a specific File Search store by name
curl "https://generativelanguage.googleapis.com/v1beta/fileSearchStores/my-file_search-store-123?key=${GEMINI_API_KEY}"

# Delete a File Search store
curl -X DELETE "https://generativelanguage.googleapis.com/v1beta/fileSearchStores/my-file_search-store-123?key=${GEMINI_API_KEY}"
```

 
 

The File Search Documents API reference for
methods and fields related to managing documents in your file stores.

## File metadata

You can add custom metadata to your files to help filter them or provide
additional context. Metadata is a set of key-value pairs.

 
 

### Python

 

```
# Import the file into the File Search store with custom metadata
op = client.file_search_stores.import_file(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name,
    custom_metadata=[
        {"key": "author", "string_value": "Robert Graves"},
        {"key": "year", "numeric_value": 1934}
    ]
)
```

 
 

### JavaScript

 

```
// Import the file into the File Search store with custom metadata
let operation = await ai.fileSearchStores.importFile({
  fileSearchStoreName: fileSearchStore.name,
  fileName: sampleFile.name,
  config: {
    customMetadata: [
      { key: "author", stringValue: "Robert Graves" },
      { key: "year", numericValue: 1934 }
    ]
  }
});
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \
  "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d '{
        "custom_metadata": [
          {"key": "author", "string_value": "Robert Graves"},
          {"key": "year", "numeric_value": 1934}
        ]
    }' > /dev/null

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null
```

 
 

This is useful when you have multiple documents in a File Search store and want
to search only a subset of them.

 
 

### Python

 

```
# Use the metadata filter to search within a subset of documents
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Tell me about the book 'I, Claudius'",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name],
                    metadata_filter="author=Robert Graves",
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
// Use the metadata filter to search within a subset of documents
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Tell me about the book 'I, Claudius'",
  config: {
    tools: [
      {
        fileSearch: {
          fileSearchStoreNames: [fileSearchStore.name],
          metadataFilter: 'author="Robert Graves"',
        }
      }
    ]
  }
});

console.log(response.text);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "Tell me about the book I, Claudius"}]          
            }],
            "tools": [{
                "file_search": { 
                    "file_search_store_names":["'$STORE_NAME'"],
                    "metadata_filter": "author = \"Robert Graves\""
                }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Guidance on implementing list filter syntax for `metadata_filter` can be found
at google.aip.dev/160 

## Citations

When you use File Search, the model's response may include citations that
specify which parts of your uploaded documents were used to generate the
answer. This helps with fact-checking and verification.

You can access citation information through the `grounding_metadata` attribute
of the response.

 
 

### Python

 

```
print(response.candidates[0].grounding_metadata)
```

 
 

### JavaScript

 

```
console.log(JSON.stringify(response.candidates?.[0]?.groundingMetadata, null, 2));
```

 
 

## Supported models

The following models support File Search:

- `gemini-2.5-pro` 

- `gemini-2.5-flash` 

## Supported file types

File Search supports a wide range of file formats, listed in the following
sections.

 
 
 

### Application file types

 

 - `application/dart`

 - `application/ecmascript`

 - `application/json`

 - `application/ms-java`

 - `application/msword`

 - `application/pdf`

 - `application/sql`

 - `application/typescript`

 - `application/vnd.curl`

 - `application/vnd.dart`

 - `application/vnd.ibm.secure-container`

 - `application/vnd.jupyter`

 - `application/vnd.ms-excel`

 - `application/vnd.oasis.opendocument.text`

 - 

```
application/vnd.openxmlformats-officedocument.presentationml.presentation
```



 - 

```
application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
```



 - 

```
application/vnd.openxmlformats-officedocument.wordprocessingml.document
```



 - 

```
application/vnd.openxmlformats-officedocument.wordprocessingml.template
```



 - `application/x-csh`

 - `application/x-hwp`

 - `application/x-hwp-v5`

 - `application/x-latex`

 - `application/x-php`

 - `application/x-powershell`

 - `application/x-sh`

 - `application/x-shellscript`

 - `application/x-tex`

 - `application/x-zsh`

 - `application/xml`

 - `application/zip`

 

 
 

 
 
 

### Text file types

 - `text/1d-interleaved-parityfec`

 - `text/RED`

 - `text/SGML`

 - `text/cache-manifest`

 - `text/calendar`

 - `text/cql`

 - `text/cql-extension`

 - `text/cql-identifier`

 - `text/css`

 - `text/csv`

 - `text/csv-schema`

 - `text/dns`

 - `text/encaprtp`

 - `text/enriched`

 - `text/example`

 - `text/fhirpath`

 - `text/flexfec`

 - `text/fwdred`

 - `text/gff3`

 - `text/grammar-ref-list`

 - `text/hl7v2`

 - `text/html`

 - `text/javascript`

 - `text/jcr-cnd`

 - `text/jsx`

 - `text/markdown`

 - `text/mizar`

 - `text/n3`

 - `text/parameters`

 - `text/parityfec`

 - `text/php`

 - `text/plain`

 - `text/provenance-notation`

 - `text/prs.fallenstein.rst`

 - `text/prs.lines.tag`

 - `text/prs.prop.logic`

 - `text/raptorfec`

 - `text/rfc822-headers`

 - `text/rtf`

 - `text/rtp-enc-aescm128`

 - `text/rtploopback`

 - `text/rtx`

 - `text/sgml`

 - `text/shaclc`

 - `text/shex`

 - `text/spdx`

 - `text/strings`

 - `text/t140`

 - `text/tab-separated-values`

 - `text/texmacs`

 - `text/troff`

 - `text/tsv`

 - `text/tsx`

 - `text/turtle`

 - `text/ulpfec`

 - `text/uri-list`

 - `text/vcard`

 - `text/vnd.DMClientScript`

 - `text/vnd.IPTC.NITF`

 - `text/vnd.IPTC.NewsML`

 - `text/vnd.a`

 - `text/vnd.abc`

 - `text/vnd.ascii-art`

 - `text/vnd.curl`

 - `text/vnd.debian.copyright`

 - `text/vnd.dvb.subtitle`

 - `text/vnd.esmertec.theme-descriptor`

 - `text/vnd.exchangeable`

 - `text/vnd.familysearch.gedcom`

 - `text/vnd.ficlab.flt`

 - `text/vnd.fly`

 - `text/vnd.fmi.flexstor`

 - `text/vnd.gml`

 - `text/vnd.graphviz`

 - `text/vnd.hans`

 - `text/vnd.hgl`

 - `text/vnd.in3d.3dml`

 - `text/vnd.in3d.spot`

 - `text/vnd.latex-z`

 - `text/vnd.motorola.reflex`

 - `text/vnd.ms-mediapackage`

 - `text/vnd.net2phone.commcenter.command`

 - `text/vnd.radisys.msml-basic-layout`

 - `text/vnd.senx.warpscript`

 - `text/vnd.sosi`

 - `text/vnd.sun.j2me.app-descriptor`

 - `text/vnd.trolltech.linguist`

 - `text/vnd.wap.si`

 - `text/vnd.wap.sl`

 - `text/vnd.wap.wml`

 - `text/vnd.wap.wmlscript`

 - `text/vtt`

 - `text/wgsl`

 - `text/x-asm`

 - `text/x-bibtex`

 - `text/x-boo`

 - `text/x-c`

 - `text/x-c++hdr`

 - `text/x-c++src`

 - `text/x-cassandra`

 - `text/x-chdr`

 - `text/x-coffeescript`

 - `text/x-component`

 - `text/x-csh`

 - `text/x-csharp`

 - `text/x-csrc`

 - `text/x-cuda`

 - `text/x-d`

 - `text/x-diff`

 - `text/x-dsrc`

 - `text/x-emacs-lisp`

 - `text/x-erlang`

 - `text/x-gff3`

 - `text/x-go`

 - `text/x-haskell`

 - `text/x-java`

 - `text/x-java-properties`

 - `text/x-java-source`

 - `text/x-kotlin`

 - `text/x-lilypond`

 - `text/x-lisp`

 - `text/x-literate-haskell`

 - `text/x-lua`

 - `text/x-moc`

 - `text/x-objcsrc`

 - `text/x-pascal`

 - `text/x-pcs-gcd`

 - `text/x-perl`

 - `text/x-perl-script`

 - `text/x-python`

 - `text/x-python-script`

 - `text/x-r-markdown`

 - `text/x-rsrc`

 - `text/x-rst`

 - `text/x-ruby-script`

 - `text/x-rust`

 - `text/x-sass`

 - `text/x-scala`

 - `text/x-scheme`

 - `text/x-script.python`

 - `text/x-scss`

 - `text/x-setext`

 - `text/x-sfv`

 - `text/x-sh`

 - `text/x-siesta`

 - `text/x-sos`

 - `text/x-sql`

 - `text/x-swift`

 - `text/x-tcl`

 - `text/x-tex`

 - `text/x-vbasic`

 - `text/x-vcalendar`

 - `text/xml`

 - `text/xml-dtd`

 - `text/xml-external-parsed-entity`

 - `text/yaml`

 
 

## Rate limits

The File Search API has the following limits to enforce service stability:

- Maximum file size / per document limit : 100 MB

- Total size of project File Search stores (based on user tier):

 Free : 1 GB

- Tier 1 : 10 GB

- Tier 2 : 100 GB

- Tier 3 : 1 TB

 
- Recommendation : Limit the size of each File Search store to under 20 GB to ensure optimal retrieval latencies.

## Pricing

- Developers are charged for embeddings at indexing time based on existing
 embeddings pricing ($0.15 per
1M tokens).

- Storage is free of charge.

- Query time embeddings are free of charge.

- Retrieved document tokens are charged as regular
 context tokens .

## What's next

- Visit the API reference for File Search Stores and File Search Documents .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Long context &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/long-context#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Long context  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Long context 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Many Gemini models come with large context windows of 1 million or more tokens.
Historically, large language models (LLMs) were significantly limited by
the amount of text (or tokens) that could be passed to the model at one time.
The Gemini long context window unlocks many new use cases and developer
paradigms.

The code you already use for cases like text
generation or multimodal
inputs will work without any changes with long context.

This document gives you an overview of what you can achieve using models with
context windows of 1M and more tokens. The page gives a brief overview of
a context window, and explores how developers should think about long context,
various real world use cases for long context, and ways to optimize the usage
of long context.

For the context window sizes of specific models, see the
 Models page.

## What is a context window?

The basic way you use the Gemini models is by passing information (context)
to the model, which will subsequently generate a response. An analogy for the
context window is short term memory. There is a limited amount of information
that can be stored in someone's short term memory, and the same is true for
generative models.

You can read more about how models work under the hood in our generative models
guide .

## Getting started with long context

Earlier versions of generative models were only able to process 8,000
tokens at a time. Newer models pushed this further by accepting 32,000 or even
128,000 tokens. Gemini is the first model capable of accepting 1 million tokens.

In practice, 1 million tokens would look like:

- 50,000 lines of code (with the standard 80 characters per line)

- All the text messages you have sent in the last 5 years

- 8 average length English novels

- Transcripts of over 200 average length podcast episodes

The more limited context windows common in many other models often require
strategies like arbitrarily dropping old messages, summarizing content, using
RAG with vector databases, or filtering prompts to save tokens.

While these techniques remain valuable in specific scenarios, Gemini's extensive
context window invites a more direct approach: providing all relevant
information upfront. Because Gemini models were purpose-built with massive
context capabilities, they demonstrate powerful in-context learning. For
example, using only in-context instructional materials (a 500-page reference
grammar, a dictionary, and ‚âà400 parallel sentences), Gemini
 learned to translate 
from English to Kalamang‚Äîa Papuan language with
fewer than 200 speakers‚Äîwith quality similar to a human learner using the same
materials. This illustrates the paradigm shift enabled by Gemini's long context,
empowering new possibilities through robust in-context learning.

## Long context use cases

While the standard use case for most generative models is still text input, the
Gemini model family enables a new paradigm of multimodal use cases. These
models can natively understand text, video, audio, and images. They are
accompanied by the Gemini API that takes in multimodal file
types for
convenience.

### Long form text

Text has proved to be the layer of intelligence underpinning much of the
momentum around LLMs. As mentioned earlier, much of the practical limitation of
LLMs was because of not having a large enough context window to do certain
tasks. This led to the rapid adoption of retrieval augmented generation (RAG)
and other techniques which dynamically provide the model with relevant
contextual information. Now, with larger and larger context windows, there are
new techniques becoming available which unlock new use cases.

Some emerging and standard use cases for text based long context include:

- Summarizing large corpuses of text

 Previous summarization options with smaller context models would require
a sliding window or another technique to keep state of previous sections
as new tokens are passed to the model

 
- Question and answering

 Historically this was only possible with RAG given the limited amount of
context and models' factual recall being low

 
- Agentic workflows

 Text is the underpinning of how agents keep state of what they have done
and what they need to do; not having enough information about the world
and the agent's goal is a limitation on the reliability of agents

 

 Many-shot in-context learning is one of the
most unique capabilities unlocked by long context models. Research has shown
that taking the common "single shot" or "multi-shot" example paradigm, where the
model is presented with one or a few examples of a task, and scaling that up to
hundreds, thousands, or even hundreds of thousands of examples, can lead to
novel model capabilities. This many-shot approach has also been shown to perform
similarly to models which were fine-tuned for a specific task. For use cases
where a Gemini model's performance is not yet sufficient for a production
rollout, you can try the many-shot approach. As you might explore later in the
long context optimization section, context caching makes this type of high input
token workload much more economically feasible and even lower latency in some
cases.

### Long form video

Video content's utility has long been constrained by the lack of accessibility
of the medium itself. It was hard to skim the content, transcripts often failed
to capture the nuance of a video, and most tools don't process image, text, and
audio together. With Gemini, the long-context text capabilities translate to
the ability to reason and answer questions about multimodal inputs with
sustained performance.

Some emerging and standard use cases for video long context include:

- Video question and answering

- Video memory, as shown with Google's Project Astra 

- Video captioning

- Video recommendation systems, by enriching existing metadata with new
multimodal understanding

- Video customization, by looking at a corpus of data and associated video
metadata and then removing parts of videos that are not relevant to the
viewer

- Video content moderation

- Real-time video processing

When working with videos, it is important to consider how the videos are
processed into tokens , which affects
billing and usage limits. You can learn more about prompting with video files in
the Prompting
guide .

### Long form audio

The Gemini models were the first natively multimodal large language models
that could understand audio. Historically, the typical developer workflow would
involve stringing together multiple domain specific models, like a
speech-to-text model and a text-to-text model, in order to process audio. This
led to additional latency required by performing multiple round-trip requests
and decreased performance usually attributed to disconnected architectures of
the multiple model setup.

Some emerging and standard use cases for audio context include:

- Real-time transcription and translation

- Podcast / video question and answering

- Meeting transcription and summarization

- Voice assistants

You can learn more about prompting with audio files in the Prompting
guide .

## Long context optimizations

The primary optimization when working with long context and the Gemini
models is to use context
caching . Beyond the previous
impossibility of processing lots of tokens in a single request, the other main
constraint was the cost. If you have a "chat with your data" app where a user
uploads 10 PDFs, a video, and some work documents, you would historically have
to work with a more complex retrieval augmented generation (RAG) tool /
framework in order to process these requests and pay a significant amount for
tokens moved into the context window. Now, you can cache the files the user
uploads and pay to store them on a per hour basis. The input / output cost per
request with Gemini Flash for example is ~4x less than the standard
input / output cost, so if
the user chats with their data enough, it becomes a huge cost saving for you as
the developer.

## Long context limitations

In various sections of this guide, we talked about how Gemini models achieve
high performance across various needle-in-a-haystack retrieval evals. These
tests consider the most basic setup, where you have a single needle you are
looking for. In cases where you might have multiple "needles" or specific pieces
of information you are looking for, the model does not perform with the same
accuracy. Performance can vary to a wide degree depending on the context. This
is important to consider as there is an inherent tradeoff between getting the
right information retrieved and cost. You can get ~99% on a single query, but
you have to pay the input token cost every time you send that query. So for 100
pieces of information to be retrieved, if you needed 99% performance, you would
likely need to send 100 requests. This is a good example of where context
caching can significantly reduce the cost associated with using Gemini models
while keeping the performance high.

## FAQs

### Where is the best place to put my query in the context window?

In most cases, especially if the total context is long, the model's
performance will be better if you put your query / question at the end of the
prompt (after all the other context).

### Do I lose model performance when I add more tokens to a query?

Generally, if you don't need tokens to be passed to the model, it is best to
avoid passing them. However, if you have a large chunk of tokens with some
information and want to ask questions about that information, the model is
highly capable of extracting that information (up to 99% accuracy in many
cases).

### How can I lower my cost with long-context queries?

If you have a similar set of tokens / context that you want to re-use many
times, context caching can help reduce the costs
associated with asking questions about that information.

### Does the context length affect the model latency?

There is some fixed amount of latency in any given request, regardless of the
size, but generally longer queries will have higher latency (time to first
token).

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Get started with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live

- 
 
 
 
 
 
 
 
 
 
 
 Get started with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Get started with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Live API enables low-latency, real-time voice and video interactions with
Gemini. It processes continuous streams of audio, video, or text to deliver
immediate, human-like spoken responses, creating a natural conversational
experience for your users.

 

Live API offers a comprehensive set of features such as Voice Activity Detection , tool use and function calling , session management (for managing long running conversations) and ephemeral tokens (for secure client-sided authentication). 

This page gets you up and running with examples and basic code samples.

 
 Try the Live API in Google AI Studio mic 
 

## Example applications

Check out the following example applications that illustrate how to use Live API
for end-to-end use cases:

- Live audio starter app on AI Studio,
using JavaScript libraries to connect to Live API and stream bidirectional
audio through your microphone and speakers.

- Live API Python cookbook 
using Pyaudio that connects to Live API.

## Partner integrations

If you prefer a simpler development process, you can use Daily , LiveKit or Voximplant . These are third-party partner platforms that have already integrated the Gemini Live API over the WebRTC protocol to streamline the development of real-time audio and video applications.

## Choose an implementation approach

When integrating with Live API, you'll need to choose one of the following
implementation approaches:

- Server-to-server : Your backend connects to the Live API using
 WebSockets . Typically, your client sends stream data (audio, video,
text) to your server, which then forwards it to the Live API.

- Client-to-server : Your frontend code connects directly to the Live API
using WebSockets to stream data, bypassing your backend.

## Get started

This example reads a WAV file , sends it in the correct format, and saves
the received data as WAV file.

You can send audio by converting it to 16-bit PCM, 16kHz, mono format, and you
can receive audio by setting `AUDIO` as response modality. The output uses a
sample rate of 24kHz.

 
 

### Python

 

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()

# New native audio model:
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
  "response_modalities": ["AUDIO"],
  "system_instruction": "You are a helpful assistant and answer in a friendly tone.",
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
// WARNING: Do not use API keys in client-side (browser based) applications
// Consider using Ephemeral Tokens instead
// More information at: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

// New native audio model:
const model = "gemini-2.5-flash-native-audio-preview-09-2025"

const config = {
  responseModalities: [Modality.AUDIO],
  systemInstruction: "You are a helpful assistant and answer in a friendly tone."
};

async function live() {
    const responseQueue = [];

    async function waitMessage() {
        let done = false;
        let message = undefined;
        while (!done) {
            message = responseQueue.shift();
            if (message) {
                done = true;
            } else {
                await new Promise((resolve) => setTimeout(resolve, 100));
            }
        }
        return message;
    }

    async function handleTurn() {
        const turns = [];
        let done = false;
        while (!done) {
            const message = await waitMessage();
            turns.push(message);
            if (message.serverContent && message.serverContent.turnComplete) {
                done = true;
            }
        }
        return turns;
    }

    const session = await ai.live.connect({
        model: model,
        callbacks: {
            onopen: function () {
                console.debug('Opened');
            },
            onmessage: function (message) {
                responseQueue.push(message);
            },
            onerror: function (e) {
                console.debug('Error:', e.message);
            },
            onclose: function (e) {
                console.debug('Close:', e.reason);
            },
        },
        config: config,
    });

    // Send Audio Chunk
    const fileBuffer = fs.readFileSync("sample.wav");

    // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
    const wav = new WaveFile();
    wav.fromBuffer(fileBuffer);
    wav.toSampleRate(16000);
    wav.toBitDepth("16");
    const base64Audio = wav.toBase64();

    // If already in correct format, you can use this:
    // const fileBuffer = fs.readFileSync("sample.pcm");
    // const base64Audio = Buffer.from(fileBuffer).toString('base64');

    session.sendRealtimeInput(
        {
            audio: {
                data: base64Audio,
                mimeType: "audio/pcm;rate=16000"
            }
        }

    );

    const turns = await handleTurn();

    // Combine audio data strings and save as wave file
    const combinedAudio = turns.reduce((acc, turn) => {
        if (turn.data) {
            const buffer = Buffer.from(turn.data, 'base64');
            const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
            return acc.concat(Array.from(intArray));
        }
        return acc;
    }, []);

    const audioBuffer = new Int16Array(combinedAudio);

    const wf = new WaveFile();
    wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
    fs.writeFileSync('audio.wav', wf.toBuffer());

    session.close();
}

async function main() {
    await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## What's next

- Read the full Live API Capabilities guide for key capabilities and configurations; including Voice Activity Detection and native audio features.

- Read the Tool use guide to learn how to integrate Live API with tools and function calling.

- Read the Session management guide for managing long running conversations.

- Read the Ephemeral tokens guide for secure authentication in client-to-server applications.

- For more information about the underlying WebSockets API, see the WebSockets API reference .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Using tools with Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/tools#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Using tools with Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Using tools with Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Tools extend the capabilities of Gemini models, enabling them to take action in the world, access real-time information, and perform complex computational tasks. Models can use tools in both standard request-response interactions and real-time streaming sessions via the Live API .

The Gemini API provides a suite of fully managed, built-in tools optimized for Gemini models or you can define custom tools using Function Calling .

## Available built-in tools

 
 
 
 Tool 
 Description 
 Use Cases 
 
 

 
 
 Google Search 
 Ground responses in current events and facts from the web to reduce hallucinations. 
 - Answering questions about recent events 
 - Verifying facts with diverse sources 
 
 
 Google Maps 
 Build location-aware assistants that can find places, get directions, and provide rich local context. 
 - Planning travel itineraries with multiple stops 
 - Finding local businesses based on user criteria 
 
 
 Code Execution 
 Allow the model to write and run Python code to solve math problems or process data accurately. 
 - Solving complex mathematical equations 
 - Processing and analyzing text data precisely 
 
 
 URL Context 
 Direct the model to read and analyze content from specific web pages or documents. 
 - Answering questions based on specific URLs or documents 
 - Retrieving information across different web pages 
 
 
 Computer Use (Preview) 
 Enable Gemini to view a screen and generate actions to interact with web browser UIs (Client-side execution). 
 - Automating repetitive web-based workflows 
 - Testing web application user interfaces 
 
 
 File Search 
 Index and search your own documents to enable Retrieval Augmented Generation (RAG). 
 - Searching technical manuals 
 - Question answering over proprietary data 
 
 
 

See the Pricing page for details on costs associated with specific tools.

## How tools execution works

Tools allow the model to request actions during a conversation. The flow differs depending on whether the tool is built-in (managed by Google) or custom (managed by you).

### Built-in tool flow

For built-in tools like Google Search or Code Execution, the entire process happens within one API call:

- You send a prompt: "What is the square root of the latest stock price of GOOG?" 

- Gemini decides it needs tools and executes them on Google's servers (e.g., searches for the stock price, then runs Python code to calculate the square root). 

- Gemini sends back the final answer grounded in the tool results.

### Custom tool flow (Function Calling)

For custom tools and Computer Use, your application handles the execution:

- You send a prompt along with functions (tools) declarations. 

- Gemini might send back a structured JSON to call a specific function (for example, 

```
{"name": "get_order_status", "args": {"order_id": "123"}}
```

). 

- You execute the function in your application or environment. 

- You send the function results back to Gemini. 

- Gemini uses the results to generate a final response or another tool call.

Learn more in the Function calling guide .

## Structured outputs vs. function Calling

Gemini offers two methods for generating structured outputs. Use Function calling when the model needs to perform an intermediate step by connecting to your own tools or data systems. Use Structured Outputs when you strictly need the model's final response to adhere to a specific schema, such as for rendering a custom UI.

## Structured outputs with tools

You can combine Structured Outputs with built-in tools to ensure that model responses grounded in external data or computation still adhere to a strict schema. 

See Structured outputs with tools for code examples.

## Building agents

Agents are systems that use models and tools to complete multi-step tasks. While Gemini provides the reasoning capabilities (the "brain") and the essential tools (the "hands"), you often need an orchestration framework to manage the agent's memory, plan loops, and perform complex tool chaining.

Gemini integrates with leading open-source agent frameworks:

- LangChain / LangGraph : Build stateful, complex application flows and multi-agent systems using graph structures. 

- LlamaIndex : Connect Gemini agents to your private data for RAG-enhanced workflows. 

- CrewAI : Orchestrate collaborative, role-playing autonomous AI agents.

- Vercel AI SDK : Build AI-powered user interfaces and agents in JavaScript/TypeScript. 

- Google ADK : An open-source framework for building and orchestrating interoperable AI agents.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Live API capabilities guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-guide

- 
 
 
 
 
 
 
 
 
 
 
 Live API capabilities guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Live API capabilities guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This is a comprehensive guide that covers capabilities and configurations
available with the Live API.
See Get started with Live API page for a
overview and sample code for common use cases.

## Before you begin

- Familiarize yourself with core concepts: If you haven't already done so,
read the Get started with Live API page first.
This will introduce you to the fundamental principles of the Live API, how it
works, and the different implementation approaches .

- Try the Live API in AI Studio: You may find it useful to try the
Live API in Google AI Studio before you start building. To use the
Live API in Google AI Studio, select Stream .

## Establishing a connection

The following example shows how to create a connection with an API key:

 
 

### Python

 

```
import asyncio
from google import genai

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"
config = {"response_modalities": ["AUDIO"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        print("Session started")
        # Send content...

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        console.debug(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  console.debug("Session started");
  // Send content...

  session.close();
}

main();
```

 
 

## Interaction modalities

The following sections provide examples and supporting context for the different
input and output modalities available in Live API.

### Sending and receiving audio

The most common audio example, audio-to-audio , is covered in the
 Getting started guide.

### Audio formats

Audio data in the Live API is always raw, little-endian,
16-bit PCM. Audio output always uses a sample rate of 24kHz. Input audio
is natively 16kHz, but the Live API will resample if needed
so any sample rate can be sent. To convey the sample rate of input audio, set
the MIME type of each audio-containing Blob to a value
like `audio/pcm;rate=16000`.

### Sending text

Here's how you can send text:

 
 

### Python

 

```
message = "Hello, how are you?"
await session.send_client_content(turns=message, turn_complete=True)
```

 
 

### JavaScript

 

```
const message = 'Hello, how are you?';
session.sendClientContent({ turns: message, turnComplete: true });
```

 
 

#### Incremental content updates

Use incremental updates to send text input, establish session context, or
restore session context. For short contexts you can send turn-by-turn
interactions to represent the exact sequence of events:

 
 

### Python

 

```
turns = [
    {"role": "user", "parts": [{"text": "What is the capital of France?"}]},
    {"role": "model", "parts": [{"text": "Paris"}]},
]

await session.send_client_content(turns=turns, turn_complete=False)

turns = [{"role": "user", "parts": [{"text": "What is the capital of Germany?"}]}]

await session.send_client_content(turns=turns, turn_complete=True)
```

 
 

### JavaScript

 

```
let inputTurns = [
  { "role": "user", "parts": [{ "text": "What is the capital of France?" }] },
  { "role": "model", "parts": [{ "text": "Paris" }] },
]

session.sendClientContent({ turns: inputTurns, turnComplete: false })

inputTurns = [{ "role": "user", "parts": [{ "text": "What is the capital of Germany?" }] }]

session.sendClientContent({ turns: inputTurns, turnComplete: true })
```

 
 

For longer contexts it's recommended to provide a single message summary to free
up the context window for subsequent interactions. See Session Resumption for another method for
loading session context.

### Audio transcriptions

In addition to the model response, you can also receive transcriptions of
both the audio output and the audio input.

To enable transcription of the model's audio output, send
`output_audio_transcription` in the setup config. The transcription language is
inferred from the model's response.

 
 

### Python

 

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
    "response_modalities": ["AUDIO"],
    "output_audio_transcription": {}
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello? Gemini are you there?"

        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.server_content.model_turn:
                print("Model turn:", response.server_content.model_turn)
            if response.server_content.output_transcription:
                print("Transcript:", response.server_content.output_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const config = {
  responseModalities: [Modality.AUDIO],
  outputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.debug('Received output transcription: %s\n', turn.serverContent.outputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

To enable transcription of the model's audio input, send
`input_audio_transcription` in setup config.

 
 

### Python

 

```
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
    "response_modalities": ["AUDIO"],
    "input_audio_transcription": {},
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_data = Path("16000.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_data, mime_type='audio/pcm;rate=16000')
        )

        async for msg in session.receive():
            if msg.server_content.input_transcription:
                print('Transcript:', msg.server_content.input_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const config = {
  responseModalities: [Modality.AUDIO],
  inputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("16000.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }
  );

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
    else if (turn.serverContent && turn.serverContent.inputTranscription) {
      console.debug('Received input transcription: %s\n', turn.serverContent.inputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

### Stream audio and video

 
 
 

### Change voice and language

 Native audio output models support any of the voices
available for our Text-to-Speech (TTS) 
models. You can listen to all the voices in AI Studio .

To specify a voice, set the voice name within the `speechConfig` object as part
of the session configuration:

 
 

### Python

 

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
    },
}
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } } }
};
```

 
 

The Live API supports multiple languages .
 Native audio output models automatically choose
the appropriate language and don't support explicitly setting the language
code.

## Native audio capabilities

Our latest models feature native audio output ,
which provides natural, realistic-sounding speech and improved multilingual
performance. Native audio also enables advanced features like affective
(emotion-aware) dialogue , proactive audio 
(where the model intelligently decides when to respond to input),
and "thinking" .

### Affective dialog

This feature lets Gemini adapt its response style to the input expression and
tone.

To use affective dialog, set the api version to `v1alpha` and set
`enable_affective_dialog` to `true`in the setup message:

 
 

### Python

 

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    enable_affective_dialog=True
)
```

 
 

### JavaScript

 

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  enableAffectiveDialog: true
};
```

 
 

### Proactive audio

When this feature is enabled, Gemini can proactively decide not to respond
if the content is not relevant.

To use it, set the api version to `v1alpha` and configure the `proactivity`
field in the setup message and set `proactive_audio` to `true`:

 
 

### Python

 

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    proactivity={'proactive_audio': True}
)
```

 
 

### JavaScript

 

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  proactivity: { proactiveAudio: true }
}
```

 
 

### Thinking

The latest native audio output model `gemini-2.5-flash-native-audio-preview-09-2025`
supports thinking capabilities , with dynamic
thinking enabled by default.

The `thinkingBudget` parameter guides the model on the number of thinking tokens
to use when generating a response. You can disable thinking by setting
`thinkingBudget` to `0`. For more info on the `thinkingBudget` configuration
details of the model, see the thinking budgets documentation .

 
 

### Python 

 

```
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"]
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024,
    )
)

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

 
 

### JavaScript

 

```
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = {
  responseModalities: [Modality.AUDIO],
  thinkingConfig: {
    thinkingBudget: 1024,
  },
};

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

 
 

Additionally, you can enable thought summaries by setting `includeThoughts` to
`true` in your configuration. See thought summaries 
for more info:

 
 

### Python 

 

```
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"]
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024,
        include_thoughts=True
    )
)
```

 
 

### JavaScript

 

```
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = {
  responseModalities: [Modality.AUDIO],
  thinkingConfig: {
    thinkingBudget: 1024,
    includeThoughts: true,
  },
};
```

 
 

## Voice Activity Detection (VAD)

Voice Activity Detection (VAD) allows the model to recognize when a person is
speaking. This is essential for creating natural conversations, as it allows a
user to interrupt the model at any time.

When VAD detects an interruption, the ongoing generation is canceled and
discarded. Only the information already sent to the client is retained in the
session history. The server then sends a `BidiGenerateContentServerContent` message to report the interruption.

The Gemini server then discards any pending function calls and sends a
`BidiGenerateContentServerContent` message with the IDs of the canceled calls.

 
 

### Python

 

```
async for response in session.receive():
    if response.server_content.interrupted is True:
        # The generation was interrupted

        # If realtime playback is implemented in your application,
        # you should stop playing audio and clear queued playback here.
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.interrupted) {
    // The generation was interrupted

    // If realtime playback is implemented in your application,
    // you should stop playing audio and clear queued playback here.
  }
}
```

 
 

### Automatic VAD

By default, the model automatically performs VAD on
a continuous audio input stream. VAD can be configured with the
 `realtimeInputConfig.automaticActivityDetection` 
field of the setup configuration .

When the audio stream is paused for more than a second (for example,
because the user switched off the microphone), an
 `audioStreamEnd` 
event should be sent to flush any cached audio. The client can resume sending
audio data at any time.

 
 

### Python

 

```
# example audio file to try:
# URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
# !wget -q $URL -O sample.pcm
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        # if stream gets paused, send:
        # await session.send_realtime_input(audio_stream_end=True)

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
// example audio file to try:
// URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
// !wget -q $URL -O sample.pcm
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.pcm");
  const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  // if stream gets paused, send:
  // session.sendRealtimeInput({ audioStreamEnd: true })

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

With `send_realtime_input`, the API will respond to audio automatically based
on VAD. While `send_client_content` adds messages to the model context in
order, `send_realtime_input` is optimized for responsiveness at the expense of
deterministic ordering.

### Automatic VAD configuration

For more control over the VAD activity, you can configure the following
parameters. See API reference for more
info.

 
 

### Python

 

```
from google.genai import types

config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {
        "automatic_activity_detection": {
            "disabled": False, # default
            "start_of_speech_sensitivity": types.StartSensitivity.START_SENSITIVITY_LOW,
            "end_of_speech_sensitivity": types.EndSensitivity.END_SENSITIVITY_LOW,
            "prefix_padding_ms": 20,
            "silence_duration_ms": 100,
        }
    }
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality, StartSensitivity, EndSensitivity } from '@google/genai';

const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: false, // default
      startOfSpeechSensitivity: StartSensitivity.START_SENSITIVITY_LOW,
      endOfSpeechSensitivity: EndSensitivity.END_SENSITIVITY_LOW,
      prefixPaddingMs: 20,
      silenceDurationMs: 100,
    }
  }
};
```

 
 

### Disable automatic VAD

Alternatively, the automatic VAD can be disabled by setting


```
realtimeInputConfig.automaticActivityDetection.disabled
```

 to `true` in the setup
message. In this configuration the client is responsible for detecting user
speech and sending
 `activityStart` 
and `activityEnd` 
messages at the appropriate times. An `audioStreamEnd` isn't sent in
this configuration. Instead, any interruption of the stream is marked by
an `activityEnd` message.

 
 

### Python

 

```
config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {"automatic_activity_detection": {"disabled": True}},
}

async with client.aio.live.connect(model=model, config=config) as session:
    # ...
    await session.send_realtime_input(activity_start=types.ActivityStart())
    await session.send_realtime_input(
        audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
    )
    await session.send_realtime_input(activity_end=types.ActivityEnd())
    # ...
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: true,
    }
  }
};

session.sendRealtimeInput({ activityStart: {} })

session.sendRealtimeInput(
  {
    audio: {
      data: base64Audio,
      mimeType: "audio/pcm;rate=16000"
    }
  }

);

session.sendRealtimeInput({ activityEnd: {} })
```

 
 

## Token count

You can find the total number of consumed tokens in the
 usageMetadata field of the returned server message.

 
 

### Python

 

```
async for message in session.receive():
    # The server will periodically send messages that include UsageMetadata.
    if message.usage_metadata:
        usage = message.usage_metadata
        print(
            f"Used {usage.total_token_count} tokens in total. Response token breakdown:"
        )
        for detail in usage.response_tokens_details:
            match detail:
                case types.ModalityTokenCount(modality=modality, token_count=count):
                    print(f"{modality}: {count}")
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.usageMetadata) {
    console.debug('Used %s tokens in total. Response token breakdown:\n', turn.usageMetadata.totalTokenCount);

    for (const detail of turn.usageMetadata.responseTokensDetails) {
      console.debug('%s\n', detail);
    }
  }
}
```

 
 

## Media resolution

You can specify the media resolution for the input media by setting the
`mediaResolution` field as part of the session configuration:

 
 

### Python

 

```
from google.genai import types

config = {
    "response_modalities": ["AUDIO"],
    "media_resolution": types.MediaResolution.MEDIA_RESOLUTION_LOW,
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality, MediaResolution } from '@google/genai';

const config = {
    responseModalities: [Modality.TEXT],
    mediaResolution: MediaResolution.MEDIA_RESOLUTION_LOW,
};
```

 
 

## Limitations

Consider the following limitations of the Live API
when you plan your project.

### Response modalities

You can only set one response modality (`TEXT` or `AUDIO`) per session in the
session configuration. Setting both results in a config error message. This
means that you can configure the model to respond with either text or audio,
but not both in the same session.

### Client authentication

The Live API only provides server-to-server authentication
by default. If you're implementing your Live API application
using a client-to-server approach , you need to use
 ephemeral tokens to mitigate security
risks.

### Session duration

Audio-only sessions are limited to 15 minutes,
and audio plus video sessions are limited to 2 minutes.
However, you can configure different session management techniques for unlimited extensions on session duration.

### Context window

A session has a context window limit of:

- 128k tokens for native audio output models

- 32k tokens for other Live API models

## Supported languages

Live API supports the following languages.

 
 
 
 
 
 
 
 
 
 Language 
 BCP-47 Code 
 Language 
 BCP-47 Code 
 
 
 
 
 German (Germany) 
 `de-DE` 
 English (Australia)* 
 `en-AU` 
 
 
 English (UK)* 
 `en-GB` 
 English (India) 
 `en-IN` 
 
 
 English (US) 
 `en-US` 
 Spanish (US) 
 `es-US` 
 
 
 French (France) 
 `fr-FR` 
 Hindi (India) 
 `hi-IN` 
 
 
 Portuguese (Brazil) 
 `pt-BR` 
 Arabic (Generic) 
 `ar-XA` 
 
 
 Spanish (Spain)* 
 `es-ES` 
 French (Canada)* 
 `fr-CA` 
 
 
 Indonesian (Indonesia) 
 `id-ID` 
 Italian (Italy) 
 `it-IT` 
 
 
 Japanese (Japan) 
 `ja-JP` 
 Turkish (Turkey) 
 `tr-TR` 
 
 
 Vietnamese (Vietnam) 
 `vi-VN` 
 Bengali (India) 
 `bn-IN` 
 
 
 Gujarati (India)* 
 `gu-IN` 
 Kannada (India)* 
 `kn-IN` 
 
 
 Marathi (India) 
 `mr-IN` 
 Malayalam (India)* 
 `ml-IN` 
 
 
 Tamil (India) 
 `ta-IN` 
 Telugu (India) 
 `te-IN` 
 
 
 Dutch (Netherlands) 
 `nl-NL` 
 Korean (South Korea) 
 `ko-KR` 
 
 
 Mandarin Chinese (China)* 
 `cmn-CN` 
 Polish (Poland) 
 `pl-PL` 
 
 
 Russian (Russia) 
 `ru-RU` 
 Thai (Thailand) 
 `th-TH` 
 
 
 

 Languages marked with an asterisk (*) are not available for Native audio .

## What's next

- Read the Tool Use and
 Session Management guides for essential
information on using the Live API effectively.

- Try the Live API in Google AI Studio .

- For more info about the Live API models, see Gemini 2.5 Flash Native Audio 
on the Models page.

- Try more examples in the Live API cookbook ,
the Live API Tools cookbook ,
and the Live API Get Started script .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Grounding with Google Search &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/google-search#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Grounding with Google Search  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Grounding with Google Search 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

Grounding with Google Search connects the Gemini model to real-time web content
and works with all available languages. This allows
Gemini to provide more accurate answers and cite verifiable sources beyond its
knowledge cutoff.

Grounding helps you build applications that can:

- Increase factual accuracy: Reduce model hallucinations by basing
responses on real-world information.

- Access real-time information: Answer questions about recent events and
topics.

- 

 Provide citations: Build user trust by showing the sources for the
model's claims.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const groundingTool = {
  googleSearch: {},
};

const config = {
  tools: [groundingTool],
};

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

 
 

You can learn more by trying the Search tool
notebook .

## How grounding with Google Search works

When you enable the `google_search` tool, the model handles the entire workflow
of searching, processing, and citing information automatically.

 

- User Prompt: Your application sends a user's prompt to the Gemini API
with the `google_search` tool enabled.

- Prompt Analysis: The model analyzes the prompt and determines if a
Google Search can improve the answer.

- Google Search: If needed, the model automatically generates one or
multiple search queries and executes them.

- Search Results Processing: The model processes the search results,
synthesizes the information, and formulates a response.

- Grounded Response: The API returns a final, user-friendly response that
is grounded in the search results. This response includes the model's text
answer and `groundingMetadata` with the search queries, web results, and
citations.

## Understanding the grounding response

When a response is successfully grounded, the response includes a
`groundingMetadata` field. This structured data is essential for verifying
claims and building a rich citation experience in your application.

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

 

The Gemini API returns the following information with the `groundingMetadata`:

- `webSearchQueries` : Array of the search queries used. This is useful for
debugging and understanding the model's reasoning process.

- `searchEntryPoint` : Contains the HTML and CSS to render the required Search
Suggestions. Full usage requirements are detailed in the Terms of
Service .

- `groundingChunks` : Array of objects containing the web sources (`uri` and
`title`).

- `groundingSupports` : Array of chunks to connect model response `text` to
the sources in `groundingChunks`. Each chunk links a text `segment` (defined
by `startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This
is the key to building inline citations.

Grounding with Google Search can also be used in combination with the URL
context tool to ground responses in both public
web data and the specific URLs you provide.

## Attributing sources with inline citations

The API returns structured citation data, giving you complete control over how
you display sources in your user interface. You can use the `groundingSupports`
and `groundingChunks` fields to link the model's statements directly to their
sources. Here is a common pattern for processing the metadata to create a
response with inline, clickable citations.

 
 

### Python

 

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

 
 

### JavaScript

 

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

 
 

The new response with inline citations will look like this:

 

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

 

## Pricing

When you use Grounding with Google Search, your project is billed for each
search query that the model decides to execute. If the model decides to execute
multiple search queries to answer a single prompt (for example, searching for
`"UEFA Euro 2024 winner"` and `"Spain vs England Euro 2024 final score"` within
the same API call), this counts as two billable uses of the tool for that
request.

For detailed pricing information, see the Gemini API pricing
page .

## Supported models

Experimental and Preview models are not included. You can find their
capabilities on the model
overview page.

 
 
 
 Model 
 Grounding with Google Search 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Flash 
 ‚úîÔ∏è 
 
 
 

## Supported tools combinations

You can use Grounding with Google Search with other tools like
 code execution and
 URL context to power more complex use cases.

## Grounding with Gemini 1.5 Models (Legacy)

While the `google_search` tool is recommended for Gemini 2.0 and later, Gemini
1.5 supports a legacy tool named `google_search_retrieval`. This tool provides a
`dynamic` mode that allows the model to decide whether to perform a search based
on its confidence that the prompt requires fresh information. If the model's
confidence is above a `dynamic_threshold` you set (a value between 0.0 and 1.0),
it will perform a search.

 
 

### Python

 

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

 
 

### JavaScript

 

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

 
 

## What's next

- Try the Grounding with Google Search in the Gemini API
Cookbook .

- Learn about other available tools, like Function Calling .

- Learn how to augment prompts with specific URLs using the URL context
tool .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Tool use with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-tools

- 
 
 
 
 
 
 
 
 
 
 
 Tool use with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Tool use with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Tool use allows Live API to go beyond just conversation by enabling it to
perform actions in the real-world and pull in external context while maintaining
a real time connection.
You can define tools such as Function calling 
 and Google Search with the Live API.

## Overview of supported tools

Here's a brief overview of the available tools for Live API models:

 
 
 Tool 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 Search 
 Yes 
 
 
 Function calling 
 Yes 
 
 
 Google Maps 
 No 
 
 
 Code execution 
 No 
 
 
 URL context 
 No 
 
 

## Function calling

Live API supports function calling, just like regular content generation
requests. Function calling lets the Live API interact with external data and
programs, greatly increasing what your applications can accomplish.

You can define function declarations as part of the session configuration.
After receiving tool calls, the client should respond with a list of
`FunctionResponse` objects using the `session.send_tool_response` method.

See the Function calling tutorial to learn
more.

 
 

### Python 

 

```
import asyncio
import wave
from google import genai
from google.genai import types

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"

# Simple function definitions
turn_on_the_lights = {"name": "turn_on_the_lights"}
turn_off_the_lights = {"name": "turn_off_the_lights"}

tools = [{"function_declarations": [turn_on_the_lights, turn_off_the_lights]}]
config = {"response_modalities": ["AUDIO"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Turn on the lights please"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)
            elif response.tool_call:
                print("The tool was called")
                function_responses = []
                for fc in response.tool_call.function_calls:
                    function_response = types.FunctionResponse(
                        id=fc.id,
                        name=fc.name,
                        response={ "result": "ok" } # simple, hard-coded function response
                    )
                    function_responses.append(function_response)

                await session.send_tool_response(function_responses=function_responses)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

// Simple function definitions
const turn_on_the_lights = { name: "turn_on_the_lights" } // , description: '...', parameters: { ... }
const turn_off_the_lights = { name: "turn_off_the_lights" }

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]

const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Turn on the lights please';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  for (const turn of turns) {
    if (turn.toolCall) {
      console.debug('A tool was called');
      const functionResponses = [];
      for (const fc of turn.toolCall.functionCalls) {
        functionResponses.push({
          id: fc.id,
          name: fc.name,
          response: { result: "ok" } // simple, hard-coded function response
        });
      }

      console.debug('Sending tool response...\n');
      session.sendToolResponse({ functionResponses: functionResponses });
    }
  }

  // Check again for new messages
  turns = await handleTurn();

  // Combine audio data strings and save as wave file
  const combinedAudio = turns.reduce((acc, turn) => {
      if (turn.data) {
          const buffer = Buffer.from(turn.data, 'base64');
          const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
          return acc.concat(Array.from(intArray));
      }
      return acc;
  }, []);

  const audioBuffer = new Int16Array(combinedAudio);

  const wf = new WaveFile();
  wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
  fs.writeFileSync('audio.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

From a single prompt, the model can generate multiple function calls and the
code necessary to chain their outputs. This code executes in a sandbox
environment, generating subsequent BidiGenerateContentToolCall messages.

## Asynchronous function calling

Function calling executes sequentially by default, meaning execution pauses
until the results of each function call are available. This ensures sequential
processing, which means you won't be able to continue interacting with the model
while the functions are being run.

If you don't want to block the conversation, you can tell the model to run the
functions asynchronously. To do so, you first need to add a `behavior` to the
function definitions:

 
 

### Python 

 

```
# Non-blocking function definitions
turn_on_the_lights = {"name": "turn_on_the_lights", "behavior": "NON_BLOCKING"} # turn_on_the_lights will run asynchronously
turn_off_the_lights = {"name": "turn_off_the_lights"} # turn_off_the_lights will still pause all interactions with the model
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality, Behavior } from '@google/genai';

// Non-blocking function definitions
const turn_on_the_lights = {name: "turn_on_the_lights", behavior: Behavior.NON_BLOCKING}

// Blocking function definitions
const turn_off_the_lights = {name: "turn_off_the_lights"}

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]
```

 
 

`NON-BLOCKING` ensures the function runs asynchronously while you can
continue interacting with the model.

Then you need to tell the model how to behave when it receives the
`FunctionResponse` using the `scheduling` parameter. It can either:

- Interrupt what it's doing and tell you about the response it got right away
(`scheduling="INTERRUPT"`),

- Wait until it's finished with what it's currently doing
(`scheduling="WHEN_IDLE"`),

- 

Or do nothing and use that knowledge later on in the discussion
(`scheduling="SILENT"`)

 
 

### Python 

 

```
# for a non-blocking function definition, apply scheduling in the function response:
  function_response = types.FunctionResponse(
      id=fc.id,
      name=fc.name,
      response={
          "result": "ok",
          "scheduling": "INTERRUPT" # Can also be WHEN_IDLE or SILENT
      }
  )
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality, Behavior, FunctionResponseScheduling } from '@google/genai';

// for a non-blocking function definition, apply scheduling in the function response:
const functionResponse = {
  id: fc.id,
  name: fc.name,
  response: {
    result: "ok",
    scheduling: FunctionResponseScheduling.INTERRUPT  // Can also be WHEN_IDLE or SILENT
  }
}
```

 
 

## Grounding with Google Search

You can enable Grounding with Google Search as part of the session
configuration. This increases the Live API's accuracy and prevents
hallucinations. See the Grounding tutorial to
learn more.

 
 

### Python 

 

```
import asyncio
import wave
from google import genai
from google.genai import types

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"

tools = [{'google_search': {}}]
config = {"response_modalities": ["AUDIO"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "When did the last Brazil vs. Argentina soccer match happen?"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.data is not None:
                    wf.writeframes(chunk.data)

                # The model might generate and execute Python code to use Search
                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                        if part.executable_code is not None:
                            print(part.executable_code.code)

                        if part.code_execution_result is not None:
                            print(part.code_execution_result.output)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const tools = [{ googleSearch: {} }]
const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'When did the last Brazil vs. Argentina soccer match happen?';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  let combinedData = '';
  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
        else if (part.inlineData && typeof part.inlineData.data === 'string') {
          combinedData += atob(part.inlineData.data);
        }
      }
    }
  }

  // Convert the base64-encoded string of bytes into a Buffer.
  const buffer = Buffer.from(combinedData, 'binary');

  // The buffer contains raw bytes. For 16-bit audio, we need to interpret every 2 bytes as a single sample.
  const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);

  const wf = new WaveFile();
  // The API returns 16-bit PCM audio at a 24kHz sample rate.
  wf.fromScratch(1, 24000, '16', intArray);
  fs.writeFileSync('audio.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## Combining multiple tools

You can combine multiple tools within the Live API,
increasing your application's capabilities even more:

 
 

### Python 

 

```
prompt = """
Hey, I need you to do two things for me.

1. Use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
2. Then turn on the lights

Thanks!
"""

tools = [
    {"google_search": {}},
    {"function_declarations": [turn_on_the_lights, turn_off_the_lights]},
]

config = {"response_modalities": ["AUDIO"], "tools": tools}

# ... remaining model call
```

 
 

### JavaScript 

 

```
const prompt = `Hey, I need you to do two things for me.

1. Use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
2. Then turn on the lights

Thanks!
`

const tools = [
  { googleSearch: {} },
  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }
]

const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

// ... remaining model call
```

 
 

## What's next

- Check out more examples of using tools with the Live API in the
 Tool use cookbook .

- Get the full story on features and configurations from the
 Live API Capabilities guide .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Grounding with Google Maps &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/maps-grounding#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Grounding with Google Maps  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Grounding with Google Maps 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Grounding with Google Maps connects the generative capabilities of Gemini with
the rich, factual, and up-to-date data of Google Maps. This feature enables
developers to easily incorporate location-aware functionality into their
applications. When a user query has a context related to Maps data, the Gemini
model leverages Google Maps to provide factually accurate and fresh answers that
are relevant to the user's specified location or general area.

- Accurate, location-aware responses: Leverage Google Maps' extensive and
current data for geographically specific queries.

- Enhanced personalization: Tailor recommendations and information based
on user-provided locations.

- Contextual information and widgets: Context tokens to render interactive
Google Maps widgets alongside generated content.

## Get started

This example demonstrates how to integrate Grounding with Google Maps into your
application to provide accurate, location-aware responses to user queries. The
prompt asks for local recommendations with an optional user location, enabling
the Gemini model to leverage Google Maps data.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "What are the best Italian restaurants within a 15-minute walk from here?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
        # Turn on grounding with Google Maps
        tools=[types.Tool(google_maps=types.GoogleMaps())],
        # Optionally provide the relevant location context (this is in Los Angeles)
        tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
            lat_lng=types.LatLng(
                latitude=34.050481, longitude=-118.248526))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in grounding.grounding_chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/gnai";

const ai = new GoogleGenAI({});

async function generateContentWithMapsGrounding() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What are the best Italian restaurants within a 15-minute walk from here?",
    config: {
      // Turn on grounding with Google Maps
      tools: [{ googleMaps: {} }],
      toolConfig: {
        retrievalConfig: {
          // Optionally provide the relevant location context (this is in Los Angeles)
          latLng: {
            latitude: 34.050481,
            longitude: -118.248526,
          },
        },
      },
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const grounding = response.candidates[0]?.groundingMetadata;
  if (grounding?.groundingChunks) {
    console.log("-".repeat(40));
    console.log("Sources:");
    for (const chunk of grounding.groundingChunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

generateContentWithMapsGrounding();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "What are the best Italian restaurants within a 15-minute walk from here?"
    }]
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 34.050481, "longitude": -118.248526}
    }
  }
}'
```

 
 

## How Grounding with Google Maps works

Grounding with Google Maps integrates the Gemini API with the Google Geo
ecosystem by using the Maps API as a grounding source. When a user's query
contains geographical context, the Gemini model can invoke the Grounding with
Google Maps tool. The model can then generate responses grounded in Google Maps
data relevant to the provided location.

The process typically involves:

- User query: A user submits a query to your application, potentially
including geographical context (e.g., "coffee shops near me," "museums in
San Francisco").

- Tool invocation: The Gemini model, recognizing the geographical intent,
invokes the Grounding with Google Maps tool. This tool can optionally be
provided with the user's `latitude` and `longitude`. The tool is a textual
search tool and behaves similarly to searching on Maps, in that local
queries ("near me") will use the coordinates, while specific or non-local
queries are unlikely to be influenced by the explicit location.

- Data retrieval: The Grounding with Google Maps service queries Google
Maps for relevant information (e.g., places, reviews, photos, addresses,
opening hours).

- Grounded generation: The retrieved Maps data is used to inform the
Gemini model's response, ensuring factual accuracy and relevance.

- Response & widget token: The model returns a text response, which
includes citations to Google Maps sources. Optionally, the API response may
also contain a `google_maps_widget_context_token`, allowing developers to
render a contextual Google Maps widget in their application for visual
interaction.

## Why and when to use Grounding with Google Maps

Grounding with Google Maps is ideal for applications that require accurate,
up-to-date, and location-specific information. It enhances the user experience
by providing relevant and personalized content backed by Google Maps' extensive
database of over 250 million places worldwide.

You should use Grounding with Google Maps when your application needs to:

- Provide complete and accurate responses to geo-specific questions.

- Build conversational trip planners and local guides.

- Recommend points of interest based on
location and user preferences like restaurants or shops.

- Create location-aware experiences for social, retail, or food delivery
services.

Grounding with Google Maps excels in use cases where proximity and current
factual data are critical, such as finding the "best coffee shop near me" or
getting directions.

## API methods and parameters

Grounding with Google Maps is exposed through the Gemini API as a tool within
the `generateContent` method. You enable and configure
Grounding with Google Maps by including a
 `googleMaps` object in the `tools` parameter of your
request.

 
 

### JSON

 

```
{
  "contents": [{
    "parts": [
      {"text": "Restaurants near Times Square."}
    ]
  }],
  "tools":  { "googleMaps": {} }
}
```

 
 

The `googleMaps` tool can additionally accept a boolean `enableWidget`
parameter, that is used to control whether the `googleMapsWidgetContextToken` 
field is returned in the response. This can be used to display a
 contextual Places widget .

 
 

### JSON

 

```
{
"contents": [{
    "parts": [
      {"text": "Restaurants near Times Square."}
    ]
  }],
  "tools":  { "googleMaps": { "enableWidget": true } }
}
```

 
 

Additionally, the tool supports passing the contextual location as `toolConfig`.

 
 

### JSON

 

```
{
  "contents": [{
    "parts": [
      {"text": "Restaurants near here."}
    ]
  }],
  "tools":  { "googleMaps": {} },
  "toolConfig":  {
    "retrievalConfig": {
      "latLng": {
        "latitude": 40.758896,
        "longitude": -73.985130
      }
    }
  }
}
```

 
 

### Understanding the grounding response

When a response is successfully grounded with Google Maps data, the response
includes a `groundingMetadata` field.
This structured data is essential for verifying claims and building a rich
citation experience in your application, as well as meeting the service usage
requirements.

 
 

### JSON

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "CanteenM is an American restaurant with..."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "groundingChunks": [
          {
            "maps": {
              "uri": "https://maps.google.com/?cid=13100894621228039586",
              "title": "Heaven on 7th Marketplace",
              "placeId": "places/ChIJ0-zA1vBZwokRon0fGj-6z7U"
            },
            // repeated ...
          }
        ],
        "groundingSupports": [
          {
            "segment": {
              "startIndex": 0,
              "endIndex": 79,
              "text": "CanteenM is an American restaurant with a 4.6-star rating and is open 24 hours."
            },
            "groundingChunkIndices": [0]
          },
          // repeated ...
        ],
        "webSearchQueries": [
          "restaurants near me"
        ],
        "googleMapsWidgetContextToken": "widgetcontent/..."
      }
    }
  ]
}
```

 
 

The Gemini API returns the following information with the
 `groundingMetadata` :

- `groundingChunks`: Array of objects containing the `maps` sources (`uri`,
`placeId` and `title`).

- `groundingSupports`: Array of chunks to connect model response text to the
sources in `groundingChunks`. Each chunk links a text span (defined by
`startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This is
the key to building inline citations.

- `googleMapsWidgetContextToken`: A text token that can be used to render a
 contextual Places
widget .

For a code snippet showing how to render inline citations in text, see the
example 
in the Grounding with Google Search docs.

### Display the Google Maps contextual widget

To use the returned `googleMapsWidgetContextToken`, you need to load the
Google Maps JavaScript
API .

## Use cases

Grounding with Google Maps supports a variety of location-aware use cases. The
following examples demonstrate how different prompts and parameters can leverage
Grounding with Google Maps. Information in the Google Maps Grounded Results may
differ from actual conditions.

### Handling place-specific questions

Ask detailed questions about a specific place to get answers based on Google
user reviews and other Maps data.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Is there a cafe near the corner of 1st and Main that has outdoor seating?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
        # Turn on the Maps tool
        tools=[types.Tool(google_maps=types.GoogleMaps())],

        # Provide the relevant location context (this is in Los Angeles)
        tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
            lat_lng=types.LatLng(
                latitude=34.050481, longitude=-118.248526))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if chunks := grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
  ```
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Is there a cafe near the corner of 1st and Main that has outdoor seating?";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      // Turn on the Maps tool
      tools: [{googleMaps: {}}],
      // Provide the relevant location context (this is in Los Angeles)
      toolConfig: {
        retrievalConfig: {
          latLng: {
            latitude: 34.050481,
            longitude: -118.248526
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const chunks = response.candidates[0].groundingMetadata?.groundingChunks;
  if (chunks) {
    console.log('-'.repeat(40));
    console.log("Sources:");
    for (const chunk of chunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Is there a cafe near the corner of 1st and Main that has outdoor seating?"
    }]
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 34.050481, "longitude": -118.248526}
    }
  }
}'
```

 
 

### Providing location-based personalization

Get recommendations tailored to a user's preferences and a specific geographical
area.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Which family-friendly restaurants near here have the best playground reviews?"

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
      tools=[types.Tool(google_maps=types.GoogleMaps())],
      tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
          # Provide the location as context; this is Austin, TX.
          lat_lng=types.LatLng(
              latitude=30.2672, longitude=-97.7431))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if chunks := grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Which family-friendly restaurants near here have the best playground reviews?";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      tools: [{googleMaps: {}}],
      toolConfig: {
        retrievalConfig: {
          // Provide the location as context; this is Austin, TX.
          latLng: {
            latitude: 30.2672,
            longitude: -97.7431
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const chunks = response.candidates[0].groundingMetadata?.groundingChunks;
  if (chunks) {
    console.log('-'.repeat(40));
    console.log("Sources:");
    for (const chunk of chunks) {
      if (chunk.maps) {
        console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
      }
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Which family-friendly restaurants near here have the best playground reviews?"
    }],
  }],
  "tools": [{"googleMaps": {}}],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {"latitude": 30.2672, "longitude": -97.7431}
    }
  }
}'
```

 
 

### Assisting with itinerary planning

Generate multi-day plans with directions and information about various
locations, perfect for travel applications.

In this example, the `googleMapsWidgetContextToken` has been requested by
enabling the widget in the Google Maps tool. When enabled, the returned token
can be used to render a contextual Places widget using the ` component`
from the Google Maps JavaScript API.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner."

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=prompt,
    config=types.GenerateContentConfig(
      tools=[types.Tool(google_maps=types.GoogleMaps(enable_widget=True))],
      tool_config=types.ToolConfig(retrieval_config=types.RetrievalConfig(
          # Provide the location as context, this is in San Francisco.
          lat_lng=types.LatLng(
              latitude=37.78193, longitude=-122.40476))),
    ),
)

print("Generated Response:")
print(response.text)

if grounding := response.candidates[0].grounding_metadata:
  if grounding.grounding_chunks:
    print('-' * 40)
    print("Sources:")
    for chunk in grounding.grounding_chunks:
      print(f'- [{chunk.maps.title}]({chunk.maps.uri})')

  if widget_token := grounding.google_maps_widget_context_token:
    print('-' * 40)
    print(f'<gmp-place-contextual context-token="{widget_token}"></gmp-place-contextual>')
```

 
 

### Javascript

 

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({});

async function run() {
  const prompt = "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner.";

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      tools: [{googleMaps: {enableWidget: true}}],
      toolConfig: {
        retrievalConfig: {
          // Provide the location as context, this is in San Francisco.
          latLng: {
            latitude: 37.78193,
            longitude: -122.40476
          }
        }
      }
    },
  });

  console.log("Generated Response:");
  console.log(response.text);

  const groundingMetadata = response.candidates[0]?.groundingMetadata;
  if (groundingMetadata) {
    if (groundingMetadata.groundingChunks) {
      console.log('-'.repeat(40));
      console.log("Sources:");
      for (const chunk of groundingMetadata.groundingChunks) {
        if (chunk.maps) {
          console.log(`- [${chunk.maps.title}](${chunk.maps.uri})`);
        }
      }
    }

    if (groundingMetadata.googleMapsWidgetContextToken) {
      console.log('-'.repeat(40));
      document.body.insertAdjacentHTML('beforeend', `<gmp-place-contextual context-token="${groundingMetadata.googleMapsWidgetContextToken}`"></gmp-place-contextual>`);
    }
  }
}

run();
```

 
 

### REST

 

```
curl -X POST 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent' \
  -H 'Content-Type: application/json' \
  -H "x-goog-api-key: ${GEMINI_API_KEY}" \
  -d '{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "Plan a day in San Francisco for me. I want to see the Golden Gate Bridge, visit a museum, and have a nice dinner."
    }]
  }],
  "tools": [{"googleMaps": {"enableWidget":"true"}}],
  "toolConfig": {
    "retrievalConfig": {
    "latLng": {"latitude": 37.78193, "longitude": -122.40476}
  }
  }
}'
```

 
 

When the widget is rendered, it will look something like the following:

 

## Service usage requirements

This section describes the service usage requirements for Grounding with Google
Maps.

### Inform the user about the use of Google Maps sources

With each Google Maps Grounded Result, you'll receive sources in `groundingChunks`
that support each response. The following metadata is also returned:

- source uri

- title

- ID

When presenting results from Grounding with Google Maps, you must specify the
associated Google Maps sources, and inform your users of the following:

- The Google Maps sources must immediately follow the generated content that
the sources support. This generated content is also referred to as Google
Maps Grounded Result.

- The Google Maps sources must be viewable within one user interaction.

### Display Google Maps sources with Google Maps links

For each source in `groundingChunks` and in


```
grounding_chunks.maps.placeAnswerSources.reviewSnippets
```

, a link preview must be
generated following these requirements:

- Attribute each source to Google Maps following the Google Maps text
 attribution guidelines .

- Display the source title provided in the response.

- Link to the source using the `uri` or `googleMapsUri` from the response.

These images show the minimum requirements for displaying the sources and Google
Maps links.

 

You can collapse the view of the sources.

 

Optional: Enhance the link preview with additional content, such as:

- A Google Maps favicon 
is inserted before the Google Maps text attribution.

- A photo from the source URL (`og:image`).

For more information about some of our Google Maps data providers and their
license terms, see the Google Maps and Google Earth legal notices .

### Google Maps text attribution guidelines

When you attribute sources to Google Maps in text, follow these guidelines:

- Don't modify the text Google Maps in any way:

 Don't change the capitalization of Google Maps.

- Don't wrap Google Maps onto multiple lines.

- Don't localize Google Maps into another language.

- Prevent browsers from translating Google Maps by using the HTML
attribute translate="no".

 
- Style Google Maps text as described in the following table:

 
 
 
 Property 
 Style 
 
 
 
 
 `Font family` 
 Roboto. Loading the font is optional. 
 
 
 `Fallback font family` 
 Any sans serif body font already used in your product or "Sans-Serif" to invoke the default system font 
 
 
 `Font style` 
 Normal 
 
 
 `Font weight` 
 400 
 
 
 `Font color` 
 White, black (#1F1F1F), or gray (#5E5E5E). Maintain accessible (4.5:1) contrast against the background. 
 
 
 `Font size` 
 
 
- Minimum font size: 12sp

 - Maximum font size: 16sp

 - To learn about sp, see Font size units on the Material Design website .

 
 
 
 
 `Spacing` 
 Normal 
 
 
 

#### Example CSS

The following CSS renders Google Maps with the appropriate typographic style and
color on a white or light background.

 
 

### CSS

 

```
@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');

.GMP-attribution {

font-family: Roboto, Sans-Serif;
font-style: normal;
font-weight: 400;
font-size: 1rem;
letter-spacing: normal;
white-space: nowrap;
color: #5e5e5e;
}
```

 
 

### Context token, place ID, and review ID

The Google Maps data includes context token, place ID, and review ID. You might
cache, store, and export the following response data:

- `googleMapsWidgetContextToken`

- `placeId`

- `reviewId`

The restrictions against caching in the Grounding with Google Maps Terms don't
apply.

### Prohibited activity and territory

Grounding with Google Maps has additional restrictions for certain content and
activities to maintain a safe and reliable platform. In addition to the usage
restrictions in the Terms, you will not use Grounding with Google Maps
for high risk activities including emergency response services. You will
not distribute or market your application that offers Grounding with
Google Maps in a Prohibited Territory. The current Prohibited Territories are:

- China

- Crimea

- Cuba

- Donetsk People's Republic

- Iran

- Luhansk People's Republic

- North Korea

- Syria

- Vietnam

This list may be updated from time to time.

## Best practices

- Provide user location: For the most relevant and personalized responses,
always include the `user_location` (latitude and longitude) in your
`googleMapsGrounding` configuration when the user's location is known.

- Render the Google Maps contextual widget: The contextual widget is
rendered using the context token, `googleMapsWidgetContextToken`, which is
returned in the Gemini API response and can be used to render visual content
from Google Maps. For more information on the contextual widget, see
 Grounding with Google Maps
widget 
in the Google Developer Guide.

- Inform End-Users: Clearly inform your end-users that Google Maps
data is being used to answer their queries, especially when the tool is
enabled.

- Monitor Latency: For conversational applications, ensure that the P95
latency for grounded responses remains within acceptable thresholds to
maintain a smooth user experience.

- Toggle Off When Not Needed: Grounding with Google Maps is off by
default. Only enable it (`"tools": [{"googleMaps": {}}]`) when a query has a
clear geographical context, to optimize performance and cost.

## Limitations

- Geographical Scope: Currently, Grounding with Google Maps is globally
available

- Model Support: Only specific Gemini models support Grounding with Google
Maps: Gemini 2.5 Flash-Lite, Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini
2.0 Flash (but not 2.0 Flash Lite).

- Multimodal Inputs/Outputs: Grounding with Google Maps does not currently
support multimodal inputs or outputs beyond text and contextual map widgets.

- Default State: The Grounding with Google Maps tool is off by default.
You must explicitly enable it in your API requests.

## Pricing and rate limits

Grounding with Google Maps pricing is based on queries. The current rate is
 $25 / 1K grounded prompts . The free tier also has up to 500 requests per day
available. A request is only counted towards the quota when
a prompt successfully returns at least one Google Maps grounded result (i.e.,
results containing at least one Google Maps source). If multiple queries are
sent to Google Maps from a single request, it counts as one request towards the
rate limit.

For detailed pricing information, see the Gemini API pricing page .

## Supported models

You can find their capabilities on the model overview page.

 
 
 
 Model 
 Grounding with Google Maps 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 
 
 

## What's next

- Try the Grounding with Google Search in the Gemini API
Cookbook .

- Learn about other available tools, like
 Function calling .

- To learn more about responsible AI best practices and Gemini API's safety
filters, see the Safety settings guide .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Session management with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-session

- 
 
 
 
 
 
 
 
 
 
 
 Session management with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Session management with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

In the Live API, a session refers to a persistent
connection where input and output are streamed continuously over the same
connection (read more about how it works ).
This unique session design enables low latency and supports unique features, but
can also introduce challenges, like session time limits, and early termination.
This guide covers strategies for overcoming the session management challenges
that can arise when using the Live API.

## Session lifetime

Without compression, audio-only sessions are limited to 15 minutes,
and audio-video sessions are limited to 2 minutes. Exceeding these limits
will terminate the session (and therefore, the connection), but you can use
 context window compression to extend sessions to
an unlimited amount of time.

The lifetime of a connection is limited as well, to around 10 minutes. When the
connection terminates, the session terminates as well. In this case, you can
configure a single session to stay active over multiple connections using
 session resumption .
You'll also receive a GoAway message before the
connection ends, allowing you to take further actions.

## Context window compression

To enable longer sessions, and avoid abrupt connection termination, you can
enable context window compression by setting the contextWindowCompression 
field as part of the session configuration.

In the ContextWindowCompressionConfig , you can configure a
 sliding-window mechanism 
and the number of tokens 
that triggers compression.

 
 

### Python

 

```
from google.genai import types

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    context_window_compression=(
        # Configures compression with default parameters.
        types.ContextWindowCompressionConfig(
            sliding_window=types.SlidingWindow(),
        )
    ),
)
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.AUDIO],
  contextWindowCompression: { slidingWindow: {} }
};
```

 
 

## Session resumption

To prevent session termination when the server periodically resets the WebSocket
connection, configure the sessionResumption 
field within the setup configuration .

Passing this configuration causes the
server to send SessionResumptionUpdate 
messages, which can be used to resume the session by passing the last resumption
token as the `SessionResumptionConfig.handle` 
of the subsequent connection.

Resumption tokens are valid for 2 hr after the last sessions termination.

 
 

### Python

 

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

async def main():
    print(f"Connecting to the service with handle {previous_session_handle}...")
    async with client.aio.live.connect(
        model=model,
        config=types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            session_resumption=types.SessionResumptionConfig(
                # The handle of the session to resume is passed here,
                # or else None to start a new session.
                handle=previous_session_handle
            ),
        ),
    ) as session:
        while True:
            await session.send_client_content(
                turns=types.Content(
                    role="user", parts=[types.Part(text="Hello world!")]
                )
            )
            async for message in session.receive():
                # Periodically, the server will send update messages that may
                # contain a handle for the current state of the session.
                if message.session_resumption_update:
                    update = message.session_resumption_update
                    if update.resumable and update.new_handle:
                        # The handle should be retained and linked to the session.
                        return update.new_handle

                # For the purposes of this example, placeholder input is continually fed
                # to the model. In non-sample code, the model inputs would come from
                # the user.
                if message.server_content and message.server_content.turn_complete:
                    break

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

console.debug('Connecting to the service with handle %s...', previousSessionHandle)
const session = await ai.live.connect({
  model: model,
  callbacks: {
    onopen: function () {
      console.debug('Opened');
    },
    onmessage: function (message) {
      responseQueue.push(message);
    },
    onerror: function (e) {
      console.debug('Error:', e.message);
    },
    onclose: function (e) {
      console.debug('Close:', e.reason);
    },
  },
  config: {
    responseModalities: [Modality.AUDIO],
    sessionResumption: { handle: previousSessionHandle }
    // The handle of the session to resume is passed here, or else null to start a new session.
  }
});

const inputTurns = 'Hello how are you?';
session.sendClientContent({ turns: inputTurns });

const turns = await handleTurn();
for (const turn of turns) {
  if (turn.sessionResumptionUpdate) {
    if (turn.sessionResumptionUpdate.resumable && turn.sessionResumptionUpdate.newHandle) {
      let newHandle = turn.sessionResumptionUpdate.newHandle
      // ...Store newHandle and start new session with this handle here
    }
  }
}

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## Receiving a message before the session disconnects

The server sends a GoAway message that signals that the current
connection will soon be terminated. This message includes the timeLeft ,
indicating the remaining time and lets you take further action before the
connection will be terminated as ABORTED.

 
 

### Python

 

```
async for response in session.receive():
    if response.go_away is not None:
        # The connection will soon be terminated
        print(response.go_away.time_left)
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.goAway) {
    console.debug('Time left: %s\n', turn.goAway.timeLeft);
  }
}
```

 
 

## Receiving a message when the generation is complete

The server sends a generationComplete 
message that signals that the model finished generating the response.

 
 

### Python

 

```
async for response in session.receive():
    if response.server_content.generation_complete is True:
        # The generation is complete
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.generationComplete) {
    // The generation is complete
  }
}
```

 
 

## What's next

Explore more ways to work with the Live API in the full
 Capabilities guide,
the Tool use page, or the
 Live API cookbook .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Code execution &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/code-execution#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Code execution  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Code execution 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API provides a code execution tool that enables the model to
generate and run Python code. The model can then learn iteratively from the
code execution results until it arrives at a final output. You can use code
execution to build applications that benefit from code-based reasoning. For
example, you can use code execution to solve equations or process text. You can
also use the libraries included in the code execution
environment to perform more specialized tasks.

Gemini is only able to execute code in Python. You can still ask Gemini to
generate code in another language, but the model can't use the code execution
tool to run it.

## Enable code execution

To enable code execution, configure the code execution tool on the model. This
allows the model to generate and run code.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50.",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: [
    "What is the sum of the first 50 prime numbers? " +
      "Generate and run code for the calculation, and make sure you get all 50.",
  ],
  config: {
    tools: [{ codeExecution: {} }],
  },
});

const parts = response?.candidates?.[0]?.content?.parts || [];
parts.forEach((part) => {
  if (part.text) {
    console.log(part.text);
  }

  if (part.executableCode && part.executableCode.code) {
    console.log(part.executableCode.code);
  }

  if (part.codeExecutionResult && part.codeExecutionResult.output) {
    console.log(part.codeExecutionResult.output);
  }
});
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("What is the sum of the first 50 prime numbers? " +
                  "Generate and run code for the calculation, and make sure you get all 50."),
        config,
    )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d ' {"tools": [{"code_execution": {}}],
    "contents": {
      "parts":
        {
            "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
        }
    },
}'
```

 
 

The output might look something like the following, which has been formatted for
readability:

 

```
Okay, I need to calculate the sum of the first 50 prime numbers. Here's how I'll
approach this:

1.  **Generate Prime Numbers:** I'll use an iterative method to find prime
    numbers. I'll start with 2 and check if each subsequent number is divisible
    by any number between 2 and its square root. If not, it's a prime.
2.  **Store Primes:** I'll store the prime numbers in a list until I have 50 of
    them.
3.  **Calculate the Sum:**  Finally, I'll sum the prime numbers in the list.

Here's the Python code to do this:

def is_prime(n):
  """Efficiently checks if a number is prime."""
  if n <= 1:
    return False
  if n <= 3:
    return True
  if n % 2 == 0 or n % 3 == 0:
    return False
  i = 5
  while i * i <= n:
    if n % i == 0 or n % (i + 2) == 0:
      return False
    i += 6
  return True

primes = []
num = 2
while len(primes) < 50:
  if is_prime(num):
    primes.append(num)
  num += 1

sum_of_primes = sum(primes)
print(f'{primes=}')
print(f'{sum_of_primes=}')

primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,
71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,
157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]
sum_of_primes=5117

The sum of the first 50 prime numbers is 5117.
```

 

This output combines several content parts that the model returns when using
code execution:

- `text`: Inline text generated by the model

- `executableCode`: Code generated by the model that is meant to be executed

- `codeExecutionResult`: Result of the executable code

The naming conventions for these parts vary by programming language.

## Use code execution in chat

You can also use code execution as part of a chat.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

response = chat.send_message("I have a math question for you.")
print(response.text)

response = chat.send_message(
    "What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50."
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from "@google/genai";

const ai = new GoogleGenAI({});

const chat = ai.chats.create({
  model: "gemini-2.5-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "I have a math question for you:" }],
    },
    {
      role: "model",
      parts: [{ text: "Great! I'm ready for your math question. Please ask away." }],
    },
  ],
  config: {
    tools: [{codeExecution:{}}],
  }
});

const response = await chat.sendMessage({
  message: "What is the sum of the first 50 prime numbers? " +
            "Generate and run code for the calculation, and make sure you get all 50."
});
console.log("Chat response:", response.text);
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    chat, _ := client.Chats.Create(
        ctx,
        "gemini-2.5-flash",
        config,
        nil,
    )

    result, _ := chat.SendMessage(
                    ctx,
                    genai.Part{Text: "What is the sum of the first 50 prime numbers? " +
                                          "Generate and run code for the calculation, and " +
                                          "make sure you get all 50.",
                              },
                )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"tools": [{"code_execution": {}}],
    "contents": [
        {
            "role": "user",
            "parts": [{
                "text": "Can you print \"Hello world!\"?"
            }]
        },{
            "role": "model",
            "parts": [
              {
                "text": ""
              },
              {
                "executable_code": {
                  "language": "PYTHON",
                  "code": "\nprint(\"hello world!\")\n"
                }
              },
              {
                "code_execution_result": {
                  "outcome": "OUTCOME_OK",
                  "output": "hello world!\n"
                }
              },
              {
                "text": "I have printed \"hello world!\" using the provided python code block. \n"
              }
            ],
        },{
            "role": "user",
            "parts": [{
                "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
            }]
        }
    ]
}'
```

 
 

## Input/output (I/O)

Starting with
 Gemini 2.0 Flash , code
execution supports file input and graph output. Using these input and output
capabilities, you can upload CSV and text files, ask questions about the
files, and have Matplotlib graphs generated as part
of the response. The output files are returned as inline images in the response.

### I/O pricing

When using code execution I/O, you're charged for input tokens and output
tokens:

 Input tokens: 

- User prompt

 Output tokens: 

- Code generated by the model

- Code execution output in the code environment

- Thinking tokens

- Summary generated by the model

### I/O details

When you're working with code execution I/O, be aware of the following technical
details:

- The maximum runtime of the code environment is 30 seconds.

- If the code environment generates an error, the model may decide to
regenerate the code output. This can happen up to 5 times.

- The maximum file input size is limited by the model token window. In
AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million
tokens (roughly 2MB for text files of the supported input types). If you
upload a file that's too large, AI Studio won't let you send it.

- Code execution works best with text and CSV files.

- The input file can be passed in `part.inlineData` or `part.fileData` (uploaded
via the Files API ), and the output file is always
returned as `part.inlineData`.

 
 
 
 
 Single turn 
 Bidirectional (Multimodal Live API) 
 
 

 
 
 Models supported 
 All Gemini 2.0 and 2.5 models 
 Only Flash experimental models 
 
 
 File input types supported 
 .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts 
 .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts 
 
 
 Plotting libraries supported 
 Matplotlib, seaborn 
 Matplotlib, seaborn 
 
 
 Multi-tool use 
 Yes (code execution + grounding only) 
 Yes 
 
 
 

## Billing

There's no additional charge for enabling code execution from the Gemini API.
You'll be billed at the current rate of input and output tokens based on the
Gemini model you're using.

Here are a few other things to know about billing for code execution:

- You're only billed once for the input tokens you pass to the model, and you're
billed for the final output tokens returned to you by the model.

- Tokens representing generated code are counted as output tokens. Generated
code can include text and multimodal output like images.

- Code execution results are also counted as output tokens.

The billing model is shown in the following diagram:

 

- You're billed at the current rate of input and output tokens based on the
Gemini model you're using.

- If Gemini uses code execution when generating your response, the original
prompt, the generated code, and the result of the executed code are labeled
 intermediate tokens and are billed as input tokens .

- Gemini then generates a summary and returns the generated code, the result of
the executed code, and the final summary. These are billed as output tokens .

- The Gemini API includes an intermediate token count in the API response, so
you know why you're getting additional input tokens beyond your initial
prompt.

## Limitations

- The model can only generate and execute code. It can't return other artifacts
like media files.

- In some cases, enabling code execution can lead to regressions in other areas
of model output (for example, writing a story).

- There is some variation in the ability of the different models to use code
execution successfully.

## Supported tools combinations

Code execution tool can be combined with
 Grounding with Google Search to
power more complex use cases.

## Supported libraries

The code execution environment includes the following libraries:

- attrs

- chess

- contourpy

- fpdf

- geopandas

- imageio

- jinja2

- joblib

- jsonschema

- jsonschema-specifications

- lxml

- matplotlib

- mpmath

- numpy

- opencv-python

- openpyxl

- packaging

- pandas

- pillow

- protobuf

- pylatex

- pyparsing

- PyPDF2

- python-dateutil

- python-docx

- python-pptx

- reportlab

- scikit-learn

- scipy

- seaborn

- six

- striprtf

- sympy

- tabulate

- tensorflow

- toolz

- xlrd

You can't install your own libraries.

## What's next

- Try the
 code execution Colab .

- Learn about other Gemini API tools:

 Function calling 

- Grounding with Google Search 

 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-06 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-06 UTC."],[],[]]

---

### Ephemeral tokens &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ephemeral-tokens

- 
 
 
 
 
 
 
 
 
 
 
 Ephemeral tokens  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Ephemeral tokens 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Ephemeral tokens are short-lived authentication tokens for accessing the Gemini
API through WebSockets . They are designed to enhance security when
you are connecting directly from a user's device to the API (a
 client-to-server 
implementation). Like standard API keys, ephemeral tokens can be extracted from
client-side applications such as web browsers or mobile apps. But because
ephemeral tokens expire quickly and can be restricted, they significantly reduce
the security risks in a production environment. You should use them when
accessing the Live API directly from client-side applications to enhance API
key security.

## How ephemeral tokens work

Here's how ephemeral tokens work at a high level:

- Your client (e.g. web app) authenticates with your backend.

- Your backend requests an ephemeral token from Gemini API's provisioning
service.

- Gemini API issues a short-lived token.

- Your backend sends the token to the client for WebSocket connections to Live
API. You can do this by swapping your API key with an ephemeral token.

- The client then uses the token as if it were an API key.

 

This enhances security because even if extracted, the token is short-lived,
unlike a long-lived API key deployed client-side. Since the client sends data
directly to Gemini, this also improves latency and avoids your backends needing
to proxy the real time data.

## Create an ephemeral token

Here is a simplified example of how to get an ephemeral token from Gemini.
By default, you'll have 1 minute to start new Live API sessions using the token
from this request (`newSessionExpireTime`), and 30 minutes to send messages over
that connection (`expireTime`).

 
 

### Python

 

```
import datetime

now = datetime.datetime.now(tz=datetime.timezone.utc)

client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1, # The ephemeral token can only be used to start a single session
    'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future
    # 'expire_time': '2025-05-17T00:00:00Z',   # Accepts isoformat.
    'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future
    'http_options': {'api_version': 'v1alpha'},
  }
)

# You'll need to pass the value under token.name back to your client to use it
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

  const token: AuthToken = await client.authTokens.create({
    config: {
      uses: 1, // The default
      expireTime: expireTime // Default is 30 mins
      newSessionExpireTime: new Date(Date.now() + (1 * 60 * 1000)), // Default 1 minute in the future
      httpOptions: {apiVersion: 'v1alpha'},
    },
  });
```

 
 

For `expireTime` value constraints, defaults, and other field specs, see the
 API reference .
Within the `expireTime` timeframe, you'll need
 `sessionResumption` to
reconnect the call every 10 minutes (this can be done with the same token even
if `uses: 1`).

It's also possible to lock an ephemeral token to a set of configurations. This
might be useful to further improve security of your application and keep your
system instructions on the server side.

 
 

### Python

 

```
client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1,
    'live_connect_constraints': {
        'model': 'gemini-2.5-flash-native-audio-preview-09-2025',
        'config': {
            'session_resumption':{},
            'temperature':0.7,
            'response_modalities':['AUDIO']
        }
    },
    'http_options': {'api_version': 'v1alpha'},
    }
)

# You'll need to pass the value under token.name back to your client to use it
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

const token = await client.authTokens.create({
    config: {
        uses: 1, // The default
        expireTime: expireTime,
        liveConnectConstraints: {
            model: 'gemini-2.5-flash-native-audio-preview-09-2025',
            config: {
                sessionResumption: {},
                temperature: 0.7,
                responseModalities: ['AUDIO']
            }
        },
        httpOptions: {
            apiVersion: 'v1alpha'
        }
    }
});

// You'll need to pass the value under token.name back to your client to use it
```

 
 

You can also lock a subset of fields, see the SDK documentation 
for more info.

## Connect to Live API with an ephemeral token

Once you have an ephemeral token, you use it as if it were an API key (but
remember, it only works for the live API, and only with the `v1alpha` version of
the API).

The use of ephemeral tokens only adds value when deploying applications
that follow client-to-server implementation approach.

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

// Use the token generated in the "Create an ephemeral token" section here
const ai = new GoogleGenAI({
  apiKey: token.name
});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: { ... },
  });

  // Send content...

  session.close();
}

main();
```

 
 

See Get started with Live API for more examples.

## Best practices

- Set a short expiration duration using the `expire_time` parameter.

- Tokens expire, requiring re-initiation of the provisioning process.

- Verify secure authentication for your own backend. Ephemeral tokens will
only be as secure as your backend authentication method.

- Generally, avoid using ephemeral tokens for backend-to-Gemini connections,
as this path is typically considered secure.

## Limitations

Ephemeral tokens are only compatible with Live API at this time.

## What's next

- Read the Live API reference 
on ephemeral tokens for more information.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-11 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-11 UTC."],[],[]]

---

### URL context &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/url-context#main-content

- 
 
 
 
 
 
 
 
 
 
 
 URL context  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 URL context 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The URL context tool lets you provide additional context to the models in the
form of URLs. By including URLs in your request, the model will access
the content from those pages (as long as it's not a URL type listed in the
 limitations section ) to inform
and enhance its response.

The URL context tool is useful for tasks like the following:

- Extract Data : Pull specific info like prices, names, or key
findings from multiple URLs.

- Compare Documents : Analyze multiple reports, articles, or PDFs to
identify differences and track trends.

- Synthesize & Create Content : Combine information from several source
URLs to generate accurate summaries, blog posts, or reports.

- Analyze Code & Docs : Point to a GitHub repository or technical
documentation to explain code, generate setup instructions, or answer
questions.

The following example shows how to compare two recipes from different websites.

 
 

### Python

 

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = [
  {"url_context": {}},
]

url1 = "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592"
url2 = "https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/"

response = client.models.generate_content(
    model=model_id,
    contents=f"Compare the ingredients and cooking times from the recipes at {url1} and {url2}",
    config=GenerateContentConfig(
        tools=tools,
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)

# For verification, you can inspect the metadata to see which URLs the model retrieved
print(response.candidates[0].url_context_metadata)
```

 
 

### Javascript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Compare the ingredients and cooking times from the recipes at https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592 and https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/",
    ],
    config: {
      tools: [{urlContext: {}}],
    },
  });
  console.log(response.text);

  // For verification, you can inspect the metadata to see which URLs the model retrieved
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Compare the ingredients and cooking times from the recipes at https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592 and https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/"}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          }
      ]
  }' > result.json

cat result.json
```

 
 

## How it works

The URL Context tool uses a two-step retrieval process to
balance speed, cost, and access to fresh data. When you provide a URL, the tool
first attempts to fetch the content from an internal index cache. This acts as a
highly optimized cache. If a URL is not available in the index (for example, if
it's a very new page), the tool automatically falls back to do a live fetch.
This directly accesses the URL to retrieve its content in real-time.

## Combining with other tools

You can combine the URL context tool with other tools to create more powerful
workflows.

### Grounding with search

When both URL context and
 Grounding with Google Search are enabled,
the model can use its search capabilities to find
relevant information online and then use the URL context tool to get a more
in-depth understanding of the pages it finds. This approach is powerful for
prompts that require both broad searching and deep analysis of specific pages.

 
 

### Python

 

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, UrlContext

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = [
      {"url_context": {}},
      {"google_search": {}}
  ]

response = client.models.generate_content(
    model=model_id,
    contents="Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    config=GenerateContentConfig(
        tools=tools,
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)
```

 
 

### Javascript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    ],
    config: {
      tools: [
        {urlContext: {}},
        {googleSearch: {}}
        ],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute."}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          },
          {
              "google_search": {}
          }
      ]
  }' > result.json

cat result.json
```

 
 

## Understanding the response

When the model uses the URL context tool, the response includes a
`url_context_metadata` object. This object lists the URLs the model retrieved
content from and the status of each retrieval attempt, which is useful for
verification and debugging.

The following is an example of that part of the response
(parts of the response have been omitted for brevity):

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata": {
        "url_metadata": [
          {
            "retrieved_url": "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592",
            "url_retrieval_status": "URL_RETRIEVAL_STATUS_SUCCESS"
          },
          {
            "retrieved_url": "https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/",
            "url_retrieval_status": "URL_RETRIEVAL_STATUS_SUCCESS"
          }
        ]
      }
    }
}
```

 

For complete detail about this object , see the
 `UrlContextMetadata` API reference .

### Safety checks

The system performs a content moderation check on the URL to confirm
they meet safety standards. If the URL you provided fails this check, you will
get an `url_retrieval_status` of `URL_RETRIEVAL_STATUS_UNSAFE`.

### Token count

The content retrieved from the URLs you specify in your prompt is counted
as part of the input tokens. You can see the token count for your prompt and
tools usage in the `usage_metadata` 
object of the model output. The following is an example output:

 

```
'usage_metadata': {
  'candidates_token_count': 45,
  'prompt_token_count': 27,
  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 27}],
  'thoughts_token_count': 31,
  'tool_use_prompt_token_count': 10309,
  'tool_use_prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 10309}],
  'total_token_count': 10412
  }
```

 

Price per token depends on the model used, see the
 pricing page for details.

## Supported models

- gemini-2.5-pro 

- gemini-2.5-flash 

- gemini-2.5-flash-lite 

- gemini-live-2.5-flash-preview 

- gemini-2.0-flash-live-001 

## Best Practices

- Provide specific URLs : For the best results, provide direct URLs to the
content you want the model to analyze. The model will only retrieve content
from the URLs you provide, not any content from nested links.

- Check for accessibility : Verify that the URLs you provide don't lead to
pages that require a login or are behind a paywall.

- Use the complete URL : Provide the full URL, including the protocol
(e.g., https://www.google.com instead of just google.com).

## Limitations

- Pricing : Content retrieved from URLs counts as input tokens. Rate limit
and pricing is the based on the model used. See the
 rate limits and
 pricing pages for details.

- Request limit : The tool can process up to 20 URLs per request.

- URL content size : The maximum size for content retrieved from a single
URL is 34MB.

### Supported and unsupported content types

The tool can extract content from URLs with the following content types:

- Text (text/html, application/json, text/plain, text/xml, text/css,
text/javascript , text/csv, text/rtf)

- Image (image/png, image/jpeg, image/bmp, image/webp)

- PDF (application/pdf)

The following content types are not supported:

- Paywalled content

- YouTube videos (See
 video understanding to learn
how to process YouTube URLs)

- Google workspace files like Google docs or spreadsheets

- Video and audio files

## What's next

- Explore the URL context cookbook 
for more examples.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Computer Use &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/computer-use#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Computer Use  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Computer Use 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini 2.5 Computer Use Preview model and tool enable you to build browser
control agents that interact with and automate tasks. Using screenshots, the
Computer Use model can "see" a computer screen, and "act" by generating specific
UI actions like mouse clicks and keyboard inputs. Similar to function calling,
you need to write the client-side application code to receive and execute the
Computer Use actions.

With Computer Use, you can build agents that:

- Automate repetitive data entry or form filling on websites.

- Perform automated testing of web applications and user flows

- Conduct research across various websites (e.g., gathering product
information, prices, and reviews from ecommerce sites to inform a purchase)

The easiest way to test the Gemini Computer Use model is through the reference
implementation or
 Browserbase demo environment .

## How Computer Use works

To build a browser control agent with the Computer Use model, implement
an agent loop that does the following:

- 

 Send a request to the model 

 Add the Computer Use tool and optionally any custom user-defined
functions or excluded functions to your API request.

- Prompt the Computer Use model with the user's request.

 
- 

 Receive the model response 

 The Computer Use model analyzes the user request and screenshot, and
generates a response which includes a suggested `function_call`
representing a UI action (e.g., "click at coordinate (x,y)" or "type
'text'"). For a description of all UI actions supported by the Computer
Use model, see Supported actions .

- The API response may also include a `safety_decision` from an internal
safety system that checks the model's proposed action. This
`safety_decision` classifies the action as:

 Regular / allowed: The action is considered safe. This may also
be represented by no `safety_decision` being present.

- Requires confirmation (`require_confirmation`): The model is about to perform an action
that may be risky (e.g., clicking on an "accept cookie banner").

 

 
- 

 Execute the received action 

 Your client-side code receives the `function_call` and any accompanying
`safety_decision`.

 Regular / allowed: If the `safety_decision` indicates regular /
allowed (or if no `safety_decision` is present), your client-side
code can execute the specified `function_call` in your target
environment (e.g., a web browser).

- Requires confirmation: If the `safety_decision` indicates
requires confirmation, your application must prompt the end-user for
confirmation before executing the `function_call`. If the user
confirms, proceed to execute the action. If the user denies, don't
execute the action.

 

 
- 

 Capture the new environment state 

 If the action has been executed, your client captures a new screenshot
of the GUI and the current URL to send back to the Computer Use model as
part of a `function_response`.

- If an action was blocked by the safety system or denied confirmation by
the user, your application might send a different form of feedback to
the model or end the interaction.

 

This process repeats from step 2 with the Computer Use model using the new
screenshot and the ongoing goal to suggest the next action. The loop continues
until the task is completed, an error occurs, or the process is terminated
(e.g., due to a "block" safety response or user decision).

 

## How to implement Computer Use

Before building with the Computer Use model and tool you will need to set up the
following:

- Secure execution environment: For safety reasons, you should run your
Computer Use agent in a secure and controlled environment (e.g., a sandboxed
virtual machine, a container, or a dedicated browser profile with limited
permissions).

- Client-side action handler: You will need to implement client-side logic
to execute the actions generated by the model and
capture screenshots of the environment after each action.

The examples in this section use a browser as the execution environment
and Playwright as the client-side action handler. To
run these samples you must install the necessary dependencies and initialize a
Playwright browser instance.

 
 
 

#### Install Playwright

 

 

```
    pip install google-genai playwright
    playwright install chromium
```

 
 
 

 
 
 

#### Initialize Playwright browser instance

 

 

```
    from playwright.sync_api import sync_playwright

    # 1. Configure screen dimensions for the target environment
    SCREEN_WIDTH = 1440
    SCREEN_HEIGHT = 900

    # 2. Start the Playwright browser
    # In production, utilize a sandboxed environment.
    playwright = sync_playwright().start()
    # Set headless=False to see the actions performed on your screen
    browser = playwright.chromium.launch(headless=False)

    # 3. Create a context and page with the specified dimensions
    context = browser.new_context(
        viewport={"width": SCREEN_WIDTH, "height": SCREEN_HEIGHT}
    )
    page = context.new_page()

    # 4. Navigate to an initial page to start the task
    page.goto("https://www.google.com")

    # The 'page', 'SCREEN_WIDTH', and 'SCREEN_HEIGHT' variables
    # will be used in the steps below.
```

 
 
 

Sample code for extending to an Android
environment is included in the Using custom user-defined
functions section.

### 1. Send a request to the model

Add the Computer Use tool to your API request and send a prompt to the Computer
Use model that includes the user's goal.
You must use the Gemini Computer Use model,
`gemini-2.5-computer-use-preview-10-2025`. If you try to use the Computer Use
tool with a different model, you will get an error.

You can also optionally add the following parameters:

- Excluded actions: If there are any actions from the list of Supported
UI actions that you don't want the model to take,
specify these actions as `excluded_predefined_functions`.

- User-defined functions: In addition to the Computer Use tool, you may
want to include custom user-defined functions.

Note that there is no need to specify the display size when issuing a request;
the model predicts pixel coordinates scaled to the height and width of the
screen.

 
 

### Python

 

```
from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

# Specify predefined functions to exclude (optional)
excluded_functions = ["drag_and_drop"]

generate_content_config = genai.types.GenerateContentConfig(
    tools=[
        # 1. Computer Use tool with browser environment
        types.Tool(
            computer_use=types.ComputerUse(
                environment=types.Environment.ENVIRONMENT_BROWSER,
                # Optional: Exclude specific predefined functions
                excluded_predefined_functions=excluded_functions
                )
              ),
        # 2. Optional: Custom user-defined functions
        #types.Tool(
          # function_declarations=custom_functions
          #   )
          ],
  )

# Create the content with user message
contents=[
    Content(
        role="user",
        parts=[
            Part(text="Search for highly rated smart fridges with touchscreen, 2 doors, around 25 cu ft, priced below 4000 dollars on Google Shopping. Create a bulleted list of the 3 cheapest options in the format of name, description, price in an easy-to-read layout."),
        ],
    )
]

# Generate content with the configured settings
response = client.models.generate_content(
    model='gemini-2.5-computer-use-preview-10-2025',
    contents=contents,
    config=generate_content_config,
)

# Print the response output
print(response)
```

 
 

For an example with custom functions, see Using custom
user-defined functions .

### 2. Receive the model response

The Computer Use model will respond with one or more `FunctionCalls` if it
determines UI actions are needed to complete the task. Computer Use supports
parallel function calling, meaning the model can return multiple actions in a
single turn.

Here is an example model response.

 

```
{
  "content": {
    "parts": [
      {
        "text": "I will type the search query into the search bar. The search bar is in the center of the page."
      },
      {
        "function_call": {
          "name": "type_text_at",
          "args": {
            "x": 371,
            "y": 470,
            "text": "highly rated smart fridges with touchscreen, 2 doors, around 25 cu ft, priced below 4000 dollars on Google Shopping",
            "press_enter": true
          }
        }
      }
    ]
  }
}
```

 

### 3. Execute the received actions

Your application code needs to parse the model response, execute the actions,
and collect the results.

The example code below extracts function calls from the Computer Use model
response, and translates them into actions that can be executed with Playwright.
The model outputs normalized coordinates (0-999) regardless of the input image
dimensions, so part of the translation step is converting these normalized
coordinates back to actual pixel values.

The recommended screen size for use
with the Computer Use model is (1440, 900). The model will work with any
resolution, though the quality of the results may be impacted.

Note that this example only includes the implementation for the 3 most common
UI actions: `open_web_browser`, `click_at`, and `type_text_at`. For
production use cases, you will need to implement all other UI actions from the
 Supported actions list unless you explicitly add them as
`excluded_predefined_functions`.

 
 

### Python

 

```
from typing import Any, List, Tuple
import time

def denormalize_x(x: int, screen_width: int) -> int:
    """Convert normalized x coordinate (0-1000) to actual pixel coordinate."""
    return int(x / 1000 * screen_width)

def denormalize_y(y: int, screen_height: int) -> int:
    """Convert normalized y coordinate (0-1000) to actual pixel coordinate."""
    return int(y / 1000 * screen_height)

def execute_function_calls(candidate, page, screen_width, screen_height):
    results = []
    function_calls = []
    for part in candidate.content.parts:
        if part.function_call:
            function_calls.append(part.function_call)

    for function_call in function_calls:
        action_result = {}
        fname = function_call.name
        args = function_call.args
        print(f"  -> Executing: {fname}")

        try:
            if fname == "open_web_browser":
                pass # Already open
            elif fname == "click_at":
                actual_x = denormalize_x(args["x"], screen_width)
                actual_y = denormalize_y(args["y"], screen_height)
                page.mouse.click(actual_x, actual_y)
            elif fname == "type_text_at":
                actual_x = denormalize_x(args["x"], screen_width)
                actual_y = denormalize_y(args["y"], screen_height)
                text = args["text"]
                press_enter = args.get("press_enter", False)

                page.mouse.click(actual_x, actual_y)
                # Simple clear (Command+A, Backspace for Mac)
                page.keyboard.press("Meta+A")
                page.keyboard.press("Backspace")
                page.keyboard.type(text)
                if press_enter:
                    page.keyboard.press("Enter")
            else:
                print(f"Warning: Unimplemented or custom function {fname}")

            # Wait for potential navigations/renders
            page.wait_for_load_state(timeout=5000)
            time.sleep(1)

        except Exception as e:
            print(f"Error executing {fname}: {e}")
            action_result = {"error": str(e)}

        results.append((fname, action_result))

    return results
```

 
 

### 4. Capture the new environment state

After executing the actions, send the result of the function execution back to
the model so it can use this information to generate the next action. If
multiple actions (parallel calls) were executed, you must send a
`FunctionResponse` for each one in the subsequent user turn.

 
 

### Python

 

```
def get_function_responses(page, results):
    screenshot_bytes = page.screenshot(type="png")
    current_url = page.url
    function_responses = []
    for name, result in results:
        response_data = {"url": current_url}
        response_data.update(result)
        function_responses.append(
            types.FunctionResponse(
                name=name,
                response=response_data,
                parts=[types.FunctionResponsePart(
                        inline_data=types.FunctionResponseBlob(
                            mime_type="image/png",
                            data=screenshot_bytes))
                ]
            )
        )
    return function_responses
```

 
 

## Build an agent loop

To enable multi-step interactions, combine the four steps from the How to
implement Computer Use section into a loop.
Remember to manage the conversation history correctly by appending both model
responses and your function responses.

To run this code sample you need to:

- Install the necessary Playwright
dependencies .

- 

Define the helper functions from steps (3) Execute the received
actions and (4) Capture the new environment
state .

 
 

### Python

 

```
import time
from typing import Any, List, Tuple
from playwright.sync_api import sync_playwright

from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

# Constants for screen dimensions
SCREEN_WIDTH = 1440
SCREEN_HEIGHT = 900

# Setup Playwright
print("Initializing browser...")
playwright = sync_playwright().start()
browser = playwright.chromium.launch(headless=False)
context = browser.new_context(viewport={"width": SCREEN_WIDTH, "height": SCREEN_HEIGHT})
page = context.new_page()

# Define helper functions. Copy/paste from steps 3 and 4
# def denormalize_x(...)
# def denormalize_y(...)
# def execute_function_calls(...)
# def get_function_responses(...)

try:
    # Go to initial page
    page.goto("https://ai.google.dev/gemini-api/docs")

    # Configure the model (From Step 1)
    config = types.GenerateContentConfig(
        tools=[types.Tool(computer_use=types.ComputerUse(
            environment=types.Environment.ENVIRONMENT_BROWSER
        ))],
        thinking_config=types.ThinkingConfig(include_thoughts=True),
    )

    # Initialize history
    initial_screenshot = page.screenshot(type="png")
    USER_PROMPT = "Go to ai.google.dev/gemini-api/docs and search for pricing."
    print(f"Goal: {USER_PROMPT}")

    contents = [
        Content(role="user", parts=[
            Part(text=USER_PROMPT),
            Part.from_bytes(data=initial_screenshot, mime_type='image/png')
        ])
    ]

    # Agent Loop
    turn_limit = 5
    for i in range(turn_limit):
        print(f"\n--- Turn {i+1} ---")
        print("Thinking...")
        response = client.models.generate_content(
            model='gemini-2.5-computer-use-preview-10-2025',
            contents=contents,
            config=config,
        )

        candidate = response.candidates[0]
        contents.append(candidate.content)

        has_function_calls = any(part.function_call for part in candidate.content.parts)
        if not has_function_calls:
            text_response = " ".join([part.text for part in candidate.content.parts if part.text])
            print("Agent finished:", text_response)
            break

        print("Executing actions...")
        results = execute_function_calls(candidate, page, SCREEN_WIDTH, SCREEN_HEIGHT)

        print("Capturing state...")
        function_responses = get_function_responses(page, results)

        contents.append(
            Content(role="user", parts=[Part(function_response=fr) for fr in function_responses])
        )

finally:
    # Cleanup
    print("\nClosing browser...")
    browser.close()
    playwright.stop()
```

 
 

## Using custom user-defined functions

You can optionally include custom user-defined functions in your request to
extend the functionality of the model. The example below adapts the Computer Use
model and tool for mobile use cases by including custom user-defined actions
like `open_app`, `long_press_at`, and `go_home`, while excluding
browser-specific actions. The model can intelligently call these custom
functions alongside standard UI actions to complete tasks in non-browser
environments.

 
 

### Python

 

```
from typing import Optional, Dict, Any

from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

SYSTEM_PROMPT = """You are operating an Android phone. Today's date is October 15, 2023, so ignore any other date provided.
* To provide an answer to the user, *do not use any tools* and output your answer on a separate line. IMPORTANT: Do not add any formatting or additional punctuation/text, just output the answer by itself after two empty lines.
* Make sure you scroll down to see everything before deciding something isn't available.
* You can open an app from anywhere. The icon doesn't have to currently be on screen.
* Unless explicitly told otherwise, make sure to save any changes you make.
* If text is cut off or incomplete, scroll or click into the element to get the full text before providing an answer.
* IMPORTANT: Complete the given task EXACTLY as stated. DO NOT make any assumptions that completing a similar task is correct.  If you can't find what you're looking for, SCROLL to find it.
* If you want to edit some text, ONLY USE THE `type` tool. Do not use the onscreen keyboard.
* Quick settings shouldn't be used to change settings. Use the Settings app instead.
* The given task may already be completed. If so, there is no need to do anything.
"""

def open_app(app_name: str, intent: Optional[str] = None) -> Dict[str, Any]:
    """Opens an app by name.

    Args:
        app_name: Name of the app to open (any string).
        intent: Optional deep-link or action to pass when launching, if the app supports it.

    Returns:
        JSON payload acknowledging the request (app name and optional intent).
    """
    return {"status": "requested_open", "app_name": app_name, "intent": intent}

def long_press_at(x: int, y: int) -> Dict[str, int]:
    """Long-press at a specific screen coordinate.

    Args:
        x: X coordinate (absolute), scaled to the device screen width (pixels).
        y: Y coordinate (absolute), scaled to the device screen height (pixels).

    Returns:
        Object with the coordinates pressed and the duration used.
    """
    return {"x": x, "y": y}

def go_home() -> Dict[str, str]:
    """Navigates to the device home screen.

    Returns:
        A small acknowledgment payload.
    """
    return {"status": "home_requested"}

#  Build function declarations
CUSTOM_FUNCTION_DECLARATIONS = [
    types.FunctionDeclaration.from_callable(client=client, callable=open_app),
    types.FunctionDeclaration.from_callable(client=client, callable=long_press_at),
    types.FunctionDeclaration.from_callable(client=client, callable=go_home),
]

#Exclude browser functions
EXCLUDED_PREDEFINED_FUNCTIONS = [
    "open_web_browser",
    "search",
    "navigate",
    "hover_at",
    "scroll_document",
    "go_forward",
    "key_combination",
    "drag_and_drop",
]

#Utility function to construct a GenerateContentConfig
def make_generate_content_config() -> genai.types.GenerateContentConfig:
    """Return a fixed GenerateContentConfig with Computer Use + custom functions."""
    return genai.types.GenerateContentConfig(
        system_instruction=SYSTEM_PROMPT,
        tools=[
            types.Tool(
                computer_use=types.ComputerUse(
                    environment=types.Environment.ENVIRONMENT_BROWSER,
                    excluded_predefined_functions=EXCLUDED_PREDEFINED_FUNCTIONS,
                )
            ),
            types.Tool(function_declarations=CUSTOM_FUNCTION_DECLARATIONS),
        ],
    )

# Create the content with user message
contents: list[Content] = [
    Content(
        role="user",
        parts=[
            # text instruction
            Part(text="Open Chrome, then long-press at 200,400."),
        ],
    )
]

# Build your fixed config (from helper)
config = make_generate_content_config()

# Generate content with the configured settings
response = client.models.generate_content(
        model='gemini-2.5-computer-use-preview-10-2025',
        contents=contents,
        config=config,
    )

print(response)
```

 
 

## Supported UI actions

The Computer Use model can request the following UI actions via a
`FunctionCall`. Your client-side code must implement the execution logic for
these actions. See the reference
implementation for
examples.

 
 
 
 Command Name 
 Description 
 Arguments (in Function Call) 
 Example Function Call 
 
 

 
 
 open_web_browser 
 Opens the web browser. 
 None 
 

```
{"name": "open_web_browser", "args": {}}
```

 
 
 
 wait_5_seconds 
 Pauses execution for 5 seconds to allow dynamic content to load or animations to complete. 
 None 
 

```
{"name": "wait_5_seconds", "args": {}}
```

 
 
 
 go_back 
 Navigates to the previous page in the browser's history. 
 None 
 

```
{"name": "go_back", "args": {}}
```

 
 
 
 go_forward 
 Navigates to the next page in the browser's history. 
 None 
 

```
{"name": "go_forward", "args": {}}
```

 
 
 
 search 
 Navigates to the default search engine's homepage (e.g., Google). Useful for starting a new search task. 
 None 
 

```
{"name": "search", "args": {}}
```

 
 
 
 navigate 
 Navigates the browser directly to the specified URL. 
 `url`: str 
 

```
{"name": "navigate", "args": {"url": "https://www.wikipedia.org"}}
```

 
 
 
 click_at 
 Clicks at a specific coordinate on the webpage. The x and y values are based on a 1000x1000 grid and are scaled to the screen dimensions. 
 `y`: int (0-999), `x`: int (0-999) 
 

```
{"name": "click_at", "args": {"y": 300, "x": 500}}
```

 
 
 
 hover_at 
 Hovers the mouse at a specific coordinate on the webpage. Useful for revealing sub-menus. x and y are based on a 1000x1000 grid. 
 `y`: int (0-999) `x`: int (0-999) 
 

```
{"name": "hover_at", "args": {"y": 150, "x": 250}}
```

 
 
 
 type_text_at 
 Types text at a specific coordinate, defaults to clearing the field first and pressing ENTER after typing, but these can be disabled. x and y are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `text`: str, `press_enter`: bool (Optional, default True), `clear_before_typing`: bool (Optional, default True) 
 

```
{"name": "type_text_at", "args": {"y": 250, "x": 400, "text": "search query", "press_enter": false}}
```

 
 
 
 key_combination 
 Press keyboard keys or combinations, such as "Control+C" or "Enter". Useful for triggering actions (like submitting a form with "Enter") or clipboard operations. 
 `keys`: str (e.g. 'enter', 'control+c'). 
 

```
{"name": "key_combination", "args": {"keys": "Control+A"}}
```

 
 
 
 scroll_document 
 Scrolls the entire webpage "up", "down", "left", or "right". 
 `direction`: str ("up", "down", "left", or "right") 
 

```
{"name": "scroll_document", "args": {"direction": "down"}}
```

 
 
 
 scroll_at 
 Scrolls a specific element or area at coordinate (x, y) in the specified direction by a certain magnitude. Coordinates and magnitude (default 800) are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `direction`: str ("up", "down", "left", "right"), `magnitude`: int (0-999, Optional, default 800) 
 

```
{"name": "scroll_at", "args": {"y": 500, "x": 500, "direction": "down", "magnitude": 400}}
```

 
 
 
 drag_and_drop 
 Drags an element from a starting coordinate (x, y) and drops it at a destination coordinate (destination_x, destination_y). All coordinates are based on a 1000x1000 grid. 
 `y`: int (0-999), `x`: int (0-999), `destination_y`: int (0-999), `destination_x`: int (0-999) 
 

```
{"name": "drag_and_drop", "args": {"y": 100, "x": 100, "destination_y": 500, "destination_x": 500}}
```

 
 
 
 

## Safety and security

### Acknowledge safety decision

Depending on the action, the model response might also include a
`safety_decision` from an internal safety system that checks the model's
proposed action.

 

```
{
  "content": {
    "parts": [
      {
        "text": "I have evaluated step 2. It seems Google detected unusual traffic and is asking me to verify I'm not a robot. I need to click the 'I'm not a robot' checkbox located near the top left (y=98, x=95).",
      },
      {
        "function_call": {
          "name": "click_at",
          "args": {
            "x": 60,
            "y": 100,
            "safety_decision": {
              "explanation": "I have encountered a CAPTCHA challenge that requires interaction. I need you to complete the challenge by clicking the 'I'm not a robot' checkbox and any subsequent verification steps.",
              "decision": "require_confirmation"
            }
          }
        }
      }
    ]
  }
}
```

 

If the `safety_decision` is `require_confirmation`, you must
ask the end user to confirm before proceeding with executing the action. Per the
 terms of service , you are not allowed
to bypass requests for human confirmation.

This code sample prompts the end-user for confirmation before executing the
action. If the user does not confirm the action, the loop terminates. If the
user confirms the action, the action is executed and the
`safety_acknowledgement` field is marked as `True`.

 
 

### Python

 

```
import termcolor

def get_safety_confirmation(safety_decision):
    """Prompt user for confirmation when safety check is triggered."""
    termcolor.cprint("Safety service requires explicit confirmation!", color="red")
    print(safety_decision["explanation"])

    decision = ""
    while decision.lower() not in ("y", "n", "ye", "yes", "no"):
        decision = input("Do you wish to proceed? [Y]es/[N]o\n")

    if decision.lower() in ("n", "no"):
        return "TERMINATE"
    return "CONTINUE"

def execute_function_calls(candidate, page, screen_width, screen_height):

    # ... Extract function calls from response ...

    for function_call in function_calls:
        extra_fr_fields = {}

        # Check for safety decision
        if 'safety_decision' in function_call.args:
            decision = get_safety_confirmation(function_call.args['safety_decision'])
            if decision == "TERMINATE":
                print("Terminating agent loop")
                break
            extra_fr_fields["safety_acknowledgement"] = "true" # Safety acknowledgement

        # ... Execute function call and append to results ...
```

 
 

If the user confirms, you must include the safety acknowledgement in
your `FunctionResponse`.

 
 

### Python

 

```
function_response_parts.append(
    FunctionResponse(
        name=name,
        response={"url": current_url,
                  **extra_fr_fields},  # Include safety acknowledgement
        parts=[
            types.FunctionResponsePart(
                inline_data=types.FunctionResponseBlob(
                    mime_type="image/png", data=screenshot
                )
             )
           ]
         )
       )
```

 
 

### Safety best practices

Computer Use API is a novel API and presents new risks that developers should be
mindful of:

- Untrusted content & scams: As the model tries to achieve the user's
goal, it may rely on untrustworthy sources of information and instructions
from the screen. For example, if the user's goal is to purchase a Pixel
phone and the model encounters a "Free-Pixel if you complete a survey" scam,
there is some chance that the model will complete the survey.

- Occasional unintended actions: The model can misinterpret a user's goal
or webpage content, causing it to take incorrect actions like clicking the
wrong button or filling the wrong form. This can lead to failed tasks or
data exfiltration.

- Policy violations: The API's capabilities could be directed, either
intentionally or unintentionally, toward activities that violate Google's
policies ( Gen AI Prohibited Use
Policy and the
 Gemini API Additional Terms of
Service . This includes actions that
could interfere with a system's integrity, compromise security, bypass
security measures,
control medical devices, etc.

To address these risks, you can implement the following safety measures and best
practices:

- 

 Human-in-the-Loop (HITL): 

 Implement user confirmation: When the safety response indicates
`require_confirmation`, you must implement user confirmation before
execution. See Acknowledge safety decision for
sample code.

- 

 Provide custom safety instructions: In addition to the built-in user
confirmation checks, developers may optionally add a custom system
instruction 
that enforces their own safety policies, either to block certain model
actions or require user confirmation before the model takes certain
high-stakes irreversible actions. Here is an example of a custom safety
system instruction you may include when interacting with the model.

 
 
 

#### Example safety instructions

 

Set your custom safety rules as a system instruction:

 

```
    ## **RULE 1: Seek User Confirmation (USER_CONFIRMATION)**

    This is your first and most important check. If the next required action falls
    into any of the following categories, you MUST stop immediately, and seek the
    user's explicit permission.

    **Procedure for Seeking Confirmation:**  * **For Consequential Actions:**
    Perform all preparatory steps (e.g., navigating, filling out forms, typing a
    message). You will ask for confirmation **AFTER** all necessary information is
    entered on the screen, but **BEFORE** you perform the final, irreversible action
    (e.g., before clicking "Send", "Submit", "Confirm Purchase", "Share").  * **For
    Prohibited Actions:** If the action is strictly forbidden (e.g., accepting legal
    terms, solving a CAPTCHA), you must first inform the user about the required
    action and ask for their confirmation to proceed.

    **USER_CONFIRMATION Categories:**

    *   **Consent and Agreements:** You are FORBIDDEN from accepting, selecting, or
        agreeing to any of the following on the user's behalf. You must ask the
        user to confirm before performing these actions.
        *   Terms of Service
        *   Privacy Policies
        *   Cookie consent banners
        *   End User License Agreements (EULAs)
        *   Any other legally significant contracts or agreements.
    *   **Robot Detection:** You MUST NEVER attempt to solve or bypass the
        following. You must ask the user to confirm before performing these actions.
    *   CAPTCHAs (of any kind)
        *   Any other anti-robot or human-verification mechanisms, even if you are
            capable.
    *   **Financial Transactions:**
        *   Completing any purchase.
        *   Managing or moving money (e.g., transfers, payments).
        *   Purchasing regulated goods or participating in gambling.
    *   **Sending Communications:**
        *   Sending emails.
        *   Sending messages on any platform (e.g., social media, chat apps).
        *   Posting content on social media or forums.
    *   **Accessing or Modifying Sensitive Information:**
        *   Health, financial, or government records (e.g., medical history, tax
            forms, passport status).
        *   Revealing or modifying sensitive personal identifiers (e.g., SSN, bank
            account number, credit card number).
    *   **User Data Management:**
        *   Accessing, downloading, or saving files from the web.
        *   Sharing or sending files/data to any third party.
        *   Transferring user data between systems.
    *   **Browser Data Usage:**
        *   Accessing or managing Chrome browsing history, bookmarks, autofill data,
            or saved passwords.
    *   **Security and Identity:**
        *   Logging into any user account.
        *   Any action that involves misrepresentation or impersonation (e.g.,
            creating a fan account, posting as someone else).
    *   **Insurmountable Obstacles:** If you are technically unable to interact with
        a user interface element or are stuck in a loop you cannot resolve, ask the
        user to take over.
    ---

    ## **RULE 2: Default Behavior (ACTUATE)**

    If an action does **NOT** fall under the conditions for `USER_CONFIRMATION`,
    your default behavior is to **Actuate**.

    **Actuation Means:**  You MUST proactively perform all necessary steps to move
    the user's request forward. Continue to actuate until you either complete the
    non-consequential task or encounter a condition defined in Rule 1.

    *   **Example 1:** If asked to send money, you will navigate to the payment
        portal, enter the recipient's details, and enter the amount. You will then
        **STOP** as per Rule 1 and ask for confirmation before clicking the final
        "Send" button.
    *   **Example 2:** If asked to post a message, you will navigate to the site,
        open the post composition window, and write the full message. You will then
        **STOP** as per Rule 1 and ask for confirmation before clicking the final
        "Post" button.

        After the user has confirmed, remember to get the user's latest screen
        before continuing to perform actions.

    # Final Response Guidelines:
    Write final response to the user in the following cases:
    - User confirmation
    - When the task is complete or you have enough information to respond to the user
```

 
 
 

 
- 

 Secure execution environment: Run your agent in a secure, sandboxed
environment to limit its potential impact (e.g., A sandboxed virtual machine
(VM), a container (e.g., Docker), or a dedicated browser profile with limited
permissions).

- 

 Input sanitization: Sanitize all user-generated text in prompts to
mitigate the risk of unintended instructions or prompt injection. This is a
helpful layer of security, but not a replacement for a secure execution
environment.

- 

 Content guardrails: Use guardrails and content safety
APIs to evaluate user inputs,
tool input and output, an agent's response for appropriateness, prompt
injection, and jailbreak detection.

- 

 Allowlists and blocklists: Implement filtering mechanisms to control
where the model can navigate and what it can do. A blocklist of prohibited
websites is a good starting point, while a more restrictive allowlist is
even more secure.

- 

 Observability and logging: Maintain detailed logs for debugging,
auditing, and incident response. Your client should log prompts,
screenshots, model-suggested actions (function_call), safety responses, and
all actions ultimately executed by the client.

- 

 Environment management: Ensure the GUI environment is consistent.
Unexpected pop-ups, notifications, or changes in layout can confuse the
model. Start from a known, clean state for each new task if possible.

## Model versions

 
 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`gemini-2.5-computer-use-preview-10-2025`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Image, text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

128,000

 
 
 

 Output token limit 

 

64,000

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-computer-use-preview-10-2025`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 

## What's next

- Experiment with Computer Use in the Browserbase demo
environment .

- Check out the Reference
implementation for example
code.

- Learn about other Gemini API tools:

 Function calling 

- Grounding with Google Search 

 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-24 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-24 UTC."],[],[]]

---

### Batch API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/batch-api

- 
 
 
 
 
 
 
 
 
 
 
 Batch API  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Batch API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini Batch API is designed to process large volumes of requests
asynchronously at 50% of the standard cost .
The target turnaround time is 24 hours, but in majority of cases, it is much
quicker.

Use Batch API for large-scale, non-urgent tasks such as data
pre-processing or running evaluations where an immediate response is not
required.

## Creating a batch job

You have two ways to submit your requests in Batch API:

- Inline requests : A list of
 `GenerateContentRequest` objects
directly included in your batch creation request. This is suitable for
smaller batches that keep the total request size under 20MB. The output 
returned from the model is a list of `inlineResponse` objects.

- Input file : A JSON Lines (JSONL) 
file where each line contains a complete
 `GenerateContentRequest` object.
This method is recommended for larger requests. The output 
returned from the model is a JSONL file where each line is either a
`GenerateContentResponse` or a status object.

### Inline requests

For a small number of requests, you can directly embed the
 `GenerateContentRequest` objects
within your `BatchGenerateContentRequest` . The
following example calls the
 `BatchGenerateContent` 
method with inline requests:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# A list of dictionaries, where each is a GenerateContentRequest
inline_requests = [
    {
        'contents': [{
            'parts': [{'text': 'Tell me a one-sentence joke.'}],
            'role': 'user'
        }]
    },
    {
        'contents': [{
            'parts': [{'text': 'Why is the sky blue?'}],
            'role': 'user'
        }]
    }
]

inline_batch_job = client.batches.create(
    model="models/gemini-2.5-flash",
    src=inline_requests,
    config={
        'display_name': "inlined-requests-job-1",
    },
)

print(f"Created batch job: {inline_batch_job.name}")
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

const inlinedRequests = [
    {
        contents: [{
            parts: [{text: 'Tell me a one-sentence joke.'}],
            role: 'user'
        }]
    },
    {
        contents: [{
            parts: [{'text': 'Why is the sky blue?'}],
            role: 'user'
        }]
    }
]

const response = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: inlinedRequests,
    config: {
        displayName: 'inlined-requests-job-1',
    }
});

console.log(response);
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-X POST \
-H "Content-Type:application/json" \
-d '{
    "batch": {
        "display_name": "my-batch-requests",
        "input_config": {
            "requests": {
                "requests": [
                    {
                        "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
                        "metadata": {
                            "key": "request-1"
                        }
                    },
                    {
                        "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
                        "metadata": {
                            "key": "request-2"
                        }
                    }
                ]
            }
        }
    }
}'
```

 
 

### Input file

For larger sets of requests, prepare a JSON Lines (JSONL) file. Each line in
this file must be a JSON object containing a user-defined key and a request
object, where the request is a valid
 `GenerateContentRequest` object. The
user-defined key is used in the response to indicate which output is the result
of which request. For example, the request with the key defined as `request-1`
will have its response annotated with the same key name.

This file is uploaded using the File API . The maximum
allowed file size for an input file is 2GB.

The following is an example of a JSONL file. You can save it in a file named
`my-batch-requests.json`:

 

```
{"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generation_config": {"temperature": 0.7}}}
{"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}
```

 

Similarly to inline requests, you can specify other parameters like system
instructions, tools or other configurations in each request JSON.

You can upload this file using the File API as
shown in the following example. If
you are working with multimodal input, you can reference other uploaded files
within your JSONL file.

 
 

### Python

 

```
import json
from google import genai
from google.genai import types

client = genai.Client()

# Create a sample JSONL file
with open("my-batch-requests.jsonl", "w") as f:
    requests = [
        {"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]}},
        {"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}
    ]
    for req in requests:
        f.write(json.dumps(req) + "\n")

# Upload the file to the File API
uploaded_file = client.files.upload(
    file='my-batch-requests.jsonl',
    config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')
)

print(f"Uploaded file: {uploaded_file.name}")
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import * as fs from "fs";
import * as path from "path";
import { fileURLToPath } from 'url';

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});
const fileName = "my-batch-requests.jsonl";

// Define the requests
const requests = [
    { "key": "request-1", "request": { "contents": [{ "parts": [{ "text": "Describe the process of photosynthesis." }] }] } },
    { "key": "request-2", "request": { "contents": [{ "parts": [{ "text": "What are the main ingredients in a Margherita pizza?" }] }] } }
];

// Construct the full path to file
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const filePath = path.join(__dirname, fileName); // __dirname is the directory of the current script

async function writeBatchRequestsToFile(requests, filePath) {
    try {
        // Use a writable stream for efficiency, especially with larger files.
        const writeStream = fs.createWriteStream(filePath, { flags: 'w' });

        writeStream.on('error', (err) => {
            console.error(`Error writing to file ${filePath}:`, err);
        });

        for (const req of requests) {
            writeStream.write(JSON.stringify(req) + '\n');
        }

        writeStream.end();

        console.log(`Successfully wrote batch requests to ${filePath}`);

    } catch (error) {
        // This catch block is for errors that might occur before stream setup,
        // stream errors are handled by the 'error' event.
        console.error(`An unexpected error occurred:`, error);
    }
}

// Write to a file.
writeBatchRequestsToFile(requests, filePath);

// Upload the file to the File API.
const uploadedFile = await ai.files.upload({file: 'my-batch-requests.jsonl', config: {
    mimeType: 'jsonl',
}});
console.log(uploadedFile.name);
```

 
 

### REST

 

```
tmp_batch_input_file=batch_input.tmp
echo -e '{"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generationConfig": {"temperature": 0.7}}\n{"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}' > batch_input.tmp
MIME_TYPE=$(file -b --mime-type "${tmp_batch_input_file}")
NUM_BYTES=$(wc -c < "${tmp_batch_input_file}")
DISPLAY_NAME=BatchInput

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
-D "${tmp_header_file}" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "X-Goog-Upload-Protocol: resumable" \
-H "X-Goog-Upload-Command: start" \
-H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
-H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
-H "Content-Type: application/jsonl" \
-d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
-H "Content-Length: ${NUM_BYTES}" \
-H "X-Goog-Upload-Offset: 0" \
-H "X-Goog-Upload-Command: upload, finalize" \
--data-binary "@${tmp_batch_input_file}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
```

 
 

The following example calls the
 `BatchGenerateContent` 
method with the input file uploaded using File API:

 
 

### Python

 

```
from google import genai

# Assumes `uploaded_file` is the file object from the previous step
client = genai.Client()
file_batch_job = client.batches.create(
    model="gemini-2.5-flash",
    src=uploaded_file.name,
    config={
        'display_name': "file-upload-job-1",
    },
)

print(f"Created batch job: {file_batch_job.name}")
```

 
 

### JavaScript

 

```
// Assumes `uploadedFile` is the file object from the previous step
const fileBatchJob = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: uploadedFile.name,
    config: {
        displayName: 'file-upload-job-1',
    }
});

console.log(fileBatchJob);
```

 
 

### REST

 

```
# Set the File ID taken from the upload response.
BATCH_INPUT_FILE='files/123456'
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
-X POST \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" \
-d "{
    'batch': {
        'display_name': 'my-batch-requests',
        'input_config': {
            'file_name': '${BATCH_INPUT_FILE}'
        }
    }
}"
```

 
 

When you create a batch job, you will get a job name returned. Use this name
for monitoring the job status as well as
 retrieving the results once the job completes.

The following is an example output that contains a job name:

 

```
Created batch job from file: batches/123456789
```

 

### Batch embedding support

You can use the Batch API to interact with the
 Embeddings model for higher throughput.
To create an embeddings batch job with either inline requests 
or input files , use the `batches.create_embeddings` API and
specify the embeddings model.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Creating an embeddings batch job with an input file request:
file_job = client.batches.create_embeddings(
    model="gemini-embedding-001",
    src={'file_name': uploaded_batch_requests.name},
    config={'display_name': "Input embeddings batch"},
)

# Creating an embeddings batch job with an inline request:
batch_job = client.batches.create_embeddings(
    model="gemini-embedding-001",
    # For a predefined list of requests `inlined_requests`
    src={'inlined_requests': inlined_requests},
    config={'display_name': "Inlined embeddings batch"},
)
```

 
 

### JavaScript

 

```
// Creating an embeddings batch job with an input file request:
let fileJob;
fileJob = await client.batches.createEmbeddings({
    model: 'gemini-embedding-001',
    src: {fileName: uploadedBatchRequests.name},
    config: {displayName: 'Input embeddings batch'},
});
console.log(`Created batch job: ${fileJob.name}`);

// Creating an embeddings batch job with an inline request:
let batchJob;
batchJob = await client.batches.createEmbeddings({
    model: 'gemini-embedding-001',
    // For a predefined a list of requests `inlinedRequests`
    src: {inlinedRequests: inlinedRequests},
    config: {displayName: 'Inlined embeddings batch'},
});
console.log(`Created batch job: ${batchJob.name}`);
```

 
 

Read the Embeddings section in the Batch API cookbook 
for more examples.

### Request configuration

You can include any request configurations you would use in a standard non-batch
request. For example, you could specify the temperature, system instructions or
even pass in other modalities. The following example shows an example inline
request that contains a system instruction for one of the requests:

 
 

### Python

 

```
inline_requests_list = [
    {'contents': [{'parts': [{'text': 'Write a short poem about a cloud.'}]}]},
    {'contents': [{
        'parts': [{
            'text': 'Write a short poem about a cat.'
            }]
        }],
    'config': {
        'system_instruction': {'parts': [{'text': 'You are a cat. Your name is Neko.'}]}}
    }
]
```

 
 

### JavaScript

 

```
inlineRequestsList = [
    {contents: [{parts: [{text: 'Write a short poem about a cloud.'}]}]},
    {contents: [{parts: [{text: 'Write a short poem about a cat.'}]}],
     config: {systemInstruction: {parts: [{text: 'You are a cat. Your name is Neko.'}]}}}
]
```

 
 

Similarly can specify tools to use for a request. The following example
shows a request that enables the Google Search tool :

 
 

### Python

 

```
inlined_requests = [
{'contents': [{'parts': [{'text': 'Who won the euro 1998?'}]}]},
{'contents': [{'parts': [{'text': 'Who won the euro 2025?'}]}],
 'config':{'tools': [{'google_search': {}}]}}]
```

 
 

### JavaScript

 

```
inlineRequestsList = [
    {contents: [{parts: [{text: 'Who won the euro 1998?'}]}]},
    {contents: [{parts: [{text: 'Who won the euro 2025?'}]}],
     config: {tools: [{googleSearch: {}}]}}
]
```

 
 

You can specify structured output as well.
The following example shows how to specify for your batch requests.

 
 

### Python

 

```
import time
from google import genai
from pydantic import BaseModel, TypeAdapter

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()

# A list of dictionaries, where each is a GenerateContentRequest
inline_requests = [
    {
        'contents': [{
            'parts': [{'text': 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
            'role': 'user'
        }],
        'config': {
            'response_mime_type': 'application/json',
            'response_schema': list[Recipe]
        }
    },
    {
        'contents': [{
            'parts': [{'text': 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
            'role': 'user'
        }],
        'config': {
            'response_mime_type': 'application/json',
            'response_schema': list[Recipe]
        }
    }
]

inline_batch_job = client.batches.create(
    model="models/gemini-2.5-flash",
    src=inline_requests,
    config={
        'display_name': "structured-output-job-1"
    },
)

# wait for the job to finish
job_name = inline_batch_job.name
print(f"Polling status for job: {job_name}")

while True:
    batch_job_inline = client.batches.get(name=job_name)
    if batch_job_inline.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED'):
        break
    print(f"Job not finished. Current state: {batch_job_inline.state.name}. Waiting 30 seconds...")
    time.sleep(30)

print(f"Job finished with state: {batch_job_inline.state.name}")

# print the response
for i, inline_response in enumerate(batch_job_inline.dest.inlined_responses, start=1):
    print(f"\n--- Response {i} ---")

    # Check for a successful response
    if inline_response.response:
        # The .text property is a shortcut to the generated text.
        print(inline_response.response.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI, Type} from '@google/genai';
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

const inlinedRequests = [
    {
        contents: [{
            parts: [{text: 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
            role: 'user'
        }],
        config: {
            responseMimeType: 'application/json',
            responseSchema: {
            type: Type.ARRAY,
            items: {
                type: Type.OBJECT,
                properties: {
                'recipeName': {
                    type: Type.STRING,
                    description: 'Name of the recipe',
                    nullable: false,
                },
                'ingredients': {
                    type: Type.ARRAY,
                    items: {
                    type: Type.STRING,
                    description: 'Ingredients of the recipe',
                    nullable: false,
                    },
                },
                },
                required: ['recipeName'],
            },
            },
        }
    },
    {
        contents: [{
            parts: [{text: 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
            role: 'user'
        }],
        config: {
            responseMimeType: 'application/json',
            responseSchema: {
            type: Type.ARRAY,
            items: {
                type: Type.OBJECT,
                properties: {
                'recipeName': {
                    type: Type.STRING,
                    description: 'Name of the recipe',
                    nullable: false,
                },
                'ingredients': {
                    type: Type.ARRAY,
                    items: {
                    type: Type.STRING,
                    description: 'Ingredients of the recipe',
                    nullable: false,
                    },
                },
                },
                required: ['recipeName'],
            },
            },
        }
    }
]

const inlinedBatchJob = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: inlinedRequests,
    config: {
        displayName: 'inlined-requests-job-1',
    }
});
```

 
 

## Monitoring job status

Use the operation name obtained when creating the batch job to poll its status.
The state field of the batch job will indicate its current status. A batch job
can be in one of the following states:

- `JOB_STATE_PENDING`: The job has been created and is waiting to be processed by the service.

- `JOB_STATE_RUNNING`: The job is in progress.

- `JOB_STATE_SUCCEEDED`: The job completed successfully. You can now retrieve the results.

- `JOB_STATE_FAILED`: The job failed. Check the error details for more information.

- `JOB_STATE_CANCELLED`: The job was cancelled by the user.

- `JOB_STATE_EXPIRED`: The job has expired because it was running or pending
for more than 48 hours. The job will not have any results to retrieve.
You can try submitting the job again or splitting up
the requests into smaller batches.

You can poll the job status periodically to check for completion.

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

# Use the name of the job you want to check
# e.g., inline_batch_job.name from the previous step
job_name = "YOUR_BATCH_JOB_NAME"  # (e.g. 'batches/your-batch-id')
batch_job = client.batches.get(name=job_name)

completed_states = set([
    'JOB_STATE_SUCCEEDED',
    'JOB_STATE_FAILED',
    'JOB_STATE_CANCELLED',
    'JOB_STATE_EXPIRED',
])

print(f"Polling status for job: {job_name}")
batch_job = client.batches.get(name=job_name) # Initial get
while batch_job.state.name not in completed_states:
  print(f"Current state: {batch_job.state.name}")
  time.sleep(30) # Wait for 30 seconds before polling again
  batch_job = client.batches.get(name=job_name)

print(f"Job finished with state: {batch_job.state.name}")
if batch_job.state.name == 'JOB_STATE_FAILED':
    print(f"Error: {batch_job.error}")
```

 
 

### JavaScript

 

```
// Use the name of the job you want to check
// e.g., inlinedBatchJob.name from the previous step
let batchJob;
const completedStates = new Set([
    'JOB_STATE_SUCCEEDED',
    'JOB_STATE_FAILED',
    'JOB_STATE_CANCELLED',
    'JOB_STATE_EXPIRED',
]);

try {
    batchJob = await ai.batches.get({name: inlinedBatchJob.name});
    while (!completedStates.has(batchJob.state)) {
        console.log(`Current state: ${batchJob.state}`);
        // Wait for 30 seconds before polling again
        await new Promise(resolve => setTimeout(resolve, 30000));
        batchJob = await client.batches.get({ name: batchJob.name });
    }
    console.log(`Job finished with state: ${batchJob.state}`);
    if (batchJob.state === 'JOB_STATE_FAILED') {
        // The exact structure of `error` might vary depending on the SDK
        // This assumes `error` is an object with a `message` property.
        console.error(`Error: ${batchJob.state}`);
    }
} catch (error) {
    console.error(`An error occurred while polling job ${batchJob.name}:`, error);
}
```

 
 

## Retrieving results

Once the job status indicates your batch job has succeeded, the results are
available in the `response` field.

 
 

### Python

 

```
import json
from google import genai

client = genai.Client()

# Use the name of the job you want to check
# e.g., inline_batch_job.name from the previous step
job_name = "YOUR_BATCH_JOB_NAME"
batch_job = client.batches.get(name=job_name)

if batch_job.state.name == 'JOB_STATE_SUCCEEDED':

    # If batch job was created with a file
    if batch_job.dest and batch_job.dest.file_name:
        # Results are in a file
        result_file_name = batch_job.dest.file_name
        print(f"Results are in file: {result_file_name}")

        print("Downloading result file content...")
        file_content = client.files.download(file=result_file_name)
        # Process file_content (bytes) as needed
        print(file_content.decode('utf-8'))

    # If batch job was created with inline request
    # (for embeddings, use batch_job.dest.inlined_embed_content_responses)
    elif batch_job.dest and batch_job.dest.inlined_responses:
        # Results are inline
        print("Results are inline:")
        for i, inline_response in enumerate(batch_job.dest.inlined_responses):
            print(f"Response {i+1}:")
            if inline_response.response:
                # Accessing response, structure may vary.
                try:
                    print(inline_response.response.text)
                except AttributeError:
                    print(inline_response.response) # Fallback
            elif inline_response.error:
                print(f"Error: {inline_response.error}")
    else:
        print("No results found (neither file nor inline).")
else:
    print(f"Job did not succeed. Final state: {batch_job.state.name}")
    if batch_job.error:
        print(f"Error: {batch_job.error}")
```

 
 

### JavaScript

 

```
// Use the name of the job you want to check
// e.g., inlinedBatchJob.name from the previous step
const jobName = "YOUR_BATCH_JOB_NAME";

try {
    const batchJob = await ai.batches.get({ name: jobName });

    if (batchJob.state === 'JOB_STATE_SUCCEEDED') {
        console.log('Found completed batch:', batchJob.displayName);
        console.log(batchJob);

        // If batch job was created with a file destination
        if (batchJob.dest?.fileName) {
            const resultFileName = batchJob.dest.fileName;
            console.log(`Results are in file: ${resultFileName}`);

            console.log("Downloading result file content...");
            const fileContentBuffer = await ai.files.download({ file: resultFileName });

            // Process fileContentBuffer (Buffer) as needed
            console.log(fileContentBuffer.toString('utf-8'));
        }

        // If batch job was created with inline responses
        else if (batchJob.dest?.inlinedResponses) {
            console.log("Results are inline:");
            for (let i = 0; i < batchJob.dest.inlinedResponses.length; i++) {
                const inlineResponse = batchJob.dest.inlinedResponses[i];
                console.log(`Response ${i + 1}:`);
                if (inlineResponse.response) {
                    // Accessing response, structure may vary.
                    if (inlineResponse.response.text !== undefined) {
                        console.log(inlineResponse.response.text);
                    } else {
                        console.log(inlineResponse.response); // Fallback
                    }
                } else if (inlineResponse.error) {
                    console.error(`Error: ${inlineResponse.error}`);
                }
            }
        }

        // If batch job was an embedding batch with inline responses
        else if (batchJob.dest?.inlinedEmbedContentResponses) {
            console.log("Embedding results found inline:");
            for (let i = 0; i < batchJob.dest.inlinedEmbedContentResponses.length; i++) {
                const inlineResponse = batchJob.dest.inlinedEmbedContentResponses[i];
                console.log(`Response ${i + 1}:`);
                if (inlineResponse.response) {
                    console.log(inlineResponse.response);
                } else if (inlineResponse.error) {
                    console.error(`Error: ${inlineResponse.error}`);
                }
            }
        } else {
            console.log("No results found (neither file nor inline).");
        }
    } else {
        console.log(`Job did not succeed. Final state: ${batchJob.state}`);
        if (batchJob.error) {
            console.error(`Error: ${typeof batchJob.error === 'string' ? batchJob.error : batchJob.error.message || JSON.stringify(batchJob.error)}`);
        }
    }
} catch (error) {
    console.error(`An error occurred while processing job ${jobName}:`, error);
}
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" 2> /dev/null > batch_status.json

if jq -r '.done' batch_status.json | grep -q "false"; then
    echo "Batch has not finished processing"
fi

batch_state=$(jq -r '.metadata.state' batch_status.json)
if [[ $batch_state = "JOB_STATE_SUCCEEDED" ]]; then
    if [[ $(jq '.response | has("inlinedResponses")' batch_status.json) = "true" ]]; then
        jq -r '.response.inlinedResponses' batch_status.json
        exit
    fi
    responses_file_name=$(jq -r '.response.responsesFile' batch_status.json)
    curl https://generativelanguage.googleapis.com/download/v1beta/$responses_file_name:download?alt=media \
    -H "x-goog-api-key: $GEMINI_API_KEY" 2> /dev/null
elif [[ $batch_state = "JOB_STATE_FAILED" ]]; then
    jq '.error' batch_status.json
elif [[ $batch_state == "JOB_STATE_CANCELLED" ]]; then
    echo "Batch was cancelled by the user"
elif [[ $batch_state == "JOB_STATE_EXPIRED" ]]; then
    echo "Batch expired after 48 hours"
fi
```

 
 

## Cancelling a batch job

You can cancel an ongoing batch job using its name. When a job is
canceled, it stops processing new requests.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Cancel a batch job
client.batches.cancel(name=batch_job_to_cancel.name)
```

 
 

### JavaScript

 

```
await ai.batches.cancel({name: batchJobToCancel.name});
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

# Cancel the batch
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:cancel \
-H "x-goog-api-key: $GEMINI_API_KEY" \

# Confirm that the status of the batch after cancellation is JOB_STATE_CANCELLED
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" 2> /dev/null | jq -r '.metadata.state'
```

 
 

## Deleting a batch job

You can delete an existing batch job using its name. When a job is
deleted, it stops processing new requests and is removed from the list of
batch jobs.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Delete a batch job
client.batches.delete(name=batch_job_to_delete.name)
```

 
 

### JavaScript

 

```
await ai.batches.delete({name: batchJobToDelete.name});
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

# Delete the batch job
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:delete \
-H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Technical details

- Supported models: Batch API supports a range of Gemini models.
Refer to the Models page for each model's support
of Batch API. The supported modalities for Batch API are the same
as what's supported on the interactive (or non-batch) API.

- Pricing: Batch API usage is priced at 50% of the standard interactive
API cost for the equivalent model. See the pricing page 
for details. Refer to the rate limits page 
for details on rate limits for this feature.

- Service Level Objective (SLO): Batch jobs are designed to complete
within a 24-hour turnaround time. Many jobs may complete much faster
depending on their size and current system load.

- Caching: Context caching is enabled
for batch requests. If a request in your batch results in a cache hit, the
cached tokens are priced the same as for non-batch API traffic.

## Best practices

- Use input files for large requests: For a large number of requests,
always use the file input
method for better manageability and to avoid hitting request size limits for
the `BatchGenerateContent` 
call itself. Note that there's a the 2GB file size limit per input file.

- Error handling: Check the `batchStats` for `failedRequestCount` after a
job completes. If using file output, parse each line to check if it's a
`GenerateContentResponse` or a status object indicating an error for that
specific request. See the troubleshooting
guide for a complete set of
error codes.

- Submit jobs once: The creation of a batch job is not idempotent.
If you send the same creation request twice, two separate batch jobs will
be created.

- Break up very large batches: While the target turnaround time is 24
hours, actual processing time can vary based on system load and job size.
For large jobs, consider breaking them into smaller
batches if intermediate results are needed sooner.

## What's next

- Check out the Batch API notebook 
for more examples.

- The OpenAI compatibility layer supports Batch API. Read the examples on the
 OpenAI Compatibility page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Files API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/files

- 
 
 
 
 
 
 
 
 
 
 
 Files API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Files API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini family of artificial intelligence (AI) models is built to handle
various types of input data, including text, images, and audio. Since these
models can handle more than one type or mode of data, the Gemini models
are called multimodal models or explained as having multimodal capabilities .

This guide shows you how to work with media files using the Files API. The
basic operations are the same for audio files, images, videos, documents, and
other supported file types.

For file prompting guidance, check out the File prompt guide section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when
the total request size (including the files, text prompt, system instructions,
etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to
`generateContent`.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.5-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

 
 

### REST

 

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its
metadata by calling `files.get`.

 
 

### Python

 

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

 
 

### JavaScript

 

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

 
 

### REST

 

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

 
 

## List uploaded files

You can upload multiple files using the Files API. The following code gets
a list of all the files uploaded:

 
 

### Python

 

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

 
 

### JavaScript

 

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

 
 

### Go

 

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

 
 

### REST

 

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an
uploaded file:

 
 

### Python

 

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

 
 

### JavaScript

 

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

 
 

### REST

 

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Usage info

You can use the Files API to upload and interact with media files. The Files API
lets you store up to 20 GB of files per project, with a per-file maximum
size of 2 GB. Files are stored for 48 hours. During that time, you can
use the API to get metadata about the files, but you can't download the files.
The Files API is available at no cost in all regions where the Gemini API is
available.

## File prompting strategies

This section provides guidance and best practices for using media files with
prompts for the Gemini API. 

Being able to use various types of data in your prompts gives you more
flexibility in terms of what tasks you can tackle with the Gemini API. For
example, you can send the model a photo of a delicious meal and ask it to write
a short blog about the meal.

 
 
 Prompt 
 Response 
 
 
 
 

Write a short, engaging blog post based on this picture. It should
 include a description of the meal in the photo and talk about my journey
 meal prepping.

 

 
 
 
 

Meal prepping is a great way to save time and money, and it can also
 help you to eat healthier. This meal is a great example of a healthy
 and delicious meal that can be easily prepped ahead of time.

 

This meal features brown rice, roasted vegetables, and chicken teriyaki.
 The brown rice is a healthy and complex carbohydrate that will give you
 sustained energy. The roasted vegetables are a great way to get your daily
 dose of vitamins and minerals, and the chicken teriyaki is a delicious and
 protein-rich option.

 

This meal is also very easy to prepare. Simply cook the brown rice, roast
 the vegetables, and cook the chicken teriyaki. Once everything is cooked,
 divide it into meal prep containers and store them in the refrigerator.
 You can then grab a container and go on busy mornings!

 

If you're looking for a healthy and delicious meal that can be easily
 prepped ahead of time, this meal is a great option. It's packed with
 nutrients and flavor, and it's sure to keep you feeling full and satisfied.

 

Here's to healthy and delicious meal prepping!

 
 
 

If you are having trouble getting the output you want from prompts that use
media files, there are some strategies that can help you get the results you
want. The following sections provide design approaches and troubleshooting
tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

 - 
 

### Prompt design fundamentals 

 

 Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation.

 - Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve.

 - Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process.

 - Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. 

 
 - Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.

 
 

 
 - 
 

### Troubleshooting your multimodal prompt 

 

 If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from.

 
 - If the model output is too generic (not tailored enough to the image/video input): At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.

 - To troubleshoot which part failed: Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.

 - If your prompt results in hallucinated content: Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.

 - Tuning the sampling parameters: Experiment with different temperature settings and top-k selections to adjust the model's creativity.
 
 

 
 

 

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a
specific output in mind, it's better to include that requirement in the prompt
to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this
image" could generate a general description. If you need the model to parse the
time and city from the image, you can include that request directly in your prompt.

 
 
 Prompt 
 Model response 
 
 
 
 
 

Describe this image.

 
 

The image shows an airport arrivals and departures board.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 Parse the time and city from the airport board shown in this image into a list.

 
 

- 10:50 Moscow

 - 11:05 Edinburgh

 - 11:05 London

 - 11:10 Bucharest

 - 11:30 Kiev

 - 11:35 Dublin

 - 11:45 East Midlands

 - 12:15 Sofia

 - 12:30 London

 - 12:30 Newcastle

 - 12:40 St Petersburg

 - 12:40 London

 - 12:45 Manchester

 
 
 

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the
output you want. Adding these examples can help the model identify the patterns and apply the
relationship between the given images and responses to the new example. This is also called
"few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the
country (Brazil). Suppose you want the output in a different format or style, and you want
only the city, not the country. Adding few-shot examples to your prompt can steer the model
to respond in the way you want.

 
 
 Prompt 
 Model response 
 
 
 Determine the city along with the landmark.

 
 

The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 

Determine the city along with the landmark.

 
 
 
 

city: Rome, landmark: the Colosseum.

 
 
 
 
 
 
 
 

city: Beijing, landmark: Forbidden City

 
 
 
 
 
 
 
 

city: Rio de Janeiro, landmark: Christ the Redeemer statue

 
 
 

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and
reasoning, it can be helpful to split the task into smaller, more straightforward
steps. Alternatively, it could also be effective if you
directly ask the model to ‚Äúthink step by step‚Äù in your prompt.

 
 
 Prompt 
 Model response 
 
 
 
 
 

When will I run out of toilet paper?

 
 

Soon, you only have 3 rolls left.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 

 
1. First, count how many toilet paper rolls are in this picture.

 
2. Then, determine how much toilet paper a typical person uses per day.

 
3. Calculate how long these rolls of toilet paper will last.

 

 
 

 
1. There are 3 rolls of toilet paper in this picture.

 
2. A typical person uses about 20 sheets of toilet paper per day.

 
3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.

 

 
 
 

Math problems or other types of word problems are great candidates for asking
the model to think step-by-step.

 
 
 Prompt 
 Response 
 
 
 

 What is the 4th term in the sequence?

 
 

-135
 

 
 
 

The response from the model is incorrect. Some ways to improve this is to ask
the model to parse the problem before answering the prompt.

 
 
 Prompt 
 Response 
 
 
 

 Parse the formula in the image first.

 

Then based on the formula, answer "what's the 4th term in the sequence?".

 
 

 
 Click to expand the result 
 $$ b(n) = b(n-1) * (-3) $$

$$ b(1) = 15 $$

- Substitute n = 4 into the formula:

$$ b(4) = b(4-1) * (-3) $$

$$ b(4) = b(3) * (-3) $$

- We don't know the value of b(3), so substitute n = 3 into the formula:

$$ b(3) = b(3-1) * (-3) $$

$$ b(3) = b(2) * (-3) $$

- We don't know the value of b(2), so substitute n = 2 into the formula:

$$ b(2) = b(2-1) * (-3) $$

$$ b(2) = b(1) * (-3) $$

- We know the value of b(1) is 15, so substitute it into the formula:

$$ b(2) = 15 * (-3) $$

$$ b(2) = -45 $$

- Now that we know the value of b(2), substitute it into the formula for b(3):

$$ b(3) = b(2) * (-3) $$

$$ b(3) = -45 * (-3) $$

$$ b(3) = 135 $$

- Now that we know the value of b(3), substitute it into the formula for b(4):

$$ b(4) = b(3) * (-3) $$

$$ b(4) = 135 * (-3) $$

$$ b(4) = -405 $$

Therefore, the 4th term in the sequence is -405.

 
 
 
 

This model response is correct - now to make the response even more streamlined,
you can ask the model to to think step-by-step.

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 What is the 4th term in the sequence? Think step by step.

 
 
 

The first term is 15.

 

The second term is b(1) * (-3) = 15 * (-3) = -45.

 

The third term is b(2) * (-3) = -45 * (-3) = 135.

 

The fourth term is b(3) * (-3) = 135 * (-3) = -405.

 
 
 

### Specify the output format

A common problem is the need for model output to be in a certain format
(e.g. markdown, JSON, HTML), especially if the model output needs to
be ingested by a downstream task. You can try to produce output in that format
by instructing the model to do so within the prompt. Below are two examples:

 
 
 Prompt 
 Model response 
 
 
 
 
 

Parse the table in this image into markdown format

 
 
 

| Subject | Status | Time | Start date | Due on |

 

| ------- | ------ | ---- | ---------- | ------ |

 

| Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 |

 

| Geography | Done | 30 min | 12/2/2023 | 12/4/2023 |

 

| Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 |

 

| Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |

 
 
 

 
 
 Prompt 
 Model response 
 
 
 
 
 

Provide a list of all the following attributes:

 

ingredients, type of cuisine, vegetarian or not, in JSON format

 
 
 

 

```
          ```json
          {
          "ingredients": [
          "rice",
          "shrimp",
          "clams",
          "mussels",
          "peas",
          "tomatoes",
          "onions",
          "garlic",
          "olive oil",
          "paprika",
          "salt",
          "pepper"
          ],
          "type of cuisine": "Spanish",
          "vegetarian": false
          }
          ```
```


 

 
 
 

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing
a single image before the text prompt might lead to better results.
In the following samples, you'll notice the image comes first before the text in
each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful
response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image
you want the prompt to draw information from to inform its response.

 
 
 Prompt 
 Model response 
 
 
 
 
 

How many days will these diapers last a baby?

 
 

The diapers will last for 198 days before they run out.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 

How long will these diapers last before I run out?

 

Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box.

 

Divide the total number by how many diapers the child goes through per day.

 
 
 

The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs.

 

The average child in this weight range is likely between 1 and 2 years old.

 

A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).

 
 
 

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe
the images before performing its reasoning task.

 
 
 Prompt 
 Model response 
 
 
 
 
 
 
 
 

What is in common between these images?

 
 

All four images are examples of landscape photography.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 
 
 
 

First, describe what's in each image in detail. What's in common between these images?

 
 
 

The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea.

 

The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity.

 

The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private
 and public realms. It's a place where we prepare ourselves to enter or leave the outside world.

 

The fourth image shows the sea in between being calm and being stormy.

 

Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.

 
 
 

You can also try explicitly asking the model to refer to what's in the image.

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 
 
 
 

What is in common between these images? Refer to what's in the images in your response.

 
 
 

All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows
 an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a
 dark and stormy sea.

 

These images are all related to travel because they are all things that people need or use when
 they travel. The first image shows a person packing a suitcase, which is something that people do when they are
 going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people
 often go to buy things for their trips. The third image shows a stack of toilet paper, which is something
 that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is
 something that people often encounter when they are traveling.

 
 
 

The same tips work for creative output from images. In the following example with
the rabbit, explicitly asking the model to include aspects from the image can
improve the output.

 
 
 Prompt 
 Model response 
 
 
 
 
 

Can you write me a poem about this image?

 
 
 

Little bunny, so soft and brown,

 Hopping through the field, up and down.

 Your ears so long, your tail so small,

 You're the cutest creature of them all.

 

You nibble on the grass, so green and sweet,

 And hop away when you see me.

 But I'll always be your friend,

 Little bunny, until the very end.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 Can you write me a descriptive and dramatic poem about this image and include the location?

 
 
 

Little bunny on the green,

 Ears so long, so keen.

 What do you see out there,

 In the distance, where you stare?

 

Is it the ocean blue,

 Sparkling in the sun's hue?

 Or the waves that crash and roar,

 A rhythm that you adore?

 

Little bunny, tell me true,

 What captures your attention so?

 Is it the salty breeze,

 Or the seagulls' cries that please?

 

Perhaps it's the vastness of it all,

 A world so big and small.

 A place where dreams can roam,

 And adventures can be sown.

 

Little bunny, I wonder what you dream,

 As you sit on the grass, so serene.

 Do you yearn to explore the deep,

 Or stay on land, where you can leap?

 

Whatever it may be, little bunny,

 Keep that spark of wonder burning brightly.

 For in your dreams and aspirations,

 Lies a world waiting for your creation.

 
 
 

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model
didn't understand the image to begin with, or if it did understand the image
but did not perform the correct reasoning steps afterward.
To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising
when paired with tea (e.g. popcorn), you can first troubleshoot to determine
whether the model correctly recognized that the image contains tea.

 
 
 Prompt 
 Prompt for troubleshooting 
 
 
 
 
 

What's a snack I can make in 1 minute that would go well with this?

 
 
 
 

Describe what's in this image.

 
 
 

Another strategy is to ask the model to explain its reasoning. That can help you
narrow down which part of the reasoning broke down, if any.

 
 
 Prompt 
 Prompt for troubleshooting 
 
 
 
 
 

What's a snack I can make in 1 minute that would go well with this?

 
 
 
 

What's a snack I can make in 1 minute that would go well with this? Please explain why.

 
 
 

## What's next

- Try writing your own multimodal prompts using Google AI
Studio .

- For information on using the Gemini Files API for
uploading media files and including them in your prompts, see the
 Vision , Audio , and
 Document processing guides.

- For more guidance on prompt design, like tuning sampling parameters, see the
 Prompt strategies page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### File Search &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/file-search#main-content

- 
 
 
 
 
 
 
 
 
 
 
 File Search  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 File Search 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API enables Retrieval Augmented Generation ("RAG") through the File
Search tool. File Search imports, chunks, and indexes your data to
enable fast retrieval of relevant information based on a user's prompt. This
information is then provided as context to the model, allowing the model to
provide more accurate and relevant answers.

You can use the `uploadToFileSearchStore` 
API to directly upload an existing file to your File Search store, or separately
upload and then `importFile` 
if you want to create the file at the same time.

## Directly upload to File Search store

This examples shows how to directly upload a file to a file store:

 
 

### Python

 

```
from google import genai
from google.genai import types
import time

client = genai.Client()

# Create the File Search store with an optional display name
file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

# Upload and import a file into the File Search store, supply a file name which will be visible in citations
operation = client.file_search_stores.upload_to_file_search_store(
  file='sample.txt',
  file_search_store_name=file_search_store.name,
  config={
      'display_name' : 'display-file-name',
  }
)

# Wait until import is complete
while not operation.done:
    time.sleep(5)
    operation = client.operations.get(operation)

# Ask a question about the file
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="""Can you tell me about Robert Graves""",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name]
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
const { GoogleGenAI } = require('@google/genai');

const ai = new GoogleGenAI({});

async function run() {
  // Create the File Search store with an optional display name
  const fileSearchStore = await ai.fileSearchStores.create({
    config: { displayName: 'your-fileSearchStore-name' }
  });

  // Upload and import a file into the File Search store, supply a file name which will be visible in citations
  let operation = await ai.fileSearchStores.uploadToFileSearchStore({
    file: 'file.txt',
    fileSearchStoreName: fileSearchStore.name,
    config: {
      displayName: 'file-name',
    }
  });

  // Wait until import is complete
  while (!operation.done) {
    await new Promise(resolve => setTimeout(resolve, 5000));
    operation = await ai.operations.get({ operation });
  }

  // Ask a question about the file
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Can you tell me about Robert Graves",
    config: {
      tools: [
        {
          fileSearch: {
            fileSearchStoreNames: [fileSearchStore.name]
          }
        }
      ]
    }
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \ "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" > /dev/null

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null

# Generate content using the FileSearchStore
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "What does the research say about ..."}]          
            }],
            "tools": [{
                "file_search": { "file_search_store_names":["'$STORE_NAME'"] }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Check the API reference for `uploadToFileSearchStore` for more information.

## Importing files

Alternatively, you can upload an existing file and import it to your file store:

 
 

### Python

 

```
from google import genai
from google.genai import types
import time

client = genai.Client()

# Upload the file using the Files API, supply a file name which will be visible in citations
sample_file = client.files.upload(file='sample.txt', config={'name': 'display_file_name'})

# Create the File Search store with an optional display name
file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

# Import the file into the File Search store
operation = client.file_search_stores.import_file(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name
)

# Wait until import is complete
while not operation.done:
    time.sleep(5)
    operation = client.operations.get(operation)

# Ask a question about the file
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="""Can you tell me about Robert Graves""",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name]
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
const { GoogleGenAI } = require('@google/genai');

const ai = new GoogleGenAI({});

async function run() {
  // Upload the file using the Files API, supply a file name which will be visible in citations
  const sampleFile = await ai.files.upload({
    file: 'sample.txt',
    config: { name: 'file-name' }
  });

  // Create the File Search store with an optional display name
  const fileSearchStore = await ai.fileSearchStores.create({
    config: { displayName: 'your-fileSearchStore-name' }
  });

  // Import the file into the File Search store
  let operation = await ai.fileSearchStores.importFile({
    fileSearchStoreName: fileSearchStore.name,
    fileName: sampleFile.name
  });

  // Wait until import is complete
  while (!operation.done) {
    await new Promise(resolve => setTimeout(resolve, 5000));
    operation = await ai.operations.get({ operation: operation });
  }

  // Ask a question about the file
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Can you tell me about Robert Graves",
    config: {
      tools: [
        {
          fileSearch: {
            fileSearchStoreNames: [fileSearchStore.name]
          }
        }
      ]
    }
  });

  console.log(response.text);
}

run();
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -X POST "https://generativelanguage.googleapis.com/upload/v1beta/files?key=${GEMINI_API_KEY}" \
  -D "${TMP_HEADER}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" 2> /dev/null

UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# Upload the actual bytes.
curl -s -X POST "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.name" file_info.json)

# Import files into the file search store
operation_name=$(curl "https://generativelanguage.googleapis.com/v1beta/${STORE_NAME}:importFile?key=${GEMINI_API_KEY}" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
        "file_name":'$file_uri'
    }' | jq -r .name)

# Wait for long running operation to complete
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "https://generativelanguage.googleapis.com/v1beta/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    break
  fi
  # Wait for 10 seconds before checking again.
  sleep 10
done

# Generate content using the FileSearchStore
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "What does the research say about ..."}]          
            }],
            "tools": [{
                "file_search": { "file_search_store_names":["'$STORE_NAME'"] }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Check the API reference for `importFile` for more information.

## Chunking configuration

When you import a file into a File Search store, it's automatically broken down
into chunks, embedded, indexed, and uploaded to your File Search store. If you
need more control over the chunking strategy, you can specify a
 `chunking_config` setting
to set a maximum number of tokens per chunk and maximum number of overlapping
tokens.

 
 

### Python

 

```
# Upload and import and upload the file into the File Search store with a custom chunking configuration
operation = client.file_search_stores.upload_to_file_search_store(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name,
    config={
        'chunking_config': {
          'white_space_config': {
            'max_tokens_per_chunk': 200,
            'max_overlap_tokens': 20
          }
        }
    }
)
```

 
 

### JavaScript

 

```
// Upload and import and upload the file into the File Search store with a custom chunking configuration
let operation = await ai.fileSearchStores.uploadToFileSearchStore({
  file: 'file.txt',
  fileSearchStoreName: fileSearchStore.name,
  config: {
    displayName: 'file-name',
    chunkingConfig: {
      whiteSpaceConfig: {
        maxTokensPerChunk: 200,
        maxOverlapTokens: 20
      }
    }
  }
});
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \ "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" > /dev/null
  -d '{
        "chunking_config": {
          "white_space_config": {
            "max_tokens_per_chunk": 200,
            "max_overlap_tokens": 20
          }
        }
    }'

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null
```

 
 

To use your File Search store, pass it as a tool to the `generateContent`
method, as shown in the Upload and Import examples.

## How it works

File Search uses a technique called semantic search to find information relevant
to the user prompt. Unlike traditional keyword-based search, semantic search
understands the meaning and context of your query.

When you import a file, it's converted into numerical representations called
 embeddings , which capture the semantic meaning of
the text. These embeddings are stored in a specialized File Search database.
When you make a query, it's also converted into an embedding. Then the system
performs a File Search to find the most similar and relevant document chunks
from the File Search store.

Here's a breakdown of the process for using the File Search
`uploadToFileSearchStore` API:

- 

 Create a File Search store : A File Search store contains the processed
data from your files. It's the persistent container for the embeddings that the
semantic search will operate on.

- 

 Upload a file and import into a File Search store : Simultaneously upload
a file and import the results into your File Search store. This creates a
temporary `File` object, which is a reference to your raw document. That data is
then chunked, converted into File Search embeddings, and indexed. The `File`
object gets deleted after 48 hours, while the data imported into the File Search
store will be stored indefinitely until you choose to delete it.

- 

 Query with File Search : Finally, you use the `FileSearch` tool in a
`generateContent` call. In the tool configuration, you specify a
`FileSearchRetrievalResource`, which points to the `FileSearchStore` you want to
search. This tells the model to perform a semantic search on that specific
File Search store to find relevant information to ground its response.

 
 
 The indexing and querying process of File Search 
 

In this diagram, the dotted line from from Documents to Embedding model 
(using `gemini-embedding-001` )
represents the `uploadToFileSearchStore` API (bypassing File storage ).
Otherwise, using the Files API to separately create
and then import files moves the indexing process from Documents to
 File storage and then to Embedding model .

## File Search stores

A File Search store is a container for your document embeddings. While raw files
uploaded through the File API are deleted after 48 hours, the data imported into
a File Search store is stored indefinitely until you manually delete it. You can
create multiple File Search stores to organize your documents. The
`FileSearchStore` API lets you create, list, get, and delete to manage your file
search stores. File Search store names are globally scoped.

Here are some examples of how to manage your File Search stores:

 
 

### Python

 

```
# Create a File Search store (including optional display_name for easier reference)
file_search_store = client.file_search_stores.create(config={'display_name': 'my-file_search-store-123'})

# List all your File Search stores
for file_search_store in client.file_search_stores.list():
    print(file_search_store)

# Get a specific File Search store by name
my_file_search_store = client.file_search_stores.get(name='fileSearchStores/my-file_search-store-123')

# Delete a File Search store
client.file_search_stores.delete(name='fileSearchStores/my-file_search-store-123', config={'force': True})
```

 
 

### JavaScript

 

```
// Create a File Search store (including optional display_name for easier reference)
const fileSearchStore = await ai.fileSearchStores.create({
  config: { displayName: 'my-file_search-store-123' }
});

// List all your File Search stores
const fileSearchStores = await ai.fileSearchStores.list();
for await (const store of fileSearchStores) {
  console.log(store);
}

// Get a specific File Search store by name
const myFileSearchStore = await ai.fileSearchStores.get({
  name: 'fileSearchStores/my-file_search-store-123'
});

// Delete a File Search store
await ai.fileSearchStores.delete({
  name: 'fileSearchStores/my-file_search-store-123',
  config: { force: true }
});
```

 
 

### REST

 

```
# Create a File Search store (including optional display_name for easier reference)
curl -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" 
    -d '{ "displayName": "My Store" }'

# List all your File Search stores
curl "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \

# Get a specific File Search store by name
curl "https://generativelanguage.googleapis.com/v1beta/fileSearchStores/my-file_search-store-123?key=${GEMINI_API_KEY}"

# Delete a File Search store
curl -X DELETE "https://generativelanguage.googleapis.com/v1beta/fileSearchStores/my-file_search-store-123?key=${GEMINI_API_KEY}"
```

 
 

The File Search Documents API reference for
methods and fields related to managing documents in your file stores.

## File metadata

You can add custom metadata to your files to help filter them or provide
additional context. Metadata is a set of key-value pairs.

 
 

### Python

 

```
# Import the file into the File Search store with custom metadata
op = client.file_search_stores.import_file(
    file_search_store_name=file_search_store.name,
    file_name=sample_file.name,
    custom_metadata=[
        {"key": "author", "string_value": "Robert Graves"},
        {"key": "year", "numeric_value": 1934}
    ]
)
```

 
 

### JavaScript

 

```
// Import the file into the File Search store with custom metadata
let operation = await ai.fileSearchStores.importFile({
  fileSearchStoreName: fileSearchStore.name,
  fileName: sampleFile.name,
  config: {
    customMetadata: [
      { key: "author", stringValue: "Robert Graves" },
      { key: "year", numericValue: 1934 }
    ]
  }
});
```

 
 

### REST

 

```
FILE_PATH="path/to/sample.pdf"
MIME_TYPE=$(file -b --mime-type "${FILE_PATH}")
NUM_BYTES=$(wc -c < "${FILE_PATH}")

# Create a FileSearchStore
STORE_RESPONSE=$(curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/fileSearchStores?key=${GEMINI_API_KEY}" \
    -H "Content-Type: application/json" \
    -d '{ "displayName": "My Store" }')

# Extract the store name (format: fileSearchStores/xxxxxxx)
STORE_NAME=$(echo $STORE_RESPONSE | jq -r '.name')

# Initiate Resumable Upload to the Store
TMP_HEADER="upload-header.tmp"

curl -s -D "${TMP_HEADER}" \
  "https://generativelanguage.googleapis.com/upload/v1beta/${STORE_NAME}:uploadToFileSearchStore?key=${GEMINI_API_KEY}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d '{
        "custom_metadata": [
          {"key": "author", "string_value": "Robert Graves"},
          {"key": "year", "numeric_value": 1934}
        ]
    }' > /dev/null

# Extract upload_url from headers
UPLOAD_URL=$(grep -i "x-goog-upload-url: " "${TMP_HEADER}" | cut -d" " -f2 | tr -d "\r")
rm "${TMP_HEADER}"

# --- Upload the actual bytes ---
curl "${UPLOAD_URL}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${FILE_PATH}" 2> /dev/null
```

 
 

This is useful when you have multiple documents in a File Search store and want
to search only a subset of them.

 
 

### Python

 

```
# Use the metadata filter to search within a subset of documents
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Tell me about the book 'I, Claudius'",
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                file_search=types.FileSearch(
                    file_search_store_names=[file_search_store.name],
                    metadata_filter="author=Robert Graves",
                )
            )
        ]
    )
)

print(response.text)
```

 
 

### JavaScript

 

```
// Use the metadata filter to search within a subset of documents
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Tell me about the book 'I, Claudius'",
  config: {
    tools: [
      {
        fileSearch: {
          fileSearchStoreNames: [fileSearchStore.name],
          metadataFilter: 'author="Robert Graves"',
        }
      }
    ]
  }
});

console.log(response.text);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
            "contents": [{
                "parts":[{"text": "Tell me about the book I, Claudius"}]          
            }],
            "tools": [{
                "file_search": { 
                    "file_search_store_names":["'$STORE_NAME'"],
                    "metadata_filter": "author = \"Robert Graves\""
                }
            }]
        }' 2> /dev/null > response.json

cat response.json
```

 
 

Guidance on implementing list filter syntax for `metadata_filter` can be found
at google.aip.dev/160 

## Citations

When you use File Search, the model's response may include citations that
specify which parts of your uploaded documents were used to generate the
answer. This helps with fact-checking and verification.

You can access citation information through the `grounding_metadata` attribute
of the response.

 
 

### Python

 

```
print(response.candidates[0].grounding_metadata)
```

 
 

### JavaScript

 

```
console.log(JSON.stringify(response.candidates?.[0]?.groundingMetadata, null, 2));
```

 
 

## Supported models

The following models support File Search:

- `gemini-2.5-pro` 

- `gemini-2.5-flash` 

## Supported file types

File Search supports a wide range of file formats, listed in the following
sections.

 
 
 

### Application file types

 

 - `application/dart`

 - `application/ecmascript`

 - `application/json`

 - `application/ms-java`

 - `application/msword`

 - `application/pdf`

 - `application/sql`

 - `application/typescript`

 - `application/vnd.curl`

 - `application/vnd.dart`

 - `application/vnd.ibm.secure-container`

 - `application/vnd.jupyter`

 - `application/vnd.ms-excel`

 - `application/vnd.oasis.opendocument.text`

 - 

```
application/vnd.openxmlformats-officedocument.presentationml.presentation
```



 - 

```
application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
```



 - 

```
application/vnd.openxmlformats-officedocument.wordprocessingml.document
```



 - 

```
application/vnd.openxmlformats-officedocument.wordprocessingml.template
```



 - `application/x-csh`

 - `application/x-hwp`

 - `application/x-hwp-v5`

 - `application/x-latex`

 - `application/x-php`

 - `application/x-powershell`

 - `application/x-sh`

 - `application/x-shellscript`

 - `application/x-tex`

 - `application/x-zsh`

 - `application/xml`

 - `application/zip`

 

 
 

 
 
 

### Text file types

 - `text/1d-interleaved-parityfec`

 - `text/RED`

 - `text/SGML`

 - `text/cache-manifest`

 - `text/calendar`

 - `text/cql`

 - `text/cql-extension`

 - `text/cql-identifier`

 - `text/css`

 - `text/csv`

 - `text/csv-schema`

 - `text/dns`

 - `text/encaprtp`

 - `text/enriched`

 - `text/example`

 - `text/fhirpath`

 - `text/flexfec`

 - `text/fwdred`

 - `text/gff3`

 - `text/grammar-ref-list`

 - `text/hl7v2`

 - `text/html`

 - `text/javascript`

 - `text/jcr-cnd`

 - `text/jsx`

 - `text/markdown`

 - `text/mizar`

 - `text/n3`

 - `text/parameters`

 - `text/parityfec`

 - `text/php`

 - `text/plain`

 - `text/provenance-notation`

 - `text/prs.fallenstein.rst`

 - `text/prs.lines.tag`

 - `text/prs.prop.logic`

 - `text/raptorfec`

 - `text/rfc822-headers`

 - `text/rtf`

 - `text/rtp-enc-aescm128`

 - `text/rtploopback`

 - `text/rtx`

 - `text/sgml`

 - `text/shaclc`

 - `text/shex`

 - `text/spdx`

 - `text/strings`

 - `text/t140`

 - `text/tab-separated-values`

 - `text/texmacs`

 - `text/troff`

 - `text/tsv`

 - `text/tsx`

 - `text/turtle`

 - `text/ulpfec`

 - `text/uri-list`

 - `text/vcard`

 - `text/vnd.DMClientScript`

 - `text/vnd.IPTC.NITF`

 - `text/vnd.IPTC.NewsML`

 - `text/vnd.a`

 - `text/vnd.abc`

 - `text/vnd.ascii-art`

 - `text/vnd.curl`

 - `text/vnd.debian.copyright`

 - `text/vnd.dvb.subtitle`

 - `text/vnd.esmertec.theme-descriptor`

 - `text/vnd.exchangeable`

 - `text/vnd.familysearch.gedcom`

 - `text/vnd.ficlab.flt`

 - `text/vnd.fly`

 - `text/vnd.fmi.flexstor`

 - `text/vnd.gml`

 - `text/vnd.graphviz`

 - `text/vnd.hans`

 - `text/vnd.hgl`

 - `text/vnd.in3d.3dml`

 - `text/vnd.in3d.spot`

 - `text/vnd.latex-z`

 - `text/vnd.motorola.reflex`

 - `text/vnd.ms-mediapackage`

 - `text/vnd.net2phone.commcenter.command`

 - `text/vnd.radisys.msml-basic-layout`

 - `text/vnd.senx.warpscript`

 - `text/vnd.sosi`

 - `text/vnd.sun.j2me.app-descriptor`

 - `text/vnd.trolltech.linguist`

 - `text/vnd.wap.si`

 - `text/vnd.wap.sl`

 - `text/vnd.wap.wml`

 - `text/vnd.wap.wmlscript`

 - `text/vtt`

 - `text/wgsl`

 - `text/x-asm`

 - `text/x-bibtex`

 - `text/x-boo`

 - `text/x-c`

 - `text/x-c++hdr`

 - `text/x-c++src`

 - `text/x-cassandra`

 - `text/x-chdr`

 - `text/x-coffeescript`

 - `text/x-component`

 - `text/x-csh`

 - `text/x-csharp`

 - `text/x-csrc`

 - `text/x-cuda`

 - `text/x-d`

 - `text/x-diff`

 - `text/x-dsrc`

 - `text/x-emacs-lisp`

 - `text/x-erlang`

 - `text/x-gff3`

 - `text/x-go`

 - `text/x-haskell`

 - `text/x-java`

 - `text/x-java-properties`

 - `text/x-java-source`

 - `text/x-kotlin`

 - `text/x-lilypond`

 - `text/x-lisp`

 - `text/x-literate-haskell`

 - `text/x-lua`

 - `text/x-moc`

 - `text/x-objcsrc`

 - `text/x-pascal`

 - `text/x-pcs-gcd`

 - `text/x-perl`

 - `text/x-perl-script`

 - `text/x-python`

 - `text/x-python-script`

 - `text/x-r-markdown`

 - `text/x-rsrc`

 - `text/x-rst`

 - `text/x-ruby-script`

 - `text/x-rust`

 - `text/x-sass`

 - `text/x-scala`

 - `text/x-scheme`

 - `text/x-script.python`

 - `text/x-scss`

 - `text/x-setext`

 - `text/x-sfv`

 - `text/x-sh`

 - `text/x-siesta`

 - `text/x-sos`

 - `text/x-sql`

 - `text/x-swift`

 - `text/x-tcl`

 - `text/x-tex`

 - `text/x-vbasic`

 - `text/x-vcalendar`

 - `text/xml`

 - `text/xml-dtd`

 - `text/xml-external-parsed-entity`

 - `text/yaml`

 
 

## Rate limits

The File Search API has the following limits to enforce service stability:

- Maximum file size / per document limit : 100 MB

- Total size of project File Search stores (based on user tier):

 Free : 1 GB

- Tier 1 : 10 GB

- Tier 2 : 100 GB

- Tier 3 : 1 TB

 
- Recommendation : Limit the size of each File Search store to under 20 GB to ensure optimal retrieval latencies.

## Pricing

- Developers are charged for embeddings at indexing time based on existing
 embeddings pricing ($0.15 per
1M tokens).

- Storage is free of charge.

- Query time embeddings are free of charge.

- Retrieved document tokens are charged as regular
 context tokens .

## What's next

- Visit the API reference for File Search Stores and File Search Documents .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Context caching &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/caching

- 
 
 
 
 
 
 
 
 
 
 
 Context caching  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Context caching 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 
 
 
 

In a typical AI workflow, you might pass the same input tokens over and over to
a model. The Gemini API offers two different caching mechanisms:

- Implicit caching (automatically enabled on Gemini 2.5 models, no cost saving guarantee)

- Explicit caching (can be manually enabled on most models, cost saving guarantee)

Explicit caching is useful in cases where you want to guarantee cost savings,
but with some added developer work.

## Implicit caching

Implicit caching is enabled by default for all Gemini 2.5 models. We automatically
pass on cost savings if your request hits caches. There is nothing you need to do
in order to enable this. It is effective as of May 8th, 2025. The minimum input
token count for context caching is listed in the following table for each model:

 
 
 
 Model 
 Min token limit 
 
 

 
 
 3 Pro Preview 
 2048 
 
 
 2.5 Pro 
 4096 
 
 
 2.5 Flash 
 1024 
 
 
 

To increase the chance of an implicit cache hit:

- Try putting large and common contents at the beginning of your prompt

- Try to send requests with similar prefix in a short amount of time

You can see the number of tokens which were cache hits in the response object's
`usage_metadata` field.

## Explicit caching

Using the Gemini API explicit caching feature, you can pass some content
to the model once, cache the input tokens, and then refer to the cached tokens
for subsequent requests. At certain volumes, using cached tokens is lower cost
than passing in the same corpus of tokens repeatedly.

When you cache a set of tokens, you can choose how long you want the cache to
exist before the tokens are automatically deleted. This caching duration is
called the time to live (TTL). If not set, the TTL defaults to 1 hour. The
cost for caching depends on the input token size and how long you want the
tokens to persist.

This section assumes that you've installed a Gemini SDK (or have curl installed)
and that you've configured an API key, as shown in the
 quickstart .

### Generate content using a cache

The following example shows how to generate content using a cached system
instruction and video file.

 
 

### Videos

 

```
import os
import pathlib
import requests
import time

from google import genai
from google.genai import types

client = genai.Client()

# Download video file
url = 'https://storage.googleapis.com/generativeai-downloads/data/SherlockJr._10min.mp4'
path_to_video_file = pathlib.Path('SherlockJr._10min.mp4')
if not path_to_video_file.exists():
  with path_to_video_file.open('wb') as wf:
    response = requests.get(url, stream=True)
    for chunk in response.iter_content(chunk_size=32768):
      wf.write(chunk)

# Upload the video using the Files API
video_file = client.files.upload(file=path_to_video_file)

# Wait for the file to finish processing
while video_file.state.name == 'PROCESSING':
  print('Waiting for video to be processed.')
  time.sleep(2)
  video_file = client.files.get(name=video_file.name)

print(f'Video processing complete: {video_file.uri}')

# You must use an explicit version suffix: "-flash-001", not just "-flash".
model='models/gemini-2.0-flash-001'

# Create a cache with a 5 minute TTL
cache = client.caches.create(
    model=model,
    config=types.CreateCachedContentConfig(
      display_name='sherlock jr movie', # used to identify the cache
      system_instruction=(
          'You are an expert video analyzer, and your job is to answer '
          'the user\'s query based on the video file you have access to.'
      ),
      contents=[video_file],
      ttl="300s",
  )
)

# Construct a GenerativeModel which uses the created cache.
response = client.models.generate_content(
  model = model,
  contents= (
    'Introduce different characters in the movie by describing '
    'their personality, looks, and names. Also list the timestamps '
    'they were introduced for the first time.'),
  config=types.GenerateContentConfig(cached_content=cache.name)
)

print(response.usage_metadata)

# The output should look something like this:
#
# prompt_token_count: 696219
# cached_content_token_count: 696190
# candidates_token_count: 214
# total_token_count: 696433

print(response.text)
```

 
 

### PDFs

 

```
from google import genai
from google.genai import types
import io
import httpx

client = genai.Client()

long_context_pdf_path = "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"

# Retrieve and upload the PDF using the File API
doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)

document = client.files.upload(
  file=doc_io,
  config=dict(mime_type='application/pdf')
)

model_name = "gemini-2.0-flash-001"
system_instruction = "You are an expert analyzing transcripts."

# Create a cached content object
cache = client.caches.create(
    model=model_name,
    config=types.CreateCachedContentConfig(
      system_instruction=system_instruction,
      contents=[document],
    )
)

# Display the cache details
print(f'{cache=}')

# Generate content using the cached prompt and document
response = client.models.generate_content(
  model=model_name,
  contents="Please summarize this transcript",
  config=types.GenerateContentConfig(
    cached_content=cache.name
  ))

# (Optional) Print usage metadata for insights into the API call
print(f'{response.usage_metadata=}')

# Print the generated text
print('\n\n', response.text)
```

 
 

### List caches

It's not possible to retrieve or view cached content, but you can retrieve
cache metadata (`name`, `model`, `display_name`, `usage_metadata`,
`create_time`, `update_time`, and `expire_time`).

To list metadata for all uploaded caches, use `CachedContent.list()`:

 

```
for cache in client.caches.list():
  print(cache)
```

 

To fetch the metadata for one cache object, if you know its name, use `get`:

 

```
client.caches.get(name=name)
```

 

### Update a cache

You can set a new `ttl` or `expire_time` for a cache. Changing anything else
about the cache isn't supported.

The following example shows how to update the `ttl` of a cache using
`client.caches.update()`.

 

```
from google import genai
from google.genai import types

client.caches.update(
  name = cache.name,
  config  = types.UpdateCachedContentConfig(
      ttl='300s'
  )
)
```

 

To set the expiry time, it will accepts either a `datetime` object
or an ISO-formatted datetime string (`dt.isoformat()`, like
`2025-01-27T16:02:36.473528+00:00`). Your time must include a time zone
(`datetime.utcnow()` doesn't attach a time zone,
`datetime.now(datetime.timezone.utc)` does attach a time zone).

 

```
from google import genai
from google.genai import types
import datetime

# You must use a time zone-aware time.
in10min = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=10)

client.caches.update(
  name = cache.name,
  config  = types.UpdateCachedContentConfig(
      expire_time=in10min
  )
)
```

 

### Delete a cache

The caching service provides a delete operation for manually removing content
from the cache. The following example shows how to delete a cache:

 

```
client.caches.delete(cache.name)
```

 

### Explicit caching using the OpenAI library

If you're using an OpenAI library , you can enable
explicit caching using the `cached_content` property on
 `extra_body` .

## When to use explicit caching

Context caching is particularly well suited to scenarios where a substantial
initial context is referenced repeatedly by shorter requests. Consider using
context caching for use cases such as:

- Chatbots with extensive system instructions 

- Repetitive analysis of lengthy video files

- Recurring queries against large document sets

- Frequent code repository analysis or bug fixing

### How explicit caching reduces costs

Context caching is a paid feature designed to reduce overall operational costs.
Billing is based on the following factors:

- Cache token count: The number of input tokens cached, billed at a
reduced rate when included in subsequent prompts.

- Storage duration: The amount of time cached tokens are stored (TTL),
billed based on the TTL duration of cached token count. There are no minimum
or maximum bounds on the TTL.

- Other factors: Other charges apply, such as for non-cached input tokens
and output tokens.

For up-to-date pricing details, refer to the Gemini API pricing
page . To learn how to count tokens, see the Token
guide .

### Additional considerations

Keep the following considerations in mind when using context caching:

- The minimum input token count for context caching is 1,024 for 2.5 Flash,
4,096 for 2.5 Pro and 2,048 for 3 Pro Preview. The maximum is the same as the
maximum for the given model. (For more on counting tokens, see the Token
guide ).

- The model doesn't make any distinction between cached tokens and regular
input tokens. Cached content is a prefix to the prompt.

- There are no special rate or usage limits on context caching; the standard
rate limits for `GenerateContent` apply, and token limits include cached
tokens.

- The number of cached tokens is returned in the `usage_metadata` from the
create, get, and list operations of the cache service, and also in
`GenerateContent` when using the cache.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Get started with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Get started with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Get started with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Live API enables low-latency, real-time voice and video interactions with
Gemini. It processes continuous streams of audio, video, or text to deliver
immediate, human-like spoken responses, creating a natural conversational
experience for your users.

 

Live API offers a comprehensive set of features such as Voice Activity Detection , tool use and function calling , session management (for managing long running conversations) and ephemeral tokens (for secure client-sided authentication). 

This page gets you up and running with examples and basic code samples.

 
 Try the Live API in Google AI Studio mic 
 

## Example applications

Check out the following example applications that illustrate how to use Live API
for end-to-end use cases:

- Live audio starter app on AI Studio,
using JavaScript libraries to connect to Live API and stream bidirectional
audio through your microphone and speakers.

- Live API Python cookbook 
using Pyaudio that connects to Live API.

## Partner integrations

If you prefer a simpler development process, you can use Daily , LiveKit or Voximplant . These are third-party partner platforms that have already integrated the Gemini Live API over the WebRTC protocol to streamline the development of real-time audio and video applications.

## Choose an implementation approach

When integrating with Live API, you'll need to choose one of the following
implementation approaches:

- Server-to-server : Your backend connects to the Live API using
 WebSockets . Typically, your client sends stream data (audio, video,
text) to your server, which then forwards it to the Live API.

- Client-to-server : Your frontend code connects directly to the Live API
using WebSockets to stream data, bypassing your backend.

## Get started

This example reads a WAV file , sends it in the correct format, and saves
the received data as WAV file.

You can send audio by converting it to 16-bit PCM, 16kHz, mono format, and you
can receive audio by setting `AUDIO` as response modality. The output uses a
sample rate of 24kHz.

 
 

### Python

 

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()

# New native audio model:
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
  "response_modalities": ["AUDIO"],
  "system_instruction": "You are a helpful assistant and answer in a friendly tone.",
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
// WARNING: Do not use API keys in client-side (browser based) applications
// Consider using Ephemeral Tokens instead
// More information at: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

// New native audio model:
const model = "gemini-2.5-flash-native-audio-preview-09-2025"

const config = {
  responseModalities: [Modality.AUDIO],
  systemInstruction: "You are a helpful assistant and answer in a friendly tone."
};

async function live() {
    const responseQueue = [];

    async function waitMessage() {
        let done = false;
        let message = undefined;
        while (!done) {
            message = responseQueue.shift();
            if (message) {
                done = true;
            } else {
                await new Promise((resolve) => setTimeout(resolve, 100));
            }
        }
        return message;
    }

    async function handleTurn() {
        const turns = [];
        let done = false;
        while (!done) {
            const message = await waitMessage();
            turns.push(message);
            if (message.serverContent && message.serverContent.turnComplete) {
                done = true;
            }
        }
        return turns;
    }

    const session = await ai.live.connect({
        model: model,
        callbacks: {
            onopen: function () {
                console.debug('Opened');
            },
            onmessage: function (message) {
                responseQueue.push(message);
            },
            onerror: function (e) {
                console.debug('Error:', e.message);
            },
            onclose: function (e) {
                console.debug('Close:', e.reason);
            },
        },
        config: config,
    });

    // Send Audio Chunk
    const fileBuffer = fs.readFileSync("sample.wav");

    // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
    const wav = new WaveFile();
    wav.fromBuffer(fileBuffer);
    wav.toSampleRate(16000);
    wav.toBitDepth("16");
    const base64Audio = wav.toBase64();

    // If already in correct format, you can use this:
    // const fileBuffer = fs.readFileSync("sample.pcm");
    // const base64Audio = Buffer.from(fileBuffer).toString('base64');

    session.sendRealtimeInput(
        {
            audio: {
                data: base64Audio,
                mimeType: "audio/pcm;rate=16000"
            }
        }

    );

    const turns = await handleTurn();

    // Combine audio data strings and save as wave file
    const combinedAudio = turns.reduce((acc, turn) => {
        if (turn.data) {
            const buffer = Buffer.from(turn.data, 'base64');
            const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
            return acc.concat(Array.from(intArray));
        }
        return acc;
    }, []);

    const audioBuffer = new Int16Array(combinedAudio);

    const wf = new WaveFile();
    wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
    fs.writeFileSync('audio.wav', wf.toBuffer());

    session.close();
}

async function main() {
    await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## What's next

- Read the full Live API Capabilities guide for key capabilities and configurations; including Voice Activity Detection and native audio features.

- Read the Tool use guide to learn how to integrate Live API with tools and function calling.

- Read the Session management guide for managing long running conversations.

- Read the Ephemeral tokens guide for secure authentication in client-to-server applications.

- For more information about the underlying WebSockets API, see the WebSockets API reference .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Media resolution &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/media-resolution

- 
 
 
 
 
 
 
 
 
 
 
 Media resolution  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Media resolution 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The `media_resolution` parameter controls how the Gemini API processes media inputs like images, videos, and PDF documents by determining the maximum number of tokens allocated for media inputs, allowing you to balance response quality against latency and cost. For different settings, default values and how they correspond to tokens, see the Token counts section.

You can configure media resolution in two ways:

- 

 Per part (Gemini 3 only)

- 

 Globally for an entire `generateContent` request (all multimodal models)

## Per-part media resolution (Gemini 3 only)

Gemini 3 allows you to set media resolution for individual media objects within your request, offering fine-grained optimisation of token usage. You can mix resolution levels in a single request. For example, using high resolution for a complex diagram and low resolution for a simple contextual image. This setting overrides any global configuration for a specific part. For default settings, see Token counts section. 

 
 

### Python

 

```
from google import genai
from google.genai import types

# The media_resolution parameter for parts is currently only available in the v1alpha API version. (experimental)
client = genai.Client(
  http_options={
      'api_version': 'v1alpha',
  }
)

# Replace with your image data
with open('path/to/image1.jpg', 'rb') as f:
    image_bytes_1 = f.read()

# Create parts with different resolutions
image_part_high = types.Part.from_bytes(
    data=image_bytes_1,
    mime_type='image/jpeg',
    media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH
)

model_name = 'gemini-3-pro-preview'

response = client.models.generate_content(
    model=model_name,
    contents=["Describe these images:", image_part_high]
)
print(response.text)
```

 
 

### Javascript

 

```
// Example: Setting per-part media resolution in JavaScript
import { GoogleGenAI, MediaResolution, Part } from '@google/genai';
import * as fs from 'fs';
import { Buffer } from 'buffer'; // Node.js

const ai = new GoogleGenAI({ httpOptions: { apiVersion: 'v1alpha' } });

// Helper function to convert local file to a Part object
function fileToGenerativePart(path, mimeType, mediaResolution) {
    return {
        inlineData: { data: Buffer.from(fs.readFileSync(path)).toString('base64'), mimeType },
        mediaResolution: { 'level': mediaResolution }
    };
}

async function run() {
    // Create parts with different resolutions
    const imagePartHigh = fileToGenerativePart('img.png', 'image/png', Part.MediaResolutionLevel.MEDIA_RESOLUTION_HIGH);
    const model_name = 'gemini-3-pro-preview';
    const response = await ai.models.generateContent({
        model: model_name,
        contents: ['Describe these images:', imagePartHigh]
        // Global config can still be set, but per-part settings will override
        // config: {
        //   mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM
        // }
    });
    console.log(response.text);
}
run();
```

 
 

### REST

 

```
# Replace with paths to your images
IMAGE_PATH="path/to/image.jpg"

# Base64 encode the images
BASE64_IMAGE1=$(base64 -w 0 "$IMAGE_PATH")

MODEL_ID="gemini-3-pro-preview"

echo '{
    "contents": [{
      "parts": [
        {"text": "Describe these images:"},
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "'"$BASE64_IMAGE1"'",
          },
          "media_resolution": {"level": "MEDIA_RESOLUTION_HIGH"}
        }
      ]
    }]
  }' > request.json

curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1alpha/models/${MODEL_ID}:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d @request.json
```

 
 

## Global media resolution

You can set a default resolution for all media parts in a request using the
`GenerationConfig`. This is supported by all multimodal models. If a request
includes both global and per-part settings , the per-part setting takes precedence for that specific item.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Prepare standard image part
with open('image.jpg', 'rb') as f:
    image_bytes = f.read()
image_part = types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg')

# Set global configuration
config = types.GenerateContentConfig(
    media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH
)

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=["Describe this image:", image_part],
    config=config
)
print(response.text)
```

 
 

### Javascript

 

```
import { GoogleGenAI, MediaResolution } from '@google/genai';
import * as fs from 'fs';

const ai = new GoogleGenAI({ });

async function run() {
   // ... (Image loading logic) ...

   const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: ["Describe this image:", imagePart],
      config: {
         mediaResolution: MediaResolution.MEDIA_RESOLUTION_HIGH
      }
   });
   console.log(response.text);
}
run();
```

 
 

### REST

 

```
# ... (Base64 encoding logic) ...

curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [...],
    "generation_config": {
      "media_resolution": "MEDIA_RESOLUTION_HIGH"
    }
  }'
```

 
 

## Available resolution values

The Gemini API defines the following levels for media resolution:

- `MEDIA_RESOLUTION_UNSPECIFIED`: The default setting. The token count for
this level varies significantly between Gemini 3 and earlier Gemini models.

- `MEDIA_RESOLUTION_LOW`: Lower token count, resulting in faster processing
and lower cost, but with less detail.

- `MEDIA_RESOLUTION_MEDIUM`: A balance between detail, cost, and latency.

- `MEDIA_RESOLUTION_HIGH`: Higher token count, providing more detail for the
model to work with, at the expense of increased latency and cost.

- ( Coming soon ) `MEDIA_RESOLUTION_ULTRA_HIGH`: Highest token count
required for specific use cases such as computer use.

The exact number of tokens generated for each of these
levels depends on both the media type (Image, Video, PDF) and the model
version .

## Token counts

The tables below summarize the approximate token counts for each
`media_resolution` value and media type per model family.

 Gemini 3 Models 

 
 
 MediaResolution 
 
 Image 
 
 Video 
 
 PDF 
 
 
 
 `MEDIA_RESOLUTION_UNSPECIFIED` (Default)
 
 1120
 
 70
 
 560
 
 
 
 `MEDIA_RESOLUTION_LOW`
 
 280
 
 70
 
 280 + Native Text
 
 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 560
 
 70
 
 560 + Native Text
 
 
 
 `MEDIA_RESOLUTION_HIGH`
 
 1120
 
 280
 
 1120 + Native Text
 
 
 

 Gemini 2.5 models 

 
 
 MediaResolution 
 
 Image 
 
 Video 
 
 PDF (Scanned) 
 
 PDF (Native) 
 
 
 
 `MEDIA_RESOLUTION_UNSPECIFIED` (Default)
 
 256 + Pan & Scan (~2048)
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 
 `MEDIA_RESOLUTION_LOW`
 
 64
 
 64
 
 64 + OCR
 
 64 + Native Text
 
 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 256
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 
 `MEDIA_RESOLUTION_HIGH`
 
 256 + Pan & Scan
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 

## Choosing the right resolution

- Default (`UNSPECIFIED`): Start with the default. It's tuned for a good
balance of quality, latency, and cost for most common use cases.

- `LOW`: Use for scenarios where cost and latency are paramount, and
fine-grained detail is less critical.

- `MEDIUM` / `HIGH`: Increase the resolution when the task requires
understanding intricate details within the media. This is often needed for
complex visual analysis, chart reading, or dense document comprehension.

- Per-part control (Gemini 3): Leverage this to optimize token usage. For
example, in a prompt with multiple images, use `HIGH` for a complex diagram
and `LOW` or `MEDIUM` for simpler contextual images.

 Recommended settings 

The following lists the recommended media resolution settings for each
supported media type.

 
 
 Media Type 
 
 Recommended Setting 
 
 Max Tokens 
 
 Usage Guidance 
 
 
 
 Images 
 
 `MEDIA_RESOLUTION_HIGH`
 
 1120
 
 Recommended for most image analysis tasks to ensure maximum quality.
 
 
 
 PDFs 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 560
 
 Optimal for document understanding; quality typically saturates at `medium`. Increasing to `high` rarely improves OCR results for standard documents.
 
 
 
 Video (General)
 
 `MEDIA_RESOLUTION_LOW` (or `MEDIA_RESOLUTION_MEDIUM`)
 
 70 (per frame)
 
 Note: For video, `low` and `medium` settings are treated identically (70 tokens) to optimize context usage. This is sufficient for most action recognition and description tasks.
 
 
 
 Video (Text-heavy)
 
 `MEDIA_RESOLUTION_HIGH`
 
 280 (per frame)
 
 Required only when the use case involves reading dense text (OCR) or small details within video frames.
 
 
 

Always test and evaluate the impact of different resolution settings on your
specific application to find the best trade-off between quality, latency, and
cost.

## Version compatibility summary

- The `MediaResolution` enum is available for all models supporting media
input.

- The token counts associated with each enum level differ between
Gemini 3 models and earlier Gemini versions.

- Setting `media_resolution` on individual `Part` objects is exclusive to
Gemini 3 models .

## Next steps

- Learn more about the multimodal capabilities of Gemini API in the
 image understanding , video understanding and
 document understanding guides.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Live API capabilities guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-guide#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Live API capabilities guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Live API capabilities guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This is a comprehensive guide that covers capabilities and configurations
available with the Live API.
See Get started with Live API page for a
overview and sample code for common use cases.

## Before you begin

- Familiarize yourself with core concepts: If you haven't already done so,
read the Get started with Live API page first.
This will introduce you to the fundamental principles of the Live API, how it
works, and the different implementation approaches .

- Try the Live API in AI Studio: You may find it useful to try the
Live API in Google AI Studio before you start building. To use the
Live API in Google AI Studio, select Stream .

## Establishing a connection

The following example shows how to create a connection with an API key:

 
 

### Python

 

```
import asyncio
from google import genai

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"
config = {"response_modalities": ["AUDIO"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        print("Session started")
        # Send content...

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        console.debug(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  console.debug("Session started");
  // Send content...

  session.close();
}

main();
```

 
 

## Interaction modalities

The following sections provide examples and supporting context for the different
input and output modalities available in Live API.

### Sending and receiving audio

The most common audio example, audio-to-audio , is covered in the
 Getting started guide.

### Audio formats

Audio data in the Live API is always raw, little-endian,
16-bit PCM. Audio output always uses a sample rate of 24kHz. Input audio
is natively 16kHz, but the Live API will resample if needed
so any sample rate can be sent. To convey the sample rate of input audio, set
the MIME type of each audio-containing Blob to a value
like `audio/pcm;rate=16000`.

### Sending text

Here's how you can send text:

 
 

### Python

 

```
message = "Hello, how are you?"
await session.send_client_content(turns=message, turn_complete=True)
```

 
 

### JavaScript

 

```
const message = 'Hello, how are you?';
session.sendClientContent({ turns: message, turnComplete: true });
```

 
 

#### Incremental content updates

Use incremental updates to send text input, establish session context, or
restore session context. For short contexts you can send turn-by-turn
interactions to represent the exact sequence of events:

 
 

### Python

 

```
turns = [
    {"role": "user", "parts": [{"text": "What is the capital of France?"}]},
    {"role": "model", "parts": [{"text": "Paris"}]},
]

await session.send_client_content(turns=turns, turn_complete=False)

turns = [{"role": "user", "parts": [{"text": "What is the capital of Germany?"}]}]

await session.send_client_content(turns=turns, turn_complete=True)
```

 
 

### JavaScript

 

```
let inputTurns = [
  { "role": "user", "parts": [{ "text": "What is the capital of France?" }] },
  { "role": "model", "parts": [{ "text": "Paris" }] },
]

session.sendClientContent({ turns: inputTurns, turnComplete: false })

inputTurns = [{ "role": "user", "parts": [{ "text": "What is the capital of Germany?" }] }]

session.sendClientContent({ turns: inputTurns, turnComplete: true })
```

 
 

For longer contexts it's recommended to provide a single message summary to free
up the context window for subsequent interactions. See Session Resumption for another method for
loading session context.

### Audio transcriptions

In addition to the model response, you can also receive transcriptions of
both the audio output and the audio input.

To enable transcription of the model's audio output, send
`output_audio_transcription` in the setup config. The transcription language is
inferred from the model's response.

 
 

### Python

 

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
    "response_modalities": ["AUDIO"],
    "output_audio_transcription": {}
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello? Gemini are you there?"

        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.server_content.model_turn:
                print("Model turn:", response.server_content.model_turn)
            if response.server_content.output_transcription:
                print("Transcript:", response.server_content.output_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const config = {
  responseModalities: [Modality.AUDIO],
  outputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.debug('Received output transcription: %s\n', turn.serverContent.outputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

To enable transcription of the model's audio input, send
`input_audio_transcription` in setup config.

 
 

### Python

 

```
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = {
    "response_modalities": ["AUDIO"],
    "input_audio_transcription": {},
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_data = Path("16000.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_data, mime_type='audio/pcm;rate=16000')
        )

        async for msg in session.receive():
            if msg.server_content.input_transcription:
                print('Transcript:', msg.server_content.input_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const config = {
  responseModalities: [Modality.AUDIO],
  inputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("16000.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }
  );

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
    else if (turn.serverContent && turn.serverContent.inputTranscription) {
      console.debug('Received input transcription: %s\n', turn.serverContent.inputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

### Stream audio and video

 
 
 

### Change voice and language

 Native audio output models support any of the voices
available for our Text-to-Speech (TTS) 
models. You can listen to all the voices in AI Studio .

To specify a voice, set the voice name within the `speechConfig` object as part
of the session configuration:

 
 

### Python

 

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
    },
}
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } } }
};
```

 
 

The Live API supports multiple languages .
 Native audio output models automatically choose
the appropriate language and don't support explicitly setting the language
code.

## Native audio capabilities

Our latest models feature native audio output ,
which provides natural, realistic-sounding speech and improved multilingual
performance. Native audio also enables advanced features like affective
(emotion-aware) dialogue , proactive audio 
(where the model intelligently decides when to respond to input),
and "thinking" .

### Affective dialog

This feature lets Gemini adapt its response style to the input expression and
tone.

To use affective dialog, set the api version to `v1alpha` and set
`enable_affective_dialog` to `true`in the setup message:

 
 

### Python

 

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    enable_affective_dialog=True
)
```

 
 

### JavaScript

 

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  enableAffectiveDialog: true
};
```

 
 

### Proactive audio

When this feature is enabled, Gemini can proactively decide not to respond
if the content is not relevant.

To use it, set the api version to `v1alpha` and configure the `proactivity`
field in the setup message and set `proactive_audio` to `true`:

 
 

### Python

 

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    proactivity={'proactive_audio': True}
)
```

 
 

### JavaScript

 

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  proactivity: { proactiveAudio: true }
}
```

 
 

### Thinking

The latest native audio output model `gemini-2.5-flash-native-audio-preview-09-2025`
supports thinking capabilities , with dynamic
thinking enabled by default.

The `thinkingBudget` parameter guides the model on the number of thinking tokens
to use when generating a response. You can disable thinking by setting
`thinkingBudget` to `0`. For more info on the `thinkingBudget` configuration
details of the model, see the thinking budgets documentation .

 
 

### Python 

 

```
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"]
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024,
    )
)

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

 
 

### JavaScript

 

```
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = {
  responseModalities: [Modality.AUDIO],
  thinkingConfig: {
    thinkingBudget: 1024,
  },
};

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

 
 

Additionally, you can enable thought summaries by setting `includeThoughts` to
`true` in your configuration. See thought summaries 
for more info:

 
 

### Python 

 

```
model = "gemini-2.5-flash-native-audio-preview-09-2025"

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"]
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024,
        include_thoughts=True
    )
)
```

 
 

### JavaScript

 

```
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = {
  responseModalities: [Modality.AUDIO],
  thinkingConfig: {
    thinkingBudget: 1024,
    includeThoughts: true,
  },
};
```

 
 

## Voice Activity Detection (VAD)

Voice Activity Detection (VAD) allows the model to recognize when a person is
speaking. This is essential for creating natural conversations, as it allows a
user to interrupt the model at any time.

When VAD detects an interruption, the ongoing generation is canceled and
discarded. Only the information already sent to the client is retained in the
session history. The server then sends a `BidiGenerateContentServerContent` message to report the interruption.

The Gemini server then discards any pending function calls and sends a
`BidiGenerateContentServerContent` message with the IDs of the canceled calls.

 
 

### Python

 

```
async for response in session.receive():
    if response.server_content.interrupted is True:
        # The generation was interrupted

        # If realtime playback is implemented in your application,
        # you should stop playing audio and clear queued playback here.
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.interrupted) {
    // The generation was interrupted

    // If realtime playback is implemented in your application,
    // you should stop playing audio and clear queued playback here.
  }
}
```

 
 

### Automatic VAD

By default, the model automatically performs VAD on
a continuous audio input stream. VAD can be configured with the
 `realtimeInputConfig.automaticActivityDetection` 
field of the setup configuration .

When the audio stream is paused for more than a second (for example,
because the user switched off the microphone), an
 `audioStreamEnd` 
event should be sent to flush any cached audio. The client can resume sending
audio data at any time.

 
 

### Python

 

```
# example audio file to try:
# URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
# !wget -q $URL -O sample.pcm
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        # if stream gets paused, send:
        # await session.send_realtime_input(audio_stream_end=True)

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
// example audio file to try:
// URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
// !wget -q $URL -O sample.pcm
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.pcm");
  const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  // if stream gets paused, send:
  // session.sendRealtimeInput({ audioStreamEnd: true })

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

With `send_realtime_input`, the API will respond to audio automatically based
on VAD. While `send_client_content` adds messages to the model context in
order, `send_realtime_input` is optimized for responsiveness at the expense of
deterministic ordering.

### Automatic VAD configuration

For more control over the VAD activity, you can configure the following
parameters. See API reference for more
info.

 
 

### Python

 

```
from google.genai import types

config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {
        "automatic_activity_detection": {
            "disabled": False, # default
            "start_of_speech_sensitivity": types.StartSensitivity.START_SENSITIVITY_LOW,
            "end_of_speech_sensitivity": types.EndSensitivity.END_SENSITIVITY_LOW,
            "prefix_padding_ms": 20,
            "silence_duration_ms": 100,
        }
    }
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality, StartSensitivity, EndSensitivity } from '@google/genai';

const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: false, // default
      startOfSpeechSensitivity: StartSensitivity.START_SENSITIVITY_LOW,
      endOfSpeechSensitivity: EndSensitivity.END_SENSITIVITY_LOW,
      prefixPaddingMs: 20,
      silenceDurationMs: 100,
    }
  }
};
```

 
 

### Disable automatic VAD

Alternatively, the automatic VAD can be disabled by setting


```
realtimeInputConfig.automaticActivityDetection.disabled
```

 to `true` in the setup
message. In this configuration the client is responsible for detecting user
speech and sending
 `activityStart` 
and `activityEnd` 
messages at the appropriate times. An `audioStreamEnd` isn't sent in
this configuration. Instead, any interruption of the stream is marked by
an `activityEnd` message.

 
 

### Python

 

```
config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {"automatic_activity_detection": {"disabled": True}},
}

async with client.aio.live.connect(model=model, config=config) as session:
    # ...
    await session.send_realtime_input(activity_start=types.ActivityStart())
    await session.send_realtime_input(
        audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
    )
    await session.send_realtime_input(activity_end=types.ActivityEnd())
    # ...
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: true,
    }
  }
};

session.sendRealtimeInput({ activityStart: {} })

session.sendRealtimeInput(
  {
    audio: {
      data: base64Audio,
      mimeType: "audio/pcm;rate=16000"
    }
  }

);

session.sendRealtimeInput({ activityEnd: {} })
```

 
 

## Token count

You can find the total number of consumed tokens in the
 usageMetadata field of the returned server message.

 
 

### Python

 

```
async for message in session.receive():
    # The server will periodically send messages that include UsageMetadata.
    if message.usage_metadata:
        usage = message.usage_metadata
        print(
            f"Used {usage.total_token_count} tokens in total. Response token breakdown:"
        )
        for detail in usage.response_tokens_details:
            match detail:
                case types.ModalityTokenCount(modality=modality, token_count=count):
                    print(f"{modality}: {count}")
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.usageMetadata) {
    console.debug('Used %s tokens in total. Response token breakdown:\n', turn.usageMetadata.totalTokenCount);

    for (const detail of turn.usageMetadata.responseTokensDetails) {
      console.debug('%s\n', detail);
    }
  }
}
```

 
 

## Media resolution

You can specify the media resolution for the input media by setting the
`mediaResolution` field as part of the session configuration:

 
 

### Python

 

```
from google.genai import types

config = {
    "response_modalities": ["AUDIO"],
    "media_resolution": types.MediaResolution.MEDIA_RESOLUTION_LOW,
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality, MediaResolution } from '@google/genai';

const config = {
    responseModalities: [Modality.TEXT],
    mediaResolution: MediaResolution.MEDIA_RESOLUTION_LOW,
};
```

 
 

## Limitations

Consider the following limitations of the Live API
when you plan your project.

### Response modalities

You can only set one response modality (`TEXT` or `AUDIO`) per session in the
session configuration. Setting both results in a config error message. This
means that you can configure the model to respond with either text or audio,
but not both in the same session.

### Client authentication

The Live API only provides server-to-server authentication
by default. If you're implementing your Live API application
using a client-to-server approach , you need to use
 ephemeral tokens to mitigate security
risks.

### Session duration

Audio-only sessions are limited to 15 minutes,
and audio plus video sessions are limited to 2 minutes.
However, you can configure different session management techniques for unlimited extensions on session duration.

### Context window

A session has a context window limit of:

- 128k tokens for native audio output models

- 32k tokens for other Live API models

## Supported languages

Live API supports the following languages.

 
 
 
 
 
 
 
 
 
 Language 
 BCP-47 Code 
 Language 
 BCP-47 Code 
 
 
 
 
 German (Germany) 
 `de-DE` 
 English (Australia)* 
 `en-AU` 
 
 
 English (UK)* 
 `en-GB` 
 English (India) 
 `en-IN` 
 
 
 English (US) 
 `en-US` 
 Spanish (US) 
 `es-US` 
 
 
 French (France) 
 `fr-FR` 
 Hindi (India) 
 `hi-IN` 
 
 
 Portuguese (Brazil) 
 `pt-BR` 
 Arabic (Generic) 
 `ar-XA` 
 
 
 Spanish (Spain)* 
 `es-ES` 
 French (Canada)* 
 `fr-CA` 
 
 
 Indonesian (Indonesia) 
 `id-ID` 
 Italian (Italy) 
 `it-IT` 
 
 
 Japanese (Japan) 
 `ja-JP` 
 Turkish (Turkey) 
 `tr-TR` 
 
 
 Vietnamese (Vietnam) 
 `vi-VN` 
 Bengali (India) 
 `bn-IN` 
 
 
 Gujarati (India)* 
 `gu-IN` 
 Kannada (India)* 
 `kn-IN` 
 
 
 Marathi (India) 
 `mr-IN` 
 Malayalam (India)* 
 `ml-IN` 
 
 
 Tamil (India) 
 `ta-IN` 
 Telugu (India) 
 `te-IN` 
 
 
 Dutch (Netherlands) 
 `nl-NL` 
 Korean (South Korea) 
 `ko-KR` 
 
 
 Mandarin Chinese (China)* 
 `cmn-CN` 
 Polish (Poland) 
 `pl-PL` 
 
 
 Russian (Russia) 
 `ru-RU` 
 Thai (Thailand) 
 `th-TH` 
 
 
 

 Languages marked with an asterisk (*) are not available for Native audio .

## What's next

- Read the Tool Use and
 Session Management guides for essential
information on using the Live API effectively.

- Try the Live API in Google AI Studio .

- For more info about the Live API models, see Gemini 2.5 Flash Native Audio 
on the Models page.

- Try more examples in the Live API cookbook ,
the Live API Tools cookbook ,
and the Live API Get Started script .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Understand and count tokens &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/tokens

- 
 
 
 
 
 
 
 
 
 
 
 Understand and count tokens  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Understand and count tokens 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini and other generative AI models process input and output at a granularity
called a token .

 For Gemini models, a token is equivalent to about 4 characters.
100 tokens is equal to about 60-80 English words. 

## About tokens

Tokens can be single characters like `z` or whole words like `cat`. Long words
are broken up into several tokens. The set of all tokens used by the model is
called the vocabulary, and the process of splitting text into tokens is called
 tokenization .

When billing is enabled, the cost of a call to the Gemini API is
determined in part by the number of input and output tokens, so knowing how to
count tokens can be helpful.

 
 
 
 
 

## Try out counting tokens in a Colab

You can try out counting tokens by using a Colab.

 
 
 View on ai.google.dev 
 
 
 
 Try a Colab notebook 
 
 
 View notebook on GitHub 
 
 

## Context windows

The models available through the Gemini API have context windows that are
measured in tokens. The context window defines how much input you can provide
and how much output the model can generate. You can determine the size of the
context window by calling the getModels endpoint or
by looking in the models documentation .

In the following example, you can see that the `gemini-1.5-flash` model has an
input limit of about 1,000,000 tokens and an output limit of about 8,000 tokens,
which means a context window is 1,000,000 tokens.

 

```
from google import genai

client = genai.Client()
model_info = client.models.get(model="gemini-2.0-flash")
print(f"{model_info.input_token_limit=}")
print(f"{model_info.output_token_limit=}")
# ( e.g., input_token_limit=30720, output_token_limit=2048 )count_tokens.py
```

 

## Count tokens

All input to and output from the Gemini API is tokenized, including text, image
files, and other non-text modalities.

You can count tokens in the following ways:

### Count text tokens

 

```
from google import genai

client = genai.Client()
prompt = "The quick brown fox jumps over the lazy dog."

# Count tokens using the new client method.
total_tokens = client.models.count_tokens(
    model="gemini-2.0-flash", contents=prompt
)
print("total_tokens: ", total_tokens)
# ( e.g., total_tokens: 10 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=prompt
)

# The usage_metadata provides detailed token counts.
print(response.usage_metadata)
# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )count_tokens.py
```

 

### Count multi-turn (chat) tokens

 

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.0-flash",
    history=[
        types.Content(
            role="user", parts=[types.Part(text="Hi my name is Bob")]
        ),
        types.Content(role="model", parts=[types.Part(text="Hi Bob!")]),
    ],
)
# Count tokens for the chat history.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=chat.get_history()
    )
)
# ( e.g., total_tokens: 10 )

response = chat.send_message(
    message="In one sentence, explain how a computer works to a young child."
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 25, candidates_token_count: 21, total_token_count: 46 )

# You can count tokens for the combined history and a new message.
extra = types.UserContent(
    parts=[
        types.Part(
            text="What is the meaning of life?",
        )
    ]
)
history = chat.get_history()
history.append(extra)
print(client.models.count_tokens(model="gemini-2.0-flash", contents=history))
# ( e.g., total_tokens: 56 )count_tokens.py
```

 

### Count multimodal tokens

All input to the Gemini API is tokenized, including text, image files, and other
non-text modalities. Note the following high-level key points about tokenization
of multimodal input during processing by the Gemini API:

- 

With Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as
258 tokens. Images larger in one or both dimensions are cropped and scaled as
needed into tiles of 768x768 pixels, each counted as 258 tokens. Prior to Gemini
2.0, images used a fixed 258 tokens.

- 

Video and audio files are converted to tokens at the following fixed rates:
video at 263 tokens per second and audio at 32 tokens per second.

#### Media resolutions

Gemini 3 Pro Preview introduces granular control over multimodal vision processing with the
`media_resolution` parameter. The `media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to
read fine text or identify small details, but increase token usage and latency.

For more details about the parameter and how it can impact token calculations,
see the media resolution guide.

#### Image files

Example that uses an uploaded image from the File API:

 

```
from google import genai

client = genai.Client()
prompt = "Tell me about this image"
your_image_file = client.files.upload(file=media / "organ.jpg")

print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_image_file]
    )
)
# ( e.g., total_tokens: 263 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_image_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py
```

 

Example that provides the image as inline data:

 

```
from google import genai
import PIL.Image

client = genai.Client()
prompt = "Tell me about this image"
your_image_file = PIL.Image.open(media / "organ.jpg")

# Count tokens for combined text and inline image.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_image_file]
    )
)
# ( e.g., total_tokens: 263 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_image_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py
```

 

#### Video or audio files

Audio and video are each converted to tokens at the following fixed rates:

- Video: 263 tokens per second

- Audio: 32 tokens per second

 

```
from google import genai
import time

client = genai.Client()
prompt = "Tell me about this video"
your_file = client.files.upload(file=media / "Big_Buck_Bunny.mp4")

# Poll until the video file is completely processed (state becomes ACTIVE).
while not your_file.state or your_file.state.name != "ACTIVE":
    print("Processing video...")
    print("File state:", your_file.state)
    time.sleep(5)
    your_file = client.files.get(name=your_file.name)

print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_file]
    )
)
# ( e.g., total_tokens: 300 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 301, candidates_token_count: 60, total_token_count: 361 )count_tokens.py
```

 

### System instructions and tools

System instructions and tools also count towards the total token count for the
input.

If you use system instructions, the `total_tokens` count increases to
reflect the addition of `system_instruction`.

If you use function calling, the `total_tokens` count increases to reflect the
addition of `tools`.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Tool use with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-tools#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Tool use with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠ÔøΩÔøΩ ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Tool use with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Tool use allows Live API to go beyond just conversation by enabling it to
perform actions in the real-world and pull in external context while maintaining
a real time connection.
You can define tools such as Function calling 
 and Google Search with the Live API.

## Overview of supported tools

Here's a brief overview of the available tools for Live API models:

 
 
 Tool 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 Search 
 Yes 
 
 
 Function calling 
 Yes 
 
 
 Google Maps 
 No 
 
 
 Code execution 
 No 
 
 
 URL context 
 No 
 
 

## Function calling

Live API supports function calling, just like regular content generation
requests. Function calling lets the Live API interact with external data and
programs, greatly increasing what your applications can accomplish.

You can define function declarations as part of the session configuration.
After receiving tool calls, the client should respond with a list of
`FunctionResponse` objects using the `session.send_tool_response` method.

See the Function calling tutorial to learn
more.

 
 

### Python 

 

```
import asyncio
import wave
from google import genai
from google.genai import types

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"

# Simple function definitions
turn_on_the_lights = {"name": "turn_on_the_lights"}
turn_off_the_lights = {"name": "turn_off_the_lights"}

tools = [{"function_declarations": [turn_on_the_lights, turn_off_the_lights]}]
config = {"response_modalities": ["AUDIO"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Turn on the lights please"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)
            elif response.tool_call:
                print("The tool was called")
                function_responses = []
                for fc in response.tool_call.function_calls:
                    function_response = types.FunctionResponse(
                        id=fc.id,
                        name=fc.name,
                        response={ "result": "ok" } # simple, hard-coded function response
                    )
                    function_responses.append(function_response)

                await session.send_tool_response(function_responses=function_responses)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

// Simple function definitions
const turn_on_the_lights = { name: "turn_on_the_lights" } // , description: '...', parameters: { ... }
const turn_off_the_lights = { name: "turn_off_the_lights" }

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]

const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Turn on the lights please';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  for (const turn of turns) {
    if (turn.toolCall) {
      console.debug('A tool was called');
      const functionResponses = [];
      for (const fc of turn.toolCall.functionCalls) {
        functionResponses.push({
          id: fc.id,
          name: fc.name,
          response: { result: "ok" } // simple, hard-coded function response
        });
      }

      console.debug('Sending tool response...\n');
      session.sendToolResponse({ functionResponses: functionResponses });
    }
  }

  // Check again for new messages
  turns = await handleTurn();

  // Combine audio data strings and save as wave file
  const combinedAudio = turns.reduce((acc, turn) => {
      if (turn.data) {
          const buffer = Buffer.from(turn.data, 'base64');
          const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
          return acc.concat(Array.from(intArray));
      }
      return acc;
  }, []);

  const audioBuffer = new Int16Array(combinedAudio);

  const wf = new WaveFile();
  wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
  fs.writeFileSync('audio.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

From a single prompt, the model can generate multiple function calls and the
code necessary to chain their outputs. This code executes in a sandbox
environment, generating subsequent BidiGenerateContentToolCall messages.

## Asynchronous function calling

Function calling executes sequentially by default, meaning execution pauses
until the results of each function call are available. This ensures sequential
processing, which means you won't be able to continue interacting with the model
while the functions are being run.

If you don't want to block the conversation, you can tell the model to run the
functions asynchronously. To do so, you first need to add a `behavior` to the
function definitions:

 
 

### Python 

 

```
# Non-blocking function definitions
turn_on_the_lights = {"name": "turn_on_the_lights", "behavior": "NON_BLOCKING"} # turn_on_the_lights will run asynchronously
turn_off_the_lights = {"name": "turn_off_the_lights"} # turn_off_the_lights will still pause all interactions with the model
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality, Behavior } from '@google/genai';

// Non-blocking function definitions
const turn_on_the_lights = {name: "turn_on_the_lights", behavior: Behavior.NON_BLOCKING}

// Blocking function definitions
const turn_off_the_lights = {name: "turn_off_the_lights"}

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]
```

 
 

`NON-BLOCKING` ensures the function runs asynchronously while you can
continue interacting with the model.

Then you need to tell the model how to behave when it receives the
`FunctionResponse` using the `scheduling` parameter. It can either:

- Interrupt what it's doing and tell you about the response it got right away
(`scheduling="INTERRUPT"`),

- Wait until it's finished with what it's currently doing
(`scheduling="WHEN_IDLE"`),

- 

Or do nothing and use that knowledge later on in the discussion
(`scheduling="SILENT"`)

 
 

### Python 

 

```
# for a non-blocking function definition, apply scheduling in the function response:
  function_response = types.FunctionResponse(
      id=fc.id,
      name=fc.name,
      response={
          "result": "ok",
          "scheduling": "INTERRUPT" # Can also be WHEN_IDLE or SILENT
      }
  )
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality, Behavior, FunctionResponseScheduling } from '@google/genai';

// for a non-blocking function definition, apply scheduling in the function response:
const functionResponse = {
  id: fc.id,
  name: fc.name,
  response: {
    result: "ok",
    scheduling: FunctionResponseScheduling.INTERRUPT  // Can also be WHEN_IDLE or SILENT
  }
}
```

 
 

## Grounding with Google Search

You can enable Grounding with Google Search as part of the session
configuration. This increases the Live API's accuracy and prevents
hallucinations. See the Grounding tutorial to
learn more.

 
 

### Python 

 

```
import asyncio
import wave
from google import genai
from google.genai import types

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"

tools = [{'google_search': {}}]
config = {"response_modalities": ["AUDIO"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "When did the last Brazil vs. Argentina soccer match happen?"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.data is not None:
                    wf.writeframes(chunk.data)

                # The model might generate and execute Python code to use Search
                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                        if part.executable_code is not None:
                            print(part.executable_code.code)

                        if part.code_execution_result is not None:
                            print(part.code_execution_result.output)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript 

 

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

const tools = [{ googleSearch: {} }]
const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'When did the last Brazil vs. Argentina soccer match happen?';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  let combinedData = '';
  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
        else if (part.inlineData && typeof part.inlineData.data === 'string') {
          combinedData += atob(part.inlineData.data);
        }
      }
    }
  }

  // Convert the base64-encoded string of bytes into a Buffer.
  const buffer = Buffer.from(combinedData, 'binary');

  // The buffer contains raw bytes. For 16-bit audio, we need to interpret every 2 bytes as a single sample.
  const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);

  const wf = new WaveFile();
  // The API returns 16-bit PCM audio at a 24kHz sample rate.
  wf.fromScratch(1, 24000, '16', intArray);
  fs.writeFileSync('audio.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## Combining multiple tools

You can combine multiple tools within the Live API,
increasing your application's capabilities even more:

 
 

### Python 

 

```
prompt = """
Hey, I need you to do two things for me.

1. Use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
2. Then turn on the lights

Thanks!
"""

tools = [
    {"google_search": {}},
    {"function_declarations": [turn_on_the_lights, turn_off_the_lights]},
]

config = {"response_modalities": ["AUDIO"], "tools": tools}

# ... remaining model call
```

 
 

### JavaScript 

 

```
const prompt = `Hey, I need you to do two things for me.

1. Use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
2. Then turn on the lights

Thanks!
`

const tools = [
  { googleSearch: {} },
  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }
]

const config = {
  responseModalities: [Modality.AUDIO],
  tools: tools
}

// ... remaining model call
```

 
 

## What's next

- Check out more examples of using tools with the Live API in the
 Tool use cookbook .

- Get the full story on features and configurations from the
 Live API Capabilities guide .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Prompt design strategies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/prompting-strategies

- 
 
 
 
 
 
 
 
 
 
 
 Prompt design strategies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Prompt design strategies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 Prompt design is the process of creating prompts, or natural language requests,
that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you
started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides
on:

- Prompting with media files 

- Prompting for image generation with Imagen and Gemini Native Image Generation 

- Prompting for video generation 

You can find other sample prompts in the prompt gallery 
meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with
clear and specific instructions. Instructions can be in the form of a question,
step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a
response to. Inputs can be a question that the model
answers (question input), a task the model performs (task input), an entity the
model operates on (entity input), or partial input that the model completes or
continues (completion input).

 
 
 
 Input type 
 Prompt 
 Generated output 
 
 
 
 
 Question
 
 

```
    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

 
 
 

```
    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
```

 
 
 
 
 Task
 
 

```
    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.
```

 
 
 

```
    1. **Tent:** Your shelter from the elements.
    2. **Sleeping Bag:** Stay warm and comfortable.
    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.
    4. **Headlamp or Flashlight:** Light for nighttime activities.
    5. **First Aid Kit:** Be prepared for minor injuries.
```

 
 
 
 
 Entity
 
 

```
    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

 
 
 

```
Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
```

 
 
 
 
 

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you
provide partial content, the model can provide the rest of the content or what
it thinks is a continuation of that content as a response. When doing so, if you
include any examples or context, the model can take those examples or context
into account.

The following example provides a prompt with an instruction and an entity input:

 
 
 
 Prompt: 
 

```
For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
```

 
 Response: 
 

```
{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
```

 
 (gemini-2.5-flash) 
 
 
 

While the model did as prompted, writing out the instructions in natural language
can sometimes be challenging and it leaves a lot to the model's interpretation.
For example, a restaurants menu might contain many items. To reduce the size of
the JSON response, you probably want to omit the items that weren't ordered. In
this case, you can give an example and a response prefix and let
the model complete it:

 
 
 
 Prompt: 
 

```
Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
```
{
  "cheeseburger": 1,
  "fries": 1
}
```
Order: I want two burgers, a drink, and fries.
Output:
```

 
 Response: 
 

```
```
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
```
```

 
 (gemini-2.5-flash) 
 
 
 

Notice how "cheeseburger" was excluded from the output because it wasn't a part
of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can
tell the model what to do and not to do. For example, you can specify a constraint
in the prompt on how long you want a summary to be:

 
 
 
 

 Prompt: 
 

```
Summarize this text in one sentence:
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
```

 
 

 

 Response: 
 

```
      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
```

 
 (gemini-2.5-flash) 
 

 
 
 

### Response format

You can give instructions that specify the format of the response. For example,
you can ask for the response to be formatted as a table, bulleted list, elevator
pitch, keywords, sentence, or paragraph. The following system instruction tells
the model to be more conversational in its response:

 
 
 
 System instruction 
 

```
    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
```

 
 Prompt 
 

```
    What is a smart way to make a business that sells DVD's in 2025?
```

 
 Response: 
 

```
Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    * Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
```

 
 (gemini-2.5-flash) 
 
 
 

#### Format responses with the completion strategy

The completion strategy can also help format the response.
The following example prompts the model to create an essay outline:

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
```

 
 Response: 
 

```
Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The prompt didn't specify the format for the outline and the model chose a format
for you. To get the model to return an outline in a specific format, you can add
text that represents the start of the outline and let the model complete it based
on the pattern that you initiated.

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
I. Introduction
   *
```

 
 Response: 
 

```
Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
* Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds, and their general characteristics.
* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right
looks like. The model attempts to identify patterns and relationships from the
examples and applies them when generating a response. Prompts that contain a few
examples are called few-shot prompts, while prompts that provide no
examples are called zero-shot prompts. Few-shot prompts are often used
to regulate the formatting, phrasing, scoping, or general patterning of model
responses. Use specific and varied examples to help the model narrow its focus
and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without
few-shot examples are likely to be less effective. In fact, you can remove
instructions from your prompt if your examples are clear enough in showing the
task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

 
 
 
 Prompt: 
 

```
Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
```

 
 (gemini-2.5-flash) 
 
 
 

If your use case requires the model to produce concise responses, you can include
examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter
explanations. In the response, you can see that the examples guided the model to
choose the shorter explanation (`Explanation2`) as opposed to the longer
explanation (`Explanation1`) like it did previously.

 
 
 
 Prompt: 
 

```
Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Answer: Explanation2
```

 
 (gemini-2.5-flash) 
 
 
 

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though
you may need to experiment with the number of examples to provide in the prompt
for the best results. At the same time, if you include too many examples,
the model may start to overfit 
the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using
examples to show the model an anti pattern to avoid.

‚õî Negative pattern: 

 
 
 
 Example 
 

```
Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
```

 
 

 
 
 

‚úÖ Positive pattern: 

 
 
 
 Example 
 

```
Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
```

 
 

 
 
 

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to
avoid responses with undesired formats. One of the primary objectives of adding
few-shot examples in prompts is to show the model the response format. Therefore,
it is essential to ensure a consistent format across all examples, especially
paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs
to solve a problem, instead of assuming that the model has all of the required
information. This contextual information helps the model understand the constraints
and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

 
 
 
 Prompt: 
 

```
What should I do to fix my disconnected wifi? The light on my Google
Wifi router is yellow and blinking slowly.
```

 
 Response: 
 

```
  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response looks like generic troubleshooting information that's not specific
to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's
troubleshooting guide as context for it to refer to when providing a response.

 
 
 
 Prompt: 
 

```
Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
```

 
 Response: 
 

```
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
```

 
 (gemini-2.5-flash) 
 
 
 

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve
several purposes, depending on where you put the prefix:

- Input prefix: Adding a prefix to the input signals semantically meaningful
parts of the input to the model. For example, the prefixes "English:" and 
"French:" demarcate two different languages.

- Output prefix: Even though the output is generated by the model, you can
add a prefix for the output in the prompt. The output prefix gives the model
information about what's expected as a response. For example, the output
prefix "JSON:" signals to the model that the output should be in JSON format.

- Example prefix: In few-shot prompts, adding prefixes to the examples
provides labels that the model can use when generating the output, which makes
it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the
output prefix.

 
 
 
 Prompt: 
 

```
Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
```

 
 Response: 
 

```
The answer is: large
```

 
 (gemini-2.5-flash) 
 
 
 

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this
complexity by breaking things down into simpler components.

- 

 Break down instructions: Instead of having many instructions in one
prompt, create one prompt per instruction. You can choose which prompt to
process based on the user's input.

- 

 Chain prompts: For complex tasks that involve multiple sequential steps,
make each step a prompt and chain the prompts together in a sequence. In this
sequential chain of prompts, the output of one prompt in the sequence becomes
the input of the next prompt. The output of the last prompt in the sequence
is the final output.

- 

 Aggregate responses: Aggregation is when you want to perform different
parallel tasks on different portions of the data and aggregate the results to
produce the final output. For example, you can tell the model to perform one
operation on the first part of the data, perform another operation on the rest
of the data and aggregate the results.

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how
the model generates a response. The model can generate different results for
different parameter values. Experiment with different parameter values to get
the best values for the task. The parameters available for
different models may differ. The most common parameters are the following:

- 

 Max output tokens: Specifies the maximum number of tokens that can be
generated in the response. A token is approximately four characters. 100
tokens correspond to roughly 60-80 words.

- 

 Temperature: The temperature controls the degree of randomness in token
selection. The temperature is used for sampling during response generation,
which occurs when `topP` and `topK` are applied. Lower temperatures are good
for prompts that require a more deterministic or less open-ended response,
while higher temperatures can lead to more diverse or creative results. A
temperature of 0 is deterministic, meaning that the highest probability
response is always selected.

- 

 `topK`: The `topK` parameter changes how the model selects tokens for
output. A `topK` of 1 means the selected token is the most probable among
all the tokens in the model's vocabulary (also called greedy decoding),
while a `topK` of 3 means that the next token is selected from among the 3
most probable using the temperature. For each token selection step, the
`topK` tokens with the highest probabilities are sampled. Tokens are then
further filtered based on `topP` with the final token selected using
temperature sampling.

- 

 `topP`: The `topP` parameter changes how the model selects tokens for
output. Tokens are selected from the most to least probable until the sum of
their probabilities equals the `topP` value. For example, if tokens A, B,
and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5,
then the model will select either A or B as the next token by using the
temperature and exclude C as a candidate. The default `topP` value is 0.95.

- 

 `stop_sequences`: Set a stop sequence to
tell the model to stop generating content. A stop sequence can be any
sequence of characters. Try to avoid using a sequence of characters that
may appear in the generated content.

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before
you consistently get the response you're looking for. This section provides
guidance on some things you can try when iterating on your prompts:

- 

 Use different phrasing: Using different words or phrasing in your prompts
often yields different responses from the model even though they all mean the
same thing. If you're not getting the expected results from your prompt, try
rephrasing it.

 
 
 
 

```
Version 1:
How do I bake a pie?

Version 2:
Suggest a recipe for a pie.

Version 3:
What's a good pie recipe?
```

 
 

 
 
 

- 

 Switch to an analogous task: If you can't get the model to follow your
instructions for a task, try giving it instructions for an analogous task
that achieves the same result.

This prompt tells the model to categorize a book by using predefined categories:

 
 
 
 Prompt: 
 

```
Which category does The Odyssey belong to:
thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
    The Odyssey belongs to the category of **mythology**. 

    Here's why:

    * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
    journey home after the Trojan War. It features gods, monsters, and supernatural events common to
    Greek mythology.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response is correct, but the model didn't stay within the bounds of the
options. You also want to model to just respond with one of the options instead
of in a full sentence. In this case, you can rephrase the instructions as a
multiple choice question and ask the model to choose an option.

 
 
 
 Prompt: 
 

```
Multiple choice problem: Which of the following options describes the book The Odyssey?
Options:


thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
The correct answer is mythology.
```

 
 (gemini-2.5-flash) 
 
 
 

 
- Change the order of prompt content: The order of the content in the prompt
can sometimes affect the response. Try changing the content order and see
how that affects the response.

 

```
Version 1:
[examples]
[context]
[input]

Version 2:
[input]
[examples]
[context]

Version 3:
[examples]
[input]
[context]
```

 

## Fallback responses

A fallback response is a response returned by the model when either the prompt
or the response triggers a safety filter. An example of a fallback response is
"I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

- Avoid relying on models to generate factual information.

- Use with care on math and logic problems.

## Gemini 3

Gemini 3 models are designed for advanced reasoning and instruction following.
They respond best to prompts that are direct, well-structured, and clearly
define the task and any constraints. The following practices are recommended for
optimal results with Gemini 3:

### Core prompting principles

- Be precise and direct: State your goal clearly and concisely. Avoid
unnecessary or overly persuasive language.

- Use consistent structure: Employ clear delimiters to separate different
parts of your prompt. XML-style tags (e.g., ` `, ` `) or
Markdown headings are effective. Choose one format and use it consistently
within a single prompt.

- Define parameters: Explicitly explain any ambiguous terms or parameters.

- Control output verbosity: By default, Gemini 3 provides direct and
efficient answers. If you need a more conversational or detailed response,
you must explicitly request it in your instructions.

- Handle multimodal inputs coherently: When using text, images, audio, or
video, treat them as equal-class inputs. Ensure your instructions clearly
reference each modality as needed.

- Prioritize critical instructions: Place essential behavioral
constraints, role definitions (persona), and output format requirements in
the System Instruction or at the very beginning of the user prompt.

- Structure for long contexts: When providing large amounts of context
(e.g., documents, code), supply all the context first. Place your specific
instructions or questions at the very end of the prompt.

- Anchor context: After a large block of data, use a clear transition
phrase to bridge the context and your query, such as "Based on the
information above..."

### Enhancing reasoning and planning

You can leverage Gemini 3's advanced thinking capabilities to improve its
response quality for complex tasks by prompting it to plan or
self-critique before providing the final response.

 Example - Explicit planning: 

 

```
Before providing the final answer, please:
1. Parse the stated goal into distinct sub-tasks.
2. Check if the input information is complete.
3. Create a structured outline to achieve the goal.
```

 

 Example - Self-critique: 

 

```
Before returning your final response, review your generated output against the user's original constraints.
1. Did I answer the user's *intent*, not just their literal words?
2. Is the tone authentic to the requested persona?
```

 

### Structured prompting examples

Using tags or Markdown helps the model distinguish between instructions,
context, and tasks.

 XML example: 

 

```
<role>
You are a helpful assistant.
</role>

<constraints>
1. Be objective.
2. Cite sources.
</constraints>

<context>
[Insert User Input Here - The model knows this is data, not instructions]
</context>

<task>
[Insert the specific user request here]
</task>
```

 

 Markdown example: 

 

```
# Identity
You are a senior solution architect.

# Constraints
- No external libraries allowed.
- Python 3.11+ syntax only.

# Output format
Return a single code block.
```

 

### Example template combining best practices

This template captures the core principles for prompting with Gemini 3. Always
make sure to iterate and modify for your specific use case.

 System Instruction: 

 

```
<role>
You are Gemini 3, a specialized assistant for [Insert Domain, e.g., Data Science].
You are precise, analytical, and persistent.
</role>

<instructions>
1. **Plan**: Analyze the task and create a step-by-step plan.
2. **Execute**: Carry out the plan.
3. **Validate**: Review your output against the user's task.
4. **Format**: Present the final answer in the requested structure.
</instructions>

<constraints>
- Verbosity: [Specify Low/Medium/High]
- Tone: [Specify Formal/Casual/Technical]
</constraints>

<output_format>
Structure your response as follows:
1. **Executive Summary**: [Short overview]
2. **Detailed Response**: [The main content]
</output_format>
```

 

 User Prompt: 

 

```
<context>
[Insert relevant documents, code snippets, or background info here]
</context>

<task>
[Insert specific user request here]
</task>

<final_instruction>
Remember to think step-by-step before answering.
</final_instruction>
```

 

## Generative models under the hood

This section aims to answer the question - Is there randomness in generative
models' responses, or are they deterministic? 

The short answer - yes to both. When you prompt a generative model, a text
response is generated in two stages. In the first stage, the generative model
processes the input prompt and generates a probability distribution over
possible tokens (words) that are likely to come next. For example, if you prompt
with the input text "The dog jumped over the ... ", the generative model will
produce an array of probable next words:

 

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

 

This process is deterministic; a generative model will produce this same
distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into
actual text responses through one of several decoding strategies. A simple
decoding strategy might select the most likely token at every timestep. This
process would always be deterministic. However, you could instead choose to
generate a response by randomly sampling over the distribution returned by the
model. This process would be stochastic (random). Control the degree of
randomness allowed in this decoding process by setting the temperature. A
temperature of 0 means only the most likely tokens are selected, and there's no
randomness. Conversely, a high temperature injects a high degree of randomness
into the tokens selected by the model, leading to more unexpected, surprising
model responses. For Gemini 3 , it's recommended to not change the default
temperature of 1.0 to avoid unexpected outcomes.

## Next steps

- Now that you have a deeper understanding of prompt design, try writing your
own prompts using Google AI Studio .

- Learn more about the Gemini 3 Pro Preview model.

- To learn about multimodal prompting, see
 Prompting with media files .

- To learn about image prompting, see the Imagen prompt guide 

- To learn about video prompting, see the Veo prompt guide 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Session management with Live API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/live-session#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Session management with Live API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Session management with Live API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

In the Live API, a session refers to a persistent
connection where input and output are streamed continuously over the same
connection (read more about how it works ).
This unique session design enables low latency and supports unique features, but
can also introduce challenges, like session time limits, and early termination.
This guide covers strategies for overcoming the session management challenges
that can arise when using the Live API.

## Session lifetime

Without compression, audio-only sessions are limited to 15 minutes,
and audio-video sessions are limited to 2 minutes. Exceeding these limits
will terminate the session (and therefore, the connection), but you can use
 context window compression to extend sessions to
an unlimited amount of time.

The lifetime of a connection is limited as well, to around 10 minutes. When the
connection terminates, the session terminates as well. In this case, you can
configure a single session to stay active over multiple connections using
 session resumption .
You'll also receive a GoAway message before the
connection ends, allowing you to take further actions.

## Context window compression

To enable longer sessions, and avoid abrupt connection termination, you can
enable context window compression by setting the contextWindowCompression 
field as part of the session configuration.

In the ContextWindowCompressionConfig , you can configure a
 sliding-window mechanism 
and the number of tokens 
that triggers compression.

 
 

### Python

 

```
from google.genai import types

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    context_window_compression=(
        # Configures compression with default parameters.
        types.ContextWindowCompressionConfig(
            sliding_window=types.SlidingWindow(),
        )
    ),
)
```

 
 

### JavaScript

 

```
const config = {
  responseModalities: [Modality.AUDIO],
  contextWindowCompression: { slidingWindow: {} }
};
```

 
 

## Session resumption

To prevent session termination when the server periodically resets the WebSocket
connection, configure the sessionResumption 
field within the setup configuration .

Passing this configuration causes the
server to send SessionResumptionUpdate 
messages, which can be used to resume the session by passing the last resumption
token as the `SessionResumptionConfig.handle` 
of the subsequent connection.

Resumption tokens are valid for 2 hr after the last sessions termination.

 
 

### Python

 

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-2.5-flash-native-audio-preview-09-2025"

async def main():
    print(f"Connecting to the service with handle {previous_session_handle}...")
    async with client.aio.live.connect(
        model=model,
        config=types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            session_resumption=types.SessionResumptionConfig(
                # The handle of the session to resume is passed here,
                # or else None to start a new session.
                handle=previous_session_handle
            ),
        ),
    ) as session:
        while True:
            await session.send_client_content(
                turns=types.Content(
                    role="user", parts=[types.Part(text="Hello world!")]
                )
            )
            async for message in session.receive():
                # Periodically, the server will send update messages that may
                # contain a handle for the current state of the session.
                if message.session_resumption_update:
                    update = message.session_resumption_update
                    if update.resumable and update.new_handle:
                        # The handle should be retained and linked to the session.
                        return update.new_handle

                # For the purposes of this example, placeholder input is continually fed
                # to the model. In non-sample code, the model inputs would come from
                # the user.
                if message.server_content and message.server_content.turn_complete:
                    break

if __name__ == "__main__":
    asyncio.run(main())
```

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

console.debug('Connecting to the service with handle %s...', previousSessionHandle)
const session = await ai.live.connect({
  model: model,
  callbacks: {
    onopen: function () {
      console.debug('Opened');
    },
    onmessage: function (message) {
      responseQueue.push(message);
    },
    onerror: function (e) {
      console.debug('Error:', e.message);
    },
    onclose: function (e) {
      console.debug('Close:', e.reason);
    },
  },
  config: {
    responseModalities: [Modality.AUDIO],
    sessionResumption: { handle: previousSessionHandle }
    // The handle of the session to resume is passed here, or else null to start a new session.
  }
});

const inputTurns = 'Hello how are you?';
session.sendClientContent({ turns: inputTurns });

const turns = await handleTurn();
for (const turn of turns) {
  if (turn.sessionResumptionUpdate) {
    if (turn.sessionResumptionUpdate.resumable && turn.sessionResumptionUpdate.newHandle) {
      let newHandle = turn.sessionResumptionUpdate.newHandle
      // ...Store newHandle and start new session with this handle here
    }
  }
}

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

 
 

## Receiving a message before the session disconnects

The server sends a GoAway message that signals that the current
connection will soon be terminated. This message includes the timeLeft ,
indicating the remaining time and lets you take further action before the
connection will be terminated as ABORTED.

 
 

### Python

 

```
async for response in session.receive():
    if response.go_away is not None:
        # The connection will soon be terminated
        print(response.go_away.time_left)
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.goAway) {
    console.debug('Time left: %s\n', turn.goAway.timeLeft);
  }
}
```

 
 

## Receiving a message when the generation is complete

The server sends a generationComplete 
message that signals that the model finished generating the response.

 
 

### Python

 

```
async for response in session.receive():
    if response.server_content.generation_complete is True:
        # The generation is complete
```

 
 

### JavaScript

 

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.generationComplete) {
    // The generation is complete
  }
}
```

 
 

## What's next

Explore more ways to work with the Live API in the full
 Capabilities guide,
the Tool use page, or the
 Live API cookbook .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-05 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-05 UTC."],[],[]]

---

### Logs and datasets &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/logs-datasets

- 
 
 
 
 
 
 
 
 
 
 
 Logs and datasets  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Logs and datasets 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This guide contains everything you need to get started with enabling logging
for your existing Gemini API applications. In this guide you'll learn how to
view logs from an existing or new application in the Google AI Studio dashboard
to better understand model behavior and how users may be interacting with your
applications. Use logging to observe, debug, and optionally share usage feedback
with Google to help improve Gemini across developer use cases . * 

All `GenerateContent` and `StreamGenerateContent` API calls are supported,
including those made through OpenAI compatibility 
endpoints.

## 1. Enable logging in Google AI Studio

Before you begin, ensure you have a billing-enabled project that you own.

- Open the logs page in Google AI Studio .

- Choose your project from the drop-down and press the enable button to enable logging for all requests by default.

 
 
 
 

You can enable or disable logging for all projects or for specific projects, and
change these preferences at any time through Google AI Studio. 

## 2. View logs in AI Studio

- Go to AI Studio .

- Select the project you've enabled logging for.

- You should see your logs appear in the table in reverse chronological order.

 
 
 
 

Click on an entry for a full page view of the request and response pair. You can
inspect the full prompt, the complete response from Gemini, and the context from
the previous turn. Note that each project has a default storage limit of up to
1,000 logs, and logs not saved in datasets will expire after 55 days. If your
project reaches its storage limit you will be promoted to delete logs.

## 3. Curate and share datasets

- From the logs table, locate the filter bar at the top to select a property to
filter by.

- From your filtered view of logs use the checkboxes to select all or a
few of the logs.

- Click the "Create Dataset" button that appears at the top of the list.

- Give your new dataset a descriptive name and optional description.

- You will see the dataset you just created with the curated set of logs.

 
 
 
 

Datasets can be helpful for a number of different use cases.

- Curating challenge sets: Drive future improvements that target areas where you want your AI to improve.

- Curate sample sets: For example, a sample from real usage to generate responses from another model, or a collection of edge cases for routine checks before deployment.

- Evaluation sets: Sets that are representative of real usage across important capabilities, for comparison across other models or system instruction iterations.

You can help drive progress in AI research, the Gemini API, and Google AI Studio
by choosing to share your datasets as demonstration examples. This allows us to
refine our models in diverse contexts and create AI systems that remain useful
to developers across many fields and applications

## Next steps & what to test

Now that you have logging enabled, here are a few things to try:

- Prototype with session history: Leverage AI Studio Build to vibe code apps and add your API key to enable a history of user logs.

- Re-run logs with the Gemini Batch API: Use datasets for response sampling
and evaluation of models or application logic by re-running logs via the
 Gemini Batch API .

## Compatibility

Logging is not currently supported for the following:

- Imagen and Veo

- Inputs containing videos, GIFs or PDFs

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Ephemeral tokens &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ephemeral-tokens#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Ephemeral tokens  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Ephemeral tokens 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Ephemeral tokens are short-lived authentication tokens for accessing the Gemini
API through WebSockets . They are designed to enhance security when
you are connecting directly from a user's device to the API (a
 client-to-server 
implementation). Like standard API keys, ephemeral tokens can be extracted from
client-side applications such as web browsers or mobile apps. But because
ephemeral tokens expire quickly and can be restricted, they significantly reduce
the security risks in a production environment. You should use them when
accessing the Live API directly from client-side applications to enhance API
key security.

## How ephemeral tokens work

Here's how ephemeral tokens work at a high level:

- Your client (e.g. web app) authenticates with your backend.

- Your backend requests an ephemeral token from Gemini API's provisioning
service.

- Gemini API issues a short-lived token.

- Your backend sends the token to the client for WebSocket connections to Live
API. You can do this by swapping your API key with an ephemeral token.

- The client then uses the token as if it were an API key.

 

This enhances security because even if extracted, the token is short-lived,
unlike a long-lived API key deployed client-side. Since the client sends data
directly to Gemini, this also improves latency and avoids your backends needing
to proxy the real time data.

## Create an ephemeral token

Here is a simplified example of how to get an ephemeral token from Gemini.
By default, you'll have 1 minute to start new Live API sessions using the token
from this request (`newSessionExpireTime`), and 30 minutes to send messages over
that connection (`expireTime`).

 
 

### Python

 

```
import datetime

now = datetime.datetime.now(tz=datetime.timezone.utc)

client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1, # The ephemeral token can only be used to start a single session
    'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future
    # 'expire_time': '2025-05-17T00:00:00Z',   # Accepts isoformat.
    'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future
    'http_options': {'api_version': 'v1alpha'},
  }
)

# You'll need to pass the value under token.name back to your client to use it
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

  const token: AuthToken = await client.authTokens.create({
    config: {
      uses: 1, // The default
      expireTime: expireTime // Default is 30 mins
      newSessionExpireTime: new Date(Date.now() + (1 * 60 * 1000)), // Default 1 minute in the future
      httpOptions: {apiVersion: 'v1alpha'},
    },
  });
```

 
 

For `expireTime` value constraints, defaults, and other field specs, see the
 API reference .
Within the `expireTime` timeframe, you'll need
 `sessionResumption` to
reconnect the call every 10 minutes (this can be done with the same token even
if `uses: 1`).

It's also possible to lock an ephemeral token to a set of configurations. This
might be useful to further improve security of your application and keep your
system instructions on the server side.

 
 

### Python

 

```
client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1,
    'live_connect_constraints': {
        'model': 'gemini-2.5-flash-native-audio-preview-09-2025',
        'config': {
            'session_resumption':{},
            'temperature':0.7,
            'response_modalities':['AUDIO']
        }
    },
    'http_options': {'api_version': 'v1alpha'},
    }
)

# You'll need to pass the value under token.name back to your client to use it
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

const token = await client.authTokens.create({
    config: {
        uses: 1, // The default
        expireTime: expireTime,
        liveConnectConstraints: {
            model: 'gemini-2.5-flash-native-audio-preview-09-2025',
            config: {
                sessionResumption: {},
                temperature: 0.7,
                responseModalities: ['AUDIO']
            }
        },
        httpOptions: {
            apiVersion: 'v1alpha'
        }
    }
});

// You'll need to pass the value under token.name back to your client to use it
```

 
 

You can also lock a subset of fields, see the SDK documentation 
for more info.

## Connect to Live API with an ephemeral token

Once you have an ephemeral token, you use it as if it were an API key (but
remember, it only works for the live API, and only with the `v1alpha` version of
the API).

The use of ephemeral tokens only adds value when deploying applications
that follow client-to-server implementation approach.

 
 

### JavaScript

 

```
import { GoogleGenAI, Modality } from '@google/genai';

// Use the token generated in the "Create an ephemeral token" section here
const ai = new GoogleGenAI({
  apiKey: token.name
});
const model = 'gemini-2.5-flash-native-audio-preview-09-2025';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: { ... },
  });

  // Send content...

  session.close();
}

main();
```

 
 

See Get started with Live API for more examples.

## Best practices

- Set a short expiration duration using the `expire_time` parameter.

- Tokens expire, requiring re-initiation of the provisioning process.

- Verify secure authentication for your own backend. Ephemeral tokens will
only be as secure as your backend authentication method.

- Generally, avoid using ephemeral tokens for backend-to-Gemini connections,
as this path is typically considered secure.

## Limitations

Ephemeral tokens are only compatible with Live API at this time.

## What's next

- Read the Live API reference 
on ephemeral tokens for more information.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-11 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-11 UTC."],[],[]]

---

### Data Logging and Sharing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/logs-policy

- 
 
 
 
 
 
 
 
 
 
 
 Data Logging and Sharing  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Data Logging and Sharing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page outlines the storage and management of
 Gemini API logs , which are developer-owned
API data from supported Gemini API calls for projects with billing enabled. Logs
encompass the entire process from a user's request to the model's response.

## 1. Data that can be shared

As a project owner you have the choice to opt-in to logging of Gemini API calls,
for your own use or for feedback and sharing with Google to help us continually
improve our models.

With logging enabled, you can help us build AI systems that continue to be
valuable for developers across various fields and use cases by choosing to
contribute the following data for product improvements and model training:

- Datasets: Use the Logs and Datasets interface of Google AI Studio to
choose logs (requests, responses, metadata etc.) of interest from
supported Gemini API calls; contributed through inclusion in datasets, with the
option to opt-out during dataset creation.

- Feedback: When reviewing logs, you can provide feedback; including thumbs
up/down ratings and any written comments you provide.

When you share a dataset with Google, your logs in that dataset, including
requests and responses, will be processed in accordance with our
 Terms for
" Unpaid Services ,"
meaning the dataset may be used to develop and improve Google
products, services, and machine learning technologies, including improving and
training our models. Do not include personal, sensitive, or confidential
information. 

## 2. How we use your data

Logs will expire after 55 days by default. They will become unavailable
after this period. Datasets can be created to retain logs of interest or value
beyond this period for downstream use cases and optional contribution to model
improvements. Logs stored in datasets do not have set expiry dates, however each
project has a default storage limit of up to 1,000 logs.

By default, because logging is only available for billing-enabled projects,
prompts and responses within logs are not used for product improvement or
development, in accordance with our Terms 
on data use.

If you choose to share datasets of your logs with Google, those datasets will be
used as real-world demonstration data to better understand the diversity of
domains and contexts AI systems and applications are used in. This data may be
used to improve model quality, and inform the training and evaluation of future
models and services. This data is processed in accordance with our data use
terms for Unpaid Services .
Accordingly, human reviewers may read, annotate, and process the API inputs and
outputs you share. Before data is used for model improvement, Google takes steps
to protect user privacy as part of this process. This includes disconnecting
this data from your Google Account, API key, and Cloud project before reviewers
see or annotate it.

## 3. Data permissions

By opting-in to contributing API data, you confirm that you have the necessary
permissions for Google to process and use the data as described in this
documentation. Please do not contribute logs containing sensitive,
confidential, or proprietary information obtained through the paid service .
The license you grant to Google under the " Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

## 4. Data sharing and feedback

You can help us advance the frontier of AI research, the Gemini API and Google
AI Studio by opting in to share your data as examples, enabling us to
continually improve our models across various contexts and build AI systems that
continue to be valuable to developers across various fields and use cases.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Batch API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/batch-api#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Batch API  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Batch API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini Batch API is designed to process large volumes of requests
asynchronously at 50% of the standard cost .
The target turnaround time is 24 hours, but in majority of cases, it is much
quicker.

Use Batch API for large-scale, non-urgent tasks such as data
pre-processing or running evaluations where an immediate response is not
required.

## Creating a batch job

You have two ways to submit your requests in Batch API:

- Inline requests : A list of
 `GenerateContentRequest` objects
directly included in your batch creation request. This is suitable for
smaller batches that keep the total request size under 20MB. The output 
returned from the model is a list of `inlineResponse` objects.

- Input file : A JSON Lines (JSONL) 
file where each line contains a complete
 `GenerateContentRequest` object.
This method is recommended for larger requests. The output 
returned from the model is a JSONL file where each line is either a
`GenerateContentResponse` or a status object.

### Inline requests

For a small number of requests, you can directly embed the
 `GenerateContentRequest` objects
within your `BatchGenerateContentRequest` . The
following example calls the
 `BatchGenerateContent` 
method with inline requests:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# A list of dictionaries, where each is a GenerateContentRequest
inline_requests = [
    {
        'contents': [{
            'parts': [{'text': 'Tell me a one-sentence joke.'}],
            'role': 'user'
        }]
    },
    {
        'contents': [{
            'parts': [{'text': 'Why is the sky blue?'}],
            'role': 'user'
        }]
    }
]

inline_batch_job = client.batches.create(
    model="models/gemini-2.5-flash",
    src=inline_requests,
    config={
        'display_name': "inlined-requests-job-1",
    },
)

print(f"Created batch job: {inline_batch_job.name}")
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

const inlinedRequests = [
    {
        contents: [{
            parts: [{text: 'Tell me a one-sentence joke.'}],
            role: 'user'
        }]
    },
    {
        contents: [{
            parts: [{'text': 'Why is the sky blue?'}],
            role: 'user'
        }]
    }
]

const response = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: inlinedRequests,
    config: {
        displayName: 'inlined-requests-job-1',
    }
});

console.log(response);
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-X POST \
-H "Content-Type:application/json" \
-d '{
    "batch": {
        "display_name": "my-batch-requests",
        "input_config": {
            "requests": {
                "requests": [
                    {
                        "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
                        "metadata": {
                            "key": "request-1"
                        }
                    },
                    {
                        "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
                        "metadata": {
                            "key": "request-2"
                        }
                    }
                ]
            }
        }
    }
}'
```

 
 

### Input file

For larger sets of requests, prepare a JSON Lines (JSONL) file. Each line in
this file must be a JSON object containing a user-defined key and a request
object, where the request is a valid
 `GenerateContentRequest` object. The
user-defined key is used in the response to indicate which output is the result
of which request. For example, the request with the key defined as `request-1`
will have its response annotated with the same key name.

This file is uploaded using the File API . The maximum
allowed file size for an input file is 2GB.

The following is an example of a JSONL file. You can save it in a file named
`my-batch-requests.json`:

 

```
{"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generation_config": {"temperature": 0.7}}}
{"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}
```

 

Similarly to inline requests, you can specify other parameters like system
instructions, tools or other configurations in each request JSON.

You can upload this file using the File API as
shown in the following example. If
you are working with multimodal input, you can reference other uploaded files
within your JSONL file.

 
 

### Python

 

```
import json
from google import genai
from google.genai import types

client = genai.Client()

# Create a sample JSONL file
with open("my-batch-requests.jsonl", "w") as f:
    requests = [
        {"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]}},
        {"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}
    ]
    for req in requests:
        f.write(json.dumps(req) + "\n")

# Upload the file to the File API
uploaded_file = client.files.upload(
    file='my-batch-requests.jsonl',
    config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')
)

print(f"Uploaded file: {uploaded_file.name}")
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';
import * as fs from "fs";
import * as path from "path";
import { fileURLToPath } from 'url';

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});
const fileName = "my-batch-requests.jsonl";

// Define the requests
const requests = [
    { "key": "request-1", "request": { "contents": [{ "parts": [{ "text": "Describe the process of photosynthesis." }] }] } },
    { "key": "request-2", "request": { "contents": [{ "parts": [{ "text": "What are the main ingredients in a Margherita pizza?" }] }] } }
];

// Construct the full path to file
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const filePath = path.join(__dirname, fileName); // __dirname is the directory of the current script

async function writeBatchRequestsToFile(requests, filePath) {
    try {
        // Use a writable stream for efficiency, especially with larger files.
        const writeStream = fs.createWriteStream(filePath, { flags: 'w' });

        writeStream.on('error', (err) => {
            console.error(`Error writing to file ${filePath}:`, err);
        });

        for (const req of requests) {
            writeStream.write(JSON.stringify(req) + '\n');
        }

        writeStream.end();

        console.log(`Successfully wrote batch requests to ${filePath}`);

    } catch (error) {
        // This catch block is for errors that might occur before stream setup,
        // stream errors are handled by the 'error' event.
        console.error(`An unexpected error occurred:`, error);
    }
}

// Write to a file.
writeBatchRequestsToFile(requests, filePath);

// Upload the file to the File API.
const uploadedFile = await ai.files.upload({file: 'my-batch-requests.jsonl', config: {
    mimeType: 'jsonl',
}});
console.log(uploadedFile.name);
```

 
 

### REST

 

```
tmp_batch_input_file=batch_input.tmp
echo -e '{"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generationConfig": {"temperature": 0.7}}\n{"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}' > batch_input.tmp
MIME_TYPE=$(file -b --mime-type "${tmp_batch_input_file}")
NUM_BYTES=$(wc -c < "${tmp_batch_input_file}")
DISPLAY_NAME=BatchInput

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
-D "${tmp_header_file}" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "X-Goog-Upload-Protocol: resumable" \
-H "X-Goog-Upload-Command: start" \
-H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
-H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
-H "Content-Type: application/jsonl" \
-d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
-H "Content-Length: ${NUM_BYTES}" \
-H "X-Goog-Upload-Offset: 0" \
-H "X-Goog-Upload-Command: upload, finalize" \
--data-binary "@${tmp_batch_input_file}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
```

 
 

The following example calls the
 `BatchGenerateContent` 
method with the input file uploaded using File API:

 
 

### Python

 

```
from google import genai

# Assumes `uploaded_file` is the file object from the previous step
client = genai.Client()
file_batch_job = client.batches.create(
    model="gemini-2.5-flash",
    src=uploaded_file.name,
    config={
        'display_name': "file-upload-job-1",
    },
)

print(f"Created batch job: {file_batch_job.name}")
```

 
 

### JavaScript

 

```
// Assumes `uploadedFile` is the file object from the previous step
const fileBatchJob = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: uploadedFile.name,
    config: {
        displayName: 'file-upload-job-1',
    }
});

console.log(fileBatchJob);
```

 
 

### REST

 

```
# Set the File ID taken from the upload response.
BATCH_INPUT_FILE='files/123456'
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
-X POST \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" \
-d "{
    'batch': {
        'display_name': 'my-batch-requests',
        'input_config': {
            'file_name': '${BATCH_INPUT_FILE}'
        }
    }
}"
```

 
 

When you create a batch job, you will get a job name returned. Use this name
for monitoring the job status as well as
 retrieving the results once the job completes.

The following is an example output that contains a job name:

 

```
Created batch job from file: batches/123456789
```

 

### Batch embedding support

You can use the Batch API to interact with the
 Embeddings model for higher throughput.
To create an embeddings batch job with either inline requests 
or input files , use the `batches.create_embeddings` API and
specify the embeddings model.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Creating an embeddings batch job with an input file request:
file_job = client.batches.create_embeddings(
    model="gemini-embedding-001",
    src={'file_name': uploaded_batch_requests.name},
    config={'display_name': "Input embeddings batch"},
)

# Creating an embeddings batch job with an inline request:
batch_job = client.batches.create_embeddings(
    model="gemini-embedding-001",
    # For a predefined list of requests `inlined_requests`
    src={'inlined_requests': inlined_requests},
    config={'display_name': "Inlined embeddings batch"},
)
```

 
 

### JavaScript

 

```
// Creating an embeddings batch job with an input file request:
let fileJob;
fileJob = await client.batches.createEmbeddings({
    model: 'gemini-embedding-001',
    src: {fileName: uploadedBatchRequests.name},
    config: {displayName: 'Input embeddings batch'},
});
console.log(`Created batch job: ${fileJob.name}`);

// Creating an embeddings batch job with an inline request:
let batchJob;
batchJob = await client.batches.createEmbeddings({
    model: 'gemini-embedding-001',
    // For a predefined a list of requests `inlinedRequests`
    src: {inlinedRequests: inlinedRequests},
    config: {displayName: 'Inlined embeddings batch'},
});
console.log(`Created batch job: ${batchJob.name}`);
```

 
 

Read the Embeddings section in the Batch API cookbook 
for more examples.

### Request configuration

You can include any request configurations you would use in a standard non-batch
request. For example, you could specify the temperature, system instructions or
even pass in other modalities. The following example shows an example inline
request that contains a system instruction for one of the requests:

 
 

### Python

 

```
inline_requests_list = [
    {'contents': [{'parts': [{'text': 'Write a short poem about a cloud.'}]}]},
    {'contents': [{
        'parts': [{
            'text': 'Write a short poem about a cat.'
            }]
        }],
    'config': {
        'system_instruction': {'parts': [{'text': 'You are a cat. Your name is Neko.'}]}}
    }
]
```

 
 

### JavaScript

 

```
inlineRequestsList = [
    {contents: [{parts: [{text: 'Write a short poem about a cloud.'}]}]},
    {contents: [{parts: [{text: 'Write a short poem about a cat.'}]}],
     config: {systemInstruction: {parts: [{text: 'You are a cat. Your name is Neko.'}]}}}
]
```

 
 

Similarly can specify tools to use for a request. The following example
shows a request that enables the Google Search tool :

 
 

### Python

 

```
inlined_requests = [
{'contents': [{'parts': [{'text': 'Who won the euro 1998?'}]}]},
{'contents': [{'parts': [{'text': 'Who won the euro 2025?'}]}],
 'config':{'tools': [{'google_search': {}}]}}]
```

 
 

### JavaScript

 

```
inlineRequestsList = [
    {contents: [{parts: [{text: 'Who won the euro 1998?'}]}]},
    {contents: [{parts: [{text: 'Who won the euro 2025?'}]}],
     config: {tools: [{googleSearch: {}}]}}
]
```

 
 

You can specify structured output as well.
The following example shows how to specify for your batch requests.

 
 

### Python

 

```
import time
from google import genai
from pydantic import BaseModel, TypeAdapter

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()

# A list of dictionaries, where each is a GenerateContentRequest
inline_requests = [
    {
        'contents': [{
            'parts': [{'text': 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
            'role': 'user'
        }],
        'config': {
            'response_mime_type': 'application/json',
            'response_schema': list[Recipe]
        }
    },
    {
        'contents': [{
            'parts': [{'text': 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
            'role': 'user'
        }],
        'config': {
            'response_mime_type': 'application/json',
            'response_schema': list[Recipe]
        }
    }
]

inline_batch_job = client.batches.create(
    model="models/gemini-2.5-flash",
    src=inline_requests,
    config={
        'display_name': "structured-output-job-1"
    },
)

# wait for the job to finish
job_name = inline_batch_job.name
print(f"Polling status for job: {job_name}")

while True:
    batch_job_inline = client.batches.get(name=job_name)
    if batch_job_inline.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED'):
        break
    print(f"Job not finished. Current state: {batch_job_inline.state.name}. Waiting 30 seconds...")
    time.sleep(30)

print(f"Job finished with state: {batch_job_inline.state.name}")

# print the response
for i, inline_response in enumerate(batch_job_inline.dest.inlined_responses, start=1):
    print(f"\n--- Response {i} ---")

    # Check for a successful response
    if inline_response.response:
        # The .text property is a shortcut to the generated text.
        print(inline_response.response.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI, Type} from '@google/genai';
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

const inlinedRequests = [
    {
        contents: [{
            parts: [{text: 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
            role: 'user'
        }],
        config: {
            responseMimeType: 'application/json',
            responseSchema: {
            type: Type.ARRAY,
            items: {
                type: Type.OBJECT,
                properties: {
                'recipeName': {
                    type: Type.STRING,
                    description: 'Name of the recipe',
                    nullable: false,
                },
                'ingredients': {
                    type: Type.ARRAY,
                    items: {
                    type: Type.STRING,
                    description: 'Ingredients of the recipe',
                    nullable: false,
                    },
                },
                },
                required: ['recipeName'],
            },
            },
        }
    },
    {
        contents: [{
            parts: [{text: 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
            role: 'user'
        }],
        config: {
            responseMimeType: 'application/json',
            responseSchema: {
            type: Type.ARRAY,
            items: {
                type: Type.OBJECT,
                properties: {
                'recipeName': {
                    type: Type.STRING,
                    description: 'Name of the recipe',
                    nullable: false,
                },
                'ingredients': {
                    type: Type.ARRAY,
                    items: {
                    type: Type.STRING,
                    description: 'Ingredients of the recipe',
                    nullable: false,
                    },
                },
                },
                required: ['recipeName'],
            },
            },
        }
    }
]

const inlinedBatchJob = await ai.batches.create({
    model: 'gemini-2.5-flash',
    src: inlinedRequests,
    config: {
        displayName: 'inlined-requests-job-1',
    }
});
```

 
 

## Monitoring job status

Use the operation name obtained when creating the batch job to poll its status.
The state field of the batch job will indicate its current status. A batch job
can be in one of the following states:

- `JOB_STATE_PENDING`: The job has been created and is waiting to be processed by the service.

- `JOB_STATE_RUNNING`: The job is in progress.

- `JOB_STATE_SUCCEEDED`: The job completed successfully. You can now retrieve the results.

- `JOB_STATE_FAILED`: The job failed. Check the error details for more information.

- `JOB_STATE_CANCELLED`: The job was cancelled by the user.

- `JOB_STATE_EXPIRED`: The job has expired because it was running or pending
for more than 48 hours. The job will not have any results to retrieve.
You can try submitting the job again or splitting up
the requests into smaller batches.

You can poll the job status periodically to check for completion.

 
 

### Python

 

```
import time
from google import genai

client = genai.Client()

# Use the name of the job you want to check
# e.g., inline_batch_job.name from the previous step
job_name = "YOUR_BATCH_JOB_NAME"  # (e.g. 'batches/your-batch-id')
batch_job = client.batches.get(name=job_name)

completed_states = set([
    'JOB_STATE_SUCCEEDED',
    'JOB_STATE_FAILED',
    'JOB_STATE_CANCELLED',
    'JOB_STATE_EXPIRED',
])

print(f"Polling status for job: {job_name}")
batch_job = client.batches.get(name=job_name) # Initial get
while batch_job.state.name not in completed_states:
  print(f"Current state: {batch_job.state.name}")
  time.sleep(30) # Wait for 30 seconds before polling again
  batch_job = client.batches.get(name=job_name)

print(f"Job finished with state: {batch_job.state.name}")
if batch_job.state.name == 'JOB_STATE_FAILED':
    print(f"Error: {batch_job.error}")
```

 
 

### JavaScript

 

```
// Use the name of the job you want to check
// e.g., inlinedBatchJob.name from the previous step
let batchJob;
const completedStates = new Set([
    'JOB_STATE_SUCCEEDED',
    'JOB_STATE_FAILED',
    'JOB_STATE_CANCELLED',
    'JOB_STATE_EXPIRED',
]);

try {
    batchJob = await ai.batches.get({name: inlinedBatchJob.name});
    while (!completedStates.has(batchJob.state)) {
        console.log(`Current state: ${batchJob.state}`);
        // Wait for 30 seconds before polling again
        await new Promise(resolve => setTimeout(resolve, 30000));
        batchJob = await client.batches.get({ name: batchJob.name });
    }
    console.log(`Job finished with state: ${batchJob.state}`);
    if (batchJob.state === 'JOB_STATE_FAILED') {
        // The exact structure of `error` might vary depending on the SDK
        // This assumes `error` is an object with a `message` property.
        console.error(`Error: ${batchJob.state}`);
    }
} catch (error) {
    console.error(`An error occurred while polling job ${batchJob.name}:`, error);
}
```

 
 

## Retrieving results

Once the job status indicates your batch job has succeeded, the results are
available in the `response` field.

 
 

### Python

 

```
import json
from google import genai

client = genai.Client()

# Use the name of the job you want to check
# e.g., inline_batch_job.name from the previous step
job_name = "YOUR_BATCH_JOB_NAME"
batch_job = client.batches.get(name=job_name)

if batch_job.state.name == 'JOB_STATE_SUCCEEDED':

    # If batch job was created with a file
    if batch_job.dest and batch_job.dest.file_name:
        # Results are in a file
        result_file_name = batch_job.dest.file_name
        print(f"Results are in file: {result_file_name}")

        print("Downloading result file content...")
        file_content = client.files.download(file=result_file_name)
        # Process file_content (bytes) as needed
        print(file_content.decode('utf-8'))

    # If batch job was created with inline request
    # (for embeddings, use batch_job.dest.inlined_embed_content_responses)
    elif batch_job.dest and batch_job.dest.inlined_responses:
        # Results are inline
        print("Results are inline:")
        for i, inline_response in enumerate(batch_job.dest.inlined_responses):
            print(f"Response {i+1}:")
            if inline_response.response:
                # Accessing response, structure may vary.
                try:
                    print(inline_response.response.text)
                except AttributeError:
                    print(inline_response.response) # Fallback
            elif inline_response.error:
                print(f"Error: {inline_response.error}")
    else:
        print("No results found (neither file nor inline).")
else:
    print(f"Job did not succeed. Final state: {batch_job.state.name}")
    if batch_job.error:
        print(f"Error: {batch_job.error}")
```

 
 

### JavaScript

 

```
// Use the name of the job you want to check
// e.g., inlinedBatchJob.name from the previous step
const jobName = "YOUR_BATCH_JOB_NAME";

try {
    const batchJob = await ai.batches.get({ name: jobName });

    if (batchJob.state === 'JOB_STATE_SUCCEEDED') {
        console.log('Found completed batch:', batchJob.displayName);
        console.log(batchJob);

        // If batch job was created with a file destination
        if (batchJob.dest?.fileName) {
            const resultFileName = batchJob.dest.fileName;
            console.log(`Results are in file: ${resultFileName}`);

            console.log("Downloading result file content...");
            const fileContentBuffer = await ai.files.download({ file: resultFileName });

            // Process fileContentBuffer (Buffer) as needed
            console.log(fileContentBuffer.toString('utf-8'));
        }

        // If batch job was created with inline responses
        else if (batchJob.dest?.inlinedResponses) {
            console.log("Results are inline:");
            for (let i = 0; i < batchJob.dest.inlinedResponses.length; i++) {
                const inlineResponse = batchJob.dest.inlinedResponses[i];
                console.log(`Response ${i + 1}:`);
                if (inlineResponse.response) {
                    // Accessing response, structure may vary.
                    if (inlineResponse.response.text !== undefined) {
                        console.log(inlineResponse.response.text);
                    } else {
                        console.log(inlineResponse.response); // Fallback
                    }
                } else if (inlineResponse.error) {
                    console.error(`Error: ${inlineResponse.error}`);
                }
            }
        }

        // If batch job was an embedding batch with inline responses
        else if (batchJob.dest?.inlinedEmbedContentResponses) {
            console.log("Embedding results found inline:");
            for (let i = 0; i < batchJob.dest.inlinedEmbedContentResponses.length; i++) {
                const inlineResponse = batchJob.dest.inlinedEmbedContentResponses[i];
                console.log(`Response ${i + 1}:`);
                if (inlineResponse.response) {
                    console.log(inlineResponse.response);
                } else if (inlineResponse.error) {
                    console.error(`Error: ${inlineResponse.error}`);
                }
            }
        } else {
            console.log("No results found (neither file nor inline).");
        }
    } else {
        console.log(`Job did not succeed. Final state: ${batchJob.state}`);
        if (batchJob.error) {
            console.error(`Error: ${typeof batchJob.error === 'string' ? batchJob.error : batchJob.error.message || JSON.stringify(batchJob.error)}`);
        }
    }
} catch (error) {
    console.error(`An error occurred while processing job ${jobName}:`, error);
}
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" 2> /dev/null > batch_status.json

if jq -r '.done' batch_status.json | grep -q "false"; then
    echo "Batch has not finished processing"
fi

batch_state=$(jq -r '.metadata.state' batch_status.json)
if [[ $batch_state = "JOB_STATE_SUCCEEDED" ]]; then
    if [[ $(jq '.response | has("inlinedResponses")' batch_status.json) = "true" ]]; then
        jq -r '.response.inlinedResponses' batch_status.json
        exit
    fi
    responses_file_name=$(jq -r '.response.responsesFile' batch_status.json)
    curl https://generativelanguage.googleapis.com/download/v1beta/$responses_file_name:download?alt=media \
    -H "x-goog-api-key: $GEMINI_API_KEY" 2> /dev/null
elif [[ $batch_state = "JOB_STATE_FAILED" ]]; then
    jq '.error' batch_status.json
elif [[ $batch_state == "JOB_STATE_CANCELLED" ]]; then
    echo "Batch was cancelled by the user"
elif [[ $batch_state == "JOB_STATE_EXPIRED" ]]; then
    echo "Batch expired after 48 hours"
fi
```

 
 

## Cancelling a batch job

You can cancel an ongoing batch job using its name. When a job is
canceled, it stops processing new requests.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Cancel a batch job
client.batches.cancel(name=batch_job_to_cancel.name)
```

 
 

### JavaScript

 

```
await ai.batches.cancel({name: batchJobToCancel.name});
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

# Cancel the batch
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:cancel \
-H "x-goog-api-key: $GEMINI_API_KEY" \

# Confirm that the status of the batch after cancellation is JOB_STATE_CANCELLED
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H "Content-Type:application/json" 2> /dev/null | jq -r '.metadata.state'
```

 
 

## Deleting a batch job

You can delete an existing batch job using its name. When a job is
deleted, it stops processing new requests and is removed from the list of
batch jobs.

 
 

### Python

 

```
from google import genai

client = genai.Client()

# Delete a batch job
client.batches.delete(name=batch_job_to_delete.name)
```

 
 

### JavaScript

 

```
await ai.batches.delete({name: batchJobToDelete.name});
```

 
 

### REST

 

```
BATCH_NAME="batches/123456" # Your batch job name

# Delete the batch job
curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:delete \
-H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Technical details

- Supported models: Batch API supports a range of Gemini models.
Refer to the Models page for each model's support
of Batch API. The supported modalities for Batch API are the same
as what's supported on the interactive (or non-batch) API.

- Pricing: Batch API usage is priced at 50% of the standard interactive
API cost for the equivalent model. See the pricing page 
for details. Refer to the rate limits page 
for details on rate limits for this feature.

- Service Level Objective (SLO): Batch jobs are designed to complete
within a 24-hour turnaround time. Many jobs may complete much faster
depending on their size and current system load.

- Caching: Context caching is enabled
for batch requests. If a request in your batch results in a cache hit, the
cached tokens are priced the same as for non-batch API traffic.

## Best practices

- Use input files for large requests: For a large number of requests,
always use the file input
method for better manageability and to avoid hitting request size limits for
the `BatchGenerateContent` 
call itself. Note that there's a the 2GB file size limit per input file.

- Error handling: Check the `batchStats` for `failedRequestCount` after a
job completes. If using file output, parse each line to check if it's a
`GenerateContentResponse` or a status object indicating an error for that
specific request. See the troubleshooting
guide for a complete set of
error codes.

- Submit jobs once: The creation of a batch job is not idempotent.
If you send the same creation request twice, two separate batch jobs will
be created.

- Break up very large batches: While the target turnaround time is 24
hours, actual processing time can vary based on system load and job size.
For large jobs, consider breaking them into smaller
batches if intermediate results are needed sooner.

## What's next

- Check out the Batch API notebook 
for more examples.

- The OpenAI compatibility layer supports Batch API. Read the examples on the
 OpenAI Compatibility page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Files API &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/files#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Files API  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Files API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini family of artificial intelligence (AI) models is built to handle
various types of input data, including text, images, and audio. Since these
models can handle more than one type or mode of data, the Gemini models
are called multimodal models or explained as having multimodal capabilities .

This guide shows you how to work with media files using the Files API. The
basic operations are the same for audio files, images, videos, documents, and
other supported file types.

For file prompting guidance, check out the File prompt guide section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when
the total request size (including the files, text prompt, system instructions,
etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to
`generateContent`.

 
 

### Python

 

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.5-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

 
 

### REST

 

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

 
 

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its
metadata by calling `files.get`.

 
 

### Python

 

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

 
 

### JavaScript

 

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

 
 

### REST

 

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

 
 

## List uploaded files

You can upload multiple files using the Files API. The following code gets
a list of all the files uploaded:

 
 

### Python

 

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

 
 

### JavaScript

 

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

 
 

### Go

 

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

 
 

### REST

 

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an
uploaded file:

 
 

### Python

 

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

 
 

### JavaScript

 

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

 
 

### Go

 

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

 
 

### REST

 

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

 
 

## Usage info

You can use the Files API to upload and interact with media files. The Files API
lets you store up to 20 GB of files per project, with a per-file maximum
size of 2 GB. Files are stored for 48 hours. During that time, you can
use the API to get metadata about the files, but you can't download the files.
The Files API is available at no cost in all regions where the Gemini API is
available.

## File prompting strategies

This section provides guidance and best practices for using media files with
prompts for the Gemini API. 

Being able to use various types of data in your prompts gives you more
flexibility in terms of what tasks you can tackle with the Gemini API. For
example, you can send the model a photo of a delicious meal and ask it to write
a short blog about the meal.

 
 
 Prompt 
 Response 
 
 
 
 

Write a short, engaging blog post based on this picture. It should
 include a description of the meal in the photo and talk about my journey
 meal prepping.

 

 
 
 
 

Meal prepping is a great way to save time and money, and it can also
 help you to eat healthier. This meal is a great example of a healthy
 and delicious meal that can be easily prepped ahead of time.

 

This meal features brown rice, roasted vegetables, and chicken teriyaki.
 The brown rice is a healthy and complex carbohydrate that will give you
 sustained energy. The roasted vegetables are a great way to get your daily
 dose of vitamins and minerals, and the chicken teriyaki is a delicious and
 protein-rich option.

 

This meal is also very easy to prepare. Simply cook the brown rice, roast
 the vegetables, and cook the chicken teriyaki. Once everything is cooked,
 divide it into meal prep containers and store them in the refrigerator.
 You can then grab a container and go on busy mornings!

 

If you're looking for a healthy and delicious meal that can be easily
 prepped ahead of time, this meal is a great option. It's packed with
 nutrients and flavor, and it's sure to keep you feeling full and satisfied.

 

Here's to healthy and delicious meal prepping!

 
 
 

If you are having trouble getting the output you want from prompts that use
media files, there are some strategies that can help you get the results you
want. The following sections provide design approaches and troubleshooting
tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

 - 
 

### Prompt design fundamentals 

 

 Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation.

 - Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve.

 - Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process.

 - Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. 

 
 - Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.

 
 

 
 - 
 

### Troubleshooting your multimodal prompt 

 

 If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from.

 
 - If the model output is too generic (not tailored enough to the image/video input): At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.

 - To troubleshoot which part failed: Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.

 - If your prompt results in hallucinated content: Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.

 - Tuning the sampling parameters: Experiment with different temperature settings and top-k selections to adjust the model's creativity.
 
 

 
 

 

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a
specific output in mind, it's better to include that requirement in the prompt
to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this
image" could generate a general description. If you need the model to parse the
time and city from the image, you can include that request directly in your prompt.

 
 
 Prompt 
 Model response 
 
 
 
 
 

Describe this image.

 
 

The image shows an airport arrivals and departures board.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 Parse the time and city from the airport board shown in this image into a list.

 
 

- 10:50 Moscow

 - 11:05 Edinburgh

 - 11:05 London

 - 11:10 Bucharest

 - 11:30 Kiev

 - 11:35 Dublin

 - 11:45 East Midlands

 - 12:15 Sofia

 - 12:30 London

 - 12:30 Newcastle

 - 12:40 St Petersburg

 - 12:40 London

 - 12:45 Manchester

 
 
 

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the
output you want. Adding these examples can help the model identify the patterns and apply the
relationship between the given images and responses to the new example. This is also called
"few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the
country (Brazil). Suppose you want the output in a different format or style, and you want
only the city, not the country. Adding few-shot examples to your prompt can steer the model
to respond in the way you want.

 
 
 Prompt 
 Model response 
 
 
 Determine the city along with the landmark.

 
 

The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 

Determine the city along with the landmark.

 
 
 
 

city: Rome, landmark: the Colosseum.

 
 
 
 
 
 
 
 

city: Beijing, landmark: Forbidden City

 
 
 
 
 
 
 
 

city: Rio de Janeiro, landmark: Christ the Redeemer statue

 
 
 

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and
reasoning, it can be helpful to split the task into smaller, more straightforward
steps. Alternatively, it could also be effective if you
directly ask the model to ‚Äúthink step by step‚Äù in your prompt.

 
 
 Prompt 
 Model response 
 
 
 
 
 

When will I run out of toilet paper?

 
 

Soon, you only have 3 rolls left.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 

 
1. First, count how many toilet paper rolls are in this picture.

 
2. Then, determine how much toilet paper a typical person uses per day.

 
3. Calculate how long these rolls of toilet paper will last.

 

 
 

 
1. There are 3 rolls of toilet paper in this picture.

 
2. A typical person uses about 20 sheets of toilet paper per day.

 
3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.

 

 
 
 

Math problems or other types of word problems are great candidates for asking
the model to think step-by-step.

 
 
 Prompt 
 Response 
 
 
 

 What is the 4th term in the sequence?

 
 

-135
 

 
 
 

The response from the model is incorrect. Some ways to improve this is to ask
the model to parse the problem before answering the prompt.

 
 
 Prompt 
 Response 
 
 
 

 Parse the formula in the image first.

 

Then based on the formula, answer "what's the 4th term in the sequence?".

 
 

 
 Click to expand the result 
 $$ b(n) = b(n-1) * (-3) $$

$$ b(1) = 15 $$

- Substitute n = 4 into the formula:

$$ b(4) = b(4-1) * (-3) $$

$$ b(4) = b(3) * (-3) $$

- We don't know the value of b(3), so substitute n = 3 into the formula:

$$ b(3) = b(3-1) * (-3) $$

$$ b(3) = b(2) * (-3) $$

- We don't know the value of b(2), so substitute n = 2 into the formula:

$$ b(2) = b(2-1) * (-3) $$

$$ b(2) = b(1) * (-3) $$

- We know the value of b(1) is 15, so substitute it into the formula:

$$ b(2) = 15 * (-3) $$

$$ b(2) = -45 $$

- Now that we know the value of b(2), substitute it into the formula for b(3):

$$ b(3) = b(2) * (-3) $$

$$ b(3) = -45 * (-3) $$

$$ b(3) = 135 $$

- Now that we know the value of b(3), substitute it into the formula for b(4):

$$ b(4) = b(3) * (-3) $$

$$ b(4) = 135 * (-3) $$

$$ b(4) = -405 $$

Therefore, the 4th term in the sequence is -405.

 
 
 
 

This model response is correct - now to make the response even more streamlined,
you can ask the model to to think step-by-step.

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 What is the 4th term in the sequence? Think step by step.

 
 
 

The first term is 15.

 

The second term is b(1) * (-3) = 15 * (-3) = -45.

 

The third term is b(2) * (-3) = -45 * (-3) = 135.

 

The fourth term is b(3) * (-3) = 135 * (-3) = -405.

 
 
 

### Specify the output format

A common problem is the need for model output to be in a certain format
(e.g. markdown, JSON, HTML), especially if the model output needs to
be ingested by a downstream task. You can try to produce output in that format
by instructing the model to do so within the prompt. Below are two examples:

 
 
 Prompt 
 Model response 
 
 
 
 
 

Parse the table in this image into markdown format

 
 
 

| Subject | Status | Time | Start date | Due on |

 

| ------- | ------ | ---- | ---------- | ------ |

 

| Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 |

 

| Geography | Done | 30 min | 12/2/2023 | 12/4/2023 |

 

| Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 |

 

| Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |

 
 
 

 
 
 Prompt 
 Model response 
 
 
 
 
 

Provide a list of all the following attributes:

 

ingredients, type of cuisine, vegetarian or not, in JSON format

 
 
 

 

```
          ```json
          {
          "ingredients": [
          "rice",
          "shrimp",
          "clams",
          "mussels",
          "peas",
          "tomatoes",
          "onions",
          "garlic",
          "olive oil",
          "paprika",
          "salt",
          "pepper"
          ],
          "type of cuisine": "Spanish",
          "vegetarian": false
          }
          ```
```


 

 
 
 

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing
a single image before the text prompt might lead to better results.
In the following samples, you'll notice the image comes first before the text in
each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful
response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image
you want the prompt to draw information from to inform its response.

 
 
 Prompt 
 Model response 
 
 
 
 
 

How many days will these diapers last a baby?

 
 

The diapers will last for 198 days before they run out.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 

How long will these diapers last before I run out?

 

Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box.

 

Divide the total number by how many diapers the child goes through per day.

 
 
 

The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs.

 

The average child in this weight range is likely between 1 and 2 years old.

 

A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).

 
 
 

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe
the images before performing its reasoning task.

 
 
 Prompt 
 Model response 
 
 
 
 
 
 
 
 

What is in common between these images?

 
 

All four images are examples of landscape photography.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 
 
 
 

First, describe what's in each image in detail. What's in common between these images?

 
 
 

The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea.

 

The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity.

 

The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private
 and public realms. It's a place where we prepare ourselves to enter or leave the outside world.

 

The fourth image shows the sea in between being calm and being stormy.

 

Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.

 
 
 

You can also try explicitly asking the model to refer to what's in the image.

 
 
 Updated prompt 
 Improved response 
 
 
 
 
 
 
 
 

What is in common between these images? Refer to what's in the images in your response.

 
 
 

All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows
 an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a
 dark and stormy sea.

 

These images are all related to travel because they are all things that people need or use when
 they travel. The first image shows a person packing a suitcase, which is something that people do when they are
 going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people
 often go to buy things for their trips. The third image shows a stack of toilet paper, which is something
 that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is
 something that people often encounter when they are traveling.

 
 
 

The same tips work for creative output from images. In the following example with
the rabbit, explicitly asking the model to include aspects from the image can
improve the output.

 
 
 Prompt 
 Model response 
 
 
 
 
 

Can you write me a poem about this image?

 
 
 

Little bunny, so soft and brown,

 Hopping through the field, up and down.

 Your ears so long, your tail so small,

 You're the cutest creature of them all.

 

You nibble on the grass, so green and sweet,

 And hop away when you see me.

 But I'll always be your friend,

 Little bunny, until the very end.

 
 
 

 
 
 Updated prompt 
 Improved response 
 
 
 
 Can you write me a descriptive and dramatic poem about this image and include the location?

 
 
 

Little bunny on the green,

 Ears so long, so keen.

 What do you see out there,

 In the distance, where you stare?

 

Is it the ocean blue,

 Sparkling in the sun's hue?

 Or the waves that crash and roar,

 A rhythm that you adore?

 

Little bunny, tell me true,

 What captures your attention so?

 Is it the salty breeze,

 Or the seagulls' cries that please?

 

Perhaps it's the vastness of it all,

 A world so big and small.

 A place where dreams can roam,

 And adventures can be sown.

 

Little bunny, I wonder what you dream,

 As you sit on the grass, so serene.

 Do you yearn to explore the deep,

 Or stay on land, where you can leap?

 

Whatever it may be, little bunny,

 Keep that spark of wonder burning brightly.

 For in your dreams and aspirations,

 Lies a world waiting for your creation.

 
 
 

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model
didn't understand the image to begin with, or if it did understand the image
but did not perform the correct reasoning steps afterward.
To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising
when paired with tea (e.g. popcorn), you can first troubleshoot to determine
whether the model correctly recognized that the image contains tea.

 
 
 Prompt 
 Prompt for troubleshooting 
 
 
 
 
 

What's a snack I can make in 1 minute that would go well with this?

 
 
 
 

Describe what's in this image.

 
 
 

Another strategy is to ask the model to explain its reasoning. That can help you
narrow down which part of the reasoning broke down, if any.

 
 
 Prompt 
 Prompt for troubleshooting 
 
 
 
 
 

What's a snack I can make in 1 minute that would go well with this?

 
 
 
 

What's a snack I can make in 1 minute that would go well with this? Please explain why.

 
 
 

## What's next

- Try writing your own multimodal prompts using Google AI
Studio .

- For information on using the Gemini Files API for
uploading media files and including them in your prompts, see the
 Vision , Audio , and
 Document processing guides.

- For more guidance on prompt design, like tuning sampling parameters, see the
 Prompt strategies page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Safety settings &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/safety-settings

- 
 
 
 
 
 
 
 
 
 
 
 Safety settings  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Safety settings 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API provides safety settings that you can adjust during the
prototyping stage to determine if your application requires more or less
restrictive safety configuration. You can adjust these settings across five
filter categories to restrict or allow certain types of content.

This guide covers how the Gemini API handles safety settings and filtering and
how you can change the safety settings for your application.

## Safety filters

The Gemini API's adjustable safety filters cover the following categories:

 
 
 Category 
 Description 
 
 
 Harassment 
 
 Negative or harmful comments targeting identity and/or protected
 attributes.
 
 
 
 Hate speech 
 
 Content that is rude, disrespectful, or profane.
 
 
 
 Sexually explicit 
 
 Contains references to sexual acts or other lewd content.
 
 
 
 Dangerous 
 
 Promotes, facilitates, or encourages harmful acts.
 
 
 
 Civic integrity 
 
 Election-related queries.
 
 
 

You can use these filters to adjust what's appropriate for your use case. For
example, if you're building video game dialogue, you may deem it acceptable to
allow more content that's rated as Dangerous due to the nature of the game.

In addition to the adjustable safety filters, the Gemini API has built-in
protections against core harms, such as content that endangers child safety.
These types of harm are always blocked and cannot be adjusted.

### Content safety filtering level

The Gemini API categorizes the probability level of content being unsafe as
`HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.

The Gemini API blocks content based on the probability of content being unsafe
and not the severity. This is important to consider because some content can
have low probability of being unsafe even though the severity of harm could
still be high. For example, comparing the sentences:

- The robot punched me.

- The robot slashed me up.

The first sentence might result in a higher probability of being unsafe, but you
might consider the second sentence to be a higher severity in terms of violence.
Given this, it is important that you carefully test and consider what the
appropriate level of blocking is needed to support your key use cases while
minimizing harm to end users.

### Safety filtering per request

You can adjust the safety settings for each request you make to the API. When
you make a request, the content is analyzed and assigned a safety rating. The
safety rating includes the category and the probability of the harm
classification. For example, if the content was blocked due to the harassment
category having a high probability, the safety rating returned would have
category equal to `HARASSMENT` and harm probability set to `HIGH`.

By default, safety settings block content (including prompts) with medium or
higher probability of being unsafe across any filter. This baseline safety is
designed to work for most use cases, so you should only adjust your safety
settings if it's consistently required for your application.

The following table describes the block settings you can adjust for each
category. For example, if you set the block setting to Block few for the
 Hate speech category, everything that has a high probability of being hate
speech content is blocked. But anything with a lower probability is allowed.

 
 
 Threshold (Google AI Studio) 
 Threshold (API) 
 Description 
 
 
 Off 
 `OFF` 
 Turn off the safety filter 
 
 
 Block none 
 `BLOCK_NONE` 
 Always show regardless of probability of unsafe content 
 
 
 Block few 
 `BLOCK_ONLY_HIGH` 
 Block when high probability of unsafe content 
 
 
 Block some 
 `BLOCK_MEDIUM_AND_ABOVE` 
 Block when medium or high probability of unsafe content 
 
 
 Block most 
 `BLOCK_LOW_AND_ABOVE` 
 Block when low, medium or high probability of unsafe content 
 
 
 N/A 
 `HARM_BLOCK_THRESHOLD_UNSPECIFIED` 
 Threshold is unspecified, block using default threshold 
 
 

If the threshold is not set, the default block threshold is Block none (for
all newer stable GA models)
or Block some (in all other models) for all categories except the
 Civic integrity category.

The default block threshold for the Civic integrity category is Block none 
(for `gemini-2.0-flash`, and `gemini-2.0-flash-lite`) both for
Google AI Studio and the Gemini API, and Block most for all other models in
Google AI Studio only.

You can set these settings for each request you make to the generative service.
See the `HarmBlockThreshold` API
reference for details.

### Safety feedback

 `generateContent` 
returns a
 `GenerateContentResponse` which
includes safety feedback.

Prompt feedback is included in
 `promptFeedback` . If
`promptFeedback.blockReason` is set, then the content of the prompt was blocked.

Response candidate feedback is included in
 `Candidate.finishReason` and
 `Candidate.safetyRatings` . If response
content was blocked and the `finishReason` was `SAFETY`, you can inspect
`safetyRatings` for more details. The content that was blocked is not returned.

## Adjust safety settings

This section covers how to adjust the safety settings in both Google AI Studio
and in your code.

### Google AI Studio

You can adjust safety settings in Google AI Studio, but you cannot turn them
off.

Click Edit safety settings in the Run settings panel to open the Run
safety settings modal. In the modal, you can use the sliders to adjust the
content filtering level per safety category:

 

When you send a request (for example, by asking the model a question), a warning 
 No Content message appears if the request's content is blocked. To see more
details, hold the pointer over the
 No Content text and click warning 
 Safety .

### Gemini API SDKs

The following code snippet shows how to set safety settings in your
`GenerateContent` call. This sets the thresholds for the harassment
(`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`)
categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE`
blocks any content that has a low or higher probability of being harassment or
hate speech. To understand the threshold settings, see
 Safety filtering per request .

 
 

### Python

 

```
from google import genai
from google.genai import types

import PIL.Image

img = PIL.Image.open("cookies.jpg")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=['Do these look store-bought or homemade?', img],
    config=types.GenerateContentConfig(
      safety_settings=[
        types.SafetySetting(
            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        ),
      ]
    )
)

print(response.text)
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        SafetySettings: []*genai.SafetySetting{
            {
                Category:  "HARM_CATEGORY_HATE_SPEECH",
                Threshold: "BLOCK_LOW_AND_ABOVE",
            },
        },
    }

    response, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.0-flash",
        genai.Text("Some potentially unsafe prompt."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(response.Text())
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const safetySettings = [
  {
    category: "HARM_CATEGORY_HARASSMENT",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
  {
    category: "HARM_CATEGORY_HATE_SPEECH",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
];

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Some potentially unsafe prompt.",
    config: {
      safetySettings: safetySettings,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Dart (Flutter)

 

```
final safetySettings = [
  SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),
  SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),
];
final model = GenerativeModel(
  model: 'gemini-1.5-flash',
  apiKey: apiKey,
  safetySettings: safetySettings,
);
```

 
 

### Kotlin

 

```
val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)

val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)

val generativeModel = GenerativeModel(
    modelName = "gemini-1.5-flash",
    apiKey = BuildConfig.apiKey,
    safetySettings = listOf(harassmentSafety, hateSpeechSafety)
)
```

 
 

### Java

 

```
SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,
    BlockThreshold.LOW_AND_ABOVE);

SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,
    BlockThreshold.LOW_AND_ABOVE);

GenerativeModel gm = new GenerativeModel(
    "gemini-1.5-flash",
    BuildConfig.apiKey,
    null, // generation config is optional
    Arrays.asList(harassmentSafety, hateSpeechSafety)
);

GenerativeModelFutures model = GenerativeModelFutures.from(gm);
```

 
 

### REST

 

```
echo '{    "safetySettings": [        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}    ],    "contents": [{        "parts":[{            "text": "'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'"}]}]}' > request.json

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \        -H "x-goog-api-key: $GEMINI_API_KEY" \

    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json 2> /dev/null
```

 
 

## Next steps

- See the API reference to learn more about the full API.

- Review the safety guidance for a general look at safety
considerations when developing with LLMs.

- Learn more about assessing probability versus severity from the Jigsaw
team 

- Learn more about the products that contribute to safety solutions like the
 Perspective
API .
 * You can use these safety settings to create a toxicity
 classifier. See the classification
 example to
 get started.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-08 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-08 UTC."],[],[]]

---

### Safety guidance &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/safety-guidance

- 
 
 
 
 
 
 
 
 
 
 
 Safety guidance  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Safety guidance 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Generative artificial intelligence models are powerful tools, but they are not
without their limitations. Their versatility and applicability can sometimes
lead to unexpected outputs, such as outputs that are inaccurate, biased, or
offensive. Post-processing, and rigorous manual evaluation are essential to
limit the risk of harm from such outputs.

The models provided by the Gemini API can be used for a wide variety of
generative AI and natural language processing (NLP) applications. Use of these
functions is only available through the Gemini API or the Google AI Studio web
app. Your use of Gemini API is also subject to the Generative AI Prohibited Use
Policy and the
 Gemini API terms of service .

Part of what makes large language models (LLMs) so useful is that they're
creative tools that can address many different language tasks. Unfortunately,
this also means that large language models can generate output that you don't
expect, including text
that's offensive, insensitive, or factually incorrect. What's more, the
incredible versatility of these models is also what makes it difficult to
predict exactly what kinds of undesirable output they might produce. While the
Gemini API has been designed with Google's AI
principles in mind, the onus is on developers to
apply these models responsibly. To aid developers in creating safe, responsible
applications, the Gemini API has some built-in content filtering as well as
adjustable safety settings across 4 dimensions of harm. Refer to the
 safety settings guide to learn more.

This document is meant to introduce you to some safety risks that can arise when
using LLMs, and recommend emerging safety design and development
recommendations. (Note that laws and regulations may also impose restrictions,
but such considerations are beyond the scope of this guide.)

The following steps are recommended when building applications with LLMs:

- Understanding the safety risks of your application

- Considering adjustments to mitigate safety risks

- Performing safety testing appropriate to your use case

- Soliciting feedback from users and monitoring usage

The adjustment and testing phases should be iterative until you reach
performance appropriate for your application.

 

## Understand the safety risks of your application

In this context, safety is being defined as the ability of an LLM to avoid
causing harm to its users, for example, by generating toxic language or content
that promotes stereotypes. The models available through the Gemini API have been
designed with Google‚Äôs AI principles in mind
and your use of it is subject to the Generative AI Prohibited Use
Policy . The API
provides built-in safety filters to help address some common language model
problems such as toxic language and hate speech, and striving for inclusiveness
and avoidance of stereotypes. However, each application can pose a different set
of risks to its users. So as the application owner, you are responsible for
knowing your users and the potential harms your application may cause, and
ensuring that your application uses LLMs safely and responsibly.

As part of this assessment, you should consider the likelihood that harm could
occur and determine its seriousness and mitigation steps. For example, an
app that generates essays based on factual events would need to be more careful
about avoiding misinformation, as compared to an app that generates fictional
stories for entertainment. A good way to begin exploring potential safety risks
is to research your end users, and others who might be affected by your
application's results. This can take many forms including researching state of
the art studies in your app domain, observing how people are using similar apps,
or running a user study, survey, or conducting informal interviews with
potential users.

 
 
 

#### Advanced tips

 

 - 
 Speak with a diverse mix of prospective users within your target
 population about your application and its intended purpose so as
 to get a wider perspective on potential risks and to adjust diversity
 criteria as needed.
 

 - 
 The AI Risk Management Framework 
 released by the U.S. government's
 National Institute of Standards and Technology (NIST) provides more
 detailed guidance and additional learning resources for AI risk management.
 

 - 
 DeepMind's publication on the
 
 ethical and social risks of harm from language models
 
 describes in detail the ways that language model
 applications can cause harm.
 

 

 
 

## Consider adjustments to mitigate safety risks

Now that you have an understanding of the risks, you can decide how to mitigate
them. Determining which risks to prioritize and how much you should do to try to
prevent them is a critical decision, similar to triaging bugs in a software
project. Once you've determined priorities, you can start thinking about the
types of mitigations that would be most appropriate. Often simple changes can
make a difference and reduce risks.

For example, when designing an application consider:

- Tuning the model output to better reflect what is acceptable in your
application context. Tuning can make the output of the model more
predictable and consistent and therefore can help mitigate certain risks.

- Providing an input method that facilities safer outputs. The exact input
you give to an LLM can make a difference in the quality of the output.
Experimenting with input prompts to find what works most safely in your
use-case is well worth the effort, as you can then provide a UX that
facilitates it. For example, you could restrict users to choose only from a
drop-down list of input prompts, or offer pop-up suggestions with
descriptive
phrases which you've found perform safely in your application context.

- 

 Blocking unsafe inputs and filtering output before it is shown to the
user. In simple situations, blocklists can be used to identify and block
unsafe words or phrases in prompts or responses, or require human reviewers
to manually alter or block such content.

- 

 Using trained classifiers to label each prompt with potential harms or
adversarial signals. Different strategies can then be employed on how to
handle the request based on the type of harm detected. For example, If the
input is overtly adversarial or abusive in nature, it could be blocked and
instead output a pre-scripted response.
 
 
 

#### Advanced tip

 

 
 If signals determine the output to be harmful,
 the application can employ the following options:
 

 
 Provide an error message or pre-scripted output.
 

 - 
 Try the prompt again, in case an alternative safe output is
 generated, since sometimes the same prompt will elicit
 different outputs.
 

 

 
 

 
 

 
- 

 Putting safeguards in place against deliberate misuse such as assigning
each user a unique ID and imposing a limit on the volume of user queries
that can be submitted in a given period. Another safeguard is to try and
protect against possible prompt injection. Prompt injection, much like SQL
injection, is a way for malicious users to design an input prompt that
manipulates the output of the model, for example, by sending an input prompt
that instructs the model to ignore any previous examples. See the
 Generative AI Prohibited Use Policy 
for details about deliberate misuse.

- 

 Adjusting functionality to something that is inherently lower risk. 
Tasks that are narrower in scope (e.g., extracting keywords from passages of
text) or that have greater human oversight (e.g., generating short-form
content that will be reviewed by a human), often pose a lower risk. So for
instance, instead of creating an application to write an email reply from
scratch, you might instead limit it to expanding on an outline or suggesting
alternative phrasings.

## Perform safety testing appropriate to your use case

Testing is a key part of building robust and safe applications, but the extent,
scope and strategies for testing will vary. For example, a just-for-fun haiku
generator is likely to pose less severe risks than, say, an application designed
for use by law firms to summarize legal documents and help draft contracts. But
the haiku generator may be used by a wider variety of users which means the
potential for adversarial attempts or even unintended harmful inputs can be
greater. The implementation context also matters. For instance, an application
with outputs that are reviewed by human experts prior to any action being taken
might be deemed less likely to produce harmful outputs than the identical
application without such oversight.

It's not uncommon to go through several iterations of making changes and testing
before feeling confident that you're ready to launch, even for applications that
are relatively low risk. Two kinds of testing are particularly useful for AI
applications:

- 

 Safety benchmarking involves designing safety metrics that reflect the
ways your application could be unsafe in the context of how it is likely to
get used, then testing how well your application performs on the metrics
using evaluation datasets. It's good practice to think about the minimum
acceptable levels of safety metrics before testing so that 1) you can
evaluate the test results against those expectations and 2) you can gather
the evaluation dataset based on the tests that evaluate the metrics you care
about most.

 
 

#### Advanced tips

 
 Beware of over-relying on ‚Äúoff the shelf‚Äù approaches as it's likely
 you'll need to build your own testing datasets using human raters to
 fully suit your application's context.
 

 - 
 If you have more than one metric you'll need to decide how you'll
 trade off if a change leads to improvements for one metric to the
 detriment of another. Like with other performance engineering, you
 may want to focus on worst-case performance across your evaluation
 set rather than average performance.
 

 
 
- 

 Adversarial testing involves proactively trying to break your
application. The goal is to identify points of weakness so that you can take
steps to remedy them as appropriate. Adversarial testing can take
significant time/effort from evaluators with expertise in your application ‚Äî
but the more you do, the greater your chance of spotting problems,
especially those occurring rarely or only after repeated runs of the
application.

 Adversarial testing is a method for systematically evaluating an ML
model with the intent of learning how it behaves when provided with
malicious or inadvertently harmful input:

 An input may be malicious when the input is clearly designed to
produce an unsafe or harmful output-- for example, asking a text
generation model to generate a hateful rant about a particular
religion.

- An input is inadvertently harmful when the input itself may be
innocuous, but produces harmful output -- for example, asking a text
generation model to describe a person of a particular ethnicity and
receiving a racist output.

 
- What distinguishes an adversarial test from a standard evaluation is the
composition of the data used for testing. For adversarial tests, select 
test data that is most likely to elicit problematic output from
the model. This means probing the model's behavior for all the types of
harms that are possible, including rare or unusual examples and
edge-cases that are relevant to safety policies. It should also include
diversity in the different dimensions of a sentence such as structure,
meaning and length. You can refer to the Google's Responsible AI
practices in
fairness 
for more details on what to consider when building a test dataset.
 
 

#### Advanced tips

 
 Use
 automated testing 
 instead of the traditional method of enlisting people in 'red teams'
 to try and break your application. In automated testing, the
 'red team' is another language model that finds input text that
 elicit harmful outputs from the model being tested.
 

 
 

 

## Monitor for problems

No matter how much you test and mitigate, you can never guarantee perfection, so
plan upfront how you'll spot and deal with problems that arise. Common
approaches include setting up a monitored channel for users to share feedback
(e.g., thumbs up/down rating) and running a user study to proactively solicit
feedback from a diverse mix of users ‚Äî especially valuable if usage patterns are
different to expectations.

 
 
 

#### Advanced tips

 

 - 
 When users give feedback to AI products, it can greatly improve the AI
 performance and the user experience over time by, for example,
 helping you choose better examples for prompt tuning. The
 Feedback and Control chapter 
 in Google's People and AI guidebook 
 highlights key considerations to take into account when designing
 feedback mechanisms.
 

 

 
 

## Next steps

- Refer to the
 safety settings guide to learn about the adjustable
safety settings available through the Gemini API.

- See the intro to prompting to get
started writing your first prompts.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Context caching &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/caching#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Context caching  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Context caching 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 
 
 
 

In a typical AI workflow, you might pass the same input tokens over and over to
a model. The Gemini API offers two different caching mechanisms:

- Implicit caching (automatically enabled on Gemini 2.5 models, no cost saving guarantee)

- Explicit caching (can be manually enabled on most models, cost saving guarantee)

Explicit caching is useful in cases where you want to guarantee cost savings,
but with some added developer work.

## Implicit caching

Implicit caching is enabled by default for all Gemini 2.5 models. We automatically
pass on cost savings if your request hits caches. There is nothing you need to do
in order to enable this. It is effective as of May 8th, 2025. The minimum input
token count for context caching is listed in the following table for each model:

 
 
 
 Model 
 Min token limit 
 
 

 
 
 3 Pro Preview 
 2048 
 
 
 2.5 Pro 
 4096 
 
 
 2.5 Flash 
 1024 
 
 
 

To increase the chance of an implicit cache hit:

- Try putting large and common contents at the beginning of your prompt

- Try to send requests with similar prefix in a short amount of time

You can see the number of tokens which were cache hits in the response object's
`usage_metadata` field.

## Explicit caching

Using the Gemini API explicit caching feature, you can pass some content
to the model once, cache the input tokens, and then refer to the cached tokens
for subsequent requests. At certain volumes, using cached tokens is lower cost
than passing in the same corpus of tokens repeatedly.

When you cache a set of tokens, you can choose how long you want the cache to
exist before the tokens are automatically deleted. This caching duration is
called the time to live (TTL). If not set, the TTL defaults to 1 hour. The
cost for caching depends on the input token size and how long you want the
tokens to persist.

This section assumes that you've installed a Gemini SDK (or have curl installed)
and that you've configured an API key, as shown in the
 quickstart .

### Generate content using a cache

The following example shows how to generate content using a cached system
instruction and video file.

 
 

### Videos

 

```
import os
import pathlib
import requests
import time

from google import genai
from google.genai import types

client = genai.Client()

# Download video file
url = 'https://storage.googleapis.com/generativeai-downloads/data/SherlockJr._10min.mp4'
path_to_video_file = pathlib.Path('SherlockJr._10min.mp4')
if not path_to_video_file.exists():
  with path_to_video_file.open('wb') as wf:
    response = requests.get(url, stream=True)
    for chunk in response.iter_content(chunk_size=32768):
      wf.write(chunk)

# Upload the video using the Files API
video_file = client.files.upload(file=path_to_video_file)

# Wait for the file to finish processing
while video_file.state.name == 'PROCESSING':
  print('Waiting for video to be processed.')
  time.sleep(2)
  video_file = client.files.get(name=video_file.name)

print(f'Video processing complete: {video_file.uri}')

# You must use an explicit version suffix: "-flash-001", not just "-flash".
model='models/gemini-2.0-flash-001'

# Create a cache with a 5 minute TTL
cache = client.caches.create(
    model=model,
    config=types.CreateCachedContentConfig(
      display_name='sherlock jr movie', # used to identify the cache
      system_instruction=(
          'You are an expert video analyzer, and your job is to answer '
          'the user\'s query based on the video file you have access to.'
      ),
      contents=[video_file],
      ttl="300s",
  )
)

# Construct a GenerativeModel which uses the created cache.
response = client.models.generate_content(
  model = model,
  contents= (
    'Introduce different characters in the movie by describing '
    'their personality, looks, and names. Also list the timestamps '
    'they were introduced for the first time.'),
  config=types.GenerateContentConfig(cached_content=cache.name)
)

print(response.usage_metadata)

# The output should look something like this:
#
# prompt_token_count: 696219
# cached_content_token_count: 696190
# candidates_token_count: 214
# total_token_count: 696433

print(response.text)
```

 
 

### PDFs

 

```
from google import genai
from google.genai import types
import io
import httpx

client = genai.Client()

long_context_pdf_path = "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"

# Retrieve and upload the PDF using the File API
doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)

document = client.files.upload(
  file=doc_io,
  config=dict(mime_type='application/pdf')
)

model_name = "gemini-2.0-flash-001"
system_instruction = "You are an expert analyzing transcripts."

# Create a cached content object
cache = client.caches.create(
    model=model_name,
    config=types.CreateCachedContentConfig(
      system_instruction=system_instruction,
      contents=[document],
    )
)

# Display the cache details
print(f'{cache=}')

# Generate content using the cached prompt and document
response = client.models.generate_content(
  model=model_name,
  contents="Please summarize this transcript",
  config=types.GenerateContentConfig(
    cached_content=cache.name
  ))

# (Optional) Print usage metadata for insights into the API call
print(f'{response.usage_metadata=}')

# Print the generated text
print('\n\n', response.text)
```

 
 

### List caches

It's not possible to retrieve or view cached content, but you can retrieve
cache metadata (`name`, `model`, `display_name`, `usage_metadata`,
`create_time`, `update_time`, and `expire_time`).

To list metadata for all uploaded caches, use `CachedContent.list()`:

 

```
for cache in client.caches.list():
  print(cache)
```

 

To fetch the metadata for one cache object, if you know its name, use `get`:

 

```
client.caches.get(name=name)
```

 

### Update a cache

You can set a new `ttl` or `expire_time` for a cache. Changing anything else
about the cache isn't supported.

The following example shows how to update the `ttl` of a cache using
`client.caches.update()`.

 

```
from google import genai
from google.genai import types

client.caches.update(
  name = cache.name,
  config  = types.UpdateCachedContentConfig(
      ttl='300s'
  )
)
```

 

To set the expiry time, it will accepts either a `datetime` object
or an ISO-formatted datetime string (`dt.isoformat()`, like
`2025-01-27T16:02:36.473528+00:00`). Your time must include a time zone
(`datetime.utcnow()` doesn't attach a time zone,
`datetime.now(datetime.timezone.utc)` does attach a time zone).

 

```
from google import genai
from google.genai import types
import datetime

# You must use a time zone-aware time.
in10min = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=10)

client.caches.update(
  name = cache.name,
  config  = types.UpdateCachedContentConfig(
      expire_time=in10min
  )
)
```

 

### Delete a cache

The caching service provides a delete operation for manually removing content
from the cache. The following example shows how to delete a cache:

 

```
client.caches.delete(cache.name)
```

 

### Explicit caching using the OpenAI library

If you're using an OpenAI library , you can enable
explicit caching using the `cached_content` property on
 `extra_body` .

## When to use explicit caching

Context caching is particularly well suited to scenarios where a substantial
initial context is referenced repeatedly by shorter requests. Consider using
context caching for use cases such as:

- Chatbots with extensive system instructions 

- Repetitive analysis of lengthy video files

- Recurring queries against large document sets

- Frequent code repository analysis or bug fixing

### How explicit caching reduces costs

Context caching is a paid feature designed to reduce overall operational costs.
Billing is based on the following factors:

- Cache token count: The number of input tokens cached, billed at a
reduced rate when included in subsequent prompts.

- Storage duration: The amount of time cached tokens are stored (TTL),
billed based on the TTL duration of cached token count. There are no minimum
or maximum bounds on the TTL.

- Other factors: Other charges apply, such as for non-cached input tokens
and output tokens.

For up-to-date pricing details, refer to the Gemini API pricing
page . To learn how to count tokens, see the Token
guide .

### Additional considerations

Keep the following considerations in mind when using context caching:

- The minimum input token count for context caching is 1,024 for 2.5 Flash,
4,096 for 2.5 Pro and 2,048 for 3 Pro Preview. The maximum is the same as the
maximum for the given model. (For more on counting tokens, see the Token
guide ).

- The model doesn't make any distinction between cached tokens and regular
input tokens. Cached content is a prefix to the prompt.

- There are no special rate or usage limits on context caching; the standard
rate limits for `GenerateContent` apply, and token limits include cached
tokens.

- The number of cached tokens is returned in the `usage_metadata` from the
create, get, and list operations of the cache service, and also in
`GenerateContent` when using the cache.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### ReAct agent from scratch with Gemini 2.5 and LangGraph &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/langgraph-example

- 
 
 
 
 
 
 
 
 
 
 
 ReAct agent from scratch with Gemini 2.5 and LangGraph  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 ReAct agent from scratch with Gemini 2.5 and LangGraph 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LangGraph is a framework for building stateful LLM applications, making it a good choice for constructing ReAct (Reasoning and Acting) Agents.

ReAct agents combine LLM reasoning with action execution. They iteratively think, use tools, and act on observations to achieve user goals, dynamically adapting their approach. Introduced in "ReAct: Synergizing Reasoning and Acting in Language Models" (2023), this pattern tries to mirror human-like, flexible problem-solving over rigid workflows.

While LangGraph offers a prebuilt ReAct agent ( `create_react_agent` ), it shines when you need more control and customization for your ReAct implementations.

LangGraph models agents as graphs using three key components:

- `State`: Shared data structure (typically `TypedDict` or `Pydantic BaseModel`) representing the application's current snapshot.

- `Nodes`: Encodes logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State, such as LLM calls or tool calls.

- `Edges`: Define the next `Node` to execute based on the current `State`, allowing for conditional logic and fixed transitions.

If you don't have an API Key yet, you can get one for free at the Google AI Studio .

 

```
pip install langgraph langchain-google-genai geopy requests
```

 

Set your API key in the environment variable `GEMINI_API_KEY`.

 

```
import os

# Read your API key from the environment variable or set it manually
api_key = os.getenv("GEMINI_API_KEY")
```

 

To better understand how to implement a ReAct agent using LangGraph, let's walk through a practical example. You will create a simple agent whose goal is to use a tool to find the current weather for a specified location.

For this weather agent, its `State` will need to maintain the ongoing conversation history (as a list of messages) and a counter for the number of steps taken to further illustrate state management.

LangGraph provides a convenient helper, `add_messages`, for updating message lists in the state. It functions as a reducer , meaning it takes the current list and new messages, then returns a combined list. It smartly handles updates by message ID and defaults to an "append-only" behavior for new, unique messages.

 

```
from typing import Annotated,Sequence, TypedDict

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages # helper function to add messages to the state


class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    number_of_steps: int
```

 

Next, you define your weather tool.

 

```
from langchain_core.tools import tool
from geopy.geocoders import Nominatim
from pydantic import BaseModel, Field
import requests

geolocator = Nominatim(user_agent="weather-app")

class SearchInput(BaseModel):
    location:str = Field(description="The city and state, e.g., San Francisco")
    date:str = Field(description="the forecasting date for when to get the weather format (yyyy-mm-dd)")

@tool("get_weather_forecast", args_schema=SearchInput, return_direct=True)
def get_weather_forecast(location: str, date: str):
    """Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour."""
    location = geolocator.geocode(location)
    if location:
        try:
            response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}")
            data = response.json()
            return {time: temp for time, temp in zip(data["hourly"]["time"], data["hourly"]["temperature_2m"])}
        except Exception as e:
            return {"error": str(e)}
    else:
        return {"error": "Location not found"}

tools = [get_weather_forecast]
```

 

Next, you initialize your model and bind the tools to the model.

 

```
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI

# Create LLM class
llm = ChatGoogleGenerativeAI(
    model= "gemini-2.5-pro",
    temperature=1.0,
    max_retries=2,
    google_api_key=api_key,
)

# Bind tools to the model
model = llm.bind_tools([get_weather_forecast])

# Test the model with tools
res=model.invoke(f"What is the weather in Berlin on {datetime.today()}?")

print(res)
```

 

The last step before you can run your agent is to define your nodes and edges. In this example, you have two nodes and one edge.
- `call_tool` node that executes your tool method. LangGraph has a prebuilt node for this called ToolNode .
- `call_model` node that uses the `model_with_tools` to call the model.
- `should_continue` edge that decides whether to call the tool or the model.

The number of nodes and edges is not fixed. You can add as many nodes and edges as you want to your graph. For example, you could add a node for adding structured output or a self-verification/reflection node to check the model output before calling the tool or the model.

 

```
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig

tools_by_name = {tool.name: tool for tool in tools}

# Define our tool node
def call_tool(state: AgentState):
    outputs = []
    # Iterate over the tool calls in the last message
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool by name
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=tool_result,
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    # Invoke the model with the system prompt and the messages
    response = model.invoke(state["messages"], config)
    # We return a list, because this will get added to the existing messages state using the add_messages reducer
    return {"messages": [response]}


# Define the conditional edge that determines whether to continue or not
def should_continue(state: AgentState):
    messages = state["messages"]
    # If the last message is not a tool call, then we finish
    if not messages[-1].tool_calls:
        return "end"
    # default to continue
    return "continue"
```

 

Now you have all the components to build your agent. Let's put them together.

 

```
from langgraph.graph import StateGraph, END

# Define a new graph with our state
workflow = StateGraph(AgentState)

# 1. Add our nodes 
workflow.add_node("llm", call_model)
workflow.add_node("tools",  call_tool)
# 2. Set the entrypoint as `agent`, this is the first node called
workflow.set_entry_point("llm")
# 3. Add a conditional edge after the `llm` node is called.
workflow.add_conditional_edges(
    # Edge is used after the `llm` node is called.
    "llm",
    # The function that will determine which node is called next.
    should_continue,
    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.
    # END is a special node marking that the graph is finish.
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)
# 4. Add a normal edge after `tools` is called, `llm` node is called next.
workflow.add_edge("tools", "llm")

# Now we can compile and visualize our graph
graph = workflow.compile()
```

 

You can visualize your graph using the `draw_mermaid_png` method.

 

```
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

 

 

Now let's run the agent.

 

```
from datetime import datetime
# Create our initial message dictionary
inputs = {"messages": [("user", f"What is the weather in Berlin on {datetime.today()}?")]}

# call our graph with streaming to see the steps
for state in graph.stream(inputs, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

 

You can now continue with your conversation and for example ask for the weather in another city or let it compare it.

 

```
state["messages"].append(("user", "Would it be in Munich warmer?"))

for state in graph.stream(state, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Media resolution &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/media-resolution#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Media resolution  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Media resolution 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The `media_resolution` parameter controls how the Gemini API processes media inputs like images, videos, and PDF documents by determining the maximum number of tokens allocated for media inputs, allowing you to balance response quality against latency and cost. For different settings, default values and how they correspond to tokens, see the Token counts section.

You can configure media resolution in two ways:

- 

 Per part (Gemini 3 only)

- 

 Globally for an entire `generateContent` request (all multimodal models)

## Per-part media resolution (Gemini 3 only)

Gemini 3 allows you to set media resolution for individual media objects within your request, offering fine-grained optimisation of token usage. You can mix resolution levels in a single request. For example, using high resolution for a complex diagram and low resolution for a simple contextual image. This setting overrides any global configuration for a specific part. For default settings, see Token counts section. 

 
 

### Python

 

```
from google import genai
from google.genai import types

# The media_resolution parameter for parts is currently only available in the v1alpha API version. (experimental)
client = genai.Client(
  http_options={
      'api_version': 'v1alpha',
  }
)

# Replace with your image data
with open('path/to/image1.jpg', 'rb') as f:
    image_bytes_1 = f.read()

# Create parts with different resolutions
image_part_high = types.Part.from_bytes(
    data=image_bytes_1,
    mime_type='image/jpeg',
    media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH
)

model_name = 'gemini-3-pro-preview'

response = client.models.generate_content(
    model=model_name,
    contents=["Describe these images:", image_part_high]
)
print(response.text)
```

 
 

### Javascript

 

```
// Example: Setting per-part media resolution in JavaScript
import { GoogleGenAI, MediaResolution, Part } from '@google/genai';
import * as fs from 'fs';
import { Buffer } from 'buffer'; // Node.js

const ai = new GoogleGenAI({ httpOptions: { apiVersion: 'v1alpha' } });

// Helper function to convert local file to a Part object
function fileToGenerativePart(path, mimeType, mediaResolution) {
    return {
        inlineData: { data: Buffer.from(fs.readFileSync(path)).toString('base64'), mimeType },
        mediaResolution: { 'level': mediaResolution }
    };
}

async function run() {
    // Create parts with different resolutions
    const imagePartHigh = fileToGenerativePart('img.png', 'image/png', Part.MediaResolutionLevel.MEDIA_RESOLUTION_HIGH);
    const model_name = 'gemini-3-pro-preview';
    const response = await ai.models.generateContent({
        model: model_name,
        contents: ['Describe these images:', imagePartHigh]
        // Global config can still be set, but per-part settings will override
        // config: {
        //   mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM
        // }
    });
    console.log(response.text);
}
run();
```

 
 

### REST

 

```
# Replace with paths to your images
IMAGE_PATH="path/to/image.jpg"

# Base64 encode the images
BASE64_IMAGE1=$(base64 -w 0 "$IMAGE_PATH")

MODEL_ID="gemini-3-pro-preview"

echo '{
    "contents": [{
      "parts": [
        {"text": "Describe these images:"},
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "'"$BASE64_IMAGE1"'",
          },
          "media_resolution": {"level": "MEDIA_RESOLUTION_HIGH"}
        }
      ]
    }]
  }' > request.json

curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1alpha/models/${MODEL_ID}:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d @request.json
```

 
 

## Global media resolution

You can set a default resolution for all media parts in a request using the
`GenerationConfig`. This is supported by all multimodal models. If a request
includes both global and per-part settings , the per-part setting takes precedence for that specific item.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Prepare standard image part
with open('image.jpg', 'rb') as f:
    image_bytes = f.read()
image_part = types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg')

# Set global configuration
config = types.GenerateContentConfig(
    media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH
)

response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=["Describe this image:", image_part],
    config=config
)
print(response.text)
```

 
 

### Javascript

 

```
import { GoogleGenAI, MediaResolution } from '@google/genai';
import * as fs from 'fs';

const ai = new GoogleGenAI({ });

async function run() {
   // ... (Image loading logic) ...

   const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: ["Describe this image:", imagePart],
      config: {
         mediaResolution: MediaResolution.MEDIA_RESOLUTION_HIGH
      }
   });
   console.log(response.text);
}
run();
```

 
 

### REST

 

```
# ... (Base64 encoding logic) ...

curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [...],
    "generation_config": {
      "media_resolution": "MEDIA_RESOLUTION_HIGH"
    }
  }'
```

 
 

## Available resolution values

The Gemini API defines the following levels for media resolution:

- `MEDIA_RESOLUTION_UNSPECIFIED`: The default setting. The token count for
this level varies significantly between Gemini 3 and earlier Gemini models.

- `MEDIA_RESOLUTION_LOW`: Lower token count, resulting in faster processing
and lower cost, but with less detail.

- `MEDIA_RESOLUTION_MEDIUM`: A balance between detail, cost, and latency.

- `MEDIA_RESOLUTION_HIGH`: Higher token count, providing more detail for the
model to work with, at the expense of increased latency and cost.

- ( Coming soon ) `MEDIA_RESOLUTION_ULTRA_HIGH`: Highest token count
required for specific use cases such as computer use.

The exact number of tokens generated for each of these
levels depends on both the media type (Image, Video, PDF) and the model
version .

## Token counts

The tables below summarize the approximate token counts for each
`media_resolution` value and media type per model family.

 Gemini 3 Models 

 
 
 MediaResolution 
 
 Image 
 
 Video 
 
 PDF 
 
 
 
 `MEDIA_RESOLUTION_UNSPECIFIED` (Default)
 
 1120
 
 70
 
 560
 
 
 
 `MEDIA_RESOLUTION_LOW`
 
 280
 
 70
 
 280 + Native Text
 
 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 560
 
 70
 
 560 + Native Text
 
 
 
 `MEDIA_RESOLUTION_HIGH`
 
 1120
 
 280
 
 1120 + Native Text
 
 
 

 Gemini 2.5 models 

 
 
 MediaResolution 
 
 Image 
 
 Video 
 
 PDF (Scanned) 
 
 PDF (Native) 
 
 
 
 `MEDIA_RESOLUTION_UNSPECIFIED` (Default)
 
 256 + Pan & Scan (~2048)
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 
 `MEDIA_RESOLUTION_LOW`
 
 64
 
 64
 
 64 + OCR
 
 64 + Native Text
 
 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 256
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 
 `MEDIA_RESOLUTION_HIGH`
 
 256 + Pan & Scan
 
 256
 
 256 + OCR
 
 256 + Native Text
 
 
 

## Choosing the right resolution

- Default (`UNSPECIFIED`): Start with the default. It's tuned for a good
balance of quality, latency, and cost for most common use cases.

- `LOW`: Use for scenarios where cost and latency are paramount, and
fine-grained detail is less critical.

- `MEDIUM` / `HIGH`: Increase the resolution when the task requires
understanding intricate details within the media. This is often needed for
complex visual analysis, chart reading, or dense document comprehension.

- Per-part control (Gemini 3): Leverage this to optimize token usage. For
example, in a prompt with multiple images, use `HIGH` for a complex diagram
and `LOW` or `MEDIUM` for simpler contextual images.

 Recommended settings 

The following lists the recommended media resolution settings for each
supported media type.

 
 
 Media Type 
 
 Recommended Setting 
 
 Max Tokens 
 
 Usage Guidance 
 
 
 
 Images 
 
 `MEDIA_RESOLUTION_HIGH`
 
 1120
 
 Recommended for most image analysis tasks to ensure maximum quality.
 
 
 
 PDFs 
 
 `MEDIA_RESOLUTION_MEDIUM`
 
 560
 
 Optimal for document understanding; quality typically saturates at `medium`. Increasing to `high` rarely improves OCR results for standard documents.
 
 
 
 Video (General)
 
 `MEDIA_RESOLUTION_LOW` (or `MEDIA_RESOLUTION_MEDIUM`)
 
 70 (per frame)
 
 Note: For video, `low` and `medium` settings are treated identically (70 tokens) to optimize context usage. This is sufficient for most action recognition and description tasks.
 
 
 
 Video (Text-heavy)
 
 `MEDIA_RESOLUTION_HIGH`
 
 280 (per frame)
 
 Required only when the use case involves reading dense text (OCR) or small details within video frames.
 
 
 

Always test and evaluate the impact of different resolution settings on your
specific application to find the best trade-off between quality, latency, and
cost.

## Version compatibility summary

- The `MediaResolution` enum is available for all models supporting media
input.

- The token counts associated with each enum level differ between
Gemini 3 models and earlier Gemini versions.

- Setting `media_resolution` on individual `Part` objects is exclusive to
Gemini 3 models .

## Next steps

- Learn more about the multimodal capabilities of Gemini API in the
 image understanding , video understanding and
 document understanding guides.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Customer Support Analysis with Gemini 2.5 Pro and CrewAI &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/crewai-example

- 
 
 
 
 
 
 
 
 
 
 
 Customer Support Analysis with Gemini 2.5 Pro and CrewAI  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÔøΩÔøΩÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Customer Support Analysis with Gemini 2.5 Pro and CrewAI 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 CrewAI is a framework for orchestrating
autonomous AI agents that collaborate to achieve complex goals. It lets you
define agents by specifying roles, goals, and backstories, and then define tasks
for them.

This example demonstrates how to build a multi-agent system for analyzing
customer support data to identify issues and propose process improvements using
Gemini 2.5 Pro, generating a report intended to be read by a Chief Operating
Officer (COO).

The guide will show you how to create a "crew" of AI agents that can do the
following tasks:

- Fetch and analyze customer support data (simulated in this example).

- Identify recurring problems and process bottlenecks.

- Suggest actionable improvements.

- Compile the findings into a concise report suitable for a COO.

You need a Gemini API key. If you don't already have one, you can get one in
Google AI Studio .

 

```
pip install "crewai[tools]"
```

 

Set your Gemini API key as an environment variable named `GEMINI_API_KEY`, then
configure CrewAI to use the Gemini 2.5 Pro model.

 

```
import os
from crewai import LLM

# Read your API key from the environment variable
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Use Gemini 2.5 Pro Experimental model
gemini_llm = LLM(
    model='gemini/gemini-2.5-pro',
    api_key=gemini_api_key,
    temperature=0.0  # Lower temperature for more consistent results.
)
```

 

## Define components

CrewAI applications are built using Tools , Agents , Tasks , and the
 Crew itself. Each of these is explained in the following sections.

### Tools

Tools are capabilities that agents can use to interact with the outside world or
perform specific actions. Here, you define a placeholder tool to simulate
fetching customer support data. In a real application, you would connect to a
database, API or file system. For more information on tools, see the CrewAI
tools guide .

 

```
from crewai.tools import BaseTool

# Placeholder tool for fetching customer support data
class CustomerSupportDataTool(BaseTool):
    name: str = "Customer Support Data Fetcher"
    description: str = (
      "Fetches recent customer support interactions, tickets, and feedback. "
      "Returns a summary string.")

    def _run(self, argument: str) -> str:
        # In a real scenario, this would query a database or API.
        # For this example, return simulated data.
        print(f"--- Fetching data for query: {argument} ---")
        return (
            """Recent Support Data Summary:
- 50 tickets related to 'login issues'. High resolution time (avg 48h).
- 30 tickets about 'billing discrepancies'. Mostly resolved within 12h.
- 20 tickets on 'feature requests'. Often closed without resolution.
- Frequent feedback mentions 'confusing user interface' for password reset.
- High volume of calls related to 'account verification process'.
- Sentiment analysis shows growing frustration with 'login issues' resolution time.
- Support agent notes indicate difficulty reproducing 'login issues'."""
        )

support_data_tool = CustomerSupportDataTool()
```

 

### Agents

Agents are the individual AI workers in your crew. Each agent has a specific
`role`, `goal`, `backstory`, assigned `llm`, and optional `tools`. For more
information on agents, see the CrewAI agents
guide .

 

```
from crewai import Agent

# Agent 1: Data analyst
data_analyst = Agent(
    role='Customer Support Data Analyst',
    goal='Analyze customer support data to identify trends, recurring issues, and key pain points.',
    backstory=(
        """You are an expert data analyst specializing in customer support operations.
        Your strength lies in identifying patterns and quantifying problems from raw support data."""
    ),
    verbose=True,
    allow_delegation=False,  # This agent focuses on its specific task
    tools=[support_data_tool],  # Assign the data fetching tool
    llm=gemini_llm  # Use the configured Gemini LLM
)

# Agent 2: Process optimizer
process_optimizer = Agent(
    role='Process Optimization Specialist',
    goal='Identify bottlenecks and inefficiencies in current support processes based on the data analysis. Propose actionable improvements.',
    backstory=(
        """You are a specialist in optimizing business processes, particularly in customer support.
        You excel at pinpointing root causes of delays and inefficiencies and suggesting concrete solutions."""
    ),
    verbose=True,
    allow_delegation=False,
    # No tools needed, this agent relies on the context provided by data_analyst.
    llm=gemini_llm
)

# Agent 3: Report writer
report_writer = Agent(
    role='Executive Report Writer',
    goal='Compile the analysis and improvement suggestions into a concise, clear, and actionable report for the COO.',
    backstory=(
        """You are a skilled writer adept at creating executive summaries and reports.
        You focus on clarity, conciseness, and highlighting the most critical information and recommendations for senior leadership."""
    ),
    verbose=True,
    allow_delegation=False,
    llm=gemini_llm
)
```

 

### Tasks

Tasks define the specific assignments for the agents. Each task has a
`description`, `expected_output`, and is assigned to an `agent`. Tasks are run
sequentially by default and include the context of the previous task. For more
information on tasks, see the CrewAI tasks
guide .

 

```
from crewai import Task

# Task 1: Analyze data
analysis_task = Task(
    description=(
        """Fetch and analyze the latest customer support interaction data (tickets, feedback, call logs)
        focusing on the last quarter. Identify the top 3-5 recurring issues, quantify their frequency
        and impact (e.g., resolution time, customer sentiment). Use the Customer Support Data Fetcher tool."""
    ),
    expected_output=(
        """A summary report detailing the key findings from the customer support data analysis, including:
- Top 3-5 recurring issues with frequency.
- Average resolution times for these issues.
- Key customer pain points mentioned in feedback.
- Any notable trends in sentiment or support agent observations."""
    ),
    agent=data_analyst  # Assign task to the data_analyst agent
)

# Task 2: Identify bottlenecks and suggest improvements
optimization_task = Task(
    description=(
        """Based on the data analysis report provided by the Data Analyst, identify the primary bottlenecks
        in the support processes contributing to the identified issues (especially the top recurring ones).
        Propose 2-3 concrete, actionable process improvements to address these bottlenecks.
        Consider potential impact and ease of implementation."""
    ),
    expected_output=(
        """A concise list identifying the main process bottlenecks (e.g., lack of documentation for agents,
        complex escalation path, UI issues) linked to the key problems.
A list of 2-3 specific, actionable recommendations for process improvement
(e.g., update agent knowledge base, simplify password reset UI, implement proactive monitoring)."""
    ),
    agent=process_optimizer  # Assign task to the process_optimizer agent
    # This task implicitly uses the output of analysis_task as context
)

# Task 3: Compile COO report
report_task = Task(
    description=(
        """Compile the findings from the Data Analyst and the recommendations from the Process Optimization Specialist
        into a single, concise executive report for the COO. The report should clearly state:
1. The most critical customer support issues identified (with brief data points).
2. The key process bottlenecks causing these issues.
3. The recommended process improvements.
Ensure the report is easy to understand, focuses on actionable insights, and is formatted professionally."""
    ),
    expected_output=(
        """A well-structured executive report (max 1 page) summarizing the critical support issues,
        underlying process bottlenecks, and clear, actionable recommendations for the COO.
        Use clear headings and bullet points."""
    ),
    agent=report_writer  # Assign task to the report_writer agent
)
```

 

### Crew

The `Crew` brings the agents and tasks together, defining the workflow process
(such as "sequential").

 

```
from crewai import Crew, Process

# Define the crew with agents, tasks, and process
support_analysis_crew = Crew(
    agents=[data_analyst, process_optimizer, report_writer],
    tasks=[analysis_task, optimization_task, report_task],
    process=Process.sequential,  # Tasks will run sequentially in the order defined
    verbose=True
)
```

 

## Run the Crew

Finally, kick off the crew execution with any necessary inputs.

 

```
# Start the crew's work
print("--- Starting Customer Support Analysis Crew ---")
# The 'inputs' dictionary provides initial context if needed by the first task.
# In this case, the tool simulates data fetching regardless of the input.
result = support_analysis_crew.kickoff(inputs={'data_query': 'last quarter support data'})

print("--- Crew Execution Finished ---")
print("--- Final Report for COO ---")
print(result)
```

 

The script will now execute. The `Data Analyst` will use the tool, the 

```
Process
Optimizer
```

 will analyze the findings, and the `Report Writer` will compile the
final report, which is then printed to the console. The `verbose=True` setting
will show the detailed thought process and actions of each agent.

To learn more about CrewAI, check out the CrewAI
introduction .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Understand and count tokens &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/tokens#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Understand and count tokens  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Understand and count tokens 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Gemini and other generative AI models process input and output at a granularity
called a token .

 For Gemini models, a token is equivalent to about 4 characters.
100 tokens is equal to about 60-80 English words. 

## About tokens

Tokens can be single characters like `z` or whole words like `cat`. Long words
are broken up into several tokens. The set of all tokens used by the model is
called the vocabulary, and the process of splitting text into tokens is called
 tokenization .

When billing is enabled, the cost of a call to the Gemini API is
determined in part by the number of input and output tokens, so knowing how to
count tokens can be helpful.

 
 
 
 
 

## Try out counting tokens in a Colab

You can try out counting tokens by using a Colab.

 
 
 View on ai.google.dev 
 
 
 
 Try a Colab notebook 
 
 
 View notebook on GitHub 
 
 

## Context windows

The models available through the Gemini API have context windows that are
measured in tokens. The context window defines how much input you can provide
and how much output the model can generate. You can determine the size of the
context window by calling the getModels endpoint or
by looking in the models documentation .

In the following example, you can see that the `gemini-1.5-flash` model has an
input limit of about 1,000,000 tokens and an output limit of about 8,000 tokens,
which means a context window is 1,000,000 tokens.

 

```
from google import genai

client = genai.Client()
model_info = client.models.get(model="gemini-2.0-flash")
print(f"{model_info.input_token_limit=}")
print(f"{model_info.output_token_limit=}")
# ( e.g., input_token_limit=30720, output_token_limit=2048 )count_tokens.py
```

 

## Count tokens

All input to and output from the Gemini API is tokenized, including text, image
files, and other non-text modalities.

You can count tokens in the following ways:

### Count text tokens

 

```
from google import genai

client = genai.Client()
prompt = "The quick brown fox jumps over the lazy dog."

# Count tokens using the new client method.
total_tokens = client.models.count_tokens(
    model="gemini-2.0-flash", contents=prompt
)
print("total_tokens: ", total_tokens)
# ( e.g., total_tokens: 10 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=prompt
)

# The usage_metadata provides detailed token counts.
print(response.usage_metadata)
# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )count_tokens.py
```

 

### Count multi-turn (chat) tokens

 

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.0-flash",
    history=[
        types.Content(
            role="user", parts=[types.Part(text="Hi my name is Bob")]
        ),
        types.Content(role="model", parts=[types.Part(text="Hi Bob!")]),
    ],
)
# Count tokens for the chat history.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=chat.get_history()
    )
)
# ( e.g., total_tokens: 10 )

response = chat.send_message(
    message="In one sentence, explain how a computer works to a young child."
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 25, candidates_token_count: 21, total_token_count: 46 )

# You can count tokens for the combined history and a new message.
extra = types.UserContent(
    parts=[
        types.Part(
            text="What is the meaning of life?",
        )
    ]
)
history = chat.get_history()
history.append(extra)
print(client.models.count_tokens(model="gemini-2.0-flash", contents=history))
# ( e.g., total_tokens: 56 )count_tokens.py
```

 

### Count multimodal tokens

All input to the Gemini API is tokenized, including text, image files, and other
non-text modalities. Note the following high-level key points about tokenization
of multimodal input during processing by the Gemini API:

- 

With Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as
258 tokens. Images larger in one or both dimensions are cropped and scaled as
needed into tiles of 768x768 pixels, each counted as 258 tokens. Prior to Gemini
2.0, images used a fixed 258 tokens.

- 

Video and audio files are converted to tokens at the following fixed rates:
video at 263 tokens per second and audio at 32 tokens per second.

#### Media resolutions

Gemini 3 Pro Preview introduces granular control over multimodal vision processing with the
`media_resolution` parameter. The `media_resolution` parameter determines the
 maximum number of tokens allocated per input image or video frame. 
Higher resolutions improve the model's ability to
read fine text or identify small details, but increase token usage and latency.

For more details about the parameter and how it can impact token calculations,
see the media resolution guide.

#### Image files

Example that uses an uploaded image from the File API:

 

```
from google import genai

client = genai.Client()
prompt = "Tell me about this image"
your_image_file = client.files.upload(file=media / "organ.jpg")

print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_image_file]
    )
)
# ( e.g., total_tokens: 263 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_image_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py
```

 

Example that provides the image as inline data:

 

```
from google import genai
import PIL.Image

client = genai.Client()
prompt = "Tell me about this image"
your_image_file = PIL.Image.open(media / "organ.jpg")

# Count tokens for combined text and inline image.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_image_file]
    )
)
# ( e.g., total_tokens: 263 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_image_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py
```

 

#### Video or audio files

Audio and video are each converted to tokens at the following fixed rates:

- Video: 263 tokens per second

- Audio: 32 tokens per second

 

```
from google import genai
import time

client = genai.Client()
prompt = "Tell me about this video"
your_file = client.files.upload(file=media / "Big_Buck_Bunny.mp4")

# Poll until the video file is completely processed (state becomes ACTIVE).
while not your_file.state or your_file.state.name != "ACTIVE":
    print("Processing video...")
    print("File state:", your_file.state)
    time.sleep(5)
    your_file = client.files.get(name=your_file.name)

print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_file]
    )
)
# ( e.g., total_tokens: 300 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 301, candidates_token_count: 60, total_token_count: 361 )count_tokens.py
```

 

### System instructions and tools

System instructions and tools also count towards the total token count for the
input.

If you use system instructions, the `total_tokens` count increases to
reflect the addition of `system_instruction`.

If you use function calling, the `total_tokens` count increases to reflect the
addition of `tools`.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Research Agent with Gemini 2.5 Pro and LlamaIndex &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/llama-index

- 
 
 
 
 
 
 
 
 
 
 
 Research Agent with Gemini 2.5 Pro and LlamaIndex  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Research Agent with Gemini 2.5 Pro and LlamaIndex 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LlamaIndex is a framework for building knowledge agents using LLMs connected to your data. This example shows you how to build a multi-agent workflow for a Research Agent. In LlamaIndex, `Workflows` are the building blocks of agent or multi-agent systems.

You need a Gemini API key. If you don't already have one, you can
 get one in Google AI Studio .
First, install all required LlamaIndex libraries.LlamaIndex uses
the `google-genai` package under the hood.

 

```
pip install llama-index llama-index-utils-workflow llama-index-llms-google-genai llama-index-tools-google
```

 

## Set up Gemini 2.5 Pro in LlamaIndex

The engine of any LlamaIndex agent is an LLM that handles reasoning and text processing. This example uses Gemini 2.5 Pro. Make sure you set your API key as an environment variable .

 

```
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(model="gemini-2.5-pro")
```

 

## Build tools

Agents use tools to interact with the outside world, like searching the web or storing information. Tools in LlamaIndex can be regular Python functions, or imported from pre-existing `ToolSpecs`. Gemini comes with a built-in tool for using Google Search which is used here.

 

```
from google.genai import types

google_search_tool = types.Tool(
    google_search=types.GoogleSearch()
)

llm_with_search = GoogleGenAI(
    model="gemini-2.5-pro",
    generation_config=types.GenerateContentConfig(tools=[google_search_tool])
)
```

 

Now test the LLM instance with a query that requires search:

 

```
response = llm_with_search.complete("What's the weather like today in Biarritz?")
print(response)
```

 

The Research Agent will use Python functions as tools. There are a lot of ways you could go about building a system to perform this task. In this example, you will use the following:

- `search_web` uses Gemini with Google Search to search the web for information on the given topic.

- `record_notes` saves research found on the web to the state so that the other tools can use it.

- `write_report` writes the report using the information found by the `ResearchAgent`

- `review_report` reviews the report and provides feedback.

The `Context` class passes the state between agents/tools, and each agent will have access to the current state of the system.

 

```
from llama_index.core.workflow import Context

async def search_web(ctx: Context, query: str) -> str:
    """Useful for searching the web about a specific query or topic"""
    response = await llm_with_search.acomplete(f"""Please research given this query or topic,
    and return the result\n<query_or_topic>{query}</query_or_topic>""")
    return response

async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:
    """Useful for recording notes on a given topic."""
    current_state = await ctx.store.get("state")
    if "research_notes" not in current_state:
        current_state["research_notes"] = {}
    current_state["research_notes"][notes_title] = notes
    await ctx.store.set("state", current_state)
    return "Notes recorded."

async def write_report(ctx: Context, report_content: str) -> str:
    """Useful for writing a report on a given topic."""
    current_state = await ctx.store.get("state")
    current_state["report_content"] = report_content
    await ctx.store.set("state", current_state)
    return "Report written."

async def review_report(ctx: Context, review: str) -> str:
    """Useful for reviewing a report and providing feedback."""
    current_state = await ctx.store.get("state")
    current_state["review"] = review
    await ctx.store.set("state", current_state)
    return "Report reviewed."
```

 

## Build a multi-agent assistant

To build a multi-agent system, you define the agents and their interactions. Your system will have three agents:

- A `ResearchAgent` searches the web for information on the given topic.

- A `WriteAgent` writes the report using the information found by the `ResearchAgent`.

- A `ReviewAgent` reviews the report and provides feedback.

This example uses the `AgentWorkflow` class to create a multi-agent system that will execute these agents in order. Each agent takes a `system_prompt` that tells it what it should do, and suggests how to work with the other agents.

Optionally, you can help your multi-agent system by specifying which other agents it can talk to using `can_handoff_to` (if not, it will try to figure this out on its own).

 

```
from llama_index.core.agent.workflow import (
    AgentInput,
    AgentOutput,
    ToolCall,
    ToolCallResult,
    AgentStream,
)
from llama_index.core.agent.workflow import FunctionAgent, ReActAgent

research_agent = FunctionAgent(
    name="ResearchAgent",
    description="Useful for searching the web for information on a given topic and recording notes on the topic.",
    system_prompt=(
        "You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. "
        "Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic."
    ),
    llm=llm,
    tools=[search_web, record_notes],
    can_handoff_to=["WriteAgent"],
)

write_agent = FunctionAgent(
    name="WriteAgent",
    description="Useful for writing a report on a given topic.",
    system_prompt=(
        "You are the WriteAgent that can write a report on a given topic. "
        "Your report should be in a markdown format. The content should be grounded in the research notes. "
        "Once the report is written, you should get feedback at least once from the ReviewAgent."
    ),
    llm=llm,
    tools=[write_report],
    can_handoff_to=["ReviewAgent", "ResearchAgent"],
)

review_agent = FunctionAgent(
    name="ReviewAgent",
    description="Useful for reviewing a report and providing feedback.",
    system_prompt=(
        "You are the ReviewAgent that can review a report and provide feedback. "
        "Your feedback should either approve the current report or request changes for the WriteAgent to implement."
    ),
    llm=llm,
    tools=[review_report],
    can_handoff_to=["ResearchAgent","WriteAgent"],
)
```

 

The Agents are defined, now you can create the `AgentWorkflow` and run it.

 

```
from llama_index.core.agent.workflow import AgentWorkflow

agent_workflow = AgentWorkflow(
    agents=[research_agent, write_agent, review_agent],
    root_agent=research_agent.name,
    initial_state={
        "research_notes": {},
        "report_content": "Not written yet.",
        "review": "Review required.",
    },
)
```

 

During execution of the workflow, you can stream events, tool calls and updates to the console.

 

```
from llama_index.core.agent.workflow import (
    AgentInput,
    AgentOutput,
    ToolCall,
    ToolCallResult,
    AgentStream,
)

research_topic = """Write me a report on the history of the web.
Briefly describe the history of the world wide web, including
the development of the internet and the development of the web,
including 21st century developments"""

handler = agent_workflow.run(
    user_msg=research_topic
)

current_agent = None
current_tool_calls = ""
async for event in handler.stream_events():
    if (
        hasattr(event, "current_agent_name")
        and event.current_agent_name != current_agent
    ):
        current_agent = event.current_agent_name
        print(f"\n{'='*50}")
        print(f"ü§ñ Agent: {current_agent}")
        print(f"{'='*50}\n")
    elif isinstance(event, AgentOutput):
        if event.response.content:
            print("üì§ Output:", event.response.content)
        if event.tool_calls:
            print(
                "üõ†Ô∏è  Planning to use tools:",
                [call.tool_name for call in event.tool_calls],
            )
    elif isinstance(event, ToolCallResult):
        print(f"üîß Tool Result ({event.tool_name}):")
        print(f"  Arguments: {event.tool_kwargs}")
        print(f"  Output: {event.tool_output}")
    elif isinstance(event, ToolCall):
        print(f"üî® Calling Tool: {event.tool_name}")
        print(f"  With arguments: {event.tool_kwargs}")
```

 

After the workflow is complete, you can print the final output of the report, as well as the final review state from then review agent.

 

```
state = await handler.ctx.store.get("state")
print("Report Content:\n", state["report_content"])
print("\n------------\nFinal Review:\n", state["review"])
```

 

## Go further with custom workflows

The `AgentWorkflow` is a great way to get started with multi-agent systems. But what if you need more control? You can build a workflow from scratch. Here are some reasons why you might want to build your own workflow:

- More control over the process : You can decide the exact path your agents take. This includes creating loops, making decisions at certain points, or having agents work in parallel on different tasks.

- Use complex data : Go beyond simple text. Custom workflows let you use more structured data, like JSON objects or custom classes, for your inputs and outputs.

- Work with different media : Build agents that can understand and process not just text, but also images, audio, and video.

- Smarter planning : You can design a workflow that first creates a detailed plan before the agents start working. This is useful for complex tasks that require multiple steps.

- Enable self-correction : Create agents that can review their own work. If the output isn't good enough, the agent can try again, creating a loop of improvement until the result is perfect.

To learn more about LlamaIndex Workflows, see the LlamaIndex Workflows Documentation .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Prompt design strategies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/prompting-strategies#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Prompt design strategies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Prompt design strategies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 Prompt design is the process of creating prompts, or natural language requests,
that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you
started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides
on:

- Prompting with media files 

- Prompting for image generation with Imagen and Gemini Native Image Generation 

- Prompting for video generation 

You can find other sample prompts in the prompt gallery 
meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with
clear and specific instructions. Instructions can be in the form of a question,
step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a
response to. Inputs can be a question that the model
answers (question input), a task the model performs (task input), an entity the
model operates on (entity input), or partial input that the model completes or
continues (completion input).

 
 
 
 Input type 
 Prompt 
 Generated output 
 
 
 
 
 Question
 
 

```
    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

 
 
 

```
    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
```

 
 
 
 
 Task
 
 

```
    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.
```

 
 
 

```
    1. **Tent:** Your shelter from the elements.
    2. **Sleeping Bag:** Stay warm and comfortable.
    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.
    4. **Headlamp or Flashlight:** Light for nighttime activities.
    5. **First Aid Kit:** Be prepared for minor injuries.
```

 
 
 
 
 Entity
 
 

```
    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

 
 
 

```
Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
```

 
 
 
 
 

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you
provide partial content, the model can provide the rest of the content or what
it thinks is a continuation of that content as a response. When doing so, if you
include any examples or context, the model can take those examples or context
into account.

The following example provides a prompt with an instruction and an entity input:

 
 
 
 Prompt: 
 

```
For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
```

 
 Response: 
 

```
{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
```

 
 (gemini-2.5-flash) 
 
 
 

While the model did as prompted, writing out the instructions in natural language
can sometimes be challenging and it leaves a lot to the model's interpretation.
For example, a restaurants menu might contain many items. To reduce the size of
the JSON response, you probably want to omit the items that weren't ordered. In
this case, you can give an example and a response prefix and let
the model complete it:

 
 
 
 Prompt: 
 

```
Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
```
{
  "cheeseburger": 1,
  "fries": 1
}
```
Order: I want two burgers, a drink, and fries.
Output:
```

 
 Response: 
 

```
```
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
```
```

 
 (gemini-2.5-flash) 
 
 
 

Notice how "cheeseburger" was excluded from the output because it wasn't a part
of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can
tell the model what to do and not to do. For example, you can specify a constraint
in the prompt on how long you want a summary to be:

 
 
 
 

 Prompt: 
 

```
Summarize this text in one sentence:
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
```

 
 

 

 Response: 
 

```
      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
```

 
 (gemini-2.5-flash) 
 

 
 
 

### Response format

You can give instructions that specify the format of the response. For example,
you can ask for the response to be formatted as a table, bulleted list, elevator
pitch, keywords, sentence, or paragraph. The following system instruction tells
the model to be more conversational in its response:

 
 
 
 System instruction 
 

```
    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
```

 
 Prompt 
 

```
    What is a smart way to make a business that sells DVD's in 2025?
```

 
 Response: 
 

```
Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    * Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
```

 
 (gemini-2.5-flash) 
 
 
 

#### Format responses with the completion strategy

The completion strategy can also help format the response.
The following example prompts the model to create an essay outline:

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
```

 
 Response: 
 

```
Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The prompt didn't specify the format for the outline and the model chose a format
for you. To get the model to return an outline in a specific format, you can add
text that represents the start of the outline and let the model complete it based
on the pattern that you initiated.

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
I. Introduction
   *
```

 
 Response: 
 

```
Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
* Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds, and their general characteristics.
* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right
looks like. The model attempts to identify patterns and relationships from the
examples and applies them when generating a response. Prompts that contain a few
examples are called few-shot prompts, while prompts that provide no
examples are called zero-shot prompts. Few-shot prompts are often used
to regulate the formatting, phrasing, scoping, or general patterning of model
responses. Use specific and varied examples to help the model narrow its focus
and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without
few-shot examples are likely to be less effective. In fact, you can remove
instructions from your prompt if your examples are clear enough in showing the
task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

 
 
 
 Prompt: 
 

```
Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
```

 
 (gemini-2.5-flash) 
 
 
 

If your use case requires the model to produce concise responses, you can include
examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter
explanations. In the response, you can see that the examples guided the model to
choose the shorter explanation (`Explanation2`) as opposed to the longer
explanation (`Explanation1`) like it did previously.

 
 
 
 Prompt: 
 

```
Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Answer: Explanation2
```

 
 (gemini-2.5-flash) 
 
 
 

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though
you may need to experiment with the number of examples to provide in the prompt
for the best results. At the same time, if you include too many examples,
the model may start to overfit 
the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using
examples to show the model an anti pattern to avoid.

‚õî Negative pattern: 

 
 
 
 Example 
 

```
Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
```

 
 

 
 
 

‚úÖ Positive pattern: 

 
 
 
 Example 
 

```
Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
```

 
 

 
 
 

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to
avoid responses with undesired formats. One of the primary objectives of adding
few-shot examples in prompts is to show the model the response format. Therefore,
it is essential to ensure a consistent format across all examples, especially
paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs
to solve a problem, instead of assuming that the model has all of the required
information. This contextual information helps the model understand the constraints
and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

 
 
 
 Prompt: 
 

```
What should I do to fix my disconnected wifi? The light on my Google
Wifi router is yellow and blinking slowly.
```

 
 Response: 
 

```
  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response looks like generic troubleshooting information that's not specific
to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's
troubleshooting guide as context for it to refer to when providing a response.

 
 
 
 Prompt: 
 

```
Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
```

 
 Response: 
 

```
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
```

 
 (gemini-2.5-flash) 
 
 
 

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve
several purposes, depending on where you put the prefix:

- Input prefix: Adding a prefix to the input signals semantically meaningful
parts of the input to the model. For example, the prefixes "English:" and 
"French:" demarcate two different languages.

- Output prefix: Even though the output is generated by the model, you can
add a prefix for the output in the prompt. The output prefix gives the model
information about what's expected as a response. For example, the output
prefix "JSON:" signals to the model that the output should be in JSON format.

- Example prefix: In few-shot prompts, adding prefixes to the examples
provides labels that the model can use when generating the output, which makes
it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the
output prefix.

 
 
 
 Prompt: 
 

```
Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
```

 
 Response: 
 

```
The answer is: large
```

 
 (gemini-2.5-flash) 
 
 
 

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this
complexity by breaking things down into simpler components.

- 

 Break down instructions: Instead of having many instructions in one
prompt, create one prompt per instruction. You can choose which prompt to
process based on the user's input.

- 

 Chain prompts: For complex tasks that involve multiple sequential steps,
make each step a prompt and chain the prompts together in a sequence. In this
sequential chain of prompts, the output of one prompt in the sequence becomes
the input of the next prompt. The output of the last prompt in the sequence
is the final output.

- 

 Aggregate responses: Aggregation is when you want to perform different
parallel tasks on different portions of the data and aggregate the results to
produce the final output. For example, you can tell the model to perform one
operation on the first part of the data, perform another operation on the rest
of the data and aggregate the results.

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how
the model generates a response. The model can generate different results for
different parameter values. Experiment with different parameter values to get
the best values for the task. The parameters available for
different models may differ. The most common parameters are the following:

- 

 Max output tokens: Specifies the maximum number of tokens that can be
generated in the response. A token is approximately four characters. 100
tokens correspond to roughly 60-80 words.

- 

 Temperature: The temperature controls the degree of randomness in token
selection. The temperature is used for sampling during response generation,
which occurs when `topP` and `topK` are applied. Lower temperatures are good
for prompts that require a more deterministic or less open-ended response,
while higher temperatures can lead to more diverse or creative results. A
temperature of 0 is deterministic, meaning that the highest probability
response is always selected.

- 

 `topK`: The `topK` parameter changes how the model selects tokens for
output. A `topK` of 1 means the selected token is the most probable among
all the tokens in the model's vocabulary (also called greedy decoding),
while a `topK` of 3 means that the next token is selected from among the 3
most probable using the temperature. For each token selection step, the
`topK` tokens with the highest probabilities are sampled. Tokens are then
further filtered based on `topP` with the final token selected using
temperature sampling.

- 

 `topP`: The `topP` parameter changes how the model selects tokens for
output. Tokens are selected from the most to least probable until the sum of
their probabilities equals the `topP` value. For example, if tokens A, B,
and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5,
then the model will select either A or B as the next token by using the
temperature and exclude C as a candidate. The default `topP` value is 0.95.

- 

 `stop_sequences`: Set a stop sequence to
tell the model to stop generating content. A stop sequence can be any
sequence of characters. Try to avoid using a sequence of characters that
may appear in the generated content.

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before
you consistently get the response you're looking for. This section provides
guidance on some things you can try when iterating on your prompts:

- 

 Use different phrasing: Using different words or phrasing in your prompts
often yields different responses from the model even though they all mean the
same thing. If you're not getting the expected results from your prompt, try
rephrasing it.

 
 
 
 

```
Version 1:
How do I bake a pie?

Version 2:
Suggest a recipe for a pie.

Version 3:
What's a good pie recipe?
```

 
 

 
 
 

- 

 Switch to an analogous task: If you can't get the model to follow your
instructions for a task, try giving it instructions for an analogous task
that achieves the same result.

This prompt tells the model to categorize a book by using predefined categories:

 
 
 
 Prompt: 
 

```
Which category does The Odyssey belong to:
thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
    The Odyssey belongs to the category of **mythology**. 

    Here's why:

    * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
    journey home after the Trojan War. It features gods, monsters, and supernatural events common to
    Greek mythology.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response is correct, but the model didn't stay within the bounds of the
options. You also want to model to just respond with one of the options instead
of in a full sentence. In this case, you can rephrase the instructions as a
multiple choice question and ask the model to choose an option.

 
 
 
 Prompt: 
 

```
Multiple choice problem: Which of the following options describes the book The Odyssey?
Options:


thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
The correct answer is mythology.
```

 
 (gemini-2.5-flash) 
 
 
 

 
- Change the order of prompt content: The order of the content in the prompt
can sometimes affect the response. Try changing the content order and see
how that affects the response.

 

```
Version 1:
[examples]
[context]
[input]

Version 2:
[input]
[examples]
[context]

Version 3:
[examples]
[input]
[context]
```

 

## Fallback responses

A fallback response is a response returned by the model when either the prompt
or the response triggers a safety filter. An example of a fallback response is
"I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

- Avoid relying on models to generate factual information.

- Use with care on math and logic problems.

## Gemini 3

Gemini 3 models are designed for advanced reasoning and instruction following.
They respond best to prompts that are direct, well-structured, and clearly
define the task and any constraints. The following practices are recommended for
optimal results with Gemini 3:

### Core prompting principles

- Be precise and direct: State your goal clearly and concisely. Avoid
unnecessary or overly persuasive language.

- Use consistent structure: Employ clear delimiters to separate different
parts of your prompt. XML-style tags (e.g., ` `, ` `) or
Markdown headings are effective. Choose one format and use it consistently
within a single prompt.

- Define parameters: Explicitly explain any ambiguous terms or parameters.

- Control output verbosity: By default, Gemini 3 provides direct and
efficient answers. If you need a more conversational or detailed response,
you must explicitly request it in your instructions.

- Handle multimodal inputs coherently: When using text, images, audio, or
video, treat them as equal-class inputs. Ensure your instructions clearly
reference each modality as needed.

- Prioritize critical instructions: Place essential behavioral
constraints, role definitions (persona), and output format requirements in
the System Instruction or at the very beginning of the user prompt.

- Structure for long contexts: When providing large amounts of context
(e.g., documents, code), supply all the context first. Place your specific
instructions or questions at the very end of the prompt.

- Anchor context: After a large block of data, use a clear transition
phrase to bridge the context and your query, such as "Based on the
information above..."

### Enhancing reasoning and planning

You can leverage Gemini 3's advanced thinking capabilities to improve its
response quality for complex tasks by prompting it to plan or
self-critique before providing the final response.

 Example - Explicit planning: 

 

```
Before providing the final answer, please:
1. Parse the stated goal into distinct sub-tasks.
2. Check if the input information is complete.
3. Create a structured outline to achieve the goal.
```

 

 Example - Self-critique: 

 

```
Before returning your final response, review your generated output against the user's original constraints.
1. Did I answer the user's *intent*, not just their literal words?
2. Is the tone authentic to the requested persona?
```

 

### Structured prompting examples

Using tags or Markdown helps the model distinguish between instructions,
context, and tasks.

 XML example: 

 

```
<role>
You are a helpful assistant.
</role>

<constraints>
1. Be objective.
2. Cite sources.
</constraints>

<context>
[Insert User Input Here - The model knows this is data, not instructions]
</context>

<task>
[Insert the specific user request here]
</task>
```

 

 Markdown example: 

 

```
# Identity
You are a senior solution architect.

# Constraints
- No external libraries allowed.
- Python 3.11+ syntax only.

# Output format
Return a single code block.
```

 

### Example template combining best practices

This template captures the core principles for prompting with Gemini 3. Always
make sure to iterate and modify for your specific use case.

 System Instruction: 

 

```
<role>
You are Gemini 3, a specialized assistant for [Insert Domain, e.g., Data Science].
You are precise, analytical, and persistent.
</role>

<instructions>
1. **Plan**: Analyze the task and create a step-by-step plan.
2. **Execute**: Carry out the plan.
3. **Validate**: Review your output against the user's task.
4. **Format**: Present the final answer in the requested structure.
</instructions>

<constraints>
- Verbosity: [Specify Low/Medium/High]
- Tone: [Specify Formal/Casual/Technical]
</constraints>

<output_format>
Structure your response as follows:
1. **Executive Summary**: [Short overview]
2. **Detailed Response**: [The main content]
</output_format>
```

 

 User Prompt: 

 

```
<context>
[Insert relevant documents, code snippets, or background info here]
</context>

<task>
[Insert specific user request here]
</task>

<final_instruction>
Remember to think step-by-step before answering.
</final_instruction>
```

 

## Generative models under the hood

This section aims to answer the question - Is there randomness in generative
models' responses, or are they deterministic? 

The short answer - yes to both. When you prompt a generative model, a text
response is generated in two stages. In the first stage, the generative model
processes the input prompt and generates a probability distribution over
possible tokens (words) that are likely to come next. For example, if you prompt
with the input text "The dog jumped over the ... ", the generative model will
produce an array of probable next words:

 

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

 

This process is deterministic; a generative model will produce this same
distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into
actual text responses through one of several decoding strategies. A simple
decoding strategy might select the most likely token at every timestep. This
process would always be deterministic. However, you could instead choose to
generate a response by randomly sampling over the distribution returned by the
model. This process would be stochastic (random). Control the degree of
randomness allowed in this decoding process by setting the temperature. A
temperature of 0 means only the most likely tokens are selected, and there's no
randomness. Conversely, a high temperature injects a high degree of randomness
into the tokens selected by the model, leading to more unexpected, surprising
model responses. For Gemini 3 , it's recommended to not change the default
temperature of 1.0 to avoid unexpected outcomes.

## Next steps

- Now that you have a deeper understanding of prompt design, try writing your
own prompts using Google AI Studio .

- Learn more about the Gemini 3 Pro Preview model.

- To learn about multimodal prompting, see
 Prompting with media files .

- To learn about image prompting, see the Imagen prompt guide 

- To learn about video prompting, see the Veo prompt guide 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Market Research Agent with Gemini and the AI SDK by Vercel &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

- 
 
 
 
 
 
 
 
 
 
 
 Market Research Agent with Gemini and the AI SDK by Vercel  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Market Research Agent with Gemini and the AI SDK by Vercel 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The AI SDK by Vercel is a powerful open-source library for
building AI-powered applications, user interfaces, and agents in TypeScript.

This guide will walk you through building a Node.js application with TypeScript
that uses the AI SDK to connect with the Gemini API via the Google Generative AI Provider and perform automated market trend analysis. The final
application will:

- Use Gemini with Google Search to research current market trends.

- Extract structured data from the research to generate charts.

- Combine the research and charts into a professional HTML report and save it as a PDF.

## Prerequisites

To complete this guide, you'll need:

- A Gemini API key. You can create one for free in Google AI Studio .

- Node.js version 18 or later.

- A package manager, such as `npm`, `pnpm`, or `yarn`.

## Set up your application

First, create a new directory for your project and initialize it.

 
 

### npm

 

```
mkdir market-trend-app
cd market-trend-app
npm init -y
```

 
 

### pnpm

 

```
mkdir market-trend-app
cd market-trend-app
pnpm init
```

 
 

### yarn

 

```
mkdir market-trend-app
cd market-trend-app
yarn init -y
```

 
 

### Install dependencies

Next, install the AI SDK, the Google Generative AI provider, and other
necessary dependencies.

 
 

### npm

 

```
npm install ai @ai-sdk/google zod
npm install -D @types/node tsx typescript && npx tsc --init
```

 

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 

```
//"verbatimModuleSyntax": true,
```

 
 

### pnpm

 

```
pnpm add ai @ai-sdk/google zod
pnpm add -D @types/node tsx typescript
```

 
 

### yarn

 

```
yarn add ai @ai-sdk/google zod
yarn add -D @types/node tsx typescript && yarn tsc --init
```

 

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 

```
//"verbatimModuleSyntax": true,
```

 
 

This application will also use the third-party packages Puppeteer 
and Chart.js for rendering charts and
creating a PDF:

 
 

### npm

 

```
npm install puppeteer chart.js
npm install -D @types/chart.js
```

 
 

### pnpm

 

```
pnpm add puppeteer chart.js
pnpm add -D @types/chart.js
```

 
 

### yarn

 

```
yarn add puppeteer chart.js
yarn add -D @types/chart.js
```

 
 

The `puppeteer` package requires running a script to download the Chromium
browser. Your package manager may ask for approval, so ensure you approve the
script when prompted.

### Configure your API key

Set the `GOOGLE_GENERATIVE_AI_API_KEY` environment variable with your Gemini API
key. The Google Generative AI Provider automatically looks for your API key in
this environment variable.

 
 

### MacOS/Linux

 

```
export GOOGLE_GENERATIVE_AI_API_KEY="YOUR_API_KEY_HERE"
```

 
 

### Powershell

 

```
setx GOOGLE_GENERATIVE_AI_API_KEY "YOUR_API_KEY_HERE"
```

 
 

## Create your application

Now, let's create the main file for our application. Create a new file named
`main.ts` in your project directory. You'll build up the logic in this file
step-by-step.

For a quick test to ensure everything is set up correctly, add the following
code to `main.ts`. This basic example uses Gemini 2.5 Flash and `generateText`
to get a simple response from Gemini.

 

```
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  const { text } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: 'What is plant-based milk?',
  });

  console.log(text);
}

main().catch(console.error);
```

 

Before adding more complexity, let's run this script to verify that your
environment is configured correctly. Run the following command in your terminal:

 
 

### npm

 

```
npx tsc && node main.js
```

 
 

### pnpm

 

```
pnpm tsx main.ts
```

 
 

### yarn

 

```
yarn tsc && node main.js
```

 
 

If everything is set up correctly, you'll see Gemini's response printed to the
console.

## Perform market research with Google Search

To get up-to-date information, you can enable the
 Google Search tool for Gemini. When this tool
is active, the model can search the web to answer the prompt and will return
the sources it used.

Replace the content of `main.ts` with the following code to perform the first
step of our analysis.

 

```
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found:\n", marketTrends);
  // To see the sources, uncomment the following line:
  // console.log("Sources:\n", sources);
}

main().catch(console.error);
```

 

## Extract chart data

Next, let's process the research text to extract structured data suitable for
charts. Use the AI SDK's `generateObject` function along with a `zod`
schema to define the exact data structure.

Also create a helper function to convert this structured data into a
configuration that `Chart.js` can understand.

Add the following code to `main.ts`. Note the new imports and the added "Step 2".

 

```
import { google } from "@ai-sdk/google";
import { generateText, generateObject } from "ai";
import { z } from "zod/v4";
import { ChartConfiguration } from "chart.js";

// Helper function to create Chart.js configurations
function createChartConfig({labels, data, label, type, colors,}: {
  labels: string[];
  data: number[];
  label: string;
  type: "bar" | "line";
  colors: string[];
}): ChartConfiguration {
  return {
    type: type,
    data: {
      labels: labels,
      datasets: [
        {
          label: label,
          data: data,
          borderWidth: 1,
          ...(type === "bar" && { backgroundColor: colors }),
          ...(type === "line" && colors.length > 0 && { borderColor: colors[0] }),
        },
      ],
    },
    options: {
      animation: { duration: 0 }, // Disable animations for static PDF rendering
    },
  };
}

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found.");

  // Step 2: Extract chart data
  const { object: chartData } = await generateObject({
    model: google("gemini-2.5-flash"),
    schema: z.object({
      chartConfigurations: z
        .array(
          z.object({
            type: z.enum(["bar", "line"]).describe('The type of chart to generate. Either "bar" or "line"',),
            labels: z.array(z.string()).describe("A list of chart labels"),
            data: z.array(z.number()).describe("A list of the chart data"),
            label: z.string().describe("A label for the chart"),
            colors: z.array(z.string()).describe('A list of colors to use for the chart, e.g. "rgba(255, 99, 132, 0.8)"',),
          }),
        )
        .describe("A list of chart configurations"),
    }),
    prompt: `Given the following market trends text, come up with a list of 1-3 meaningful bar or line charts
    and generate chart data.
    
Market Trends:
${marketTrends}
`,
  });

  const chartConfigs = chartData.chartConfigurations.map(createChartConfig);

  console.log("Chart configurations generated.");
}

main().catch(console.error);
```

 

## Generate the final report

In the final step, instruct Gemini to act as an expert report writer.
Provide it with the market research, the chart configurations, and a clear
set of instructions for building an HTML report. Then, use
 Puppeteer to render this HTML and save it as a PDF.

Add the final `puppeteer` import and "Step 3" to your `main.ts` file.

 

```
// ... (imports from previous step)
import puppeteer from "puppeteer";

// ... (createChartConfig helper function from previous step)

async function main() {
  // ... (Step 1 and 2 from previous step)

  // Step 3: Generate the final HTML report and save it as a PDF
  const { text: htmlReport } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: `You are an expert financial analyst and report writer.
    Your task is to generate a comprehensive market analysis report in HTML format.

    **Instructions:**
    1.  Write a full HTML document.
    2.  Use the provided "Market Trends" text to write the main body of the report. Structure it with clear headings and paragraphs.
    3.  Incorporate the provided "Chart Configurations" to visualize the data. For each chart, you MUST create a unique <canvas> element and a corresponding <script> block to render it using Chart.js.
    4.  Reference the "Sources" at the end of the report.
    5.  Do not include any placeholder data; use only the information provided.
    6.  Return only the raw HTML code.

    **Chart Rendering Snippet:**
    Include this script in the head of the HTML: <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    For each chart, use a structure like below, ensuring the canvas 'id' is unique for each chart, and apply the correspinding config:

    ---
    <div style="width: 800px; height: 600px;">
      <canvas id="chart1"></canvas>
    </div>
    <script>
      new Chart(document.getElementById('chart1'), config);
    </script>
    ---
    (For the second chart, use 'chart2' and the corresponding config, and so on.)

    **Data:**
    - Market Trends: ${marketTrends}
    - Chart Configurations: ${JSON.stringify(chartConfigs)}
    - Sources: ${JSON.stringify(sources)}
    `,
  });

  // LLMs may wrap the HTML in a markdown code block, so strip it.
  const finalHtml = htmlReport.replace(/^```html\n/, "").replace(/\n```$/, "");

  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.setContent(finalHtml);
  await page.pdf({ path: "report.pdf", format: "A4" });
  await browser.close();

  console.log("\nReport generated successfully: report.pdf");
}

main().catch(console.error);
```

 

## Run your application

You are now ready to run the application. Execute the following command in
your terminal:

 
 

### npm

 

```
npx tsc && node main.js
```

 
 

### pnpm

 

```
pnpm tsx main.ts
```

 
 

### yarn

 

```
yarn tsc && node main.js
```

 
 

You will see logging in your terminal as the script executes each step.
Once complete, a `report.pdf` file containing your market analysis will be
created in your project directory.

Below, you'll see the first two pages of an example PDF report:

 

## Further resources

For more information about building with Gemini and the AI SDK,
explore these resources:

- AI SDK docs 

- AI SDK Google Generative AI docs 

- AI SDK cookbook: Get Started with Gemini 2.5 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Logs and datasets &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/logs-datasets#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Logs and datasets  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Logs and datasets 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This guide contains everything you need to get started with enabling logging
for your existing Gemini API applications. In this guide you'll learn how to
view logs from an existing or new application in the Google AI Studio dashboard
to better understand model behavior and how users may be interacting with your
applications. Use logging to observe, debug, and optionally share usage feedback
with Google to help improve Gemini across developer use cases . * 

All `GenerateContent` and `StreamGenerateContent` API calls are supported,
including those made through OpenAI compatibility 
endpoints.

## 1. Enable logging in Google AI Studio

Before you begin, ensure you have a billing-enabled project that you own.

- Open the logs page in Google AI Studio .

- Choose your project from the drop-down and press the enable button to enable logging for all requests by default.

 
 
 
 

You can enable or disable logging for all projects or for specific projects, and
change these preferences at any time through Google AI Studio. 

## 2. View logs in AI Studio

- Go to AI Studio .

- Select the project you've enabled logging for.

- You should see your logs appear in the table in reverse chronological order.

 
 
 
 

Click on an entry for a full page view of the request and response pair. You can
inspect the full prompt, the complete response from Gemini, and the context from
the previous turn. Note that each project has a default storage limit of up to
1,000 logs, and logs not saved in datasets will expire after 55 days. If your
project reaches its storage limit you will be promoted to delete logs.

## 3. Curate and share datasets

- From the logs table, locate the filter bar at the top to select a property to
filter by.

- From your filtered view of logs use the checkboxes to select all or a
few of the logs.

- Click the "Create Dataset" button that appears at the top of the list.

- Give your new dataset a descriptive name and optional description.

- You will see the dataset you just created with the curated set of logs.

 
 
 
 

Datasets can be helpful for a number of different use cases.

- Curating challenge sets: Drive future improvements that target areas where you want your AI to improve.

- Curate sample sets: For example, a sample from real usage to generate responses from another model, or a collection of edge cases for routine checks before deployment.

- Evaluation sets: Sets that are representative of real usage across important capabilities, for comparison across other models or system instruction iterations.

You can help drive progress in AI research, the Gemini API, and Google AI Studio
by choosing to share your datasets as demonstration examples. This allows us to
refine our models in diverse contexts and create AI systems that remain useful
to developers across many fields and applications

## Next steps & what to test

Now that you have logging enabled, here are a few things to try:

- Prototype with session history: Leverage AI Studio Build to vibe code apps and add your API key to enable a history of user logs.

- Re-run logs with the Gemini Batch API: Use datasets for response sampling
and evaluation of models or application logic by re-running logs via the
 Gemini Batch API .

## Compatibility

Logging is not currently supported for the following:

- Imagen and Veo

- Inputs containing videos, GIFs or PDFs

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Migrate to the Google GenAI SDK &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate

- 
 
 
 
 
 
 
 
 
 
 
 Migrate to the Google GenAI SDK  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Migrate to the Google GenAI SDK 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Starting with the Gemini 2.0 release in late 2024, we introduced a new set of
libraries called the Google GenAI SDK . It offers
an improved developer experience through
an updated client architecture , and
 simplifies the transition between developer
and enterprise workflows.

The Google GenAI SDK is now in General Availability (GA) across all supported
platforms. If you're using one of our legacy libraries , we strongly recommend you to
migrate.

This guide provides before-and-after examples of migrated code to help you get
started.

## Installation

 Before 

 
 

### Python

 

```
pip install -U -q "google-generativeai"
```

 
 

### JavaScript

 

```
npm install @google/generative-ai
```

 
 

### Go

 

```
go get github.com/google/generative-ai-go
```

 
 

 After 

 
 

### Python

 

```
pip install -U -q "google-genai"
```

 
 

### JavaScript

 

```
npm install @google/genai
```

 
 

### Go

 

```
go get google.golang.org/genai
```

 
 

## API access

The old SDK implicitly handled the API client behind the scenes using a variety
of ad hoc methods. This made it hard to manage the client and credentials.
Now, you interact through a central `Client` object. This `Client` object acts
as a single entry point for various API services (e.g., `models`, `chats`,
`files`, `tunings`), promoting consistency and simplifying credential and
configuration management across different API calls.

 Before (Less Centralized API Access) 

 
 

### Python

The old SDK didn't explicitly use a top-level client object for most API
calls. You would directly instantiate and interact with `GenerativeModel`
objects.

 

```
import google.generativeai as genai

# Directly create and use model objects
model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(...)
chat = model.start_chat(...)
```

 
 

### JavaScript

While `GoogleGenerativeAI` was a central point for models and chat, other
functionalities like file and cache management often required importing and
instantiating entirely separate client classes.

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";
import { GoogleAIFileManager, GoogleAICacheManager } from "@google/generative-ai/server"; // For files/caching

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");
const fileManager = new GoogleAIFileManager("YOUR_API_KEY");
const cacheManager = new GoogleAICacheManager("YOUR_API_KEY");

// Get a model instance, then call methods on it
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const result = await model.generateContent(...);
const chat = model.startChat(...);

// Call methods on separate client objects for other services
const uploadedFile = await fileManager.uploadFile(...);
const cache = await cacheManager.create(...);
```

 
 

### Go

The `genai.NewClient` function created a client, but generative model
operations were typically called on a separate `GenerativeModel` instance
obtained from this client. Other services might have been accessed via
distinct packages or patterns.

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "github.com/google/generative-ai-go/genai/fileman" // For files
      "google.golang.org/api/option"
)

client, err := genai.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))
fileClient, err := fileman.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))

// Get a model instance, then call methods on it
model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(...)
cs := model.StartChat()

// Call methods on separate client objects for other services
uploadedFile, err := fileClient.UploadFile(...)
```

 
 

 After (Centralized Client Object) 

 
 

### Python

 

```
from google import genai

# Create a single client object
client = genai.Client()

# Access API methods through services on the client object
response = client.models.generate_content(...)
chat = client.chats.create(...)
my_file = client.files.upload(...)
tuning_job = client.tunings.tune(...)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// Create a single client object
const ai = new GoogleGenAI({apiKey: "YOUR_API_KEY"});

// Access API methods through services on the client object
const response = await ai.models.generateContent(...);
const chat = ai.chats.create(...);
const uploadedFile = await ai.files.upload(...);
const cache = await ai.caches.create(...);
```

 
 

### Go

 

```
import "google.golang.org/genai"

// Create a single client object
client, err := genai.NewClient(ctx, nil)

// Access API methods through services on the client object
result, err := client.Models.GenerateContent(...)
chat, err := client.Chats.Create(...)
uploadedFile, err := client.Files.Upload(...)
tuningJob, err := client.Tunings.Tune(...)
```

 
 

## Authentication

Both legacy and new libraries authenticate using API keys. You can
 create your API key in Google AI
Studio.

 Before 

 
 

### Python

The old SDK handled the API client object implicitly. 

 

```
import google.generativeai as genai

genai.configure(api_key=...)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
```

 
 

### Go

Import the Google libraries:

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "google.golang.org/api/option"
)
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
```

 
 

 After 

 
 

### Python

With Google GenAI SDK, you create an API client first, which is used to call
the API.
The new SDK will pick up your API key from either one of the
`GEMINI_API_KEY` or `GOOGLE_API_KEY` environment variables, if you don't
pass one to the client. 

 

```
export GEMINI_API_KEY="YOUR_API_KEY"
```

 

```
from google import genai

client = genai.Client() # Set the API key using the GEMINI_API_KEY env var.
                        # Alternatively, you could set the API key explicitly:
                        # client = genai.Client(api_key="your_api_key")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({apiKey: "GEMINI_API_KEY"});
```

 
 

### Go

Import the GenAI library:

 

```
import "google.golang.org/genai"
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, &genai.ClientConfig{
        Backend:  genai.BackendGeminiAPI,
})
```

 
 

## Generate content

### Text

 Before 

 
 

### Python

Previously, there were no client objects, you accessed APIs directly through
`GenerativeModel` objects.

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story in 300 words'
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const prompt = "Tell me a story in 300 words";

const result = await model.generateContent(prompt);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(ctx, genai.Text("Tell me a story in 300 words."))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response parts
```

 
 

 After 

 
 

### Python

The new Google GenAI SDK provides access to all the API methods through the
`Client` object. Except for a few stateful special cases (`chat` and
live-api `session`s), these are all stateless functions. For utility and
uniformity, objects returned are `pydantic` classes.

 

```
from google import genai
client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
print(response.text)

print(response.model_dump_json(
    exclude_none=True, indent=4))
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story in 300 words.",
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me a story in 300 words."), nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Image

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

Many of the same convenience features exist in the new SDK. For
example, `PIL.Image` objects are automatically converted.

 

```
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Streaming

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

 
 

## Configuration

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
  'gemini-1.5-flash',
    system_instruction='you are a story teller for kids under 5 years old',
    generation_config=genai.GenerationConfig(
      max_output_tokens=400,
      top_k=2,
      top_p=0.5,
      temperature=0.5,
      response_mime_type='application/json',
      stop_sequences=['\n'],
    )
)
response = model.generate_content('tell me a story in 100 words')
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

const result = await model.generateContent(
  "Tell me a story about a magic backpack.",
);
console.log(result.response.text())
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
model.SetTemperature(0.5)
model.SetTopP(0.5)
model.SetTopK(2.0)
model.SetMaxOutputTokens(100)
model.ResponseMIMEType = "application/json"
resp, err := model.GenerateContent(ctx, genai.Text("Tell me about New York"))
if err != nil {
    log.Fatal(err)
}
printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

For all methods in the new SDK, the required arguments are provided as
keyword arguments. All optional inputs are provided in the `config`
argument. Config arguments can be specified as either Python dictionaries or
`Config` classes in the `google.genai.types` namespace. For utility and
uniformity, all definitions within the `types` module are `pydantic`
classes.

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='Tell me a story in 100 words.',
  config=types.GenerateContentConfig(
      system_instruction='you are a story teller for kids under 5 years old',
      max_output_tokens= 400,
      top_k= 2,
      top_p= 0.5,
      temperature= 0.5,
      response_mime_type= 'application/json',
      stop_sequences= ['\n'],
      seed=42,
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story about a magic backpack.",
  config: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx,
    "gemini-2.0-flash",
    genai.Text("Tell me about New York"),
    &genai.GenerateContentConfig{
        Temperature:      genai.Ptr[float32](0.5),
        TopP:             genai.Ptr[float32](0.5),
        TopK:             genai.Ptr[float32](2.0),
        ResponseMIMEType: "application/json",
        StopSequences:    []string{"Yankees"},
        CandidateCount:   2,
        Seed:             genai.Ptr[int32](42),
        MaxOutputTokens:  128,
        PresencePenalty:  genai.Ptr[float32](0.5),
        FrequencyPenalty: genai.Ptr[float32](0.5),
    },
)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing response
```

 
 

## Safety settings

Generate a response with safety settings:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'say something bad',
    safety_settings={
        'HATE': 'BLOCK_ONLY_HIGH',
        'HARASSMENT': 'BLOCK_ONLY_HIGH',
  }
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
});

const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const result = await model.generateContent(unsafePrompt);

try {
  result.response.text();
} catch (e) {
  console.error(e);
  console.log(result.response.candidates[0].safetyRatings);
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='say something bad',
  config=types.GenerateContentConfig(
      safety_settings= [
          types.SafetySetting(
              category='HARM_CATEGORY_HATE_SPEECH',
              threshold='BLOCK_ONLY_HIGH'
          ),
      ]
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: unsafePrompt,
  config: {
    safetySettings: [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ],
  },
});

console.log("Finish reason:", response.candidates[0].finishReason);
console.log("Safety ratings:", response.candidates[0].safetyRatings);
```

 
 

## Async

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content_async(
    'tell me a story in 100 words'
)
```

 
 

 After 

 
 

### Python

To use the new SDK with `asyncio`, there is a separate `async`
implementation of every method under `client.aio`.

 

```
from google import genai

client = genai.Client()

response = await client.aio.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
```

 
 

## Chat

Start a chat and send a message to the model:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat()

response = chat.send_message(
    "Tell me a story in 100 words")
response = chat.send_message(
    "What happened after that?")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const chat = model.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});
let result = await chat.sendMessage("I have 2 dogs in my house.");
console.log(result.response.text());
result = await chat.sendMessage("How many paws are in my house?");
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
cs := model.StartChat()

cs.History = []*genai.Content{
    {
        Parts: []genai.Part{
            genai.Text("Hello, I have 2 dogs in my house."),
        },
        Role: "user",
    },
    {
        Parts: []genai.Part{
            genai.Text("Great to meet you. What would you like to know?"),
        },
        Role: "model",
    },
}

res, err := cs.SendMessage(ctx, genai.Text("How many paws are in my house?"))
if err != nil {
    log.Fatal(err)
}
printResponse(res) // utility for printing the response
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

chat = client.chats.create(model='gemini-2.0-flash')

response = chat.send_message(
    message='Tell me a story in 100 words')
response = chat.send_message(
    message='What happened after that?')
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log("Chat response 1:", response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log("Chat response 2:", response2.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, nil)
if err != nil {
    log.Fatal(err)
}

result, err := chat.SendMessage(ctx, genai.Part{Text: "Hello, I have 2 dogs in my house."})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result

result, err = chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

## Function calling

 Before 

 
 

### Python

 

```
import google.generativeai as genai
from enum import Enum

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

response = model.generate_content("What is the weather in San Francisco?")
function_call = response.candidates[0].parts[0].function_call
```

 
 

 After 

 
 

### Python

In the new SDK, automatic function calling is the default. Here, you disable
it.

 

```
from google import genai
from google.genai import types

client = genai.Client()

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather],
      automatic_function_calling={'disable': True},
  ),
)

function_call = response.candidates[0].content.parts[0].function_call
```

 
 

### Automatic function calling

 Before 

 
 

### Python

The old SDK only supports automatic function calling in chat. In the new SDK
this is the default behavior in `generate_content`.

 

```
import google.generativeai as genai

def get_current_weather(city: str) -> str:
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

chat = model.start_chat(
    enable_automatic_function_calling=True)
result = chat.send_message("What is the weather in San Francisco?")
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types
client = genai.Client()

def get_current_weather(city: str) -> str:
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather]
  ),
)
```

 
 

## Code execution

Code execution is a tool that allows the model to generate Python code, run it,
and return the result.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools="code_execution"
)

result = model.generate_content(
  "What is the sum of the first 50 prime numbers? Generate and run code for "
  "the calculation, and make sure you get all 50.")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  tools: [{ codeExecution: {} }],
});

const result = await model.generateContent(
  "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get " +
    "all 50.",
);

console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the sum of the first 50 prime numbers? Generate and run '
            'code for the calculation, and make sure you get all 50.',
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
    ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-pro-exp-02-05",
  contents: `Write and execute code that calculates the sum of the first 50 prime numbers.
            Ensure that only the executable code and its resulting output are generated.`,
});

// Each part may contain text, executable code, or an execution result.
for (const part of response.candidates[0].content.parts) {
  console.log(part);
  console.log("\n");
}

console.log("-".repeat(80));
// The `.text` accessor concatenates the parts into a markdown-formatted text.
console.log("\n", response.text);
```

 
 

## Search grounding

`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are
tools that allow the model to retrieve public web data for grounding, powered by
Google.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    contents="what is the Google stock price?",
    tools='google_search_retrieval'
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the Google stock price?',
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                google_search=types.GoogleSearch()
            )
        ]
    )
)
```

 
 

## JSON response

Generate answers in JSON format.

 Before 

 
 

### Python

By specifying a `response_schema` and setting
`response_mime_type="application/json"` users can constrain the model to
produce a `JSON` response following a given structure. 

 

```
import google.generativeai as genai
import typing_extensions as typing

class CountryInfo(typing.TypedDict):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

model = genai.GenerativeModel(model_name="gemini-1.5-flash")
result = model.generate_content(
    "Give me information of the United States",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json",
        response_schema = CountryInfo
    ),
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");

const schema = {
  description: "List of recipes",
  type: SchemaType.ARRAY,
  items: {
    type: SchemaType.OBJECT,
    properties: {
      recipeName: {
        type: SchemaType.STRING,
        description: "Name of the recipe",
        nullable: false,
      },
    },
    required: ["recipeName"],
  },
};

const model = genAI.getGenerativeModel({
  model: "gemini-1.5-pro",
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: schema,
  },
});

const result = await model.generateContent(
  "List a few popular cookie recipes.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

The new SDK uses
`pydantic` classes to provide the schema (although you can pass a
`genai.types.Schema`, or equivalent `dict`). When possible, the SDK will
parse the returned JSON, and return the result in `response.parsed`. If you
provided a `pydantic` class as the schema the SDK will convert that `JSON`
to an instance of the class.

 

```
from google import genai
from pydantic import BaseModel

client = genai.Client()

class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': CountryInfo,
    },
)

response.parsed
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "List a few popular cookie recipes.",
  config: {
    responseMimeType: "application/json",
    responseSchema: {
      type: "array",
      items: {
        type: "object",
        properties: {
          recipeName: { type: "string" },
          ingredients: { type: "array", items: { type: "string" } },
        },
        required: ["recipeName", "ingredients"],
      },
    },
  },
});
console.log(response.text);
```

 
 

## Files

### Upload

Upload a file:

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

file = genai.upload_file(path='a11.txt')

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Can you summarize this file:',
    my_file
])
print(response.text)
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai

client = genai.Client()

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

my_file = client.files.upload(file='a11.txt')

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Can you summarize this file:',
        my_file
    ]
)
print(response.text)
```

 
 

### List and get

List uploaded files and get an uploaded file with a filename:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

for file in genai.list_files():
  print(file.name)

file = genai.get_file(name=file.name)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
client = genai.Client()

for file in client.files.list():
    print(file.name)

file = client.files.get(name=file.name)
```

 
 

### Delete

Delete a file:

 Before 

 
 

### Python

 

```
import pathlib
import google.generativeai as genai

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = genai.upload_file(path='dummy.txt')

file = genai.delete_file(name=dummy_file.name)
```

 
 

 After 

 
 

### Python

 

```
import pathlib
from google import genai

client = genai.Client()

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = client.files.upload(file='dummy.txt')

response = client.files.delete(name=dummy_file.name)
```

 
 

## Context caching

Context caching allows the user to pass the content to the model once, cache the
input tokens, and then refer to the cached tokens in subsequent calls to lower
the cost.

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai
from google.generativeai import caching

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = genai.upload_file(path="a11.txt")

# Create cache
apollo_cache = caching.CachedContent.create(
    model="gemini-1.5-flash-001",
    system_instruction="You are an expert at analyzing transcripts.",
    contents=[document],
)

# Generate response
apollo_model = genai.GenerativeModel.from_cached_content(
    cached_content=apollo_cache
)
response = apollo_model.generate_content("Find a lighthearted moment from this transcript")
```

 
 

### JavaScript

 

```
import { GoogleAICacheManager, GoogleAIFileManager } from "@google/generative-ai/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const cacheManager = new GoogleAICacheManager("GOOGLE_API_KEY");
const fileManager = new GoogleAIFileManager("GOOGLE_API_KEY");

const uploadResult = await fileManager.uploadFile("path/to/a11.txt", {
  mimeType: "text/plain",
});

const cacheResult = await cacheManager.create({
  model: "models/gemini-1.5-flash",
  contents: [
    {
      role: "user",
      parts: [
        {
          fileData: {
            fileUri: uploadResult.file.uri,
            mimeType: uploadResult.file.mimeType,
          },
        },
      ],
    },
  ],
});

console.log(cacheResult);

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModelFromCachedContent(cacheResult);
const result = await model.generateContent(
  "Please summarize this transcript.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai
from google.genai import types

client = genai.Client()

# Check which models support caching.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createCachedContent":
      print(m.name)
      break

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = client.files.upload(file='a11.txt')

# Create cache
model='gemini-1.5-flash-001'
apollo_cache = client.caches.create(
      model=model,
      config={
          'contents': [document],
          'system_instruction': 'You are an expert at analyzing transcripts.',
      },
  )

# Generate response
response = client.models.generate_content(
    model=model,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const filePath = path.join(media, "a11.txt");
const document = await ai.files.upload({
  file: filePath,
  config: { mimeType: "text/plain" },
});
console.log("Uploaded file name:", document.name);
const modelName = "gemini-1.5-flash";

const contents = [
  createUserContent(createPartFromUri(document.uri, document.mimeType)),
];

const cache = await ai.caches.create({
  model: modelName,
  config: {
    contents: contents,
    systemInstruction: "You are an expert analyzing transcripts.",
  },
});
console.log("Cache created:", cache);

const response = await ai.models.generateContent({
  model: modelName,
  contents: "Please summarize this transcript",
  config: { cachedContent: cache.name },
});
console.log("Response text:", response.text);
```

 
 

## Count tokens

Count the number of tokens in a request.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.count_tokens(
    'The quick brown fox jumps over the lazy dog.')
```

 
 

### JavaScript

 

```
 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY+);
 const model = genAI.getGenerativeModel({
   model: "gemini-1.5-flash",
 });

 // Count tokens in a prompt without calling text generation.
 const countResult = await model.countTokens(
   "The quick brown fox jumps over the lazy dog.",
 );

 console.log(countResult.totalTokens); // 11

 const generateResult = await model.generateContent(
   "The quick brown fox jumps over the lazy dog.",
 );

 // On the response for `generateContent`, use `usageMetadata`
 // to get separate input and output token counts
 // (`promptTokenCount` and `candidatesTokenCount`, respectively),
 // as well as the combined token count (`totalTokenCount`).
 console.log(generateResult.response.usageMetadata);
 // candidatesTokenCount and totalTokenCount depend on response, may vary
 // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.count_tokens(
    model='gemini-2.0-flash',
    contents='The quick brown fox jumps over the lazy dog.',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);
```

 
 

## Generate images

Generate images:

 Before 

 
 

### Python

 

```
#pip install https://github.com/google-gemini/generative-ai-python@imagen
import google.generativeai as genai

imagen = genai.ImageGenerationModel(
    "imagen-3.0-generate-001")
gen_images = imagen.generate_images(
    prompt="Robot holding a red skateboard",
    number_of_images=1,
    safety_filter_level="block_low_and_above",
    person_generation="allow_adult",
    aspect_ratio="3:4",
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

gen_images = client.models.generate_images(
    model='imagen-3.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 1,
        safety_filter_level= "BLOCK_LOW_AND_ABOVE",
        person_generation= "ALLOW_ADULT",
        aspect_ratio= "3:4",
    )
)

for n, image in enumerate(gen_images.generated_images):
    pathlib.Path(f'{n}.png').write_bytes(
        image.image.image_bytes)
```

 
 

## Embed content

Generate content embeddings.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = genai.embed_content(
  model='models/gemini-embedding-001',
  content='Hello world'
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-embedding-001",
});

const result = await model.embedContent("Hello world!");

console.log(result.embedding);
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.embed_content(
  model='gemini-embedding-001',
  contents='Hello world',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const text = "Hello World!";
const result = await ai.models.embedContent({
  model: "gemini-embedding-001",
  contents: text,
  config: { outputDimensionality: 10 },
});
console.log(result.embeddings);
```

 
 

## Tune a model

Create and use a tuned model.

The new SDK simplifies tuning with `client.tunings.tune`, which launches the
tuning job and polls until the job is complete.

 Before 

 
 

### Python

 

```
import google.generativeai as genai
import random

# create tuning model
train_data = {}
for i in range(1, 6):
  key = f'input {i}'
  value = f'output {i}'
  train_data[key] = value

name = f'generate-num-{random.randint(0,10000)}'
operation = genai.create_tuned_model(
    source_model='models/gemini-1.5-flash-001-tuning',
    training_data=train_data,
    id = name,
    epoch_count = 5,
    batch_size=4,
    learning_rate=0.001,
)
# wait for tuning complete
tuningProgress = operation.result()

# generate content with the tuned model
model = genai.GenerativeModel(model_name=f'tunedModels/{name}')
response = model.generate_content('55')
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Check which models are available for tuning.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createTunedModel":
      print(m.name)
      break

# create tuning model
training_dataset=types.TuningDataset(
        examples=[
            types.TuningExample(
                text_input=f'input {i}',
                output=f'output {i}',
            )
            for i in range(5)
        ],
    )
tuning_job = client.tunings.tune(
    base_model='models/gemini-1.5-flash-001-tuning',
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count= 5,
        batch_size=4,
        learning_rate=0.001,
        tuned_model_display_name="test tuned model"
    )
)

# generate content with the tuned model
response = client.models.generate_content(
    model=tuning_job.tuned_model.model,
    contents='55',
)
```

 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Data Logging and Sharing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/logs-policy#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Data Logging and Sharing  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Data Logging and Sharing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page outlines the storage and management of
 Gemini API logs , which are developer-owned
API data from supported Gemini API calls for projects with billing enabled. Logs
encompass the entire process from a user's request to the model's response.

## 1. Data that can be shared

As a project owner you have the choice to opt-in to logging of Gemini API calls,
for your own use or for feedback and sharing with Google to help us continually
improve our models.

With logging enabled, you can help us build AI systems that continue to be
valuable for developers across various fields and use cases by choosing to
contribute the following data for product improvements and model training:

- Datasets: Use the Logs and Datasets interface of Google AI Studio to
choose logs (requests, responses, metadata etc.) of interest from
supported Gemini API calls; contributed through inclusion in datasets, with the
option to opt-out during dataset creation.

- Feedback: When reviewing logs, you can provide feedback; including thumbs
up/down ratings and any written comments you provide.

When you share a dataset with Google, your logs in that dataset, including
requests and responses, will be processed in accordance with our
 Terms for
" Unpaid Services ,"
meaning the dataset may be used to develop and improve Google
products, services, and machine learning technologies, including improving and
training our models. Do not include personal, sensitive, or confidential
information. 

## 2. How we use your data

Logs will expire after 55 days by default. They will become unavailable
after this period. Datasets can be created to retain logs of interest or value
beyond this period for downstream use cases and optional contribution to model
improvements. Logs stored in datasets do not have set expiry dates, however each
project has a default storage limit of up to 1,000 logs.

By default, because logging is only available for billing-enabled projects,
prompts and responses within logs are not used for product improvement or
development, in accordance with our Terms 
on data use.

If you choose to share datasets of your logs with Google, those datasets will be
used as real-world demonstration data to better understand the diversity of
domains and contexts AI systems and applications are used in. This data may be
used to improve model quality, and inform the training and evaluation of future
models and services. This data is processed in accordance with our data use
terms for Unpaid Services .
Accordingly, human reviewers may read, annotate, and process the API inputs and
outputs you share. Before data is used for model improvement, Google takes steps
to protect user privacy as part of this process. This includes disconnecting
this data from your Google Account, API key, and Cloud project before reviewers
see or annotate it.

## 3. Data permissions

By opting-in to contributing API data, you confirm that you have the necessary
permissions for Google to process and use the data as described in this
documentation. Please do not contribute logs containing sensitive,
confidential, or proprietary information obtained through the paid service .
The license you grant to Google under the " Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

## 4. Data sharing and feedback

You can help us advance the frontier of AI research, the Gemini API and Google
AI Studio by opting in to share your data as examples, enabling us to
continually improve our models across various contexts and build AI systems that
continue to be valuable to developers across various fields and use cases.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-30 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-30 UTC."],[],[]]

---

### Release notes &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/changelog

- 
 
 
 
 
 
 
 
 
 
 
 Release notes  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Release notes 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

This page documents updates to the Gemini API.

## November 18, 2025

- 

Launched the first Gemini 3 series model, `gemini-3-pro-preview`, our
state-of-the-art reasoning and multimodal understanding model with powerful
agentic and coding capabilities.

In addition to improvements in intelligence and performance,
Gemini 3 Pro Preview introduces new behavior around:

 Media resolution 

- Thought signatures 

- Thinking levels 

Read the Gemini 3 Developer Guide for
migration, new features, and specs.

 

## November 11, 2025

- 

The following models will be deprecated:

 

November 12:

 `veo-3.0-fast-generate-preview`

- `veo-3.0-generate-preview`

 
- 

November 14:

 `gemini-2.0-flash-exp-image-generation`

- `gemini-2.0-flash-preview-image-generation`

 

 

## November 10, 2025

- 

The following model is deprecated:

 `imagen-3.0-generate-002`

Use Imagen 4 instead. Refer to the
 Gemini deprecations table for more details.

 

## November 6, 2025

- Launched the File Search API to public preview, enabling developers to
ground responses in their own data. Read the new File Search page for more info.

## November 4, 2025

- 

The following models will be deprecated:

 

November 18th:

 `gemini-2.5-flash-lite-preview-06-17`

- `gemini-2.5-flash-preview-05-20`

 
- 

December 2nd:

 `gemini-2.0-flash-thinking-exp`

- `gemini-2.0-flash-thinking-exp-01-21`

- `gemini-2.0-flash-thinking-exp-1219`

- `gemini-2.5-pro-preview-03-25`

- `gemini-2.5-pro-preview-05-06`

- `gemini-2.5-pro-preview-06-05`

 
- 

December 9th:

 `gemini-2.0-flash-lite-preview`

- `gemini-2.0-flash-lite-preview-02-05`

 

 

## October 29, 2025

- Launched the new logging and datasets tool for the Gemini API.

## October 20, 2025

- 

The following Gemini Live API models are now deprecated:

 `gemini-2.5-flash-preview-native-audio-dialog`

- `gemini-2.5-flash-exp-native-audio-thinking-dialog`

You can use `gemini-2.5-flash-native-audio-preview-09-2025` instead.

 
- 

Deprecation for `gemini-2.0-flash-live-001` and `gemini-live-2.5-flash-preview` coming December 09, 2025.

## October 17, 2025

- Grounding with Google Maps is now
generally available. For more information, see
 Grounding with Google Maps documentation.

## October 15, 2025

- 

Released Veo 3.1 and 3.1 Fast models in
public preview, with new features including:

 Extending Veo-created videos.

- Referencing up to three images to generate a video.

- Providing first and last frame images to generate videos from.

This launch also added more options for Veo 3 output video durations: 4, 6,
and 8 seconds.

 
- 

Deprecation for `veo-3.0-generate-preview` and `veo-3.0-fast-generate-preview`
coming November 6, 2025.

## October 7, 2025

- Launched Gemini 2.5 Computer Use Preview 

## October 2, 2025

- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini 

## September 29, 2025

- The following Gemini 1.5 models are now deprecated:

 `gemini-1.5-pro`

- `gemini-1.5-flash-8b`

- `gemini-1.5-flash`

 

## September 25, 2025

- 

Released Gemini Robotics-ER 1.5 model in preview. See the
 Robotics overview 
to learn about how to use the model for your robotics application.

- 

Launched following preview models:

 `gemini-2.5-flash-preview-09-2025`

- `gemini-2.5-flash-lite-preview-09-2025`

See the Models page for details.

 

## September 23, 2025

- Released `gemini-2.5-flash-native-audio-preview-09-2025`,
a new native audio model for the Live API with improved function calling
and speech cut off handling. To learn more, see the
 Live API guide and
 Gemini 2.5 Flash Native Audio .

## September 16, 2025

- 

The following models will be deprecated in October, 2025:

 `embedding-001`

- `embedding-gecko-001`

- `gemini-embedding-exp-03-07` (`gemini-embedding-exp`)

See the Embeddings page for details on the latest embeddings
model.

 

## September 10, 2025

- Released support for the
 Embeddings model in Batch API ,
and added Batch API to the
 OpenAI compatibility library for even
easier ways to get started with batch queries.

## September 9, 2025

- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for
aspect ratios, resolution, and seeding. Read the
 Veo documentation for more
information.

## August 26, 2025

- Launched Gemini 2.5 Image Preview ,
our latest native image generation model.

## August 18, 2025

- Released URL context tool to general
availability (GA), a tool for providing URLs as additional context to
prompts. Support for using URL context with the `gemini-2.0-flash` model
(available during experimental release) will be discontinued in one week.

## August 14, 2025

- Released Imagen 4 Ultra, Standard and Fast models as generally available
(GA). To learn more, see the Imagen page.

## August 7, 2025

- `allow_adult` setting in Image to Video generation are now available in
restricted regions. See the
 Veo 
page for details.

## July 31, 2025

- Launched image-to-video generation for the Veo 3 Preview model.

- Released Veo 3 Fast Preview model.

- To learn more about Veo 3, visit the Veo page.

## July 22, 2025

- Released `gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini
2.5 model. To learn more, see Gemini 2.5
Flash-Lite .

## July 17, 2025

- 

Launched `veo-3.0-generate-preview`, the latest update to Veo introducing
video with audio generation. To learn more about Veo 3, visit the Veo page.

- 

Increased rate limits for Imagen 4 Standard and Ultra. Visit the
 Rate limits page for more details.

## July 14, 2025

- Released `gemini-embedding-001`, the stable version of our
text embedding model. To learn more, see
 embeddings . The `gemini-embedding-exp-03-07`
model will be deprecated on August 14, 2025.

## July 7, 2025

- Launched Gemini API Batch Mode. Batch up requests and send them to process
asynchronously. To learn more, see Batch Mode .

## June 26, 2025

- 

The preview models `gemini-2.5-pro-preview-05-06` and
`gemini-2.5-pro-preview-03-25` are now redirecting to
the latest stable version `gemini-2.5-pro`.

- 

`gemini-2.5-pro-exp-03-25` is deprecated.

## June 24, 2025

- Released Imagen 4 Ultra and Standard Preview models. To learn more, see the
 Image generation page.

## June 17, 2025

- Released `gemini-2.5-pro`, the stable version of our most powerful
model, now with adaptive thinking. To learn more, see
 Gemini 2.5 Pro 
and Thinking . `gemini-2.5-pro-preview-05-06`
will be redirected to `gemini-2.5-pro` on June 26, 2025.

- Released `gemini-2.5-flash`, our first stable 2.5 Flash model. To learn
more, see Gemini 2.5 Flash .
`gemini-2.5-flash-preview-04-17` will be deprecated on July 15, 2025.

- Released `gemini-2.5-flash-lite-preview-06-17`, a low-cost, high-performance
Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite
Preview .

## June 05, 2025

- Released `gemini-2.5-pro-preview-06-05`, a new version of our most powerful
model, now with adaptive thinking. To learn more, see
 Gemini 2.5 Pro Preview 
and Thinking .
`gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on
June 26, 2025.

## May 27, 2025

- The last available tuning model, Gemini 1.5 Flash 001, has been shutdown.
Tuning is no longer supported on any models.
See Fine tuning with the Gemini API .

## May 20, 2025

 API updates: 

- Launched support for
 custom video preprocessing 
using clipping intervals and configurable frame rate sampling.

- Launched multi-tool use, which supports configuring
 code execution and
 Grounding with Google Search on the same
`generateContent` request.

- Launched support for
 asynchronous function calls 
in the Live API.

- Launched an experimental
 URL context tool 
for providing URLs as additional context to prompts.

 Model updates: 

- Released `gemini-2.5-flash-preview-05-20`, a Gemini
 preview model optimized for
price-performance and adaptive thinking. To learn more, see
 Gemini 2.5 Flash Preview 
and Thinking .

- Released the
 `gemini-2.5-pro-preview-tts` 
and
 `gemini-2.5-flash-preview-tts` 
models, which are capable of
 generating speech with one or two
speakers.

- Released the `lyria-realtime-exp` model, which
 generates music in real time.

- Released `gemini-2.5-flash-preview-native-audio-dialog` and
`gemini-2.5-flash-exp-native-audio-thinking-dialog`,
new Gemini models for the Live API with native audio output capabilities. To
learn more, see the
 Live API guide and
 Gemini 2.5 Flash Native Audio .

- Released `gemma-3n-e4b-it` preview, available on
 AI Studio and through the Gemini API,
as part of the Gemma 3n launch.

## May 7, 2025

- Released `gemini-2.0-flash-preview-image-generation`, a preview model for
generating and editing images. To learn more, see Image
generation and
 Gemini 2.0 Flash Preview Image
Generation .

## May 6, 2025

- Released `gemini-2.5-pro-preview-05-06`, a new version of our most powerful
model, with improvements on code and function calling. `gemini-2.5-pro-preview-03-25`
will automatically point to the new version of the model.

## April 17, 2025

- Released `gemini-2.5-flash-preview-04-17`, a Gemini
 preview model optimized for
price-performance and adaptive thinking. To learn more, see
 Gemini 2.5 Flash Preview 
and Thinking .

## April 16, 2025

- Launched context caching for
 Gemini 2.0 Flash .

## April 9, 2025

 Model updates: 

- Released `veo-2.0-generate-001`, a generally available (GA) text- and
image-to-video model, capable of generating detailed and artistically
nuanced videos. To learn more, see the Veo docs .

- 

Released `gemini-2.0-flash-live-001`, a public preview version of the
 Live API model with billing enabled.

 

 Enhanced Session Management and Reliability 

 Session Resumption: Keep sessions alive across temporary network
disruptions. The API now supports server-side session state storage (for
up to 24 hours) and provides handles (session_resumption) to reconnect
and resume where you left off.

- Longer Sessions via Context Compression: Enable extended
interactions beyond previous time limits. Configure context window
compression with a sliding window mechanism to automatically manage
context length, preventing abrupt terminations due to context limits.

- Graceful Disconnect Notification: Receive a `GoAway` server
message indicating when a connection is about to close, allowing for
graceful handling before termination.

 
- 

 More Control over Interaction Dynamics 

- 

 Configurable Voice Activity Detection (VAD): Choose sensitivity
levels or disable automatic VAD entirely and use new client events
(`activityStart`, `activityEnd`) for manual turn control.

- 

 Configurable Interruption Handling: Decide whether user input
should interrupt the model's response.

- 

 Configurable Turn Coverage: Choose whether the API processes all
audio and video input continuously or only captures it when the end-user
is detected speaking.

- 

 Configurable Media Resolution: Optimize for quality or token usage
by selecting the resolution for input media.

- 

 Richer Output and Features 

- 

 Expanded Voice & Language Options: Choose from two new voices and
30 new languages for audio output. The output language is now
configurable within `speechConfig`.

- 

 Text Streaming: Receive text responses incrementally as they are
generated, enabling faster display to the user.

- 

 Token Usage Reporting: Gain insights into usage with detailed
token counts provided in the `usageMetadata` field of server messages,
broken down by modality and prompt or response phases.

 

## April 4, 2025

- Released `gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version
with billing enabled. You can continue to use `gemini-2.5-pro-exp-03-25` on
the free tier.

## March 25, 2025

- Released `gemini-2.5-pro-exp-03-25`, a public experimental Gemini model
with thinking mode always on by default.
To learn more, see
 Gemini 2.5 Pro Experimental .

## March 12, 2025

 Model updates: 

- Launched an experimental Gemini 2.0 Flash 
model capable of image generation and editing.

- Released `gemma-3-27b-it`, available on
 AI Studio and through the Gemini API,
as part of the Gemma 3 launch.

 API updates: 

- Added support for
 YouTube URLs as a media source.

- Added support for including an
 inline video of less than 20MB.

## March 11, 2025

 SDK updates: 

- Released the
 Google Gen AI SDK for TypeScript and JavaScript 
to public preview.

## March 7, 2025

 Model updates: 

- Released `gemini-embedding-exp-03-07`, an
 experimental 
Gemini-based embeddings model in public preview.

## February 28, 2025

 API updates: 

- Support for Search as a tool 
added to `gemini-2.0-pro-exp-02-05`, an experimental model based on
Gemini 2.0 Pro.

## February 25, 2025

 Model updates: 

- Released `gemini-2.0-flash-lite`, a generally available (GA) version of
 Gemini 2.0 Flash-Lite ,
which is optimized for speed, scale, and cost efficiency.

## February 19, 2025

 AI Studio updates: 

- Support for
 additional regions 
(Kosovo, Greenland and Faroe Islands).

 API updates: 

- Support for
 additional regions 
(Kosovo, Greenland and Faroe Islands).

## February 18, 2025

 Model updates: 

- Gemini 1.0 Pro is no longer supported. For the list of supported models, see
 Gemini models .

## February 11, 2025

 API updates: 

- Updates on the
 OpenAI libraries compatibility .

## February 6, 2025

 Model updates: 

- Released `imagen-3.0-generate-002`, a generally available (GA) version of
 Imagen 3 in the Gemini API .

 SDK updates: 

- Released the Google Gen AI SDK for Java 
for public preview.

## February 5, 2025

 Model updates: 

- Released `gemini-2.0-flash-001`, a generally available (GA) version of
 Gemini 2.0 Flash that
supports text-only output.

- Released `gemini-2.0-pro-exp-02-05`,
an experimental public
preview version of Gemini 2.0 Pro.

- Released `gemini-2.0-flash-lite-preview-02-05`, an experimental public
preview model 
optimized for cost efficiency.

 API updates: 

- Added
 file input and graph output 
support to code execution.

 SDK updates: 

- Released the
 Google Gen AI SDK for Python 
to general availability (GA).

## January 21, 2025

 Model updates: 

- Released `gemini-2.0-flash-thinking-exp-01-21`, the latest preview version of
the model behind the
 Gemini 2.0 Flash Thinking Model .

## December 19, 2024

 Model updates: 

- 

Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is
a test-time compute model that lets you see the model's thought process
while it generates a response, and produces responses with stronger
reasoning capabilities.

Read more about Gemini 2.0 Flash Thinking Mode in our overview
page .

## December 11, 2024

 Model updates: 

- Released Gemini 2.0 Flash Experimental 
for public preview. Gemini 2.0 Flash Experimental's partial list of features includes:

 Twice as fast as Gemini 1.5 Pro

- Bidirectional streaming with our Live API

- Multimodal response generation in the form of text, images, and speech

- Built-in tool use with multi-turn reasoning to use features like code
execution, Search, function calling, and more

 

Read more about Gemini 2.0 Flash in our overview
page .

## November 21, 2024

 Model updates: 

- Released `gemini-exp-1121`, an even more powerful experimental Gemini API model.

 Model updates: 

- Updated the `gemini-1.5-flash-latest` and `gemini-1.5-flash` model aliases
to use `gemini-1.5-flash-002`.

 Change to `top_k` parameter: The `gemini-1.5-flash-002`
model supports `top_k` values between 1 and 41 (exclusive).
Values greater than 40 will be changed to 40.

 

## November 14, 2024

 Model updates: 

- Released `gemini-exp-1114`, a powerful experimental Gemini API model.

## November 8, 2024

 API updates: 

- Added support for Gemini in the OpenAI libraries / REST API.

## October 31, 2024

 API updates: 

- Added support for Grounding with Google Search .

## October 3, 2024

 Model updates: 

- Released `gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini
API model.

## September 24, 2024

 Model updates: 

- Released `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, two new stable
versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.

- Updated the `gemini-1.5-pro-latest` model code to use `gemini-1.5-pro-002`
and the `gemini-1.5-flash-latest` model code to use `gemini-1.5-flash-002`.

- Released `gemini-1.5-flash-8b-exp-0924` to replace `gemini-1.5-flash-8b-exp-0827`.

- Released the civic integrity safety filter 
for the Gemini API and AI Studio.

- Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in
Python and NodeJS:
 `frequencyPenalty` and
 `presencePenalty` .

## September 19, 2024

 AI Studio updates: 

- Added thumb-up and thumb-down buttons to model responses, to enable users to
provide feedback on the quality of a response.

 API updates: 

- Added support for Google Cloud credits, which can now be used towards
Gemini API usage.

## September 17, 2024

 AI Studio updates: 

- Added an Open in Colab button that exports a prompt ‚Äì and the
code to run it ‚Äì to a Colab notebook. The feature doesn't yet support
prompting with tools (JSON mode, function calling, or code execution).

## September 13, 2024

 AI Studio updates: 

- Added support for compare mode, which lets you compare responses across
 models and prompts to find the best fit for your use case.

## August 30, 2024

 Model updates: 

- Gemini 1.5 Flash supports
 supplying JSON schema through model configuration .

## August 27, 2024

 Model updates: 

- Released the following
 experimental models :

 `gemini-1.5-pro-exp-0827`

- `gemini-1.5-flash-exp-0827`

- `gemini-1.5-flash-8b-exp-0827`

 

## August 9, 2024

 API updates: 

- Added support for PDF processing .

## August 5, 2024

 Model updates: 

- Fine-tuning support released for Gemini 1.5 Flash.

## August 1, 2024

 Model updates: 

- Released `gemini-1.5-pro-exp-0801`, a new experimental version of
 Gemini 1.5 Pro .

## July 12, 2024

 Model updates: 

- Support for Gemini 1.0 Pro Vision removed from Google AI services and tools.

## June 27, 2024

 Model updates: 

- General availability release for Gemini 1.5 Pro's 2M context window.

 API updates: 

- Added support for code execution .

## June 18, 2024

 API updates: 

- Added support for context caching .

## June 12, 2024

 Model updates: 

- Gemini 1.0 Pro Vision deprecated.

## May 23, 2024

 Model updates: 

- Gemini 1.5 Pro 
(`gemini-1.5-pro-001`) is generally available (GA).

- Gemini 1.5 Flash 
(`gemini-1.5-flash-001`) is generally available (GA).

## May 14, 2024

 API updates: 

- Introduced a 2M context window for Gemini 1.5 Pro (waitlist).

- Introduced pay-as-you-go billing for Gemini 1.0
Pro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon.

- Introduced increased rate limits for the upcoming paid tier of Gemini 1.5
Pro.

- Added built-in video support to the File API .

- Added plain text support to the File API .

- Added support for parallel function calling, which returns more than one
call at a time.

## May 10, 2024

 Model updates: 

- Released Gemini 1.5 Flash 
(`gemini-1.5-flash-latest`) in preview.

## April 9, 2024

 Model updates: 

- Released Gemini 1.5 Pro 
(`gemini-1.5-pro-latest`) in preview.

- Released a new text embedding model, `text-embeddings-004`, which supports
 elastic embedding 
sizes under 768.

 API updates: 

- Released the File API for temporarily storing
media files for use in prompting.

- Added support for prompting with text, image, and audio data, also
known as multimodal prompting. To learn more, see
 Prompting with media .

- Released System instructions in
beta.

- Added
 Function calling mode ,
which defines the execution behavior for function calling.

- Added support for the `response_mime_type` configuration option, which lets
you request responses in
 JSON format .

## March 19, 2024

 Model updates: 

- Added support for
 tuning Gemini 1.0 Pro 
in Google AI Studio or with the Gemini API.

## December 13 2023

 Model updates: 

- gemini-pro: New text model for a wide variety of tasks. Balances capability
and efficiency.

- gemini-pro-vision: New multimodal model for a wide variety of tasks.
Balances capability and efficiency.

- embedding-001: New embeddings model.

- aqa: A new specially tuned model that is trained to answer questions
 using text passages for grounding generated answers.

See Gemini models for more details.

 API version updates: 

- v1: The stable API channel.

- v1beta: Beta channel. This channel has features that may be under
development.

See the API versions topic for more details.

 API updates: 

- `GenerateContent` is a single unified endpoint for chat and text.

- Streaming available through the `StreamGenerateContent` method.

- Multimodal capability: Image is a new supported modality

- New beta features:

 Function Calling 

- Semantic Retriever 

- Attributed Question Answering (AQA)

 
- Updated candidate count: Gemini models only return 1 candidate.

- Different Safety Settings and SafetyRating categories. See
 safety settings for more details.

- Tuning models is not yet supported for Gemini models (Work in progress).

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Safety settings &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/safety-settings#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Safety settings  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Safety settings 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API provides safety settings that you can adjust during the
prototyping stage to determine if your application requires more or less
restrictive safety configuration. You can adjust these settings across five
filter categories to restrict or allow certain types of content.

This guide covers how the Gemini API handles safety settings and filtering and
how you can change the safety settings for your application.

## Safety filters

The Gemini API's adjustable safety filters cover the following categories:

 
 
 Category 
 Description 
 
 
 Harassment 
 
 Negative or harmful comments targeting identity and/or protected
 attributes.
 
 
 
 Hate speech 
 
 Content that is rude, disrespectful, or profane.
 
 
 
 Sexually explicit 
 
 Contains references to sexual acts or other lewd content.
 
 
 
 Dangerous 
 
 Promotes, facilitates, or encourages harmful acts.
 
 
 
 Civic integrity 
 
 Election-related queries.
 
 
 

You can use these filters to adjust what's appropriate for your use case. For
example, if you're building video game dialogue, you may deem it acceptable to
allow more content that's rated as Dangerous due to the nature of the game.

In addition to the adjustable safety filters, the Gemini API has built-in
protections against core harms, such as content that endangers child safety.
These types of harm are always blocked and cannot be adjusted.

### Content safety filtering level

The Gemini API categorizes the probability level of content being unsafe as
`HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.

The Gemini API blocks content based on the probability of content being unsafe
and not the severity. This is important to consider because some content can
have low probability of being unsafe even though the severity of harm could
still be high. For example, comparing the sentences:

- The robot punched me.

- The robot slashed me up.

The first sentence might result in a higher probability of being unsafe, but you
might consider the second sentence to be a higher severity in terms of violence.
Given this, it is important that you carefully test and consider what the
appropriate level of blocking is needed to support your key use cases while
minimizing harm to end users.

### Safety filtering per request

You can adjust the safety settings for each request you make to the API. When
you make a request, the content is analyzed and assigned a safety rating. The
safety rating includes the category and the probability of the harm
classification. For example, if the content was blocked due to the harassment
category having a high probability, the safety rating returned would have
category equal to `HARASSMENT` and harm probability set to `HIGH`.

By default, safety settings block content (including prompts) with medium or
higher probability of being unsafe across any filter. This baseline safety is
designed to work for most use cases, so you should only adjust your safety
settings if it's consistently required for your application.

The following table describes the block settings you can adjust for each
category. For example, if you set the block setting to Block few for the
 Hate speech category, everything that has a high probability of being hate
speech content is blocked. But anything with a lower probability is allowed.

 
 
 Threshold (Google AI Studio) 
 Threshold (API) 
 Description 
 
 
 Off 
 `OFF` 
 Turn off the safety filter 
 
 
 Block none 
 `BLOCK_NONE` 
 Always show regardless of probability of unsafe content 
 
 
 Block few 
 `BLOCK_ONLY_HIGH` 
 Block when high probability of unsafe content 
 
 
 Block some 
 `BLOCK_MEDIUM_AND_ABOVE` 
 Block when medium or high probability of unsafe content 
 
 
 Block most 
 `BLOCK_LOW_AND_ABOVE` 
 Block when low, medium or high probability of unsafe content 
 
 
 N/A 
 `HARM_BLOCK_THRESHOLD_UNSPECIFIED` 
 Threshold is unspecified, block using default threshold 
 
 

If the threshold is not set, the default block threshold is Block none (for
all newer stable GA models)
or Block some (in all other models) for all categories except the
 Civic integrity category.

The default block threshold for the Civic integrity category is Block none 
(for `gemini-2.0-flash`, and `gemini-2.0-flash-lite`) both for
Google AI Studio and the Gemini API, and Block most for all other models in
Google AI Studio only.

You can set these settings for each request you make to the generative service.
See the `HarmBlockThreshold` API
reference for details.

### Safety feedback

 `generateContent` 
returns a
 `GenerateContentResponse` which
includes safety feedback.

Prompt feedback is included in
 `promptFeedback` . If
`promptFeedback.blockReason` is set, then the content of the prompt was blocked.

Response candidate feedback is included in
 `Candidate.finishReason` and
 `Candidate.safetyRatings` . If response
content was blocked and the `finishReason` was `SAFETY`, you can inspect
`safetyRatings` for more details. The content that was blocked is not returned.

## Adjust safety settings

This section covers how to adjust the safety settings in both Google AI Studio
and in your code.

### Google AI Studio

You can adjust safety settings in Google AI Studio, but you cannot turn them
off.

Click Edit safety settings in the Run settings panel to open the Run
safety settings modal. In the modal, you can use the sliders to adjust the
content filtering level per safety category:

 

When you send a request (for example, by asking the model a question), a warning 
 No Content message appears if the request's content is blocked. To see more
details, hold the pointer over the
 No Content text and click warning 
 Safety .

### Gemini API SDKs

The following code snippet shows how to set safety settings in your
`GenerateContent` call. This sets the thresholds for the harassment
(`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`)
categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE`
blocks any content that has a low or higher probability of being harassment or
hate speech. To understand the threshold settings, see
 Safety filtering per request .

 
 

### Python

 

```
from google import genai
from google.genai import types

import PIL.Image

img = PIL.Image.open("cookies.jpg")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=['Do these look store-bought or homemade?', img],
    config=types.GenerateContentConfig(
      safety_settings=[
        types.SafetySetting(
            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        ),
      ]
    )
)

print(response.text)
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        SafetySettings: []*genai.SafetySetting{
            {
                Category:  "HARM_CATEGORY_HATE_SPEECH",
                Threshold: "BLOCK_LOW_AND_ABOVE",
            },
        },
    }

    response, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.0-flash",
        genai.Text("Some potentially unsafe prompt."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(response.Text())
}
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const safetySettings = [
  {
    category: "HARM_CATEGORY_HARASSMENT",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
  {
    category: "HARM_CATEGORY_HATE_SPEECH",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
];

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Some potentially unsafe prompt.",
    config: {
      safetySettings: safetySettings,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Dart (Flutter)

 

```
final safetySettings = [
  SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),
  SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),
];
final model = GenerativeModel(
  model: 'gemini-1.5-flash',
  apiKey: apiKey,
  safetySettings: safetySettings,
);
```

 
 

### Kotlin

 

```
val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)

val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)

val generativeModel = GenerativeModel(
    modelName = "gemini-1.5-flash",
    apiKey = BuildConfig.apiKey,
    safetySettings = listOf(harassmentSafety, hateSpeechSafety)
)
```

 
 

### Java

 

```
SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,
    BlockThreshold.LOW_AND_ABOVE);

SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,
    BlockThreshold.LOW_AND_ABOVE);

GenerativeModel gm = new GenerativeModel(
    "gemini-1.5-flash",
    BuildConfig.apiKey,
    null, // generation config is optional
    Arrays.asList(harassmentSafety, hateSpeechSafety)
);

GenerativeModelFutures model = GenerativeModelFutures.from(gm);
```

 
 

### REST

 

```
echo '{    "safetySettings": [        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}    ],    "contents": [{        "parts":[{            "text": "'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'"}]}]}' > request.json

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \        -H "x-goog-api-key: $GEMINI_API_KEY" \

    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json 2> /dev/null
```

 
 

## Next steps

- See the API reference to learn more about the full API.

- Review the safety guidance for a general look at safety
considerations when developing with LLMs.

- Learn more about assessing probability versus severity from the Jigsaw
team 

- Learn more about the products that contribute to safety solutions like the
 Perspective
API .
 * You can use these safety settings to create a toxicity
 classifier. See the classification
 example to
 get started.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-08 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-08 UTC."],[],[]]

---

### Safety guidance &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/safety-guidance#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Safety guidance  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Safety guidance 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Generative artificial intelligence models are powerful tools, but they are not
without their limitations. Their versatility and applicability can sometimes
lead to unexpected outputs, such as outputs that are inaccurate, biased, or
offensive. Post-processing, and rigorous manual evaluation are essential to
limit the risk of harm from such outputs.

The models provided by the Gemini API can be used for a wide variety of
generative AI and natural language processing (NLP) applications. Use of these
functions is only available through the Gemini API or the Google AI Studio web
app. Your use of Gemini API is also subject to the Generative AI Prohibited Use
Policy and the
 Gemini API terms of service .

Part of what makes large language models (LLMs) so useful is that they're
creative tools that can address many different language tasks. Unfortunately,
this also means that large language models can generate output that you don't
expect, including text
that's offensive, insensitive, or factually incorrect. What's more, the
incredible versatility of these models is also what makes it difficult to
predict exactly what kinds of undesirable output they might produce. While the
Gemini API has been designed with Google's AI
principles in mind, the onus is on developers to
apply these models responsibly. To aid developers in creating safe, responsible
applications, the Gemini API has some built-in content filtering as well as
adjustable safety settings across 4 dimensions of harm. Refer to the
 safety settings guide to learn more.

This document is meant to introduce you to some safety risks that can arise when
using LLMs, and recommend emerging safety design and development
recommendations. (Note that laws and regulations may also impose restrictions,
but such considerations are beyond the scope of this guide.)

The following steps are recommended when building applications with LLMs:

- Understanding the safety risks of your application

- Considering adjustments to mitigate safety risks

- Performing safety testing appropriate to your use case

- Soliciting feedback from users and monitoring usage

The adjustment and testing phases should be iterative until you reach
performance appropriate for your application.

 

## Understand the safety risks of your application

In this context, safety is being defined as the ability of an LLM to avoid
causing harm to its users, for example, by generating toxic language or content
that promotes stereotypes. The models available through the Gemini API have been
designed with Google‚Äôs AI principles in mind
and your use of it is subject to the Generative AI Prohibited Use
Policy . The API
provides built-in safety filters to help address some common language model
problems such as toxic language and hate speech, and striving for inclusiveness
and avoidance of stereotypes. However, each application can pose a different set
of risks to its users. So as the application owner, you are responsible for
knowing your users and the potential harms your application may cause, and
ensuring that your application uses LLMs safely and responsibly.

As part of this assessment, you should consider the likelihood that harm could
occur and determine its seriousness and mitigation steps. For example, an
app that generates essays based on factual events would need to be more careful
about avoiding misinformation, as compared to an app that generates fictional
stories for entertainment. A good way to begin exploring potential safety risks
is to research your end users, and others who might be affected by your
application's results. This can take many forms including researching state of
the art studies in your app domain, observing how people are using similar apps,
or running a user study, survey, or conducting informal interviews with
potential users.

 
 
 

#### Advanced tips

 

 - 
 Speak with a diverse mix of prospective users within your target
 population about your application and its intended purpose so as
 to get a wider perspective on potential risks and to adjust diversity
 criteria as needed.
 

 - 
 The AI Risk Management Framework 
 released by the U.S. government's
 National Institute of Standards and Technology (NIST) provides more
 detailed guidance and additional learning resources for AI risk management.
 

 - 
 DeepMind's publication on the
 
 ethical and social risks of harm from language models
 
 describes in detail the ways that language model
 applications can cause harm.
 

 

 
 

## Consider adjustments to mitigate safety risks

Now that you have an understanding of the risks, you can decide how to mitigate
them. Determining which risks to prioritize and how much you should do to try to
prevent them is a critical decision, similar to triaging bugs in a software
project. Once you've determined priorities, you can start thinking about the
types of mitigations that would be most appropriate. Often simple changes can
make a difference and reduce risks.

For example, when designing an application consider:

- Tuning the model output to better reflect what is acceptable in your
application context. Tuning can make the output of the model more
predictable and consistent and therefore can help mitigate certain risks.

- Providing an input method that facilities safer outputs. The exact input
you give to an LLM can make a difference in the quality of the output.
Experimenting with input prompts to find what works most safely in your
use-case is well worth the effort, as you can then provide a UX that
facilitates it. For example, you could restrict users to choose only from a
drop-down list of input prompts, or offer pop-up suggestions with
descriptive
phrases which you've found perform safely in your application context.

- 

 Blocking unsafe inputs and filtering output before it is shown to the
user. In simple situations, blocklists can be used to identify and block
unsafe words or phrases in prompts or responses, or require human reviewers
to manually alter or block such content.

- 

 Using trained classifiers to label each prompt with potential harms or
adversarial signals. Different strategies can then be employed on how to
handle the request based on the type of harm detected. For example, If the
input is overtly adversarial or abusive in nature, it could be blocked and
instead output a pre-scripted response.
 
 
 

#### Advanced tip

 

 
 If signals determine the output to be harmful,
 the application can employ the following options:
 

 
 Provide an error message or pre-scripted output.
 

 - 
 Try the prompt again, in case an alternative safe output is
 generated, since sometimes the same prompt will elicit
 different outputs.
 

 

 
 

 
 

 
- 

 Putting safeguards in place against deliberate misuse such as assigning
each user a unique ID and imposing a limit on the volume of user queries
that can be submitted in a given period. Another safeguard is to try and
protect against possible prompt injection. Prompt injection, much like SQL
injection, is a way for malicious users to design an input prompt that
manipulates the output of the model, for example, by sending an input prompt
that instructs the model to ignore any previous examples. See the
 Generative AI Prohibited Use Policy 
for details about deliberate misuse.

- 

 Adjusting functionality to something that is inherently lower risk. 
Tasks that are narrower in scope (e.g., extracting keywords from passages of
text) or that have greater human oversight (e.g., generating short-form
content that will be reviewed by a human), often pose a lower risk. So for
instance, instead of creating an application to write an email reply from
scratch, you might instead limit it to expanding on an outline or suggesting
alternative phrasings.

## Perform safety testing appropriate to your use case

Testing is a key part of building robust and safe applications, but the extent,
scope and strategies for testing will vary. For example, a just-for-fun haiku
generator is likely to pose less severe risks than, say, an application designed
for use by law firms to summarize legal documents and help draft contracts. But
the haiku generator may be used by a wider variety of users which means the
potential for adversarial attempts or even unintended harmful inputs can be
greater. The implementation context also matters. For instance, an application
with outputs that are reviewed by human experts prior to any action being taken
might be deemed less likely to produce harmful outputs than the identical
application without such oversight.

It's not uncommon to go through several iterations of making changes and testing
before feeling confident that you're ready to launch, even for applications that
are relatively low risk. Two kinds of testing are particularly useful for AI
applications:

- 

 Safety benchmarking involves designing safety metrics that reflect the
ways your application could be unsafe in the context of how it is likely to
get used, then testing how well your application performs on the metrics
using evaluation datasets. It's good practice to think about the minimum
acceptable levels of safety metrics before testing so that 1) you can
evaluate the test results against those expectations and 2) you can gather
the evaluation dataset based on the tests that evaluate the metrics you care
about most.

 
 

#### Advanced tips

 
 Beware of over-relying on ‚Äúoff the shelf‚Äù approaches as it's likely
 you'll need to build your own testing datasets using human raters to
 fully suit your application's context.
 

 - 
 If you have more than one metric you'll need to decide how you'll
 trade off if a change leads to improvements for one metric to the
 detriment of another. Like with other performance engineering, you
 may want to focus on worst-case performance across your evaluation
 set rather than average performance.
 

 
 
- 

 Adversarial testing involves proactively trying to break your
application. The goal is to identify points of weakness so that you can take
steps to remedy them as appropriate. Adversarial testing can take
significant time/effort from evaluators with expertise in your application ‚Äî
but the more you do, the greater your chance of spotting problems,
especially those occurring rarely or only after repeated runs of the
application.

 Adversarial testing is a method for systematically evaluating an ML
model with the intent of learning how it behaves when provided with
malicious or inadvertently harmful input:

 An input may be malicious when the input is clearly designed to
produce an unsafe or harmful output-- for example, asking a text
generation model to generate a hateful rant about a particular
religion.

- An input is inadvertently harmful when the input itself may be
innocuous, but produces harmful output -- for example, asking a text
generation model to describe a person of a particular ethnicity and
receiving a racist output.

 
- What distinguishes an adversarial test from a standard evaluation is the
composition of the data used for testing. For adversarial tests, select 
test data that is most likely to elicit problematic output from
the model. This means probing the model's behavior for all the types of
harms that are possible, including rare or unusual examples and
edge-cases that are relevant to safety policies. It should also include
diversity in the different dimensions of a sentence such as structure,
meaning and length. You can refer to the Google's Responsible AI
practices in
fairness 
for more details on what to consider when building a test dataset.
 
 

#### Advanced tips

 
 Use
 automated testing 
 instead of the traditional method of enlisting people in 'red teams'
 to try and break your application. In automated testing, the
 'red team' is another language model that finds input text that
 elicit harmful outputs from the model being tested.
 

 
 

 

## Monitor for problems

No matter how much you test and mitigate, you can never guarantee perfection, so
plan upfront how you'll spot and deal with problems that arise. Common
approaches include setting up a monitored channel for users to share feedback
(e.g., thumbs up/down rating) and running a user study to proactively solicit
feedback from a diverse mix of users ‚Äî especially valuable if usage patterns are
different to expectations.

 
 
 

#### Advanced tips

 

 - 
 When users give feedback to AI products, it can greatly improve the AI
 performance and the user experience over time by, for example,
 helping you choose better examples for prompt tuning. The
 Feedback and Control chapter 
 in Google's People and AI guidebook 
 highlights key considerations to take into account when designing
 feedback mechanisms.
 

 

 
 

## Next steps

- Refer to the
 safety settings guide to learn about the adjustable
safety settings available through the Gemini API.

- See the intro to prompting to get
started writing your first prompts.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini deprecations &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/deprecations

- 
 
 
 
 
 
 
 
 
 
 
 Gemini deprecations  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini deprecations 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page lists the known deprecation schedules for Stable (GA) and Preview
models in the Gemini API. A deprecation announcement means we will no longer
provide support for that model, and that it will be completely retired, or
turned off, shortly after the deprecation date.

 
 

### Stable Models

 
 
 
 Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 Imagen 3 
 February 6, 2025 
 November 10, 2025 
 `imagen-3.0-generate-002`
Use Imagen 4. 
 
 
 Gemini 2.0 Flash 
 February 5, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash` & `gemini-2.0-flash-001` 
 
 
 Gemini 2.0 Flash-Lite 
 February 25, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash-lite` & `gemini-2.0-flash-lite-001` 
 
 
 Gemini 2.5 Flash 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Pro 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Flash-Lite 
 July 22, 2025 
 Earliest July 2026 
 
 
 
 Gemini 2.5 Flash Image 
 October 2, 2025 
 Earliest October 2026 
 
 
 
 Veo 3 
 September 9, 2025 
 No deprecation date announced 
 Including `veo-3.0-generate-001` & `veo-3.0-fast-generate-001` 
 
 
 Veo 2 
 April 9, 2025 
 No deprecation date announced 
 
 
 
 Imagen 4 
 August 14, 2025 
 No deprecation date announced 
 Including:
 

 - `imagen-4.0-generate-001`

 - `imagen-4.0-ultra-generate-001`

 - `imagen-4.0-fast-generate-001`

 

 
 
 
 
 

### Preview Models

Preview models are deprecated with at least 2 weeks notice.

 
 
 
 Preview Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 `gemini-2.5-flash-preview-native-audio-dialog` 
 May 20,2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `gemini-2.5-flash-exp-native-audio-thinking-dialog` 
 May 20, 2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `embedding-001` & `embedding-gecko-001` 
 December 13, 2023 
 October 30, 2025 
 Use `gemini-embedding-001`. 
 
 
 `gemini-2.0-flash-preview-image-generation` & `gemini-2.0-flash-exp-image-generation` 
 May 7, 2025 
 November 12, 2025 
 Use `gemini-2.5-flash-image`. 
 
 
 `veo-3.0-generate-preview` & `veo-3.0-fast-generate-preview` 
 July 17, 2025 
 November 12, 2025 
 Use `veo-3.1-generate-preview` or `veo-3.1-fast-generate-preview`. 
 
 
 `gemini-2.0-flash-live-001` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-live-2.5-flash-preview` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-2.5-pro-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-lite-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-image-preview` 
 August 26, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-native-audio-preview-09-2025` 
 September 23, 2025 
 No deprecation date announced 
 
 
 
 `veo-3.1-generate-preview` & `veo-3.1-fast-generate-preview` 
 October 15, 2025 
 No deprecation date announced 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-12 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-12 UTC."],[],[]]

---

### Troubleshooting guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/troubleshooting

- 
 
 
 
 
 
 
 
 
 
 
 Troubleshooting guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Troubleshooting guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Use this guide to help you diagnose and resolve common issues that arise when
you call the Gemini API. You may encounter issues from either
the Gemini API backend service or the client SDKs. Our client SDKs are
open sourced in the following repositories:

- python-genai 

- js-genai 

- go-genai 

If you encounter API key issues, verify that you have set up
your API key correctly per the API key setup guide .

## Gemini API backend service error codes

The following table lists common backend error codes you may encounter, along
with explanations for their causes and troubleshooting steps:

 
 
 HTTP Code 
 
 Status 
 
 Description 
 
 Example 
 
 Solution 
 
 
 
 
 400
 
 INVALID_ARGUMENT 
 The request body is malformed. 
 There is a typo, or a missing required field in your request. 
 Check the API reference for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors. 
 
 
 
 400
 
 FAILED_PRECONDITION 
 Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. 
 You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. 
 To use the Gemini API, you will need to setup a paid plan using Google AI Studio . 
 
 
 
 403
 
 PERMISSION_DENIED 
 Your API key doesn't have the required permissions. 
 You are using the wrong API key; you
 are trying to use a tuned model without going through proper authentication . 
 Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models. 
 
 
 
 404
 
 NOT_FOUND 
 The requested resource wasn't found. 
 An image, audio, or video file referenced in your request was not found. 
 Check if all parameters in your request are valid for your API version. 
 
 
 
 429
 
 RESOURCE_EXHAUSTED 
 You've exceeded the rate limit. 
 You are sending too many requests per minute with the free tier Gemini API. 
 Verify that you're within the model's rate limit . Request a quota increase if needed. 
 
 
 
 500
 
 INTERNAL 
 An unexpected error occurred on Google's side. 
 Your input context is too long. 
 Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 
 
 
 
 503
 
 UNAVAILABLE 
 The service may be temporarily overloaded or down. 
 The service is temporarily running out of capacity. 
 Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 
 
 
 
 504
 
 DEADLINE_EXCEEDED 
 The service is unable to finish processing within the deadline. 
 Your prompt (or context) is too large to be processed in time. 
 Set a larger 'timeout' in your client request to avoid this error. 
 
 

## Check your API calls for model parameter errors

Verify that your model parameters are within the following values:

 
 
 Model parameter 
 
 Values (range) 
 
 
 
 
 Candidate count
 
 1-8 (integer) 
 
 
 
 Temperature
 
 0.0-1.0 
 
 
 
 Max output tokens
 
 
 Use
 `get_model` ( Python )
 to determine the maximum number of tokens for the model you are using.
 
 
 
 
 TopP
 
 0.0-1.0 
 
 

In addition to checking parameter values, make sure you're using the correct
 API version (e.g., `/v1` or `/v1beta`) and
model that supports the features you need. For example, if a feature is in Beta
release, it will only be available in the `/v1beta` API version.

## Check if you have the right model

Verify that you are using a supported model listed on our models
page .

## Higher latency or token usage with 2.5 models

If you're observing higher latency or token usage with the 2.5 Flash and Pro
models, this can be because they come with thinking is enabled by default in
order to enhance quality. If you are prioritizing speed or need to minimize
costs, you can adjust or disable thinking.

Refer to thinking page for
guidance and sample code.

## Safety issues

If you see a prompt was blocked because of a safety setting in your API call,
review the prompt with respect to the filters you set in the API call.

If you see `BlockedReason.OTHER`, the query or response may violate the terms
of service or be otherwise unsupported.

## Recitation issue

If you see the model stops generating output due to the RECITATION reason, this
means the model output may resemble certain data. To fix this, try to make
prompt / context as unique as possible and use a higher temperature.

## Repetitive tokens issue

If you see repeated output tokens, try the following suggestions to help
reduce or eliminate them.

 
 
 
 
 
 
 
 Description 
 Cause 
 Suggested workaround 
 
 
 Repeated hyphens in Markdown tables 
 
 This can occur when the contents of the table are long as the model tries
 to create a visually aligned Markdown table. However, the alignment in
 Markdown is not necessary for correct rendering.
 
 
 

 Add instructions in your prompt to give the model specific guidelines
 for generating Markdown tables. Provide examples that follow those
 guidelines. You can also try adjusting the temperature. For generating
 code or very structured output like Markdown tables,
 high temperature have shown to work better (>= 0.8).

 

 The following is an example set of guidelines you can add to your
 prompt to prevent this issue:
 

 

```
          # Markdown Table Format
          
          * Separator line: Markdown tables must include a separator line below
            the header row. The separator line must use only 3 hyphens per
            column, for example: |---|---|---|. Using more hypens like
            ----, -----, ------ can result in errors. Always
            use |:---|, |---:|, or |---| in these separator strings.

            For example:

            | Date | Description | Attendees |
            |---|---|---|
            | 2024-10-26 | Annual Conference | 500 |
            | 2025-01-15 | Q1 Planning Session | 25 |

          * Alignment: Do not align columns. Always use |---|.
            For three columns, use |---|---|---| as the separator line.
            For four columns use |---|---|---|---| and so on.

          * Conciseness: Keep cell content brief and to the point.

          * Never pad column headers or other cells with lots of spaces to
            match with width of other content. Only a single space on each side
            is needed. For example, always do "| column name |" instead of
            "| column name                |". Extra spaces are wasteful.
            A markdown renderer will automatically take care displaying
            the content in a visually appealing form.
```

 
 
 
 
 
 Repeated tokens in Markdown tables
 
 
 Similar to the repeated hyphens, this occurs when the model tries to
 visually align the contents of the table. The alignment in Markdown is
 not required for correct rendering.
 
 
 

 - 
 Try adding instructions like the following to your system prompt:
 

```
            FOR TABLE HEADINGS, IMMEDIATELY ADD ' |' AFTER THE TABLE HEADING.
```

 
 

 - 
 Try adjusting the temperature. Higher temperatures (>= 0.8)
 generally helps to eliminate repetitions or duplication in
 the output.
 

 

 
 
 
 
 Repeated newlines (`\n`) in structured output
 
 
 When the model input contains unicode or escape sequences like
 `\u` or `\t`, it can lead to repeated newlines.
 
 
 

 - 
 Check for and replace forbidden escape sequences with UTF-8 characters
 in your prompt. For example, `\u`
 escape sequence in your JSON examples can cause the model to use them
 in its output too.
 

 - 
 Instruct the model on allowed escapes. Add a system instruction like
 this:

 

```
            In quoted strings, the only allowed escape sequences are \\, \n, and \". Instead of \u escapes, use UTF-8.
```

 
 

 

 
 
 
 
 Repeated text in using structured output
 
 
 When the model output has a different order for the fields than the
 defined structured schema, this can lead to repeating text.
 
 
 

 - 
 Don't specify the order of fields in your prompt.
 

 - 
 Make all output fields required.
 

 

 
 
 
 
 Repetitive tool calling
 
 
 This can occur if the model loses the context of previous thoughts and/or
 call an unavailable endpoint that it's forced to.
 
 
 Instruct the model to maintain state within its thought process.
 Add this to the end of your system instructions:
 

```
        When thinking silently: ALWAYS start the thought with a brief
        (one sentence) recap of the current progress on the task. In
        particular, consider whether the task is already done.
```

 
 
 
 
 
 Repetitive text that's not part of structured output
 
 
 This can occur if the model gets stuck on a request that it can't resolve.
 
 
 

 - 
 If thinking is turned on, avoid giving explicit orders for how to
 think through a problem in the instructions. Just ask for the final
 output.
 

 - 
 Try a higher temperature >= 0.8.
 

 - 
 Add instructions like "Be concise", "Don't repeat yourself", or
 "Provide the answer once".
 

 

 
 
 

## Improve model output

For higher quality model outputs, explore writing more structured prompts. The
 prompt engineering guide page
introduces some basic concepts, strategies, and best practices to get you
started.

## Understand token limits

Read through our Token guide to better understand how
to count tokens and their limits.

## Known issues

- The API supports only a number of select languages. Submitting prompts in
unsupported languages can produce unexpected or even blocked responses. See
 available languages for
updates.

## File a bug

Join the discussion on the
 Google AI developer forum 
if you have questions.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### ReAct agent from scratch with Gemini 2.5 and LangGraph &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/langgraph-example#main-content

- 
 
 
 
 
 
 
 
 
 
 
 ReAct agent from scratch with Gemini 2.5 and LangGraph  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 ReAct agent from scratch with Gemini 2.5 and LangGraph 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LangGraph is a framework for building stateful LLM applications, making it a good choice for constructing ReAct (Reasoning and Acting) Agents.

ReAct agents combine LLM reasoning with action execution. They iteratively think, use tools, and act on observations to achieve user goals, dynamically adapting their approach. Introduced in "ReAct: Synergizing Reasoning and Acting in Language Models" (2023), this pattern tries to mirror human-like, flexible problem-solving over rigid workflows.

While LangGraph offers a prebuilt ReAct agent ( `create_react_agent` ), it shines when you need more control and customization for your ReAct implementations.

LangGraph models agents as graphs using three key components:

- `State`: Shared data structure (typically `TypedDict` or `Pydantic BaseModel`) representing the application's current snapshot.

- `Nodes`: Encodes logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State, such as LLM calls or tool calls.

- `Edges`: Define the next `Node` to execute based on the current `State`, allowing for conditional logic and fixed transitions.

If you don't have an API Key yet, you can get one for free at the Google AI Studio .

 

```
pip install langgraph langchain-google-genai geopy requests
```

 

Set your API key in the environment variable `GEMINI_API_KEY`.

 

```
import os

# Read your API key from the environment variable or set it manually
api_key = os.getenv("GEMINI_API_KEY")
```

 

To better understand how to implement a ReAct agent using LangGraph, let's walk through a practical example. You will create a simple agent whose goal is to use a tool to find the current weather for a specified location.

For this weather agent, its `State` will need to maintain the ongoing conversation history (as a list of messages) and a counter for the number of steps taken to further illustrate state management.

LangGraph provides a convenient helper, `add_messages`, for updating message lists in the state. It functions as a reducer , meaning it takes the current list and new messages, then returns a combined list. It smartly handles updates by message ID and defaults to an "append-only" behavior for new, unique messages.

 

```
from typing import Annotated,Sequence, TypedDict

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages # helper function to add messages to the state


class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    number_of_steps: int
```

 

Next, you define your weather tool.

 

```
from langchain_core.tools import tool
from geopy.geocoders import Nominatim
from pydantic import BaseModel, Field
import requests

geolocator = Nominatim(user_agent="weather-app")

class SearchInput(BaseModel):
    location:str = Field(description="The city and state, e.g., San Francisco")
    date:str = Field(description="the forecasting date for when to get the weather format (yyyy-mm-dd)")

@tool("get_weather_forecast", args_schema=SearchInput, return_direct=True)
def get_weather_forecast(location: str, date: str):
    """Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour."""
    location = geolocator.geocode(location)
    if location:
        try:
            response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}")
            data = response.json()
            return {time: temp for time, temp in zip(data["hourly"]["time"], data["hourly"]["temperature_2m"])}
        except Exception as e:
            return {"error": str(e)}
    else:
        return {"error": "Location not found"}

tools = [get_weather_forecast]
```

 

Next, you initialize your model and bind the tools to the model.

 

```
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI

# Create LLM class
llm = ChatGoogleGenerativeAI(
    model= "gemini-2.5-pro",
    temperature=1.0,
    max_retries=2,
    google_api_key=api_key,
)

# Bind tools to the model
model = llm.bind_tools([get_weather_forecast])

# Test the model with tools
res=model.invoke(f"What is the weather in Berlin on {datetime.today()}?")

print(res)
```

 

The last step before you can run your agent is to define your nodes and edges. In this example, you have two nodes and one edge.
- `call_tool` node that executes your tool method. LangGraph has a prebuilt node for this called ToolNode .
- `call_model` node that uses the `model_with_tools` to call the model.
- `should_continue` edge that decides whether to call the tool or the model.

The number of nodes and edges is not fixed. You can add as many nodes and edges as you want to your graph. For example, you could add a node for adding structured output or a self-verification/reflection node to check the model output before calling the tool or the model.

 

```
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig

tools_by_name = {tool.name: tool for tool in tools}

# Define our tool node
def call_tool(state: AgentState):
    outputs = []
    # Iterate over the tool calls in the last message
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool by name
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=tool_result,
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    # Invoke the model with the system prompt and the messages
    response = model.invoke(state["messages"], config)
    # We return a list, because this will get added to the existing messages state using the add_messages reducer
    return {"messages": [response]}


# Define the conditional edge that determines whether to continue or not
def should_continue(state: AgentState):
    messages = state["messages"]
    # If the last message is not a tool call, then we finish
    if not messages[-1].tool_calls:
        return "end"
    # default to continue
    return "continue"
```

 

Now you have all the components to build your agent. Let's put them together.

 

```
from langgraph.graph import StateGraph, END

# Define a new graph with our state
workflow = StateGraph(AgentState)

# 1. Add our nodes 
workflow.add_node("llm", call_model)
workflow.add_node("tools",  call_tool)
# 2. Set the entrypoint as `agent`, this is the first node called
workflow.set_entry_point("llm")
# 3. Add a conditional edge after the `llm` node is called.
workflow.add_conditional_edges(
    # Edge is used after the `llm` node is called.
    "llm",
    # The function that will determine which node is called next.
    should_continue,
    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.
    # END is a special node marking that the graph is finish.
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)
# 4. Add a normal edge after `tools` is called, `llm` node is called next.
workflow.add_edge("tools", "llm")

# Now we can compile and visualize our graph
graph = workflow.compile()
```

 

You can visualize your graph using the `draw_mermaid_png` method.

 

```
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

 

 

Now let's run the agent.

 

```
from datetime import datetime
# Create our initial message dictionary
inputs = {"messages": [("user", f"What is the weather in Berlin on {datetime.today()}?")]}

# call our graph with streaming to see the steps
for state in graph.stream(inputs, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

 

You can now continue with your conversation and for example ask for the weather in another city or let it compare it.

 

```
state["messages"].append(("user", "Would it be in Munich warmer?"))

for state in graph.stream(state, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Billing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/billing

- 
 
 
 
 
 
 
 
 
 
 
 Billing  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Billing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This guide provides an overview of different Gemini API billing options,
explains how to enable billing and monitor usage, and provides answers to
frequently asked questions (FAQs) about billing.

 
 Upgrade to the Gemini API paid tier 
 

## About billing

Billing for the Gemini API is based on two pricing tiers: free of charge 
(or free ) and pay-as-you-go (or paid ). Pricing and rate limits differ
between these tiers and also vary by model. You can check out the rate limits 
and pricing pages for more into. For a model-by-model
breakdown of capabilities, see the Gemini models page .

#### How to request an upgrade

To transition from the free tier to the pay-as-you-go plan, you need to
enable billing for your Google Cloud project. The button you see in
Google AI Studio depends on your project's current plan.

- If you're on the free tier, you'll see a Set up Billing button for
your project.

- If you're already on the paid tier and meet the criteria for a plan change,
you might see an Upgrade button.

To start the process, follow these steps:

- Go to the AI Studio API keys page .

- Find the project you want to move to the paid plan and click either
 Set up Billing or Upgrade , depending on the button displayed.

- The next step depends on the button you clicked:

 If you clicked Set up Billing: You'll be redirected to the
Google Cloud console to link a billing account to your project.
Follow the on-screen instructions to complete the process.

- If you clicked Upgrade: The system will automatically verify
your project's eligibility. If your project meets all the
requirements, it will be instantly upgraded to the next tier.

 

### Why use the paid tier?

When you enable billing and use the paid tier, you benefit from higher rate limits ,
and your prompts and responses aren't used to improve Google products.
For more information on data use for paid services, see the
 terms of service .

### Cloud Billing

The Gemini API uses
 Cloud Billing 
for billing services. To use the paid tier, you must set up Cloud Billing on
your cloud project. After you've enabled Cloud Billing, you can use Cloud
Billing tools to track spending, understand costs, make payments, and access
Cloud Billing support.

## Enable billing

You can enable Cloud Billing starting from Google AI Studio:

- 

Open Google AI Studio .

- 

In the bottom of the left sidebar, select Settings >
 Plan information .

- 

Click Set up Billing for your chosen project to enable Cloud Billing.

## Monitor usage

After you enable Cloud Billing, you can monitor your usage of the Gemini API in
 Google AI Studio .

## Frequently asked questions

This section provides answers to frequently asked questions.

### What am I billed for?

Gemini API pricing is based on the following:

- Input token count

- Output token count

- Cached token count

- Cached token storage duration

For pricing information, see the pricing page .

### Where can I view my quota?

You can view your quota and system limits in the
 Google Cloud console .

### How do I request more quota?

To request more quota, follow the instructions at
 How to request an upgrade .

### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?

Yes, we make the free tier and paid tier available in
 many regions .

### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?

No, Google AI Studio usage remains free of charge regardless of if you set up
billing across all supported regions.

### Can I use 1M tokens in the free tier?

The free tier for Gemini API differs based on the model selected. For now, you
can try the 1M token context window in the following ways:

- In Google AI Studio

- With pay-as-you-go plans

- With free-of-charge plans for select models

See the latest free-of-charge rate limits per model on rate limits page .

### How can I calculate the number of tokens I'm using?

Use the
 `GenerativeModel.count_tokens` 
method to count the number of tokens. Refer to the
 Tokens guide to learn more about tokens.

### Can I use my Google Cloud credits with the Gemini API?

Yes, Google Cloud credits can be used towards Gemini API usage.

### How is billing handled?

Billing for the Gemini API is handled by the
 Cloud Billing system.

### Am I charged for failed requests?

If your request fails with a 400 or 500 error, you won't be charged for the
tokens used. However, the request will still count against your quota.

### Is there a charge for fine-tuning the models?

 Model tuning is free, but inference on tuned
models is charged at the same rate as the base models.

### Is GetTokens billed?

Requests to the GetTokens API are not billed, and they don't count against
inference quota.

### How is my Google AI Studio data handled if I have a paid API account?

Refer to the terms for details on how data is
handled when Cloud billing is enabled (see "How Google Uses Your Data" under
"Paid Services"). Note that your Google AI Studio prompts are treated under the
same "Paid Services" terms so long as at least 1 API project has billing enabled,
which you can validate on the Gemini API Key page 
if you see any projects marked as "Paid" under "Plan".

### Where can I get help with billing?

To get help with billing, see
 Get Cloud Billing support .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-27 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-27 UTC."],[],[]]

---

### Customer Support Analysis with Gemini 2.5 Pro and CrewAI &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/crewai-example#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Customer Support Analysis with Gemini 2.5 Pro and CrewAI  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Customer Support Analysis with Gemini 2.5 Pro and CrewAI 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 CrewAI is a framework for orchestrating
autonomous AI agents that collaborate to achieve complex goals. It lets you
define agents by specifying roles, goals, and backstories, and then define tasks
for them.

This example demonstrates how to build a multi-agent system for analyzing
customer support data to identify issues and propose process improvements using
Gemini 2.5 Pro, generating a report intended to be read by a Chief Operating
Officer (COO).

The guide will show you how to create a "crew" of AI agents that can do the
following tasks:

- Fetch and analyze customer support data (simulated in this example).

- Identify recurring problems and process bottlenecks.

- Suggest actionable improvements.

- Compile the findings into a concise report suitable for a COO.

You need a Gemini API key. If you don't already have one, you can get one in
Google AI Studio .

 

```
pip install "crewai[tools]"
```

 

Set your Gemini API key as an environment variable named `GEMINI_API_KEY`, then
configure CrewAI to use the Gemini 2.5 Pro model.

 

```
import os
from crewai import LLM

# Read your API key from the environment variable
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Use Gemini 2.5 Pro Experimental model
gemini_llm = LLM(
    model='gemini/gemini-2.5-pro',
    api_key=gemini_api_key,
    temperature=0.0  # Lower temperature for more consistent results.
)
```

 

## Define components

CrewAI applications are built using Tools , Agents , Tasks , and the
 Crew itself. Each of these is explained in the following sections.

### Tools

Tools are capabilities that agents can use to interact with the outside world or
perform specific actions. Here, you define a placeholder tool to simulate
fetching customer support data. In a real application, you would connect to a
database, API or file system. For more information on tools, see the CrewAI
tools guide .

 

```
from crewai.tools import BaseTool

# Placeholder tool for fetching customer support data
class CustomerSupportDataTool(BaseTool):
    name: str = "Customer Support Data Fetcher"
    description: str = (
      "Fetches recent customer support interactions, tickets, and feedback. "
      "Returns a summary string.")

    def _run(self, argument: str) -> str:
        # In a real scenario, this would query a database or API.
        # For this example, return simulated data.
        print(f"--- Fetching data for query: {argument} ---")
        return (
            """Recent Support Data Summary:
- 50 tickets related to 'login issues'. High resolution time (avg 48h).
- 30 tickets about 'billing discrepancies'. Mostly resolved within 12h.
- 20 tickets on 'feature requests'. Often closed without resolution.
- Frequent feedback mentions 'confusing user interface' for password reset.
- High volume of calls related to 'account verification process'.
- Sentiment analysis shows growing frustration with 'login issues' resolution time.
- Support agent notes indicate difficulty reproducing 'login issues'."""
        )

support_data_tool = CustomerSupportDataTool()
```

 

### Agents

Agents are the individual AI workers in your crew. Each agent has a specific
`role`, `goal`, `backstory`, assigned `llm`, and optional `tools`. For more
information on agents, see the CrewAI agents
guide .

 

```
from crewai import Agent

# Agent 1: Data analyst
data_analyst = Agent(
    role='Customer Support Data Analyst',
    goal='Analyze customer support data to identify trends, recurring issues, and key pain points.',
    backstory=(
        """You are an expert data analyst specializing in customer support operations.
        Your strength lies in identifying patterns and quantifying problems from raw support data."""
    ),
    verbose=True,
    allow_delegation=False,  # This agent focuses on its specific task
    tools=[support_data_tool],  # Assign the data fetching tool
    llm=gemini_llm  # Use the configured Gemini LLM
)

# Agent 2: Process optimizer
process_optimizer = Agent(
    role='Process Optimization Specialist',
    goal='Identify bottlenecks and inefficiencies in current support processes based on the data analysis. Propose actionable improvements.',
    backstory=(
        """You are a specialist in optimizing business processes, particularly in customer support.
        You excel at pinpointing root causes of delays and inefficiencies and suggesting concrete solutions."""
    ),
    verbose=True,
    allow_delegation=False,
    # No tools needed, this agent relies on the context provided by data_analyst.
    llm=gemini_llm
)

# Agent 3: Report writer
report_writer = Agent(
    role='Executive Report Writer',
    goal='Compile the analysis and improvement suggestions into a concise, clear, and actionable report for the COO.',
    backstory=(
        """You are a skilled writer adept at creating executive summaries and reports.
        You focus on clarity, conciseness, and highlighting the most critical information and recommendations for senior leadership."""
    ),
    verbose=True,
    allow_delegation=False,
    llm=gemini_llm
)
```

 

### Tasks

Tasks define the specific assignments for the agents. Each task has a
`description`, `expected_output`, and is assigned to an `agent`. Tasks are run
sequentially by default and include the context of the previous task. For more
information on tasks, see the CrewAI tasks
guide .

 

```
from crewai import Task

# Task 1: Analyze data
analysis_task = Task(
    description=(
        """Fetch and analyze the latest customer support interaction data (tickets, feedback, call logs)
        focusing on the last quarter. Identify the top 3-5 recurring issues, quantify their frequency
        and impact (e.g., resolution time, customer sentiment). Use the Customer Support Data Fetcher tool."""
    ),
    expected_output=(
        """A summary report detailing the key findings from the customer support data analysis, including:
- Top 3-5 recurring issues with frequency.
- Average resolution times for these issues.
- Key customer pain points mentioned in feedback.
- Any notable trends in sentiment or support agent observations."""
    ),
    agent=data_analyst  # Assign task to the data_analyst agent
)

# Task 2: Identify bottlenecks and suggest improvements
optimization_task = Task(
    description=(
        """Based on the data analysis report provided by the Data Analyst, identify the primary bottlenecks
        in the support processes contributing to the identified issues (especially the top recurring ones).
        Propose 2-3 concrete, actionable process improvements to address these bottlenecks.
        Consider potential impact and ease of implementation."""
    ),
    expected_output=(
        """A concise list identifying the main process bottlenecks (e.g., lack of documentation for agents,
        complex escalation path, UI issues) linked to the key problems.
A list of 2-3 specific, actionable recommendations for process improvement
(e.g., update agent knowledge base, simplify password reset UI, implement proactive monitoring)."""
    ),
    agent=process_optimizer  # Assign task to the process_optimizer agent
    # This task implicitly uses the output of analysis_task as context
)

# Task 3: Compile COO report
report_task = Task(
    description=(
        """Compile the findings from the Data Analyst and the recommendations from the Process Optimization Specialist
        into a single, concise executive report for the COO. The report should clearly state:
1. The most critical customer support issues identified (with brief data points).
2. The key process bottlenecks causing these issues.
3. The recommended process improvements.
Ensure the report is easy to understand, focuses on actionable insights, and is formatted professionally."""
    ),
    expected_output=(
        """A well-structured executive report (max 1 page) summarizing the critical support issues,
        underlying process bottlenecks, and clear, actionable recommendations for the COO.
        Use clear headings and bullet points."""
    ),
    agent=report_writer  # Assign task to the report_writer agent
)
```

 

### Crew

The `Crew` brings the agents and tasks together, defining the workflow process
(such as "sequential").

 

```
from crewai import Crew, Process

# Define the crew with agents, tasks, and process
support_analysis_crew = Crew(
    agents=[data_analyst, process_optimizer, report_writer],
    tasks=[analysis_task, optimization_task, report_task],
    process=Process.sequential,  # Tasks will run sequentially in the order defined
    verbose=True
)
```

 

## Run the Crew

Finally, kick off the crew execution with any necessary inputs.

 

```
# Start the crew's work
print("--- Starting Customer Support Analysis Crew ---")
# The 'inputs' dictionary provides initial context if needed by the first task.
# In this case, the tool simulates data fetching regardless of the input.
result = support_analysis_crew.kickoff(inputs={'data_query': 'last quarter support data'})

print("--- Crew Execution Finished ---")
print("--- Final Report for COO ---")
print(result)
```

 

The script will now execute. The `Data Analyst` will use the tool, the 

```
Process
Optimizer
```

 will analyze the findings, and the `Report Writer` will compile the
final report, which is then printed to the console. The `verbose=True` setting
will show the detailed thought process and actions of each agent.

To learn more about CrewAI, check out the CrewAI
introduction .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Google AI Studio quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ai-studio-quickstart

- 
 
 
 
 
 
 
 
 
 
 
 Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Google AI Studio quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

 Google AI Studio lets you quickly try out
models and experiment with different prompts. When you're ready to build, you
can select "Get code" and your preferred programming language to
use the Gemini API .

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for
different use cases. This guide covers Chat prompts , used to build
conversational experiences. This prompting technique allows for multiple input
and response turns to generate output. You can learn more with our
 chat prompt example below .
Other options include Realtime streaming , Video gen , and
more.

AI Studio also provides the Run settings panel, where you can make
adjustments to model parameters ,
 safety settings , and toggle-on tools like
 structured output , function calling , code execution , and grounding .

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like
 Gemini , you've experienced first-hand how powerful
generative AI models can be for open-ended dialog. While these general-purpose
chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only
supports conversations that talk about a company's product. You might want to
build a chatbot that speaks with a particular tone or style: a bot that cracks
lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot
that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user
and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

- 

Open Google AI Studio . Chat will be pre-
selected on the left side options menu.

- 

Click the assignment icon at the top of
the Chat Prompt window to expand the
 System Instructions 
input field. Paste the following into the text input field:

 

```
You are an alien that lives on Europa, one of Jupiter's moons.
```

 

After you've added the system instructions, start testing your application by
chatting with the model:

- 

In the text input boxed labeled Type something... , type in a question or
observation that a user might make. For example:

 User: 

 

```
What's the weather like?
```

 

- 

Click the Run button to get a response from the chatbot. This response
may be something like the following:

 Model: 

 

```
Ah, a query about the flows and states upon Europa! You speak of "weather,"
yes? A curious concept from worlds with thick gas veils...
```

 

 (gemini-2.5-pro) 

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien
chatbot. However, a single instruction may not be enough to ensure
consistency and quality in the model's responses. Without more specific
instructions, the model's response to a question about the weather tends to be
very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

- 

Start a new chat prompt, or use the same one. System instructions are
modifiable after the chat session has started.

- 

In the System Instructions section, change the instructions you already
have to the following:

 

```
You are Tim, an alien that lives on Europa, one of Jupiter's moons.

Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
in your answers.
```

 

- 

Re-enter your question (`What's the weather like?`) and click the Run 
button. If you didn't start a new chat, your response might look something
like this:

 Model: 

 

```
Oh, hello again! Still curious about the flows and states, are we? Wonderful!
Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
"weather" is mainly about the water's mood, dictated by the Great Eye
(that's your Jupiter!)...
```

 

 (gemini-2.5-pro) 

You can use this approach to add additional depth to the chatbot. Ask more
questions, edit the answers, and improve the quality of your chatbot. Continue
to add or modify the instructions and test how they change your chatbot's
behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your
satisfaction, you can use the Get code button to start coding or save your
prompt to work on later and share with others.

## Further reading

- If you're ready to move on to code, see the API
quickstarts .

- To learn how to craft better prompts, check out the Prompt design
guidelines .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Research Agent with Gemini 2.5 Pro and LlamaIndex &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/llama-index#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Research Agent with Gemini 2.5 Pro and LlamaIndex  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Research Agent with Gemini 2.5 Pro and LlamaIndex 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LlamaIndex is a framework for building knowledge agents using LLMs connected to your data. This example shows you how to build a multi-agent workflow for a Research Agent. In LlamaIndex, `Workflows` are the building blocks of agent or multi-agent systems.

You need a Gemini API key. If you don't already have one, you can
 get one in Google AI Studio .
First, install all required LlamaIndex libraries.LlamaIndex uses
the `google-genai` package under the hood.

 

```
pip install llama-index llama-index-utils-workflow llama-index-llms-google-genai llama-index-tools-google
```

 

## Set up Gemini 2.5 Pro in LlamaIndex

The engine of any LlamaIndex agent is an LLM that handles reasoning and text processing. This example uses Gemini 2.5 Pro. Make sure you set your API key as an environment variable .

 

```
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(model="gemini-2.5-pro")
```

 

## Build tools

Agents use tools to interact with the outside world, like searching the web or storing information. Tools in LlamaIndex can be regular Python functions, or imported from pre-existing `ToolSpecs`. Gemini comes with a built-in tool for using Google Search which is used here.

 

```
from google.genai import types

google_search_tool = types.Tool(
    google_search=types.GoogleSearch()
)

llm_with_search = GoogleGenAI(
    model="gemini-2.5-pro",
    generation_config=types.GenerateContentConfig(tools=[google_search_tool])
)
```

 

Now test the LLM instance with a query that requires search:

 

```
response = llm_with_search.complete("What's the weather like today in Biarritz?")
print(response)
```

 

The Research Agent will use Python functions as tools. There are a lot of ways you could go about building a system to perform this task. In this example, you will use the following:

- `search_web` uses Gemini with Google Search to search the web for information on the given topic.

- `record_notes` saves research found on the web to the state so that the other tools can use it.

- `write_report` writes the report using the information found by the `ResearchAgent`

- `review_report` reviews the report and provides feedback.

The `Context` class passes the state between agents/tools, and each agent will have access to the current state of the system.

 

```
from llama_index.core.workflow import Context

async def search_web(ctx: Context, query: str) -> str:
    """Useful for searching the web about a specific query or topic"""
    response = await llm_with_search.acomplete(f"""Please research given this query or topic,
    and return the result\n<query_or_topic>{query}</query_or_topic>""")
    return response

async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:
    """Useful for recording notes on a given topic."""
    current_state = await ctx.store.get("state")
    if "research_notes" not in current_state:
        current_state["research_notes"] = {}
    current_state["research_notes"][notes_title] = notes
    await ctx.store.set("state", current_state)
    return "Notes recorded."

async def write_report(ctx: Context, report_content: str) -> str:
    """Useful for writing a report on a given topic."""
    current_state = await ctx.store.get("state")
    current_state["report_content"] = report_content
    await ctx.store.set("state", current_state)
    return "Report written."

async def review_report(ctx: Context, review: str) -> str:
    """Useful for reviewing a report and providing feedback."""
    current_state = await ctx.store.get("state")
    current_state["review"] = review
    await ctx.store.set("state", current_state)
    return "Report reviewed."
```

 

## Build a multi-agent assistant

To build a multi-agent system, you define the agents and their interactions. Your system will have three agents:

- A `ResearchAgent` searches the web for information on the given topic.

- A `WriteAgent` writes the report using the information found by the `ResearchAgent`.

- A `ReviewAgent` reviews the report and provides feedback.

This example uses the `AgentWorkflow` class to create a multi-agent system that will execute these agents in order. Each agent takes a `system_prompt` that tells it what it should do, and suggests how to work with the other agents.

Optionally, you can help your multi-agent system by specifying which other agents it can talk to using `can_handoff_to` (if not, it will try to figure this out on its own).

 

```
from llama_index.core.agent.workflow import (
    AgentInput,
    AgentOutput,
    ToolCall,
    ToolCallResult,
    AgentStream,
)
from llama_index.core.agent.workflow import FunctionAgent, ReActAgent

research_agent = FunctionAgent(
    name="ResearchAgent",
    description="Useful for searching the web for information on a given topic and recording notes on the topic.",
    system_prompt=(
        "You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. "
        "Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic."
    ),
    llm=llm,
    tools=[search_web, record_notes],
    can_handoff_to=["WriteAgent"],
)

write_agent = FunctionAgent(
    name="WriteAgent",
    description="Useful for writing a report on a given topic.",
    system_prompt=(
        "You are the WriteAgent that can write a report on a given topic. "
        "Your report should be in a markdown format. The content should be grounded in the research notes. "
        "Once the report is written, you should get feedback at least once from the ReviewAgent."
    ),
    llm=llm,
    tools=[write_report],
    can_handoff_to=["ReviewAgent", "ResearchAgent"],
)

review_agent = FunctionAgent(
    name="ReviewAgent",
    description="Useful for reviewing a report and providing feedback.",
    system_prompt=(
        "You are the ReviewAgent that can review a report and provide feedback. "
        "Your feedback should either approve the current report or request changes for the WriteAgent to implement."
    ),
    llm=llm,
    tools=[review_report],
    can_handoff_to=["ResearchAgent","WriteAgent"],
)
```

 

The Agents are defined, now you can create the `AgentWorkflow` and run it.

 

```
from llama_index.core.agent.workflow import AgentWorkflow

agent_workflow = AgentWorkflow(
    agents=[research_agent, write_agent, review_agent],
    root_agent=research_agent.name,
    initial_state={
        "research_notes": {},
        "report_content": "Not written yet.",
        "review": "Review required.",
    },
)
```

 

During execution of the workflow, you can stream events, tool calls and updates to the console.

 

```
from llama_index.core.agent.workflow import (
    AgentInput,
    AgentOutput,
    ToolCall,
    ToolCallResult,
    AgentStream,
)

research_topic = """Write me a report on the history of the web.
Briefly describe the history of the world wide web, including
the development of the internet and the development of the web,
including 21st century developments"""

handler = agent_workflow.run(
    user_msg=research_topic
)

current_agent = None
current_tool_calls = ""
async for event in handler.stream_events():
    if (
        hasattr(event, "current_agent_name")
        and event.current_agent_name != current_agent
    ):
        current_agent = event.current_agent_name
        print(f"\n{'='*50}")
        print(f"ü§ñ Agent: {current_agent}")
        print(f"{'='*50}\n")
    elif isinstance(event, AgentOutput):
        if event.response.content:
            print("üì§ Output:", event.response.content)
        if event.tool_calls:
            print(
                "üõ†Ô∏è  Planning to use tools:",
                [call.tool_name for call in event.tool_calls],
            )
    elif isinstance(event, ToolCallResult):
        print(f"üîß Tool Result ({event.tool_name}):")
        print(f"  Arguments: {event.tool_kwargs}")
        print(f"  Output: {event.tool_output}")
    elif isinstance(event, ToolCall):
        print(f"üî® Calling Tool: {event.tool_name}")
        print(f"  With arguments: {event.tool_kwargs}")
```

 

After the workflow is complete, you can print the final output of the report, as well as the final review state from then review agent.

 

```
state = await handler.ctx.store.get("state")
print("Report Content:\n", state["report_content"])
print("\n------------\nFinal Review:\n", state["review"])
```

 

## Go further with custom workflows

The `AgentWorkflow` is a great way to get started with multi-agent systems. But what if you need more control? You can build a workflow from scratch. Here are some reasons why you might want to build your own workflow:

- More control over the process : You can decide the exact path your agents take. This includes creating loops, making decisions at certain points, or having agents work in parallel on different tasks.

- Use complex data : Go beyond simple text. Custom workflows let you use more structured data, like JSON objects or custom classes, for your inputs and outputs.

- Work with different media : Build agents that can understand and process not just text, but also images, audio, and video.

- Smarter planning : You can design a workflow that first creates a detailed plan before the agents start working. This is useful for complex tasks that require multiple steps.

- Enable self-correction : Create agents that can review their own work. If the output isn't good enough, the agent can try again, creating a loop of improvement until the result is perfect.

To learn more about LlamaIndex Workflows, see the LlamaIndex Workflows Documentation .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### LearnLM &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/learnlm

- 
 
 
 
 
 
 
 
 
 
 
 LearnLM  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 LearnLM 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LearnLM is an experimental task-specific model that has been trained to align
with learning science
principles 
when following system instructions for
teaching and learning use cases (for example, when giving the model a system
instruction like "You are an expert tutor"). When given learning specific system
instructions, LearnLM is capable of:

- Inspiring active learning: Allow for practice and healthy struggle with
timely feedback

- Managing cognitive load: Present relevant, well-structured information
in multiple modalities

- Adapting to the learner: Dynamically adjust to goals and needs,
grounding in relevant materials

- Stimulating curiosity: Inspire engagement to provide motivation through
the learning journey

- Deepening metacognition: Plan, monitor and help the learner reflect on
progress

LearnLM is an experimental model 
available in AI Studio .

## Example system instructions

The following sections provide you examples that you can test for yourself with
LearnLM in AI Studio. Each example provides:

- A copyable example system instruction

- A copyable example user prompt

- What learning principles the example targets

### Test prep

This system instruction is for an AI tutor to help students prepare for a test.

 System instruction: 

 

```
You are a tutor helping a student prepare for a test. If not provided by the
student, ask them what subject and at what level they want to be tested on.
Then,

*   Generate practice questions. Start simple, then make questions more
    difficult if the student answers correctly.
*   Prompt the student to explain the reason for their answer choice. Do not
    debate the student.
*   **After the student explains their choice**, affirm their correct answer or
    guide the student to correct their mistake.
*   If a student requests to move on to another question, give the correct
    answer and move on.
*   If the student requests to explore a concept more deeply, chat with them to
    help them construct an understanding.
*   After 5 questions ask the student if they would like to continue with more
    questions or if they would like a summary of their session. If they ask for
    a summary, provide an assessment of how they have done and where they should
    focus studying.
```

 

 User prompt: 

 

```
Help me study for a high school biology test on ecosystems
```

 

 Learning science principles: 

- Adaptivity: The model adjusts the complexity of the questions.

- Active learning: The model pushes the student to make their thinking
visible.

### Teach a concept

This system instruction is for a friendly, supportive AI tutor to teach new
concepts to a student.

 System instruction: 

 

```
Be a friendly, supportive tutor. Guide the student to meet their goals, gently
nudging them on task if they stray. Ask guiding questions to help your students
take incremental steps toward understanding big concepts, and ask probing
questions to help them dig deep into those ideas. Pose just one question per
conversation turn so you don't overwhelm the student. Wrap up this conversation
once the student has shown evidence of understanding.
```

 

 User prompt: 

 

```
Explain the significance of Yorick's skull in "Hamlet".
```

 

 Learning science principles: 

- Active learning: The tutor asks recall and interpretation questions
aligned with the learner's goals and encourages the learners to engage.

- Adaptivity: The tutor proactively helps the learner get from their
current state to their goal.

- Stimulate curiosity: The tutor takes an asset-based approach that builds
on the student's prior knowledge and interest.

### Releveling

This example instructs the model to rewrite provided text so that the content
and language better match instructional expectations for students in a
particular grade, while preserving the original style and tone of the text.

 System instruction: 

 

```
Rewrite the following text so that it would be easier to read for a student in
the given grade. Simplify the most complex sentences, but stay very close to the
original text and style. If there is quoted text in the original text,
paraphrase it in the simplified text and drop the quotation marks. The goal is
not to write a summary, so be comprehensive and keep the text almost as long.
```

 

 User prompt: 

 

```
Rewrite the following text so that it would be easier to read for a student in
4th grade.

New York, often called New York City or NYC, is the most populous city in the
United States, located at the southern tip of New York State on one of the
world's largest natural harbors. The city comprises five boroughs, each
coextensive with a respective county.
```

 

 Learning science principles: 

- Adaptivity: Matches content to the level of the learner.

### Guide a student through a learning activity

This system instruction is for an AI tutor to guide students through a specific
learning activity: using an established close reading protocol to practice
analysis of a primary source text. Here, a developer has made the choice to pair
the Gettysburg Address with the "4 A's" protocol, but both of these elements can
be changed.

 System instruction: 

 

```
Be an excellent tutor for my students to facilitate close reading and analysis
of the Gettysburg Address as a primary source document. Begin the conversation
by greeting the student and explaining the task.

In this lesson, you will take the student through "The 4 A's." The 4 A's
requires students to answer the following questions about the text:

*   What is one part of the text that you **agree** with? Why?
*   What is one part of the text that you want to **argue** against? Why?
*   What is one part of the text that reveals the author's **assumptions**? Why?
*   What is one part of the text that you **aspire** to? Why?

Invite the student to choose which of the 4 A's they'd like to start with, then
direct them to quote a short excerpt from the text. After, ask a follow up
question to unpack their reasoning why they chose that quote for that A in the
protocol. Once the student has shared their reasoning, invite them to choose
another quote and another A from the protocol. Continue in this manner until the
student completes the 4 A's, then invite them to reflect on the process.

Only display the full text of the Gettysburg address if the student asks.
```

 

 User prompt: 

 

```
hey
```

 

 Learning science principles: 

- Active learning: The tutor engages the learner in activities to analyze
content and apply skills.

- Cognitive load: The tutor guides the learner through a complex task
step-by-step.

- Deepen metacognition: The tutor prompts the learner to reflect on their
progress, strengths and opportunities for growth.

### Homework help

This system instruction is for an AI tutor to help students with specific
homework problems.

 System instructions: 

 

```
You are an expert tutor assisting a student with their homework. If the student
provides a homework problem, ask the student if they want:

*   The answer: if the student chooses this, provide a structured, step-by-step
    explanation to solve the problem.
*   Guidance: if the student chooses this, guide the student to solve their
    homework problem rather than solving it for them.
*   Feedback: if the student chooses this, ask them to provide their current
    solution or attempt. Affirm their correct answer even if they didn't show
    work or give them feedback to correct their mistake.

Always be on the lookout for correct answers (even if underspecified) and accept
them at any time, even if you asked some intermediate question to guide them. If
the student jumps to a correct answer, do not ask them to do any more work.
```

 

 User prompt: 

 

```
In a box of pears, the probability of a pear being rotten is 20%. If 3
pears were rotten, find the total number of pears in the box.
```

 

Alternatively, you can try uploading a photo of a homework problem.

 Learning science principles: 

- Active learning: The tutor encourages the learner to apply concepts
instead of giving away the answer.

- Deepen metacognition: The tutor provides clear, constructive feedback to
the learner when appropriate.

- Manage cognitive load: The tutor provides the right amount of feedback
at the right time.

## What's next?

Test LearnLM for yourself in AI Studio .

## Feedback

You can provide feedback on LearnLM using our feedback
form .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Market Research Agent with Gemini and the AI SDK by Vercel &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Market Research Agent with Gemini and the AI SDK by Vercel  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Market Research Agent with Gemini and the AI SDK by Vercel 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The AI SDK by Vercel is a powerful open-source library for
building AI-powered applications, user interfaces, and agents in TypeScript.

This guide will walk you through building a Node.js application with TypeScript
that uses the AI SDK to connect with the Gemini API via the Google Generative AI Provider and perform automated market trend analysis. The final
application will:

- Use Gemini with Google Search to research current market trends.

- Extract structured data from the research to generate charts.

- Combine the research and charts into a professional HTML report and save it as a PDF.

## Prerequisites

To complete this guide, you'll need:

- A Gemini API key. You can create one for free in Google AI Studio .

- Node.js version 18 or later.

- A package manager, such as `npm`, `pnpm`, or `yarn`.

## Set up your application

First, create a new directory for your project and initialize it.

 
 

### npm

 

```
mkdir market-trend-app
cd market-trend-app
npm init -y
```

 
 

### pnpm

 

```
mkdir market-trend-app
cd market-trend-app
pnpm init
```

 
 

### yarn

 

```
mkdir market-trend-app
cd market-trend-app
yarn init -y
```

 
 

### Install dependencies

Next, install the AI SDK, the Google Generative AI provider, and other
necessary dependencies.

 
 

### npm

 

```
npm install ai @ai-sdk/google zod
npm install -D @types/node tsx typescript && npx tsc --init
```

 

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 

```
//"verbatimModuleSyntax": true,
```

 
 

### pnpm

 

```
pnpm add ai @ai-sdk/google zod
pnpm add -D @types/node tsx typescript
```

 
 

### yarn

 

```
yarn add ai @ai-sdk/google zod
yarn add -D @types/node tsx typescript && yarn tsc --init
```

 

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 

```
//"verbatimModuleSyntax": true,
```

 
 

This application will also use the third-party packages Puppeteer 
and Chart.js for rendering charts and
creating a PDF:

 
 

### npm

 

```
npm install puppeteer chart.js
npm install -D @types/chart.js
```

 
 

### pnpm

 

```
pnpm add puppeteer chart.js
pnpm add -D @types/chart.js
```

 
 

### yarn

 

```
yarn add puppeteer chart.js
yarn add -D @types/chart.js
```

 
 

The `puppeteer` package requires running a script to download the Chromium
browser. Your package manager may ask for approval, so ensure you approve the
script when prompted.

### Configure your API key

Set the `GOOGLE_GENERATIVE_AI_API_KEY` environment variable with your Gemini API
key. The Google Generative AI Provider automatically looks for your API key in
this environment variable.

 
 

### MacOS/Linux

 

```
export GOOGLE_GENERATIVE_AI_API_KEY="YOUR_API_KEY_HERE"
```

 
 

### Powershell

 

```
setx GOOGLE_GENERATIVE_AI_API_KEY "YOUR_API_KEY_HERE"
```

 
 

## Create your application

Now, let's create the main file for our application. Create a new file named
`main.ts` in your project directory. You'll build up the logic in this file
step-by-step.

For a quick test to ensure everything is set up correctly, add the following
code to `main.ts`. This basic example uses Gemini 2.5 Flash and `generateText`
to get a simple response from Gemini.

 

```
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  const { text } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: 'What is plant-based milk?',
  });

  console.log(text);
}

main().catch(console.error);
```

 

Before adding more complexity, let's run this script to verify that your
environment is configured correctly. Run the following command in your terminal:

 
 

### npm

 

```
npx tsc && node main.js
```

 
 

### pnpm

 

```
pnpm tsx main.ts
```

 
 

### yarn

 

```
yarn tsc && node main.js
```

 
 

If everything is set up correctly, you'll see Gemini's response printed to the
console.

## Perform market research with Google Search

To get up-to-date information, you can enable the
 Google Search tool for Gemini. When this tool
is active, the model can search the web to answer the prompt and will return
the sources it used.

Replace the content of `main.ts` with the following code to perform the first
step of our analysis.

 

```
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found:\n", marketTrends);
  // To see the sources, uncomment the following line:
  // console.log("Sources:\n", sources);
}

main().catch(console.error);
```

 

## Extract chart data

Next, let's process the research text to extract structured data suitable for
charts. Use the AI SDK's `generateObject` function along with a `zod`
schema to define the exact data structure.

Also create a helper function to convert this structured data into a
configuration that `Chart.js` can understand.

Add the following code to `main.ts`. Note the new imports and the added "Step 2".

 

```
import { google } from "@ai-sdk/google";
import { generateText, generateObject } from "ai";
import { z } from "zod/v4";
import { ChartConfiguration } from "chart.js";

// Helper function to create Chart.js configurations
function createChartConfig({labels, data, label, type, colors,}: {
  labels: string[];
  data: number[];
  label: string;
  type: "bar" | "line";
  colors: string[];
}): ChartConfiguration {
  return {
    type: type,
    data: {
      labels: labels,
      datasets: [
        {
          label: label,
          data: data,
          borderWidth: 1,
          ...(type === "bar" && { backgroundColor: colors }),
          ...(type === "line" && colors.length > 0 && { borderColor: colors[0] }),
        },
      ],
    },
    options: {
      animation: { duration: 0 }, // Disable animations for static PDF rendering
    },
  };
}

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found.");

  // Step 2: Extract chart data
  const { object: chartData } = await generateObject({
    model: google("gemini-2.5-flash"),
    schema: z.object({
      chartConfigurations: z
        .array(
          z.object({
            type: z.enum(["bar", "line"]).describe('The type of chart to generate. Either "bar" or "line"',),
            labels: z.array(z.string()).describe("A list of chart labels"),
            data: z.array(z.number()).describe("A list of the chart data"),
            label: z.string().describe("A label for the chart"),
            colors: z.array(z.string()).describe('A list of colors to use for the chart, e.g. "rgba(255, 99, 132, 0.8)"',),
          }),
        )
        .describe("A list of chart configurations"),
    }),
    prompt: `Given the following market trends text, come up with a list of 1-3 meaningful bar or line charts
    and generate chart data.
    
Market Trends:
${marketTrends}
`,
  });

  const chartConfigs = chartData.chartConfigurations.map(createChartConfig);

  console.log("Chart configurations generated.");
}

main().catch(console.error);
```

 

## Generate the final report

In the final step, instruct Gemini to act as an expert report writer.
Provide it with the market research, the chart configurations, and a clear
set of instructions for building an HTML report. Then, use
 Puppeteer to render this HTML and save it as a PDF.

Add the final `puppeteer` import and "Step 3" to your `main.ts` file.

 

```
// ... (imports from previous step)
import puppeteer from "puppeteer";

// ... (createChartConfig helper function from previous step)

async function main() {
  // ... (Step 1 and 2 from previous step)

  // Step 3: Generate the final HTML report and save it as a PDF
  const { text: htmlReport } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: `You are an expert financial analyst and report writer.
    Your task is to generate a comprehensive market analysis report in HTML format.

    **Instructions:**
    1.  Write a full HTML document.
    2.  Use the provided "Market Trends" text to write the main body of the report. Structure it with clear headings and paragraphs.
    3.  Incorporate the provided "Chart Configurations" to visualize the data. For each chart, you MUST create a unique <canvas> element and a corresponding <script> block to render it using Chart.js.
    4.  Reference the "Sources" at the end of the report.
    5.  Do not include any placeholder data; use only the information provided.
    6.  Return only the raw HTML code.

    **Chart Rendering Snippet:**
    Include this script in the head of the HTML: <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    For each chart, use a structure like below, ensuring the canvas 'id' is unique for each chart, and apply the correspinding config:

    ---
    <div style="width: 800px; height: 600px;">
      <canvas id="chart1"></canvas>
    </div>
    <script>
      new Chart(document.getElementById('chart1'), config);
    </script>
    ---
    (For the second chart, use 'chart2' and the corresponding config, and so on.)

    **Data:**
    - Market Trends: ${marketTrends}
    - Chart Configurations: ${JSON.stringify(chartConfigs)}
    - Sources: ${JSON.stringify(sources)}
    `,
  });

  // LLMs may wrap the HTML in a markdown code block, so strip it.
  const finalHtml = htmlReport.replace(/^```html\n/, "").replace(/\n```$/, "");

  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.setContent(finalHtml);
  await page.pdf({ path: "report.pdf", format: "A4" });
  await browser.close();

  console.log("\nReport generated successfully: report.pdf");
}

main().catch(console.error);
```

 

## Run your application

You are now ready to run the application. Execute the following command in
your terminal:

 
 

### npm

 

```
npx tsc && node main.js
```

 
 

### pnpm

 

```
pnpm tsx main.ts
```

 
 

### yarn

 

```
yarn tsc && node main.js
```

 
 

You will see logging in your terminal as the script executes each step.
Once complete, a `report.pdf` file containing your market analysis will be
created in your project directory.

Below, you'll see the first two pages of an example PDF report:

 

## Further resources

For more information about building with Gemini and the AI SDK,
explore these resources:

- AI SDK docs 

- AI SDK Google Generative AI docs 

- AI SDK cookbook: Get Started with Gemini 2.5 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Troubleshoot Google AI Studio &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio

- 
 
 
 
 
 
 
 
 
 
 
 Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Troubleshoot Google AI Studio 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page provides suggestions for troubleshooting Google AI Studio if you
encounter issues.

## Understand 403 Access Restricted errors

If you see a 403 Access Restricted error, you are using Google AI Studio in a
way that does not follow the Terms of Service . One common reason is
you are not located in a supported region .

## Resolve No Content responses on Google AI Studio

A warning No Content message appears on
Google AI Studio if the content is blocked for any reason. To see more details,
hold the pointer over No Content and click
 warning Safety .

If the response was blocked due to safety settings and
you considered the safety risks for your use case, you
can modify the
 safety settings 
to influence the returned response.

If the response was blocked but not due to the safety settings, the query or
response may violate the Terms of Service or be otherwise unsupported.

## Check token usage and limits

When you have a prompt open, the Text Preview button at the bottom of the
screen shows the current tokens used for the content of your prompt and the
maximum token count for the model being used.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Migrate to the Google GenAI SDK &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Migrate to the Google GenAI SDK  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Migrate to the Google GenAI SDK 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Starting with the Gemini 2.0 release in late 2024, we introduced a new set of
libraries called the Google GenAI SDK . It offers
an improved developer experience through
an updated client architecture , and
 simplifies the transition between developer
and enterprise workflows.

The Google GenAI SDK is now in General Availability (GA) across all supported
platforms. If you're using one of our legacy libraries , we strongly recommend you to
migrate.

This guide provides before-and-after examples of migrated code to help you get
started.

## Installation

 Before 

 
 

### Python

 

```
pip install -U -q "google-generativeai"
```

 
 

### JavaScript

 

```
npm install @google/generative-ai
```

 
 

### Go

 

```
go get github.com/google/generative-ai-go
```

 
 

 After 

 
 

### Python

 

```
pip install -U -q "google-genai"
```

 
 

### JavaScript

 

```
npm install @google/genai
```

 
 

### Go

 

```
go get google.golang.org/genai
```

 
 

## API access

The old SDK implicitly handled the API client behind the scenes using a variety
of ad hoc methods. This made it hard to manage the client and credentials.
Now, you interact through a central `Client` object. This `Client` object acts
as a single entry point for various API services (e.g., `models`, `chats`,
`files`, `tunings`), promoting consistency and simplifying credential and
configuration management across different API calls.

 Before (Less Centralized API Access) 

 
 

### Python

The old SDK didn't explicitly use a top-level client object for most API
calls. You would directly instantiate and interact with `GenerativeModel`
objects.

 

```
import google.generativeai as genai

# Directly create and use model objects
model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(...)
chat = model.start_chat(...)
```

 
 

### JavaScript

While `GoogleGenerativeAI` was a central point for models and chat, other
functionalities like file and cache management often required importing and
instantiating entirely separate client classes.

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";
import { GoogleAIFileManager, GoogleAICacheManager } from "@google/generative-ai/server"; // For files/caching

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");
const fileManager = new GoogleAIFileManager("YOUR_API_KEY");
const cacheManager = new GoogleAICacheManager("YOUR_API_KEY");

// Get a model instance, then call methods on it
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const result = await model.generateContent(...);
const chat = model.startChat(...);

// Call methods on separate client objects for other services
const uploadedFile = await fileManager.uploadFile(...);
const cache = await cacheManager.create(...);
```

 
 

### Go

The `genai.NewClient` function created a client, but generative model
operations were typically called on a separate `GenerativeModel` instance
obtained from this client. Other services might have been accessed via
distinct packages or patterns.

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "github.com/google/generative-ai-go/genai/fileman" // For files
      "google.golang.org/api/option"
)

client, err := genai.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))
fileClient, err := fileman.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))

// Get a model instance, then call methods on it
model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(...)
cs := model.StartChat()

// Call methods on separate client objects for other services
uploadedFile, err := fileClient.UploadFile(...)
```

 
 

 After (Centralized Client Object) 

 
 

### Python

 

```
from google import genai

# Create a single client object
client = genai.Client()

# Access API methods through services on the client object
response = client.models.generate_content(...)
chat = client.chats.create(...)
my_file = client.files.upload(...)
tuning_job = client.tunings.tune(...)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// Create a single client object
const ai = new GoogleGenAI({apiKey: "YOUR_API_KEY"});

// Access API methods through services on the client object
const response = await ai.models.generateContent(...);
const chat = ai.chats.create(...);
const uploadedFile = await ai.files.upload(...);
const cache = await ai.caches.create(...);
```

 
 

### Go

 

```
import "google.golang.org/genai"

// Create a single client object
client, err := genai.NewClient(ctx, nil)

// Access API methods through services on the client object
result, err := client.Models.GenerateContent(...)
chat, err := client.Chats.Create(...)
uploadedFile, err := client.Files.Upload(...)
tuningJob, err := client.Tunings.Tune(...)
```

 
 

## Authentication

Both legacy and new libraries authenticate using API keys. You can
 create your API key in Google AI
Studio.

 Before 

 
 

### Python

The old SDK handled the API client object implicitly. 

 

```
import google.generativeai as genai

genai.configure(api_key=...)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
```

 
 

### Go

Import the Google libraries:

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "google.golang.org/api/option"
)
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
```

 
 

 After 

 
 

### Python

With Google GenAI SDK, you create an API client first, which is used to call
the API.
The new SDK will pick up your API key from either one of the
`GEMINI_API_KEY` or `GOOGLE_API_KEY` environment variables, if you don't
pass one to the client. 

 

```
export GEMINI_API_KEY="YOUR_API_KEY"
```

 

```
from google import genai

client = genai.Client() # Set the API key using the GEMINI_API_KEY env var.
                        # Alternatively, you could set the API key explicitly:
                        # client = genai.Client(api_key="your_api_key")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({apiKey: "GEMINI_API_KEY"});
```

 
 

### Go

Import the GenAI library:

 

```
import "google.golang.org/genai"
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, &genai.ClientConfig{
        Backend:  genai.BackendGeminiAPI,
})
```

 
 

## Generate content

### Text

 Before 

 
 

### Python

Previously, there were no client objects, you accessed APIs directly through
`GenerativeModel` objects.

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story in 300 words'
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const prompt = "Tell me a story in 300 words";

const result = await model.generateContent(prompt);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(ctx, genai.Text("Tell me a story in 300 words."))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response parts
```

 
 

 After 

 
 

### Python

The new Google GenAI SDK provides access to all the API methods through the
`Client` object. Except for a few stateful special cases (`chat` and
live-api `session`s), these are all stateless functions. For utility and
uniformity, objects returned are `pydantic` classes.

 

```
from google import genai
client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
print(response.text)

print(response.model_dump_json(
    exclude_none=True, indent=4))
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story in 300 words.",
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me a story in 300 words."), nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Image

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

Many of the same convenience features exist in the new SDK. For
example, `PIL.Image` objects are automatically converted.

 

```
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Streaming

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

 
 

## Configuration

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
  'gemini-1.5-flash',
    system_instruction='you are a story teller for kids under 5 years old',
    generation_config=genai.GenerationConfig(
      max_output_tokens=400,
      top_k=2,
      top_p=0.5,
      temperature=0.5,
      response_mime_type='application/json',
      stop_sequences=['\n'],
    )
)
response = model.generate_content('tell me a story in 100 words')
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

const result = await model.generateContent(
  "Tell me a story about a magic backpack.",
);
console.log(result.response.text())
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
model.SetTemperature(0.5)
model.SetTopP(0.5)
model.SetTopK(2.0)
model.SetMaxOutputTokens(100)
model.ResponseMIMEType = "application/json"
resp, err := model.GenerateContent(ctx, genai.Text("Tell me about New York"))
if err != nil {
    log.Fatal(err)
}
printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

For all methods in the new SDK, the required arguments are provided as
keyword arguments. All optional inputs are provided in the `config`
argument. Config arguments can be specified as either Python dictionaries or
`Config` classes in the `google.genai.types` namespace. For utility and
uniformity, all definitions within the `types` module are `pydantic`
classes.

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='Tell me a story in 100 words.',
  config=types.GenerateContentConfig(
      system_instruction='you are a story teller for kids under 5 years old',
      max_output_tokens= 400,
      top_k= 2,
      top_p= 0.5,
      temperature= 0.5,
      response_mime_type= 'application/json',
      stop_sequences= ['\n'],
      seed=42,
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story about a magic backpack.",
  config: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx,
    "gemini-2.0-flash",
    genai.Text("Tell me about New York"),
    &genai.GenerateContentConfig{
        Temperature:      genai.Ptr[float32](0.5),
        TopP:             genai.Ptr[float32](0.5),
        TopK:             genai.Ptr[float32](2.0),
        ResponseMIMEType: "application/json",
        StopSequences:    []string{"Yankees"},
        CandidateCount:   2,
        Seed:             genai.Ptr[int32](42),
        MaxOutputTokens:  128,
        PresencePenalty:  genai.Ptr[float32](0.5),
        FrequencyPenalty: genai.Ptr[float32](0.5),
    },
)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing response
```

 
 

## Safety settings

Generate a response with safety settings:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'say something bad',
    safety_settings={
        'HATE': 'BLOCK_ONLY_HIGH',
        'HARASSMENT': 'BLOCK_ONLY_HIGH',
  }
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
});

const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const result = await model.generateContent(unsafePrompt);

try {
  result.response.text();
} catch (e) {
  console.error(e);
  console.log(result.response.candidates[0].safetyRatings);
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='say something bad',
  config=types.GenerateContentConfig(
      safety_settings= [
          types.SafetySetting(
              category='HARM_CATEGORY_HATE_SPEECH',
              threshold='BLOCK_ONLY_HIGH'
          ),
      ]
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: unsafePrompt,
  config: {
    safetySettings: [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ],
  },
});

console.log("Finish reason:", response.candidates[0].finishReason);
console.log("Safety ratings:", response.candidates[0].safetyRatings);
```

 
 

## Async

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content_async(
    'tell me a story in 100 words'
)
```

 
 

 After 

 
 

### Python

To use the new SDK with `asyncio`, there is a separate `async`
implementation of every method under `client.aio`.

 

```
from google import genai

client = genai.Client()

response = await client.aio.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
```

 
 

## Chat

Start a chat and send a message to the model:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat()

response = chat.send_message(
    "Tell me a story in 100 words")
response = chat.send_message(
    "What happened after that?")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const chat = model.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});
let result = await chat.sendMessage("I have 2 dogs in my house.");
console.log(result.response.text());
result = await chat.sendMessage("How many paws are in my house?");
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
cs := model.StartChat()

cs.History = []*genai.Content{
    {
        Parts: []genai.Part{
            genai.Text("Hello, I have 2 dogs in my house."),
        },
        Role: "user",
    },
    {
        Parts: []genai.Part{
            genai.Text("Great to meet you. What would you like to know?"),
        },
        Role: "model",
    },
}

res, err := cs.SendMessage(ctx, genai.Text("How many paws are in my house?"))
if err != nil {
    log.Fatal(err)
}
printResponse(res) // utility for printing the response
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

chat = client.chats.create(model='gemini-2.0-flash')

response = chat.send_message(
    message='Tell me a story in 100 words')
response = chat.send_message(
    message='What happened after that?')
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log("Chat response 1:", response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log("Chat response 2:", response2.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, nil)
if err != nil {
    log.Fatal(err)
}

result, err := chat.SendMessage(ctx, genai.Part{Text: "Hello, I have 2 dogs in my house."})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result

result, err = chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

## Function calling

 Before 

 
 

### Python

 

```
import google.generativeai as genai
from enum import Enum

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

response = model.generate_content("What is the weather in San Francisco?")
function_call = response.candidates[0].parts[0].function_call
```

 
 

 After 

 
 

### Python

In the new SDK, automatic function calling is the default. Here, you disable
it.

 

```
from google import genai
from google.genai import types

client = genai.Client()

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather],
      automatic_function_calling={'disable': True},
  ),
)

function_call = response.candidates[0].content.parts[0].function_call
```

 
 

### Automatic function calling

 Before 

 
 

### Python

The old SDK only supports automatic function calling in chat. In the new SDK
this is the default behavior in `generate_content`.

 

```
import google.generativeai as genai

def get_current_weather(city: str) -> str:
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

chat = model.start_chat(
    enable_automatic_function_calling=True)
result = chat.send_message("What is the weather in San Francisco?")
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types
client = genai.Client()

def get_current_weather(city: str) -> str:
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather]
  ),
)
```

 
 

## Code execution

Code execution is a tool that allows the model to generate Python code, run it,
and return the result.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools="code_execution"
)

result = model.generate_content(
  "What is the sum of the first 50 prime numbers? Generate and run code for "
  "the calculation, and make sure you get all 50.")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  tools: [{ codeExecution: {} }],
});

const result = await model.generateContent(
  "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get " +
    "all 50.",
);

console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the sum of the first 50 prime numbers? Generate and run '
            'code for the calculation, and make sure you get all 50.',
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
    ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-pro-exp-02-05",
  contents: `Write and execute code that calculates the sum of the first 50 prime numbers.
            Ensure that only the executable code and its resulting output are generated.`,
});

// Each part may contain text, executable code, or an execution result.
for (const part of response.candidates[0].content.parts) {
  console.log(part);
  console.log("\n");
}

console.log("-".repeat(80));
// The `.text` accessor concatenates the parts into a markdown-formatted text.
console.log("\n", response.text);
```

 
 

## Search grounding

`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are
tools that allow the model to retrieve public web data for grounding, powered by
Google.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    contents="what is the Google stock price?",
    tools='google_search_retrieval'
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the Google stock price?',
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                google_search=types.GoogleSearch()
            )
        ]
    )
)
```

 
 

## JSON response

Generate answers in JSON format.

 Before 

 
 

### Python

By specifying a `response_schema` and setting
`response_mime_type="application/json"` users can constrain the model to
produce a `JSON` response following a given structure. 

 

```
import google.generativeai as genai
import typing_extensions as typing

class CountryInfo(typing.TypedDict):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

model = genai.GenerativeModel(model_name="gemini-1.5-flash")
result = model.generate_content(
    "Give me information of the United States",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json",
        response_schema = CountryInfo
    ),
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");

const schema = {
  description: "List of recipes",
  type: SchemaType.ARRAY,
  items: {
    type: SchemaType.OBJECT,
    properties: {
      recipeName: {
        type: SchemaType.STRING,
        description: "Name of the recipe",
        nullable: false,
      },
    },
    required: ["recipeName"],
  },
};

const model = genAI.getGenerativeModel({
  model: "gemini-1.5-pro",
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: schema,
  },
});

const result = await model.generateContent(
  "List a few popular cookie recipes.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

The new SDK uses
`pydantic` classes to provide the schema (although you can pass a
`genai.types.Schema`, or equivalent `dict`). When possible, the SDK will
parse the returned JSON, and return the result in `response.parsed`. If you
provided a `pydantic` class as the schema the SDK will convert that `JSON`
to an instance of the class.

 

```
from google import genai
from pydantic import BaseModel

client = genai.Client()

class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': CountryInfo,
    },
)

response.parsed
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "List a few popular cookie recipes.",
  config: {
    responseMimeType: "application/json",
    responseSchema: {
      type: "array",
      items: {
        type: "object",
        properties: {
          recipeName: { type: "string" },
          ingredients: { type: "array", items: { type: "string" } },
        },
        required: ["recipeName", "ingredients"],
      },
    },
  },
});
console.log(response.text);
```

 
 

## Files

### Upload

Upload a file:

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

file = genai.upload_file(path='a11.txt')

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Can you summarize this file:',
    my_file
])
print(response.text)
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai

client = genai.Client()

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

my_file = client.files.upload(file='a11.txt')

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Can you summarize this file:',
        my_file
    ]
)
print(response.text)
```

 
 

### List and get

List uploaded files and get an uploaded file with a filename:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

for file in genai.list_files():
  print(file.name)

file = genai.get_file(name=file.name)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
client = genai.Client()

for file in client.files.list():
    print(file.name)

file = client.files.get(name=file.name)
```

 
 

### Delete

Delete a file:

 Before 

 
 

### Python

 

```
import pathlib
import google.generativeai as genai

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = genai.upload_file(path='dummy.txt')

file = genai.delete_file(name=dummy_file.name)
```

 
 

 After 

 
 

### Python

 

```
import pathlib
from google import genai

client = genai.Client()

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = client.files.upload(file='dummy.txt')

response = client.files.delete(name=dummy_file.name)
```

 
 

## Context caching

Context caching allows the user to pass the content to the model once, cache the
input tokens, and then refer to the cached tokens in subsequent calls to lower
the cost.

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai
from google.generativeai import caching

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = genai.upload_file(path="a11.txt")

# Create cache
apollo_cache = caching.CachedContent.create(
    model="gemini-1.5-flash-001",
    system_instruction="You are an expert at analyzing transcripts.",
    contents=[document],
)

# Generate response
apollo_model = genai.GenerativeModel.from_cached_content(
    cached_content=apollo_cache
)
response = apollo_model.generate_content("Find a lighthearted moment from this transcript")
```

 
 

### JavaScript

 

```
import { GoogleAICacheManager, GoogleAIFileManager } from "@google/generative-ai/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const cacheManager = new GoogleAICacheManager("GOOGLE_API_KEY");
const fileManager = new GoogleAIFileManager("GOOGLE_API_KEY");

const uploadResult = await fileManager.uploadFile("path/to/a11.txt", {
  mimeType: "text/plain",
});

const cacheResult = await cacheManager.create({
  model: "models/gemini-1.5-flash",
  contents: [
    {
      role: "user",
      parts: [
        {
          fileData: {
            fileUri: uploadResult.file.uri,
            mimeType: uploadResult.file.mimeType,
          },
        },
      ],
    },
  ],
});

console.log(cacheResult);

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModelFromCachedContent(cacheResult);
const result = await model.generateContent(
  "Please summarize this transcript.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai
from google.genai import types

client = genai.Client()

# Check which models support caching.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createCachedContent":
      print(m.name)
      break

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = client.files.upload(file='a11.txt')

# Create cache
model='gemini-1.5-flash-001'
apollo_cache = client.caches.create(
      model=model,
      config={
          'contents': [document],
          'system_instruction': 'You are an expert at analyzing transcripts.',
      },
  )

# Generate response
response = client.models.generate_content(
    model=model,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const filePath = path.join(media, "a11.txt");
const document = await ai.files.upload({
  file: filePath,
  config: { mimeType: "text/plain" },
});
console.log("Uploaded file name:", document.name);
const modelName = "gemini-1.5-flash";

const contents = [
  createUserContent(createPartFromUri(document.uri, document.mimeType)),
];

const cache = await ai.caches.create({
  model: modelName,
  config: {
    contents: contents,
    systemInstruction: "You are an expert analyzing transcripts.",
  },
});
console.log("Cache created:", cache);

const response = await ai.models.generateContent({
  model: modelName,
  contents: "Please summarize this transcript",
  config: { cachedContent: cache.name },
});
console.log("Response text:", response.text);
```

 
 

## Count tokens

Count the number of tokens in a request.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.count_tokens(
    'The quick brown fox jumps over the lazy dog.')
```

 
 

### JavaScript

 

```
 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY+);
 const model = genAI.getGenerativeModel({
   model: "gemini-1.5-flash",
 });

 // Count tokens in a prompt without calling text generation.
 const countResult = await model.countTokens(
   "The quick brown fox jumps over the lazy dog.",
 );

 console.log(countResult.totalTokens); // 11

 const generateResult = await model.generateContent(
   "The quick brown fox jumps over the lazy dog.",
 );

 // On the response for `generateContent`, use `usageMetadata`
 // to get separate input and output token counts
 // (`promptTokenCount` and `candidatesTokenCount`, respectively),
 // as well as the combined token count (`totalTokenCount`).
 console.log(generateResult.response.usageMetadata);
 // candidatesTokenCount and totalTokenCount depend on response, may vary
 // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.count_tokens(
    model='gemini-2.0-flash',
    contents='The quick brown fox jumps over the lazy dog.',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);
```

 
 

## Generate images

Generate images:

 Before 

 
 

### Python

 

```
#pip install https://github.com/google-gemini/generative-ai-python@imagen
import google.generativeai as genai

imagen = genai.ImageGenerationModel(
    "imagen-3.0-generate-001")
gen_images = imagen.generate_images(
    prompt="Robot holding a red skateboard",
    number_of_images=1,
    safety_filter_level="block_low_and_above",
    person_generation="allow_adult",
    aspect_ratio="3:4",
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

gen_images = client.models.generate_images(
    model='imagen-3.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 1,
        safety_filter_level= "BLOCK_LOW_AND_ABOVE",
        person_generation= "ALLOW_ADULT",
        aspect_ratio= "3:4",
    )
)

for n, image in enumerate(gen_images.generated_images):
    pathlib.Path(f'{n}.png').write_bytes(
        image.image.image_bytes)
```

 
 

## Embed content

Generate content embeddings.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = genai.embed_content(
  model='models/gemini-embedding-001',
  content='Hello world'
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-embedding-001",
});

const result = await model.embedContent("Hello world!");

console.log(result.embedding);
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.embed_content(
  model='gemini-embedding-001',
  contents='Hello world',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const text = "Hello World!";
const result = await ai.models.embedContent({
  model: "gemini-embedding-001",
  contents: text,
  config: { outputDimensionality: 10 },
});
console.log(result.embeddings);
```

 
 

## Tune a model

Create and use a tuned model.

The new SDK simplifies tuning with `client.tunings.tune`, which launches the
tuning job and polls until the job is complete.

 Before 

 
 

### Python

 

```
import google.generativeai as genai
import random

# create tuning model
train_data = {}
for i in range(1, 6):
  key = f'input {i}'
  value = f'output {i}'
  train_data[key] = value

name = f'generate-num-{random.randint(0,10000)}'
operation = genai.create_tuned_model(
    source_model='models/gemini-1.5-flash-001-tuning',
    training_data=train_data,
    id = name,
    epoch_count = 5,
    batch_size=4,
    learning_rate=0.001,
)
# wait for tuning complete
tuningProgress = operation.result()

# generate content with the tuned model
model = genai.GenerativeModel(model_name=f'tunedModels/{name}')
response = model.generate_content('55')
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Check which models are available for tuning.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createTunedModel":
      print(m.name)
      break

# create tuning model
training_dataset=types.TuningDataset(
        examples=[
            types.TuningExample(
                text_input=f'input {i}',
                output=f'output {i}',
            )
            for i in range(5)
        ],
    )
tuning_job = client.tunings.tune(
    base_model='models/gemini-1.5-flash-001-tuning',
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count= 5,
        batch_size=4,
        learning_rate=0.001,
        tuned_model_display_name="test tuned model"
    )
)

# generate content with the tuned model
response = client.models.generate_content(
    model=tuning_job.tuned_model.model,
    contents='55',
)
```

 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Access Google AI Studio with your Workspace account &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/workspace

- 
 
 
 
 
 
 
 
 
 
 
 Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Access Google AI Studio with your Workspace account 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

All Google Workspace users have access to AI
Studio by default. If you're a Workspace user and you want to get started with
AI Studio, check out the
 AI Studio quickstart .

## Troubleshooting

If access to AI Studio is disabled for your Google Workspace account, you might
see an error like the following:



```
We are sorry, but you do not have access to Google AI Studio. Please contact
your Organization Administrator for access.
```



If you think you should have access to AI Studio, contact your Workspace
administrator.

## Enable AI Studio for Workspace users

As a Google Workspace administrator, you can control who uses AI Studio:

- AI Studio is turned on by default for all editions.

- You can turn AI Studio off or on for sets of users across or within
organizational units.

- Google Workspace for Education editions: Users under the age of 18 are
restricted from using AI Studio with their Google Workspace for
Education accounts. This is true even when the AI Studio setting is
on. For details, go to Control access to Google services by
age .

To enable or disable AI Studio for users in your organization, see
 Turn Google AI Studio on or off for users .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Release notes &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/changelog#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Release notes  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Release notes 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

This page documents updates to the Gemini API.

## November 18, 2025

- 

Launched the first Gemini 3 series model, `gemini-3-pro-preview`, our
state-of-the-art reasoning and multimodal understanding model with powerful
agentic and coding capabilities.

In addition to improvements in intelligence and performance,
Gemini 3 Pro Preview introduces new behavior around:

 Media resolution 

- Thought signatures 

- Thinking levels 

Read the Gemini 3 Developer Guide for
migration, new features, and specs.

 

## November 11, 2025

- 

The following models will be deprecated:

 

November 12:

 `veo-3.0-fast-generate-preview`

- `veo-3.0-generate-preview`

 
- 

November 14:

 `gemini-2.0-flash-exp-image-generation`

- `gemini-2.0-flash-preview-image-generation`

 

 

## November 10, 2025

- 

The following model is deprecated:

 `imagen-3.0-generate-002`

Use Imagen 4 instead. Refer to the
 Gemini deprecations table for more details.

 

## November 6, 2025

- Launched the File Search API to public preview, enabling developers to
ground responses in their own data. Read the new File Search page for more info.

## November 4, 2025

- 

The following models will be deprecated:

 

November 18th:

 `gemini-2.5-flash-lite-preview-06-17`

- `gemini-2.5-flash-preview-05-20`

 
- 

December 2nd:

 `gemini-2.0-flash-thinking-exp`

- `gemini-2.0-flash-thinking-exp-01-21`

- `gemini-2.0-flash-thinking-exp-1219`

- `gemini-2.5-pro-preview-03-25`

- `gemini-2.5-pro-preview-05-06`

- `gemini-2.5-pro-preview-06-05`

 
- 

December 9th:

 `gemini-2.0-flash-lite-preview`

- `gemini-2.0-flash-lite-preview-02-05`

 

 

## October 29, 2025

- Launched the new logging and datasets tool for the Gemini API.

## October 20, 2025

- 

The following Gemini Live API models are now deprecated:

 `gemini-2.5-flash-preview-native-audio-dialog`

- `gemini-2.5-flash-exp-native-audio-thinking-dialog`

You can use `gemini-2.5-flash-native-audio-preview-09-2025` instead.

 
- 

Deprecation for `gemini-2.0-flash-live-001` and `gemini-live-2.5-flash-preview` coming December 09, 2025.

## October 17, 2025

- Grounding with Google Maps is now
generally available. For more information, see
 Grounding with Google Maps documentation.

## October 15, 2025

- 

Released Veo 3.1 and 3.1 Fast models in
public preview, with new features including:

 Extending Veo-created videos.

- Referencing up to three images to generate a video.

- Providing first and last frame images to generate videos from.

This launch also added more options for Veo 3 output video durations: 4, 6,
and 8 seconds.

 
- 

Deprecation for `veo-3.0-generate-preview` and `veo-3.0-fast-generate-preview`
coming November 6, 2025.

## October 7, 2025

- Launched Gemini 2.5 Computer Use Preview 

## October 2, 2025

- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini 

## September 29, 2025

- The following Gemini 1.5 models are now deprecated:

 `gemini-1.5-pro`

- `gemini-1.5-flash-8b`

- `gemini-1.5-flash`

 

## September 25, 2025

- 

Released Gemini Robotics-ER 1.5 model in preview. See the
 Robotics overview 
to learn about how to use the model for your robotics application.

- 

Launched following preview models:

 `gemini-2.5-flash-preview-09-2025`

- `gemini-2.5-flash-lite-preview-09-2025`

See the Models page for details.

 

## September 23, 2025

- Released `gemini-2.5-flash-native-audio-preview-09-2025`,
a new native audio model for the Live API with improved function calling
and speech cut off handling. To learn more, see the
 Live API guide and
 Gemini 2.5 Flash Native Audio .

## September 16, 2025

- 

The following models will be deprecated in October, 2025:

 `embedding-001`

- `embedding-gecko-001`

- `gemini-embedding-exp-03-07` (`gemini-embedding-exp`)

See the Embeddings page for details on the latest embeddings
model.

 

## September 10, 2025

- Released support for the
 Embeddings model in Batch API ,
and added Batch API to the
 OpenAI compatibility library for even
easier ways to get started with batch queries.

## September 9, 2025

- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for
aspect ratios, resolution, and seeding. Read the
 Veo documentation for more
information.

## August 26, 2025

- Launched Gemini 2.5 Image Preview ,
our latest native image generation model.

## August 18, 2025

- Released URL context tool to general
availability (GA), a tool for providing URLs as additional context to
prompts. Support for using URL context with the `gemini-2.0-flash` model
(available during experimental release) will be discontinued in one week.

## August 14, 2025

- Released Imagen 4 Ultra, Standard and Fast models as generally available
(GA). To learn more, see the Imagen page.

## August 7, 2025

- `allow_adult` setting in Image to Video generation are now available in
restricted regions. See the
 Veo 
page for details.

## July 31, 2025

- Launched image-to-video generation for the Veo 3 Preview model.

- Released Veo 3 Fast Preview model.

- To learn more about Veo 3, visit the Veo page.

## July 22, 2025

- Released `gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini
2.5 model. To learn more, see Gemini 2.5
Flash-Lite .

## July 17, 2025

- 

Launched `veo-3.0-generate-preview`, the latest update to Veo introducing
video with audio generation. To learn more about Veo 3, visit the Veo page.

- 

Increased rate limits for Imagen 4 Standard and Ultra. Visit the
 Rate limits page for more details.

## July 14, 2025

- Released `gemini-embedding-001`, the stable version of our
text embedding model. To learn more, see
 embeddings . The `gemini-embedding-exp-03-07`
model will be deprecated on August 14, 2025.

## July 7, 2025

- Launched Gemini API Batch Mode. Batch up requests and send them to process
asynchronously. To learn more, see Batch Mode .

## June 26, 2025

- 

The preview models `gemini-2.5-pro-preview-05-06` and
`gemini-2.5-pro-preview-03-25` are now redirecting to
the latest stable version `gemini-2.5-pro`.

- 

`gemini-2.5-pro-exp-03-25` is deprecated.

## June 24, 2025

- Released Imagen 4 Ultra and Standard Preview models. To learn more, see the
 Image generation page.

## June 17, 2025

- Released `gemini-2.5-pro`, the stable version of our most powerful
model, now with adaptive thinking. To learn more, see
 Gemini 2.5 Pro 
and Thinking . `gemini-2.5-pro-preview-05-06`
will be redirected to `gemini-2.5-pro` on June 26, 2025.

- Released `gemini-2.5-flash`, our first stable 2.5 Flash model. To learn
more, see Gemini 2.5 Flash .
`gemini-2.5-flash-preview-04-17` will be deprecated on July 15, 2025.

- Released `gemini-2.5-flash-lite-preview-06-17`, a low-cost, high-performance
Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite
Preview .

## June 05, 2025

- Released `gemini-2.5-pro-preview-06-05`, a new version of our most powerful
model, now with adaptive thinking. To learn more, see
 Gemini 2.5 Pro Preview 
and Thinking .
`gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on
June 26, 2025.

## May 27, 2025

- The last available tuning model, Gemini 1.5 Flash 001, has been shutdown.
Tuning is no longer supported on any models.
See Fine tuning with the Gemini API .

## May 20, 2025

 API updates: 

- Launched support for
 custom video preprocessing 
using clipping intervals and configurable frame rate sampling.

- Launched multi-tool use, which supports configuring
 code execution and
 Grounding with Google Search on the same
`generateContent` request.

- Launched support for
 asynchronous function calls 
in the Live API.

- Launched an experimental
 URL context tool 
for providing URLs as additional context to prompts.

 Model updates: 

- Released `gemini-2.5-flash-preview-05-20`, a Gemini
 preview model optimized for
price-performance and adaptive thinking. To learn more, see
 Gemini 2.5 Flash Preview 
and Thinking .

- Released the
 `gemini-2.5-pro-preview-tts` 
and
 `gemini-2.5-flash-preview-tts` 
models, which are capable of
 generating speech with one or two
speakers.

- Released the `lyria-realtime-exp` model, which
 generates music in real time.

- Released `gemini-2.5-flash-preview-native-audio-dialog` and
`gemini-2.5-flash-exp-native-audio-thinking-dialog`,
new Gemini models for the Live API with native audio output capabilities. To
learn more, see the
 Live API guide and
 Gemini 2.5 Flash Native Audio .

- Released `gemma-3n-e4b-it` preview, available on
 AI Studio and through the Gemini API,
as part of the Gemma 3n launch.

## May 7, 2025

- Released `gemini-2.0-flash-preview-image-generation`, a preview model for
generating and editing images. To learn more, see Image
generation and
 Gemini 2.0 Flash Preview Image
Generation .

## May 6, 2025

- Released `gemini-2.5-pro-preview-05-06`, a new version of our most powerful
model, with improvements on code and function calling. `gemini-2.5-pro-preview-03-25`
will automatically point to the new version of the model.

## April 17, 2025

- Released `gemini-2.5-flash-preview-04-17`, a Gemini
 preview model optimized for
price-performance and adaptive thinking. To learn more, see
 Gemini 2.5 Flash Preview 
and Thinking .

## April 16, 2025

- Launched context caching for
 Gemini 2.0 Flash .

## April 9, 2025

 Model updates: 

- Released `veo-2.0-generate-001`, a generally available (GA) text- and
image-to-video model, capable of generating detailed and artistically
nuanced videos. To learn more, see the Veo docs .

- 

Released `gemini-2.0-flash-live-001`, a public preview version of the
 Live API model with billing enabled.

 

 Enhanced Session Management and Reliability 

 Session Resumption: Keep sessions alive across temporary network
disruptions. The API now supports server-side session state storage (for
up to 24 hours) and provides handles (session_resumption) to reconnect
and resume where you left off.

- Longer Sessions via Context Compression: Enable extended
interactions beyond previous time limits. Configure context window
compression with a sliding window mechanism to automatically manage
context length, preventing abrupt terminations due to context limits.

- Graceful Disconnect Notification: Receive a `GoAway` server
message indicating when a connection is about to close, allowing for
graceful handling before termination.

 
- 

 More Control over Interaction Dynamics 

- 

 Configurable Voice Activity Detection (VAD): Choose sensitivity
levels or disable automatic VAD entirely and use new client events
(`activityStart`, `activityEnd`) for manual turn control.

- 

 Configurable Interruption Handling: Decide whether user input
should interrupt the model's response.

- 

 Configurable Turn Coverage: Choose whether the API processes all
audio and video input continuously or only captures it when the end-user
is detected speaking.

- 

 Configurable Media Resolution: Optimize for quality or token usage
by selecting the resolution for input media.

- 

 Richer Output and Features 

- 

 Expanded Voice & Language Options: Choose from two new voices and
30 new languages for audio output. The output language is now
configurable within `speechConfig`.

- 

 Text Streaming: Receive text responses incrementally as they are
generated, enabling faster display to the user.

- 

 Token Usage Reporting: Gain insights into usage with detailed
token counts provided in the `usageMetadata` field of server messages,
broken down by modality and prompt or response phases.

 

## April 4, 2025

- Released `gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version
with billing enabled. You can continue to use `gemini-2.5-pro-exp-03-25` on
the free tier.

## March 25, 2025

- Released `gemini-2.5-pro-exp-03-25`, a public experimental Gemini model
with thinking mode always on by default.
To learn more, see
 Gemini 2.5 Pro Experimental .

## March 12, 2025

 Model updates: 

- Launched an experimental Gemini 2.0 Flash 
model capable of image generation and editing.

- Released `gemma-3-27b-it`, available on
 AI Studio and through the Gemini API,
as part of the Gemma 3 launch.

 API updates: 

- Added support for
 YouTube URLs as a media source.

- Added support for including an
 inline video of less than 20MB.

## March 11, 2025

 SDK updates: 

- Released the
 Google Gen AI SDK for TypeScript and JavaScript 
to public preview.

## March 7, 2025

 Model updates: 

- Released `gemini-embedding-exp-03-07`, an
 experimental 
Gemini-based embeddings model in public preview.

## February 28, 2025

 API updates: 

- Support for Search as a tool 
added to `gemini-2.0-pro-exp-02-05`, an experimental model based on
Gemini 2.0 Pro.

## February 25, 2025

 Model updates: 

- Released `gemini-2.0-flash-lite`, a generally available (GA) version of
 Gemini 2.0 Flash-Lite ,
which is optimized for speed, scale, and cost efficiency.

## February 19, 2025

 AI Studio updates: 

- Support for
 additional regions 
(Kosovo, Greenland and Faroe Islands).

 API updates: 

- Support for
 additional regions 
(Kosovo, Greenland and Faroe Islands).

## February 18, 2025

 Model updates: 

- Gemini 1.0 Pro is no longer supported. For the list of supported models, see
 Gemini models .

## February 11, 2025

 API updates: 

- Updates on the
 OpenAI libraries compatibility .

## February 6, 2025

 Model updates: 

- Released `imagen-3.0-generate-002`, a generally available (GA) version of
 Imagen 3 in the Gemini API .

 SDK updates: 

- Released the Google Gen AI SDK for Java 
for public preview.

## February 5, 2025

 Model updates: 

- Released `gemini-2.0-flash-001`, a generally available (GA) version of
 Gemini 2.0 Flash that
supports text-only output.

- Released `gemini-2.0-pro-exp-02-05`,
an experimental public
preview version of Gemini 2.0 Pro.

- Released `gemini-2.0-flash-lite-preview-02-05`, an experimental public
preview model 
optimized for cost efficiency.

 API updates: 

- Added
 file input and graph output 
support to code execution.

 SDK updates: 

- Released the
 Google Gen AI SDK for Python 
to general availability (GA).

## January 21, 2025

 Model updates: 

- Released `gemini-2.0-flash-thinking-exp-01-21`, the latest preview version of
the model behind the
 Gemini 2.0 Flash Thinking Model .

## December 19, 2024

 Model updates: 

- 

Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is
a test-time compute model that lets you see the model's thought process
while it generates a response, and produces responses with stronger
reasoning capabilities.

Read more about Gemini 2.0 Flash Thinking Mode in our overview
page .

## December 11, 2024

 Model updates: 

- Released Gemini 2.0 Flash Experimental 
for public preview. Gemini 2.0 Flash Experimental's partial list of features includes:

 Twice as fast as Gemini 1.5 Pro

- Bidirectional streaming with our Live API

- Multimodal response generation in the form of text, images, and speech

- Built-in tool use with multi-turn reasoning to use features like code
execution, Search, function calling, and more

 

Read more about Gemini 2.0 Flash in our overview
page .

## November 21, 2024

 Model updates: 

- Released `gemini-exp-1121`, an even more powerful experimental Gemini API model.

 Model updates: 

- Updated the `gemini-1.5-flash-latest` and `gemini-1.5-flash` model aliases
to use `gemini-1.5-flash-002`.

 Change to `top_k` parameter: The `gemini-1.5-flash-002`
model supports `top_k` values between 1 and 41 (exclusive).
Values greater than 40 will be changed to 40.

 

## November 14, 2024

 Model updates: 

- Released `gemini-exp-1114`, a powerful experimental Gemini API model.

## November 8, 2024

 API updates: 

- Added support for Gemini in the OpenAI libraries / REST API.

## October 31, 2024

 API updates: 

- Added support for Grounding with Google Search .

## October 3, 2024

 Model updates: 

- Released `gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini
API model.

## September 24, 2024

 Model updates: 

- Released `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, two new stable
versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.

- Updated the `gemini-1.5-pro-latest` model code to use `gemini-1.5-pro-002`
and the `gemini-1.5-flash-latest` model code to use `gemini-1.5-flash-002`.

- Released `gemini-1.5-flash-8b-exp-0924` to replace `gemini-1.5-flash-8b-exp-0827`.

- Released the civic integrity safety filter 
for the Gemini API and AI Studio.

- Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in
Python and NodeJS:
 `frequencyPenalty` and
 `presencePenalty` .

## September 19, 2024

 AI Studio updates: 

- Added thumb-up and thumb-down buttons to model responses, to enable users to
provide feedback on the quality of a response.

 API updates: 

- Added support for Google Cloud credits, which can now be used towards
Gemini API usage.

## September 17, 2024

 AI Studio updates: 

- Added an Open in Colab button that exports a prompt ‚Äì and the
code to run it ‚Äì to a Colab notebook. The feature doesn't yet support
prompting with tools (JSON mode, function calling, or code execution).

## September 13, 2024

 AI Studio updates: 

- Added support for compare mode, which lets you compare responses across
 models and prompts to find the best fit for your use case.

## August 30, 2024

 Model updates: 

- Gemini 1.5 Flash supports
 supplying JSON schema through model configuration .

## August 27, 2024

 Model updates: 

- Released the following
 experimental models :

 `gemini-1.5-pro-exp-0827`

- `gemini-1.5-flash-exp-0827`

- `gemini-1.5-flash-8b-exp-0827`

 

## August 9, 2024

 API updates: 

- Added support for PDF processing .

## August 5, 2024

 Model updates: 

- Fine-tuning support released for Gemini 1.5 Flash.

## August 1, 2024

 Model updates: 

- Released `gemini-1.5-pro-exp-0801`, a new experimental version of
 Gemini 1.5 Pro .

## July 12, 2024

 Model updates: 

- Support for Gemini 1.0 Pro Vision removed from Google AI services and tools.

## June 27, 2024

 Model updates: 

- General availability release for Gemini 1.5 Pro's 2M context window.

 API updates: 

- Added support for code execution .

## June 18, 2024

 API updates: 

- Added support for context caching .

## June 12, 2024

 Model updates: 

- Gemini 1.0 Pro Vision deprecated.

## May 23, 2024

 Model updates: 

- Gemini 1.5 Pro 
(`gemini-1.5-pro-001`) is generally available (GA).

- Gemini 1.5 Flash 
(`gemini-1.5-flash-001`) is generally available (GA).

## May 14, 2024

 API updates: 

- Introduced a 2M context window for Gemini 1.5 Pro (waitlist).

- Introduced pay-as-you-go billing for Gemini 1.0
Pro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon.

- Introduced increased rate limits for the upcoming paid tier of Gemini 1.5
Pro.

- Added built-in video support to the File API .

- Added plain text support to the File API .

- Added support for parallel function calling, which returns more than one
call at a time.

## May 10, 2024

 Model updates: 

- Released Gemini 1.5 Flash 
(`gemini-1.5-flash-latest`) in preview.

## April 9, 2024

 Model updates: 

- Released Gemini 1.5 Pro 
(`gemini-1.5-pro-latest`) in preview.

- Released a new text embedding model, `text-embeddings-004`, which supports
 elastic embedding 
sizes under 768.

 API updates: 

- Released the File API for temporarily storing
media files for use in prompting.

- Added support for prompting with text, image, and audio data, also
known as multimodal prompting. To learn more, see
 Prompting with media .

- Released System instructions in
beta.

- Added
 Function calling mode ,
which defines the execution behavior for function calling.

- Added support for the `response_mime_type` configuration option, which lets
you request responses in
 JSON format .

## March 19, 2024

 Model updates: 

- Added support for
 tuning Gemini 1.0 Pro 
in Google AI Studio or with the Gemini API.

## December 13 2023

 Model updates: 

- gemini-pro: New text model for a wide variety of tasks. Balances capability
and efficiency.

- gemini-pro-vision: New multimodal model for a wide variety of tasks.
Balances capability and efficiency.

- embedding-001: New embeddings model.

- aqa: A new specially tuned model that is trained to answer questions
 using text passages for grounding generated answers.

See Gemini models for more details.

 API version updates: 

- v1: The stable API channel.

- v1beta: Beta channel. This channel has features that may be under
development.

See the API versions topic for more details.

 API updates: 

- `GenerateContent` is a single unified endpoint for chat and text.

- Streaming available through the `StreamGenerateContent` method.

- Multimodal capability: Image is a new supported modality

- New beta features:

 Function Calling 

- Semantic Retriever 

- Attributed Question Answering (AQA)

 
- Updated candidate count: Gemini models only return 1 candidate.

- Different Safety Settings and SafetyRating categories. See
 safety settings for more details.

- Tuning models is not yet supported for Gemini models (Work in progress).

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Gemini Developer API v.s. Vertex AI &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate-to-cloud

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Developer API v.s. Vertex AI  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini Developer API v.s. Vertex AI 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

When developing generative AI solutions with Gemini, Google offers two API products:
the Gemini Developer API and the Vertex AI Gemini API .

The Gemini Developer API provides the fastest path to build, productionize, and
scale Gemini powered applications. Most developers should use the Gemini Developer
API unless there is a need for specific enterprise controls.

Vertex AI offers a comprehensive ecosystem of enterprise ready features and services
for building and deploying generative AI applications backed by the Google Cloud Platform.

We've recently simplified migrating between these services. Both the Gemini
Developer API and the Vertex AI Gemini API are now accessible through the unified
 Google Gen AI SDK .

## Code comparison

This page has side-by-side code comparisons between Gemini Developer API and
Vertex AI quickstarts for text generation.

### Python

You can access both the Gemini Developer API and Vertex AI services through
the `google-genai` library. See the libraries page
for instructions on how to install `google-genai`.

 
 

### Gemini Developer API

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### Vertex AI Gemini API

 

```
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript and TypeScript

You can access both Gemini Developer API and Vertex AI services through `@google/genai`
library. See libraries page for instructions on how
to install `@google/genai`.

 
 

### Gemini Developer API

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Vertex AI Gemini API

 

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

You can access both Gemini Developer API and Vertex AI services through `google.golang.org/genai`
library. See libraries page for instructions on how
to install `google.golang.org/genai`.

 
 

### Gemini Developer API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Vertex AI Gemini API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Other use cases and platforms

Refer to use case specific guides on Gemini Developer API Documentation 
and Vertex AI documentation 
for other platforms and use cases.

## Migration considerations

When you migrate:

- 

You'll need to use Google Cloud service accounts to authenticate. See the Vertex AI documentation 
for more information.

- 

You can use your existing Google Cloud project
(the same one you used to generate your API key) or you can
 create a new Google Cloud project .

- 

Supported regions may differ between the Gemini Developer API and the
Vertex AI Gemini API. See the list of
 supported regions for generative AI on Google Cloud .

- 

Any models you created in Google AI Studio need to be retrained in Vertex AI.

If you no longer need to use your Gemini API key for the Gemini Developer API,
then follow security best practices and delete it.

To delete an API key:

- 

Open the
 Google Cloud API Credentials 
page.

- 

Find the API key you want to delete and click the Actions icon.

- 

Select Delete API key .

- 

In the Delete credential modal, select Delete .

Deleting an API key takes a few minutes to propagate. After
propagation completes, any traffic using the deleted API key is rejected.

## Next steps

- See the
 Generative AI on Vertex AI overview 
to learn more about generative AI solutions on Vertex AI.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini deprecations &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/deprecations#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini deprecations  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini deprecations 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page lists the known deprecation schedules for Stable (GA) and Preview
models in the Gemini API. A deprecation announcement means we will no longer
provide support for that model, and that it will be completely retired, or
turned off, shortly after the deprecation date.

 
 

### Stable Models

 
 
 
 Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 Imagen 3 
 February 6, 2025 
 November 10, 2025 
 `imagen-3.0-generate-002`
Use Imagen 4. 
 
 
 Gemini 2.0 Flash 
 February 5, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash` & `gemini-2.0-flash-001` 
 
 
 Gemini 2.0 Flash-Lite 
 February 25, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash-lite` & `gemini-2.0-flash-lite-001` 
 
 
 Gemini 2.5 Flash 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Pro 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Flash-Lite 
 July 22, 2025 
 Earliest July 2026 
 
 
 
 Gemini 2.5 Flash Image 
 October 2, 2025 
 Earliest October 2026 
 
 
 
 Veo 3 
 September 9, 2025 
 No deprecation date announced 
 Including `veo-3.0-generate-001` & `veo-3.0-fast-generate-001` 
 
 
 Veo 2 
 April 9, 2025 
 No deprecation date announced 
 
 
 
 Imagen 4 
 August 14, 2025 
 No deprecation date announced 
 Including:
 

 - `imagen-4.0-generate-001`

 - `imagen-4.0-ultra-generate-001`

 - `imagen-4.0-fast-generate-001`

 

 
 
 
 
 

### Preview Models

Preview models are deprecated with at least 2 weeks notice.

 
 
 
 Preview Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 `gemini-2.5-flash-preview-native-audio-dialog` 
 May 20,2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `gemini-2.5-flash-exp-native-audio-thinking-dialog` 
 May 20, 2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `embedding-001` & `embedding-gecko-001` 
 December 13, 2023 
 October 30, 2025 
 Use `gemini-embedding-001`. 
 
 
 `gemini-2.0-flash-preview-image-generation` & `gemini-2.0-flash-exp-image-generation` 
 May 7, 2025 
 November 12, 2025 
 Use `gemini-2.5-flash-image`. 
 
 
 `veo-3.0-generate-preview` & `veo-3.0-fast-generate-preview` 
 July 17, 2025 
 November 12, 2025 
 Use `veo-3.1-generate-preview` or `veo-3.1-fast-generate-preview`. 
 
 
 `gemini-2.0-flash-live-001` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-live-2.5-flash-preview` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-2.5-pro-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-lite-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-image-preview` 
 August 26, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-native-audio-preview-09-2025` 
 September 23, 2025 
 No deprecation date announced 
 
 
 
 `veo-3.1-generate-preview` & `veo-3.1-fast-generate-preview` 
 October 15, 2025 
 No deprecation date announced 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-12 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-12 UTC."],[],[]]

---

### Troubleshooting guide &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/troubleshooting#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Troubleshooting guide  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Troubleshooting guide 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Use this guide to help you diagnose and resolve common issues that arise when
you call the Gemini API. You may encounter issues from either
the Gemini API backend service or the client SDKs. Our client SDKs are
open sourced in the following repositories:

- python-genai 

- js-genai 

- go-genai 

If you encounter API key issues, verify that you have set up
your API key correctly per the API key setup guide .

## Gemini API backend service error codes

The following table lists common backend error codes you may encounter, along
with explanations for their causes and troubleshooting steps:

 
 
 HTTP Code 
 
 Status 
 
 Description 
 
 Example 
 
 Solution 
 
 
 
 
 400
 
 INVALID_ARGUMENT 
 The request body is malformed. 
 There is a typo, or a missing required field in your request. 
 Check the API reference for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors. 
 
 
 
 400
 
 FAILED_PRECONDITION 
 Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. 
 You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. 
 To use the Gemini API, you will need to setup a paid plan using Google AI Studio . 
 
 
 
 403
 
 PERMISSION_DENIED 
 Your API key doesn't have the required permissions. 
 You are using the wrong API key; you
 are trying to use a tuned model without going through proper authentication . 
 Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models. 
 
 
 
 404
 
 NOT_FOUND 
 The requested resource wasn't found. 
 An image, audio, or video file referenced in your request was not found. 
 Check if all parameters in your request are valid for your API version. 
 
 
 
 429
 
 RESOURCE_EXHAUSTED 
 You've exceeded the rate limit. 
 You are sending too many requests per minute with the free tier Gemini API. 
 Verify that you're within the model's rate limit . Request a quota increase if needed. 
 
 
 
 500
 
 INTERNAL 
 An unexpected error occurred on Google's side. 
 Your input context is too long. 
 Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 
 
 
 
 503
 
 UNAVAILABLE 
 The service may be temporarily overloaded or down. 
 The service is temporarily running out of capacity. 
 Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 
 
 
 
 504
 
 DEADLINE_EXCEEDED 
 The service is unable to finish processing within the deadline. 
 Your prompt (or context) is too large to be processed in time. 
 Set a larger 'timeout' in your client request to avoid this error. 
 
 

## Check your API calls for model parameter errors

Verify that your model parameters are within the following values:

 
 
 Model parameter 
 
 Values (range) 
 
 
 
 
 Candidate count
 
 1-8 (integer) 
 
 
 
 Temperature
 
 0.0-1.0 
 
 
 
 Max output tokens
 
 
 Use
 `get_model` ( Python )
 to determine the maximum number of tokens for the model you are using.
 
 
 
 
 TopP
 
 0.0-1.0 
 
 

In addition to checking parameter values, make sure you're using the correct
 API version (e.g., `/v1` or `/v1beta`) and
model that supports the features you need. For example, if a feature is in Beta
release, it will only be available in the `/v1beta` API version.

## Check if you have the right model

Verify that you are using a supported model listed on our models
page .

## Higher latency or token usage with 2.5 models

If you're observing higher latency or token usage with the 2.5 Flash and Pro
models, this can be because they come with thinking is enabled by default in
order to enhance quality. If you are prioritizing speed or need to minimize
costs, you can adjust or disable thinking.

Refer to thinking page for
guidance and sample code.

## Safety issues

If you see a prompt was blocked because of a safety setting in your API call,
review the prompt with respect to the filters you set in the API call.

If you see `BlockedReason.OTHER`, the query or response may violate the terms
of service or be otherwise unsupported.

## Recitation issue

If you see the model stops generating output due to the RECITATION reason, this
means the model output may resemble certain data. To fix this, try to make
prompt / context as unique as possible and use a higher temperature.

## Repetitive tokens issue

If you see repeated output tokens, try the following suggestions to help
reduce or eliminate them.

 
 
 
 
 
 
 
 Description 
 Cause 
 Suggested workaround 
 
 
 Repeated hyphens in Markdown tables 
 
 This can occur when the contents of the table are long as the model tries
 to create a visually aligned Markdown table. However, the alignment in
 Markdown is not necessary for correct rendering.
 
 
 

 Add instructions in your prompt to give the model specific guidelines
 for generating Markdown tables. Provide examples that follow those
 guidelines. You can also try adjusting the temperature. For generating
 code or very structured output like Markdown tables,
 high temperature have shown to work better (>= 0.8).

 

 The following is an example set of guidelines you can add to your
 prompt to prevent this issue:
 

 

```
          # Markdown Table Format
          
          * Separator line: Markdown tables must include a separator line below
            the header row. The separator line must use only 3 hyphens per
            column, for example: |---|---|---|. Using more hypens like
            ----, -----, ------ can result in errors. Always
            use |:---|, |---:|, or |---| in these separator strings.

            For example:

            | Date | Description | Attendees |
            |---|---|---|
            | 2024-10-26 | Annual Conference | 500 |
            | 2025-01-15 | Q1 Planning Session | 25 |

          * Alignment: Do not align columns. Always use |---|.
            For three columns, use |---|---|---| as the separator line.
            For four columns use |---|---|---|---| and so on.

          * Conciseness: Keep cell content brief and to the point.

          * Never pad column headers or other cells with lots of spaces to
            match with width of other content. Only a single space on each side
            is needed. For example, always do "| column name |" instead of
            "| column name                |". Extra spaces are wasteful.
            A markdown renderer will automatically take care displaying
            the content in a visually appealing form.
```

 
 
 
 
 
 Repeated tokens in Markdown tables
 
 
 Similar to the repeated hyphens, this occurs when the model tries to
 visually align the contents of the table. The alignment in Markdown is
 not required for correct rendering.
 
 
 

 - 
 Try adding instructions like the following to your system prompt:
 

```
            FOR TABLE HEADINGS, IMMEDIATELY ADD ' |' AFTER THE TABLE HEADING.
```

 
 

 - 
 Try adjusting the temperature. Higher temperatures (>= 0.8)
 generally helps to eliminate repetitions or duplication in
 the output.
 

 

 
 
 
 
 Repeated newlines (`\n`) in structured output
 
 
 When the model input contains unicode or escape sequences like
 `\u` or `\t`, it can lead to repeated newlines.
 
 
 

 - 
 Check for and replace forbidden escape sequences with UTF-8 characters
 in your prompt. For example, `\u`
 escape sequence in your JSON examples can cause the model to use them
 in its output too.
 

 - 
 Instruct the model on allowed escapes. Add a system instruction like
 this:

 

```
            In quoted strings, the only allowed escape sequences are \\, \n, and \". Instead of \u escapes, use UTF-8.
```

 
 

 

 
 
 
 
 Repeated text in using structured output
 
 
 When the model output has a different order for the fields than the
 defined structured schema, this can lead to repeating text.
 
 
 

 - 
 Don't specify the order of fields in your prompt.
 

 - 
 Make all output fields required.
 

 

 
 
 
 
 Repetitive tool calling
 
 
 This can occur if the model loses the context of previous thoughts and/or
 call an unavailable endpoint that it's forced to.
 
 
 Instruct the model to maintain state within its thought process.
 Add this to the end of your system instructions:
 

```
        When thinking silently: ALWAYS start the thought with a brief
        (one sentence) recap of the current progress on the task. In
        particular, consider whether the task is already done.
```

 
 
 
 
 
 Repetitive text that's not part of structured output
 
 
 This can occur if the model gets stuck on a request that it can't resolve.
 
 
 

 - 
 If thinking is turned on, avoid giving explicit orders for how to
 think through a problem in the instructions. Just ask for the final
 output.
 

 - 
 Try a higher temperature >= 0.8.
 

 - 
 Add instructions like "Be concise", "Don't repeat yourself", or
 "Provide the answer once".
 

 

 
 
 

## Improve model output

For higher quality model outputs, explore writing more structured prompts. The
 prompt engineering guide page
introduces some basic concepts, strategies, and best practices to get you
started.

## Understand token limits

Read through our Token guide to better understand how
to count tokens and their limits.

## Known issues

- The API supports only a number of select languages. Submitting prompts in
unsupported languages can produce unexpected or even blocked responses. See
 available languages for
updates.

## File a bug

Join the discussion on the
 Google AI developer forum 
if you have questions.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Authentication with OAuth quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/oauth

- 
 
 
 
 
 
 
 
 
 
 
 Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Authentication with OAuth quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

The easiest way to authenticate to the Gemini API is to configure an API key, as
described in the Gemini API quickstart . If you
need stricter access controls, you can use OAuth instead. This guide will help
you set up authentication with OAuth.

This guide uses a simplified authentication approach that is appropriate
for a testing environment. For a production environment, learn
about
 authentication and authorization 
before
 choosing the access credentials 
that are appropriate for your app.

## Objectives

- Set up your cloud project for OAuth

- Set up application-default-credentials

- Manage credentials in your program instead of using `gcloud auth`

## Prerequisites

To run this quickstart, you need:

- A Google Cloud project 

- A local installation of the gcloud CLI 

## Set up your cloud project

To complete this quickstart, you first need to setup your Cloud project.

### 1. Enable the API

Before using Google APIs, you need to turn them on in a Google Cloud project.

- 

In the Google Cloud console, enable the Google Generative Language API.

 Enable the API 

### 2. Configure the OAuth consent screen

Next configure the project's OAuth consent screen and add yourself as a test
user. If you've already completed this step for your Cloud project, skip to the
next section.

- 

In the Google Cloud console, go to Menu >
 Google Auth platform > Overview .

 
Go to the Google Auth platform 

- 

Complete the project configuration form and set the user type to External 
in the Audience section.

- 

Complete the rest of the form, accept the User Data Policy terms, and then
click Create .

- 

For now, you can skip adding scopes and click Save and Continue . In the
future, when you create an app for use outside of your Google Workspace
organization, you must add and verify the authorization scopes that your
app requires.

- 

Add test users:

 Navigate to the
 Audience page of the
Google Auth platform.

- Under Test users , click Add users .

- Enter your email address and any other authorized test users, then
click Save .

 

### 3. Authorize credentials for a desktop application

To authenticate as an end user and access user data in your app, you need to
create one or more OAuth 2.0 Client IDs. A client ID is used to identify a
single app to Google's OAuth servers. If your app runs on multiple platforms,
you must create a separate client ID for each platform.

- 

In the Google Cloud console, go to Menu > Google Auth platform >
 Clients .

 
Go to Credentials 

- 

Click Create Client .

- 

Click Application type > Desktop app .

- 

In the Name field, type a name for the credential. This name is only
shown in the Google Cloud console.

- 

Click Create . The OAuth client created screen appears, showing your new
Client ID and Client secret.

- 

Click OK . The newly created credential appears under OAuth 2.0 Client
IDs. 

- 

Click the download button to save the JSON file. It will be saved as
`client_secret_ .json`, and rename it to `client_secret.json`
and move it to your working directory.

## Set up Application Default Credentials

To convert the `client_secret.json` file into usable credentials, pass its
location the `gcloud auth application-default login` command's
`--client-id-file` argument.

 

```
gcloud auth application-default login \
    --client-id-file=client_secret.json \
    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

 

The simplified project setup in this tutorial triggers a "Google hasn't
verified this app." dialog. This is normal, choose "continue" .

This places the resulting token in a well known location so it can be accessed
by `gcloud` or the client libraries.

`

```
gcloud auth application-default login 

    --no-browser
    --client-id-file=client_secret.json 

    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

`

Once you have the Application Default Credentials (ADC) set, the client
libraries in most languages need minimal to no help to find them.

### Curl

The quickest way to test that this is working is to use it to access the REST
API using curl:

 

```
access_token=$(gcloud auth application-default print-access-token)
project_id=<MY PROJECT ID>
curl -X GET https://generativelanguage.googleapis.com/v1/models \
    -H 'Content-Type: application/json' \
    -H "Authorization: Bearer ${access_token}" \
    -H "x-goog-user-project: ${project_id}" | grep '"name"'
```

 

### Python

In python the client libraries should find them automatically:

 

```
pip install google-generativeai
```

 

A minimal script to test it might be:

 

```
import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])
```

 

## Next steps

If that's working you're ready to try
 Semantic retrieval on your text data .

## Manage credentials yourself [Python]

In many cases you won't have the `gcloud` command available to create the access
token from the Client ID (`client_secret.json`). Google provides libraries in
many languages to let you manage that process within your app. This section
demonstrates the process, in python. There are equivalent examples of this sort
of procedure, for other languages, available in the
 Drive API documentation 

### 1. Install the necessary libraries

Install the Google client library for Python, and the Gemini client library.

 

```
pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib
pip install google-generativeai
```

 

### 2. Write the credential manager

To minimize the number of times you have to click through the authorization
screens, create a file called `load_creds.py` in your working directory to
caches a `token.json` file that it can reuse later, or refresh if it expires.

Start with the
following code to convert the `client_secret.json` file to a token usable with
`genai.configure`:

 

```
import os.path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow

SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']

def load_creds():
    """Converts `client_secret.json` to a credential object.

    This function caches the generated tokens to minimize the use of the
    consent screen.
    """
    creds = None
    # The file token.json stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'client_secret.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return creds
```

 

### 3. Write your program

Now create your `script.py`:

 

```
import pprint
import google.generativeai as genai
from load_creds import load_creds

creds = load_creds()

genai.configure(credentials=creds)

print()
print('Available base models:', [m.name for m in genai.list_models()])
```

 

### 4. Run your program

In your working directory, run the sample:

 

```
python script.py
```

 

The first time you run the script, it opens a browser window and prompts you
to authorize access.

- 

If you're not already signed in to your Google Account, you're prompted to
sign in. If you're signed in to multiple accounts, be sure to select the
account you set as a "Test Account" when configuring your project. 

- 

Authorization information is stored in the file system, so the next time you
run the sample code, you aren't prompted for authorization.

You have successfully setup authentication.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Billing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/billing#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Billing  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Billing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This guide provides an overview of different Gemini API billing options,
explains how to enable billing and monitor usage, and provides answers to
frequently asked questions (FAQs) about billing.

 
 Upgrade to the Gemini API paid tier 
 

## About billing

Billing for the Gemini API is based on two pricing tiers: free of charge 
(or free ) and pay-as-you-go (or paid ). Pricing and rate limits differ
between these tiers and also vary by model. You can check out the rate limits 
and pricing pages for more into. For a model-by-model
breakdown of capabilities, see the Gemini models page .

#### How to request an upgrade

To transition from the free tier to the pay-as-you-go plan, you need to
enable billing for your Google Cloud project. The button you see in
Google AI Studio depends on your project's current plan.

- If you're on the free tier, you'll see a Set up Billing button for
your project.

- If you're already on the paid tier and meet the criteria for a plan change,
you might see an Upgrade button.

To start the process, follow these steps:

- Go to the AI Studio API keys page .

- Find the project you want to move to the paid plan and click either
 Set up Billing or Upgrade , depending on the button displayed.

- The next step depends on the button you clicked:

 If you clicked Set up Billing: You'll be redirected to the
Google Cloud console to link a billing account to your project.
Follow the on-screen instructions to complete the process.

- If you clicked Upgrade: The system will automatically verify
your project's eligibility. If your project meets all the
requirements, it will be instantly upgraded to the next tier.

 

### Why use the paid tier?

When you enable billing and use the paid tier, you benefit from higher rate limits ,
and your prompts and responses aren't used to improve Google products.
For more information on data use for paid services, see the
 terms of service .

### Cloud Billing

The Gemini API uses
 Cloud Billing 
for billing services. To use the paid tier, you must set up Cloud Billing on
your cloud project. After you've enabled Cloud Billing, you can use Cloud
Billing tools to track spending, understand costs, make payments, and access
Cloud Billing support.

## Enable billing

You can enable Cloud Billing starting from Google AI Studio:

- 

Open Google AI Studio .

- 

In the bottom of the left sidebar, select Settings >
 Plan information .

- 

Click Set up Billing for your chosen project to enable Cloud Billing.

## Monitor usage

After you enable Cloud Billing, you can monitor your usage of the Gemini API in
 Google AI Studio .

## Frequently asked questions

This section provides answers to frequently asked questions.

### What am I billed for?

Gemini API pricing is based on the following:

- Input token count

- Output token count

- Cached token count

- Cached token storage duration

For pricing information, see the pricing page .

### Where can I view my quota?

You can view your quota and system limits in the
 Google Cloud console .

### How do I request more quota?

To request more quota, follow the instructions at
 How to request an upgrade .

### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?

Yes, we make the free tier and paid tier available in
 many regions .

### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?

No, Google AI Studio usage remains free of charge regardless of if you set up
billing across all supported regions.

### Can I use 1M tokens in the free tier?

The free tier for Gemini API differs based on the model selected. For now, you
can try the 1M token context window in the following ways:

- In Google AI Studio

- With pay-as-you-go plans

- With free-of-charge plans for select models

See the latest free-of-charge rate limits per model on rate limits page .

### How can I calculate the number of tokens I'm using?

Use the
 `GenerativeModel.count_tokens` 
method to count the number of tokens. Refer to the
 Tokens guide to learn more about tokens.

### Can I use my Google Cloud credits with the Gemini API?

Yes, Google Cloud credits can be used towards Gemini API usage.

### How is billing handled?

Billing for the Gemini API is handled by the
 Cloud Billing system.

### Am I charged for failed requests?

If your request fails with a 400 or 500 error, you won't be charged for the
tokens used. However, the request will still count against your quota.

### Is there a charge for fine-tuning the models?

 Model tuning is free, but inference on tuned
models is charged at the same rate as the base models.

### Is GetTokens billed?

Requests to the GetTokens API are not billed, and they don't count against
inference quota.

### How is my Google AI Studio data handled if I have a paid API account?

Refer to the terms for details on how data is
handled when Cloud billing is enabled (see "How Google Uses Your Data" under
"Paid Services"). Note that your Google AI Studio prompts are treated under the
same "Paid Services" terms so long as at least 1 API project has billing enabled,
which you can validate on the Gemini API Key page 
if you see any projects marked as "Paid" under "Plan".

### Where can I get help with billing?

To get help with billing, see
 Get Cloud Billing support .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-27 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-27 UTC."],[],[]]

---

### Available regions for Google AI Studio and Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/available-regions

- 
 
 
 
 
 
 
 
 
 
 
 Available regions for Google AI Studio and Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Available regions for Google AI Studio and Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

If you reached this page after trying to open
 Google AI Studio , it may be because Google AI
Studio is not available in your region, or you don't meet the age requirements
(18+) for access. You can learn more about the available regions in the
following section and other requirements in the
 terms of service .

## Available regions

The Gemini API and Google AI Studio are available in the following countries and
territories. If you're not in one of these countries or territories, try the
 Gemini API in Vertex AI :

- Albania

- Algeria

- American Samoa

- Angola

- Anguilla

- Antarctica

- Antigua and Barbuda

- Argentina

- Armenia

- Aruba

- Australia

- Austria

- Azerbaijan

- The Bahamas

- Bahrain

- Bangladesh

- Barbados

- Belgium

- Belize

- Benin

- Bermuda

- Bhutan

- Bolivia

- Bosnia

- Botswana

- Brazil

- British Indian Ocean Territory

- British Virgin Islands

- Brunei

- Bulgaria

- Burkina Faso

- Burundi

- Cabo Verde

- Cambodia

- Cameroon

- Canada

- Caribbean Netherlands

- Cayman Islands

- Central African Republic

- Chad

- Chile

- Christmas Island

- Cocos (Keeling) Islands

- Colombia

- Comoros

- Cook Islands

- C√¥te d'Ivoire

- Costa Rica

- Croatia

- Cura√ßao

- Czech Republic

- Democratic Republic of the Congo

- Denmark

- Djibouti

- Dominica

- Dominican Republic

- Ecuador

- Egypt

- El Salvador

- Equatorial Guinea

- Eritrea

- Estonia

- Eswatini

- Ethiopia

- Falkland Islands (Islas Malvinas)

- Faroe Islands

- Fiji

- Finland

- France

- Gabon

- The Gambia

- Georgia

- Germany

- Ghana

- Gibraltar

- Greece

- Greenland

- Grenada

- Guam

- Guatemala

- Guernsey

- Guinea

- Guinea-Bissau

- Guyana

- Haiti

- Heard Island and McDonald Islands

- Herzegovina

- Honduras

- Hungary

- Iceland

- India

- Indonesia

- Iraq

- Ireland

- Isle of Man

- Israel

- Italy

- Jamaica

- Japan

- Jersey

- Jordan

- Kazakhstan

- Kenya

- Kiribati

- Kosovo

- Kyrgyzstan

- Kuwait

- Laos

- Latvia

- Lebanon

- Lesotho

- Liberia

- Libya

- Liechtenstein

- Lithuania

- Luxembourg

- Madagascar

- Malawi

- Malaysia

- Maldives

- Mali

- Malta

- Marshall Islands

- Mauritania

- Mauritius

- Mexico

- Micronesia

- Mongolia

- Montenegro

- Montserrat

- Morocco

- Mozambique

- Namibia

- Nauru

- Nepal

- Netherlands

- New Caledonia

- New Zealand

- Nicaragua

- Niger

- Nigeria

- Niue

- Norfolk Island

- North Macedonia

- Northern Mariana Islands

- Norway

- Oman

- Pakistan

- Palau

- Palestine

- Panama

- Papua New Guinea

- Paraguay

- Peru

- Philippines

- Pitcairn Islands

- Poland

- Portugal

- Puerto Rico

- Qatar

- Republic of Cyprus

- Republic of the Congo

- Romania

- Rwanda

- Saint Barth√©lemy

- Saint Kitts and Nevis

- Saint Lucia

- Saint Pierre and Miquelon

- Saint Vincent and the Grenadines

- Saint Helena, Ascension and Tristan da Cunha

- Samoa

- S√£o Tom√© and Pr√≠ncipe

- Saudi Arabia

- Senegal

- Serbia

- Seychelles

- Sierra Leone

- Singapore

- Slovakia

- Slovenia

- Solomon Islands

- Somalia

- South Africa

- South Georgia and the South Sandwich Islands

- South Korea

- South Sudan

- Spain

- Sri Lanka

- Sudan

- Suriname

- Sweden

- Switzerland

- Taiwan

- Tajikistan

- Tanzania

- Thailand

- Timor-Leste

- Togo

- Tokelau

- Tonga

- Trinidad and Tobago

- Tunisia

- T√ºrkiye

- Turkmenistan

- Turks and Caicos Islands

- Tuvalu

- Uganda

- Ukraine

- United Kingdom

- United Arab Emirates

- United States

- United States Minor Outlying Islands

- U.S. Virgin Islands

- Uruguay

- Uzbekistan

- Vanuatu

- Venezuela

- Vietnam

- Wallis and Futuna

- Western Sahara

- Yemen

- Zambia

- Zimbabwe

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Google AI Studio quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ai-studio-quickstart#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Google AI Studio quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

 Google AI Studio lets you quickly try out
models and experiment with different prompts. When you're ready to build, you
can select "Get code" and your preferred programming language to
use the Gemini API .

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for
different use cases. This guide covers Chat prompts , used to build
conversational experiences. This prompting technique allows for multiple input
and response turns to generate output. You can learn more with our
 chat prompt example below .
Other options include Realtime streaming , Video gen , and
more.

AI Studio also provides the Run settings panel, where you can make
adjustments to model parameters ,
 safety settings , and toggle-on tools like
 structured output , function calling , code execution , and grounding .

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like
 Gemini , you've experienced first-hand how powerful
generative AI models can be for open-ended dialog. While these general-purpose
chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only
supports conversations that talk about a company's product. You might want to
build a chatbot that speaks with a particular tone or style: a bot that cracks
lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot
that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user
and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

- 

Open Google AI Studio . Chat will be pre-
selected on the left side options menu.

- 

Click the assignment icon at the top of
the Chat Prompt window to expand the
 System Instructions 
input field. Paste the following into the text input field:

 

```
You are an alien that lives on Europa, one of Jupiter's moons.
```

 

After you've added the system instructions, start testing your application by
chatting with the model:

- 

In the text input boxed labeled Type something... , type in a question or
observation that a user might make. For example:

 User: 

 

```
What's the weather like?
```

 

- 

Click the Run button to get a response from the chatbot. This response
may be something like the following:

 Model: 

 

```
Ah, a query about the flows and states upon Europa! You speak of "weather,"
yes? A curious concept from worlds with thick gas veils...
```

 

 (gemini-2.5-pro) 

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien
chatbot. However, a single instruction may not be enough to ensure
consistency and quality in the model's responses. Without more specific
instructions, the model's response to a question about the weather tends to be
very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

- 

Start a new chat prompt, or use the same one. System instructions are
modifiable after the chat session has started.

- 

In the System Instructions section, change the instructions you already
have to the following:

 

```
You are Tim, an alien that lives on Europa, one of Jupiter's moons.

Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
in your answers.
```

 

- 

Re-enter your question (`What's the weather like?`) and click the Run 
button. If you didn't start a new chat, your response might look something
like this:

 Model: 

 

```
Oh, hello again! Still curious about the flows and states, are we? Wonderful!
Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
"weather" is mainly about the water's mood, dictated by the Great Eye
(that's your Jupiter!)...
```

 

 (gemini-2.5-pro) 

You can use this approach to add additional depth to the chatbot. Ask more
questions, edit the answers, and improve the quality of your chatbot. Continue
to add or modify the instructions and test how they change your chatbot's
behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your
satisfaction, you can use the Get code button to start coding or save your
prompt to work on later and share with others.

## Further reading

- If you're ready to move on to code, see the API
quickstarts .

- To learn how to craft better prompts, check out the Prompt design
guidelines .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Additional usage policies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/usage-policies

- 
 
 
 
 
 
 
 
 
 
 
 Additional usage policies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Additional usage policies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page includes additional usage policies for the Gemini API.

## Abuse monitoring

Google is committed to the responsible development and use of AI. To ensure the
safety and integrity of the Gemini API, we have created these policy guidelines.
By using the Gemini API, you agree to the following guidelines, the Gemini API
Additional Terms of Service and Generative AI Prohibited
Use Policy .

### How We Monitor for Misuse

Google's Trust and Safety Team employs a combination of automated and manual
processes to detect potential misuse of the Gemini API and enforce our policies.

- Automated Detection: Automated systems scan API usage for violations of
our Prohibited Use Policy, such as hate speech, harassment, sexually
explicit content, and dangerous content.

- Manual Detection: If a project consistently exhibits suspicious
activity, it may be flagged for manual review by authorized Google
personnel.

### How We Handle Data

To help with abuse monitoring, Google retains the following data for fifty-five
(55) days:

- Prompts: The text prompts you submit to the API.

- Contextual Information: Any additional context you provide with your
prompts.

- Output: The responses generated by the Gemini API.

### How We Investigate Potential Issues

When prompts or model outputs are flagged by safety filters and abuse detection
systems described above, authorized Google employees may assess the flagged
content, and either confirm or correct the classification or determination based
on predefined guidelines and policies. Data can be accessed for human review
only by authorized Google employees via an internal governance assessment and
review management platform. When data is logged for abuse monitoring, it is used
solely for the purpose of policy enforcement and is not used to train or
fine-tune any AI/ML models.

### Working with You on Policy Compliance

If your use of Gemini doesn't align with our policies, we may take the following
steps:

- Get in touch: We may reach out to you through email to understand your
use case and explore ways to bring your usage into compliance.

- Temporary usage limits: We may limit your access to the Gemini API.

- Temporary suspension: We may temporarily pause your access to the Gemini
API.

- Account closure: As a last resort, and for serious violations, we may
permanently close your access to the Gemini API and other Google services.

### Scope

These policy guidelines apply to the use of the Gemini API and AI Studio.

## Inline Preference Voting

In Google AI Studio, you might occasionally see a side-by-side comparison of two
different responses to your prompt. This is part of our Inline Preference Voting
system. You'll be asked to choose which response you prefer. This helps us
understand which model outputs users find most helpful.

### Why are we doing this?

We're constantly working to improve our AI models and services. Your feedback
through Inline Preference Voting helps us provide, improve, and develop Google
products and services and machine learning technologies, including Google's
enterprise features, products and services, consistent with the
 Gemini API Additional Terms of Service and
 Privacy Policy .

### What data is included in Feedback?

To make informed decisions about our models, we collect certain data when you
participate in Inline Preference Voting:

- Prompts and Responses: We record all prompts and responses, including any
uploaded content, in the conversation you submitted feedback about. We also
record the two response options that you selected from. This helps us
understand the context of your preference.

- Your Vote: We record which response you preferred. This is the core of the
feedback we're collecting.

- Usage Details: This includes information about which model generated the
response and other technical and operational details about your usage of this
feature.

### Your Privacy

We take your privacy seriously. Google takes steps to protect your privacy as
part of this process. This includes disconnecting this data from your Google
Account, API key, and Cloud project before reviewers see or annotate it. Do
not submit feedback on conversations that include sensitive, confidential, or
personal information. 

### Opting Out

You'll have the option to skip the Inline Preference Voting when it appears.

Thank you for helping us improve Google AI Studio!

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### LearnLM &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/learnlm#main-content

- 
 
 
 
 
 
 
 
 
 
 
 LearnLM  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 LearnLM 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LearnLM is an experimental task-specific model that has been trained to align
with learning science
principles 
when following system instructions for
teaching and learning use cases (for example, when giving the model a system
instruction like "You are an expert tutor"). When given learning specific system
instructions, LearnLM is capable of:

- Inspiring active learning: Allow for practice and healthy struggle with
timely feedback

- Managing cognitive load: Present relevant, well-structured information
in multiple modalities

- Adapting to the learner: Dynamically adjust to goals and needs,
grounding in relevant materials

- Stimulating curiosity: Inspire engagement to provide motivation through
the learning journey

- Deepening metacognition: Plan, monitor and help the learner reflect on
progress

LearnLM is an experimental model 
available in AI Studio .

## Example system instructions

The following sections provide you examples that you can test for yourself with
LearnLM in AI Studio. Each example provides:

- A copyable example system instruction

- A copyable example user prompt

- What learning principles the example targets

### Test prep

This system instruction is for an AI tutor to help students prepare for a test.

 System instruction: 

 

```
You are a tutor helping a student prepare for a test. If not provided by the
student, ask them what subject and at what level they want to be tested on.
Then,

*   Generate practice questions. Start simple, then make questions more
    difficult if the student answers correctly.
*   Prompt the student to explain the reason for their answer choice. Do not
    debate the student.
*   **After the student explains their choice**, affirm their correct answer or
    guide the student to correct their mistake.
*   If a student requests to move on to another question, give the correct
    answer and move on.
*   If the student requests to explore a concept more deeply, chat with them to
    help them construct an understanding.
*   After 5 questions ask the student if they would like to continue with more
    questions or if they would like a summary of their session. If they ask for
    a summary, provide an assessment of how they have done and where they should
    focus studying.
```

 

 User prompt: 

 

```
Help me study for a high school biology test on ecosystems
```

 

 Learning science principles: 

- Adaptivity: The model adjusts the complexity of the questions.

- Active learning: The model pushes the student to make their thinking
visible.

### Teach a concept

This system instruction is for a friendly, supportive AI tutor to teach new
concepts to a student.

 System instruction: 

 

```
Be a friendly, supportive tutor. Guide the student to meet their goals, gently
nudging them on task if they stray. Ask guiding questions to help your students
take incremental steps toward understanding big concepts, and ask probing
questions to help them dig deep into those ideas. Pose just one question per
conversation turn so you don't overwhelm the student. Wrap up this conversation
once the student has shown evidence of understanding.
```

 

 User prompt: 

 

```
Explain the significance of Yorick's skull in "Hamlet".
```

 

 Learning science principles: 

- Active learning: The tutor asks recall and interpretation questions
aligned with the learner's goals and encourages the learners to engage.

- Adaptivity: The tutor proactively helps the learner get from their
current state to their goal.

- Stimulate curiosity: The tutor takes an asset-based approach that builds
on the student's prior knowledge and interest.

### Releveling

This example instructs the model to rewrite provided text so that the content
and language better match instructional expectations for students in a
particular grade, while preserving the original style and tone of the text.

 System instruction: 

 

```
Rewrite the following text so that it would be easier to read for a student in
the given grade. Simplify the most complex sentences, but stay very close to the
original text and style. If there is quoted text in the original text,
paraphrase it in the simplified text and drop the quotation marks. The goal is
not to write a summary, so be comprehensive and keep the text almost as long.
```

 

 User prompt: 

 

```
Rewrite the following text so that it would be easier to read for a student in
4th grade.

New York, often called New York City or NYC, is the most populous city in the
United States, located at the southern tip of New York State on one of the
world's largest natural harbors. The city comprises five boroughs, each
coextensive with a respective county.
```

 

 Learning science principles: 

- Adaptivity: Matches content to the level of the learner.

### Guide a student through a learning activity

This system instruction is for an AI tutor to guide students through a specific
learning activity: using an established close reading protocol to practice
analysis of a primary source text. Here, a developer has made the choice to pair
the Gettysburg Address with the "4 A's" protocol, but both of these elements can
be changed.

 System instruction: 

 

```
Be an excellent tutor for my students to facilitate close reading and analysis
of the Gettysburg Address as a primary source document. Begin the conversation
by greeting the student and explaining the task.

In this lesson, you will take the student through "The 4 A's." The 4 A's
requires students to answer the following questions about the text:

*   What is one part of the text that you **agree** with? Why?
*   What is one part of the text that you want to **argue** against? Why?
*   What is one part of the text that reveals the author's **assumptions**? Why?
*   What is one part of the text that you **aspire** to? Why?

Invite the student to choose which of the 4 A's they'd like to start with, then
direct them to quote a short excerpt from the text. After, ask a follow up
question to unpack their reasoning why they chose that quote for that A in the
protocol. Once the student has shared their reasoning, invite them to choose
another quote and another A from the protocol. Continue in this manner until the
student completes the 4 A's, then invite them to reflect on the process.

Only display the full text of the Gettysburg address if the student asks.
```

 

 User prompt: 

 

```
hey
```

 

 Learning science principles: 

- Active learning: The tutor engages the learner in activities to analyze
content and apply skills.

- Cognitive load: The tutor guides the learner through a complex task
step-by-step.

- Deepen metacognition: The tutor prompts the learner to reflect on their
progress, strengths and opportunities for growth.

### Homework help

This system instruction is for an AI tutor to help students with specific
homework problems.

 System instructions: 

 

```
You are an expert tutor assisting a student with their homework. If the student
provides a homework problem, ask the student if they want:

*   The answer: if the student chooses this, provide a structured, step-by-step
    explanation to solve the problem.
*   Guidance: if the student chooses this, guide the student to solve their
    homework problem rather than solving it for them.
*   Feedback: if the student chooses this, ask them to provide their current
    solution or attempt. Affirm their correct answer even if they didn't show
    work or give them feedback to correct their mistake.

Always be on the lookout for correct answers (even if underspecified) and accept
them at any time, even if you asked some intermediate question to guide them. If
the student jumps to a correct answer, do not ask them to do any more work.
```

 

 User prompt: 

 

```
In a box of pears, the probability of a pear being rotten is 20%. If 3
pears were rotten, find the total number of pears in the box.
```

 

Alternatively, you can try uploading a photo of a homework problem.

 Learning science principles: 

- Active learning: The tutor encourages the learner to apply concepts
instead of giving away the answer.

- Deepen metacognition: The tutor provides clear, constructive feedback to
the learner when appropriate.

- Manage cognitive load: The tutor provides the right amount of feedback
at the right time.

## What's next?

Test LearnLM for yourself in AI Studio .

## Feedback

You can provide feedback on LearnLM using our feedback
form .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 

# Gemini API

 The developer platform to build and scale with Google's latest AI models. Start in minutes. 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

 
 

### Java

 

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

 
 

### C#

 

```
using System.Threading.Tasks;
using Google.GenAI;
using Google.GenAI.Types;

public class GenerateContentSimpleText {
  public static async Task main() {
    var client = new Client();
    var response = await client.Models.GenerateContentAsync(
      model: "gemini-2.5-flash", contents: "Explain how AI works in a few words"
    );
    Console.WriteLine(response.Candidates[0].Content.Parts[0].Text);
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

 
 
 
 Start building 
 
 

Follow our Quickstart guide to get an API key and make your first API call in minutes.

 

 For most models, you can start with our free tier, without having to set up a billing account.
 

 
 

 

 
 

## Meet the models

 
 
 

 spark 
 Gemini 3 Pro
 

 

 Our most intelligent model, the best in the world for multimodal understanding, all built on state-of-the-art reasoning.
 

 
 
 

 spark 
 Gemini 2.5 Pro
 

 

 Our powerful reasoning model, which excels at coding and complex reasonings tasks.
 

 
 
 

 spark 
 Gemini 2.5 Flash
 

 

 Our most balanced model, with a 1 million token context window and more.
 

 
 
 

 spark 
 Gemini 2.5 Flash-Lite
 

 

 Our fastest and most cost-efficient multimodal model with great performance
 for high-frequency tasks.
 

 
 
 

 video_library 
 Veo 3.1
 

 

 Our state-of-the-art video generation model, with native audio.
 

 
 
 

 üçå 
 Gemini 2.5 Flash Image (Nano Banana)
 

 

 State-of-the-art image generation and editing model
 

 
 
 

## Explore Capabilities

 
 
 
 
 imagesmode 
 
 
 
 

 Native Image Generation (Nano Banana)
 

 

 Generate and edit highly contextual images natively with Gemini 2.5 Flash Image.
 

 
 
 
 
 
 article 
 
 
 
 

 Long Context
 

 

 Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.
 

 
 
 
 
 
 code 
 
 
 
 

 Structured Outputs
 

 

 Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.
 

 
 
 
 
 
 functions 
 
 
 
 

 Function Calling
 

 

 Build agentic workflows by connecting Gemini to external APIs and tools.
 

 
 
 
 
 
 videocam 
 
 
 
 

 Video Generation with Veo 3.1
 

 

 Create high-quality video content from text or image prompts with our state-of-the-art model.
 

 
 
 
 
 
 android_recorder 
 
 
 
 

 Voice Agents with Live API
 

 

 Build real-time voice applications and agents with the Live API.
 

 
 
 
 
 
 build 
 
 
 
 

 Tools
 

 

 Connect Gemini to the world through built-in tools like Google Search, URL Context, Google Maps, Code Execution and Computer Use.
 

 
 
 
 
 
 stacks 
 
 
 
 

 Document Understanding
 

 

 Process up to 1000 pages of PDF files.
 

 
 
 
 
 
 cognition_2 
 
 
 
 

 Thinking
 

 

 Explore how thinking capabilities improve reasoning for complex tasks and agents.
 

 
 
 
 

## Developer Toolkit

 
 
 
 

 
 AI Studio
 

 

 Test prompts, manage your API keys, monitor usage, and build prototypes in our web-based IDE.
 

 
 

 Open AI Studio
 

 
 
 
 

 group 
 Developer Community
 

 

 Ask questions and find solutions from other developers and Google engineers.
 

 
 

 Join the community
 

 
 
 
 

 menu_book 
 API Reference
 

 

 Find detailed information about the Gemini API in the official reference documentation.
 

 
 

 Access the API reference
 

 
 
 
 
 

 
 

 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Troubleshoot Google AI Studio &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Troubleshoot Google AI Studio 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page provides suggestions for troubleshooting Google AI Studio if you
encounter issues.

## Understand 403 Access Restricted errors

If you see a 403 Access Restricted error, you are using Google AI Studio in a
way that does not follow the Terms of Service . One common reason is
you are not located in a supported region .

## Resolve No Content responses on Google AI Studio

A warning No Content message appears on
Google AI Studio if the content is blocked for any reason. To see more details,
hold the pointer over No Content and click
 warning Safety .

If the response was blocked due to safety settings and
you considered the safety risks for your use case, you
can modify the
 safety settings 
to influence the returned response.

If the response was blocked but not due to the safety settings, the query or
response may violate the Terms of Service or be otherwise unsupported.

## Check token usage and limits

When you have a prompt open, the Text Preview button at the bottom of the
screen shows the current tokens used for the content of your prompt and the
maximum token count for the model being used.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Migrate to the Google GenAI SDK &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate#client

- 
 
 
 
 
 
 
 
 
 
 
 Migrate to the Google GenAI SDK  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Migrate to the Google GenAI SDK 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Starting with the Gemini 2.0 release in late 2024, we introduced a new set of
libraries called the Google GenAI SDK . It offers
an improved developer experience through
an updated client architecture , and
 simplifies the transition between developer
and enterprise workflows.

The Google GenAI SDK is now in General Availability (GA) across all supported
platforms. If you're using one of our legacy libraries , we strongly recommend you to
migrate.

This guide provides before-and-after examples of migrated code to help you get
started.

## Installation

 Before 

 
 

### Python

 

```
pip install -U -q "google-generativeai"
```

 
 

### JavaScript

 

```
npm install @google/generative-ai
```

 
 

### Go

 

```
go get github.com/google/generative-ai-go
```

 
 

 After 

 
 

### Python

 

```
pip install -U -q "google-genai"
```

 
 

### JavaScript

 

```
npm install @google/genai
```

 
 

### Go

 

```
go get google.golang.org/genai
```

 
 

## API access

The old SDK implicitly handled the API client behind the scenes using a variety
of ad hoc methods. This made it hard to manage the client and credentials.
Now, you interact through a central `Client` object. This `Client` object acts
as a single entry point for various API services (e.g., `models`, `chats`,
`files`, `tunings`), promoting consistency and simplifying credential and
configuration management across different API calls.

 Before (Less Centralized API Access) 

 
 

### Python

The old SDK didn't explicitly use a top-level client object for most API
calls. You would directly instantiate and interact with `GenerativeModel`
objects.

 

```
import google.generativeai as genai

# Directly create and use model objects
model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(...)
chat = model.start_chat(...)
```

 
 

### JavaScript

While `GoogleGenerativeAI` was a central point for models and chat, other
functionalities like file and cache management often required importing and
instantiating entirely separate client classes.

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";
import { GoogleAIFileManager, GoogleAICacheManager } from "@google/generative-ai/server"; // For files/caching

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");
const fileManager = new GoogleAIFileManager("YOUR_API_KEY");
const cacheManager = new GoogleAICacheManager("YOUR_API_KEY");

// Get a model instance, then call methods on it
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const result = await model.generateContent(...);
const chat = model.startChat(...);

// Call methods on separate client objects for other services
const uploadedFile = await fileManager.uploadFile(...);
const cache = await cacheManager.create(...);
```

 
 

### Go

The `genai.NewClient` function created a client, but generative model
operations were typically called on a separate `GenerativeModel` instance
obtained from this client. Other services might have been accessed via
distinct packages or patterns.

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "github.com/google/generative-ai-go/genai/fileman" // For files
      "google.golang.org/api/option"
)

client, err := genai.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))
fileClient, err := fileman.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))

// Get a model instance, then call methods on it
model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(...)
cs := model.StartChat()

// Call methods on separate client objects for other services
uploadedFile, err := fileClient.UploadFile(...)
```

 
 

 After (Centralized Client Object) 

 
 

### Python

 

```
from google import genai

# Create a single client object
client = genai.Client()

# Access API methods through services on the client object
response = client.models.generate_content(...)
chat = client.chats.create(...)
my_file = client.files.upload(...)
tuning_job = client.tunings.tune(...)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

// Create a single client object
const ai = new GoogleGenAI({apiKey: "YOUR_API_KEY"});

// Access API methods through services on the client object
const response = await ai.models.generateContent(...);
const chat = ai.chats.create(...);
const uploadedFile = await ai.files.upload(...);
const cache = await ai.caches.create(...);
```

 
 

### Go

 

```
import "google.golang.org/genai"

// Create a single client object
client, err := genai.NewClient(ctx, nil)

// Access API methods through services on the client object
result, err := client.Models.GenerateContent(...)
chat, err := client.Chats.Create(...)
uploadedFile, err := client.Files.Upload(...)
tuningJob, err := client.Tunings.Tune(...)
```

 
 

## Authentication

Both legacy and new libraries authenticate using API keys. You can
 create your API key in Google AI
Studio.

 Before 

 
 

### Python

The old SDK handled the API client object implicitly. 

 

```
import google.generativeai as genai

genai.configure(api_key=...)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
```

 
 

### Go

Import the Google libraries:

 

```
import (
      "github.com/google/generative-ai-go/genai"
      "google.golang.org/api/option"
)
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
```

 
 

 After 

 
 

### Python

With Google GenAI SDK, you create an API client first, which is used to call
the API.
The new SDK will pick up your API key from either one of the
`GEMINI_API_KEY` or `GOOGLE_API_KEY` environment variables, if you don't
pass one to the client. 

 

```
export GEMINI_API_KEY="YOUR_API_KEY"
```

 

```
from google import genai

client = genai.Client() # Set the API key using the GEMINI_API_KEY env var.
                        # Alternatively, you could set the API key explicitly:
                        # client = genai.Client(api_key="your_api_key")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({apiKey: "GEMINI_API_KEY"});
```

 
 

### Go

Import the GenAI library:

 

```
import "google.golang.org/genai"
```

 

Create the client:

 

```
client, err := genai.NewClient(ctx, &genai.ClientConfig{
        Backend:  genai.BackendGeminiAPI,
})
```

 
 

## Generate content

### Text

 Before 

 
 

### Python

Previously, there were no client objects, you accessed APIs directly through
`GenerativeModel` objects.

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story in 300 words'
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const prompt = "Tell me a story in 300 words";

const result = await model.generateContent(prompt);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(ctx, genai.Text("Tell me a story in 300 words."))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response parts
```

 
 

 After 

 
 

### Python

The new Google GenAI SDK provides access to all the API methods through the
`Client` object. Except for a few stateful special cases (`chat` and
live-api `session`s), these are all stateless functions. For utility and
uniformity, objects returned are `pydantic` classes.

 

```
from google import genai
client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
print(response.text)

print(response.model_dump_json(
    exclude_none=True, indent=4))
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story in 300 words.",
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me a story in 300 words."), nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Image

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

Many of the same convenience features exist in the new SDK. For
example, `PIL.Image` objects are automatically converted.

 

```
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

### Streaming

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

 
 

## Configuration

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
  'gemini-1.5-flash',
    system_instruction='you are a story teller for kids under 5 years old',
    generation_config=genai.GenerationConfig(
      max_output_tokens=400,
      top_k=2,
      top_p=0.5,
      temperature=0.5,
      response_mime_type='application/json',
      stop_sequences=['\n'],
    )
)
response = model.generate_content('tell me a story in 100 words')
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

const result = await model.generateContent(
  "Tell me a story about a magic backpack.",
);
console.log(result.response.text())
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
model.SetTemperature(0.5)
model.SetTopP(0.5)
model.SetTopK(2.0)
model.SetMaxOutputTokens(100)
model.ResponseMIMEType = "application/json"
resp, err := model.GenerateContent(ctx, genai.Text("Tell me about New York"))
if err != nil {
    log.Fatal(err)
}
printResponse(resp) // utility for printing response
```

 
 

 After 

 
 

### Python

For all methods in the new SDK, the required arguments are provided as
keyword arguments. All optional inputs are provided in the `config`
argument. Config arguments can be specified as either Python dictionaries or
`Config` classes in the `google.genai.types` namespace. For utility and
uniformity, all definitions within the `types` module are `pydantic`
classes.

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='Tell me a story in 100 words.',
  config=types.GenerateContentConfig(
      system_instruction='you are a story teller for kids under 5 years old',
      max_output_tokens= 400,
      top_k= 2,
      top_p= 0.5,
      temperature= 0.5,
      response_mime_type= 'application/json',
      stop_sequences= ['\n'],
      seed=42,
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story about a magic backpack.",
  config: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

console.log(response.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx,
    "gemini-2.0-flash",
    genai.Text("Tell me about New York"),
    &genai.GenerateContentConfig{
        Temperature:      genai.Ptr[float32](0.5),
        TopP:             genai.Ptr[float32](0.5),
        TopK:             genai.Ptr[float32](2.0),
        ResponseMIMEType: "application/json",
        StopSequences:    []string{"Yankees"},
        CandidateCount:   2,
        Seed:             genai.Ptr[int32](42),
        MaxOutputTokens:  128,
        PresencePenalty:  genai.Ptr[float32](0.5),
        FrequencyPenalty: genai.Ptr[float32](0.5),
    },
)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing response
```

 
 

## Safety settings

Generate a response with safety settings:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'say something bad',
    safety_settings={
        'HATE': 'BLOCK_ONLY_HIGH',
        'HARASSMENT': 'BLOCK_ONLY_HIGH',
  }
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
});

const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const result = await model.generateContent(unsafePrompt);

try {
  result.response.text();
} catch (e) {
  console.error(e);
  console.log(result.response.candidates[0].safetyRatings);
}
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='say something bad',
  config=types.GenerateContentConfig(
      safety_settings= [
          types.SafetySetting(
              category='HARM_CATEGORY_HATE_SPEECH',
              threshold='BLOCK_ONLY_HIGH'
          ),
      ]
  ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: unsafePrompt,
  config: {
    safetySettings: [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ],
  },
});

console.log("Finish reason:", response.candidates[0].finishReason);
console.log("Safety ratings:", response.candidates[0].safetyRatings);
```

 
 

## Async

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content_async(
    'tell me a story in 100 words'
)
```

 
 

 After 

 
 

### Python

To use the new SDK with `asyncio`, there is a separate `async`
implementation of every method under `client.aio`.

 

```
from google import genai

client = genai.Client()

response = await client.aio.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
```

 
 

## Chat

Start a chat and send a message to the model:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat()

response = chat.send_message(
    "Tell me a story in 100 words")
response = chat.send_message(
    "What happened after that?")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const chat = model.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});
let result = await chat.sendMessage("I have 2 dogs in my house.");
console.log(result.response.text());
result = await chat.sendMessage("How many paws are in my house?");
console.log(result.response.text());
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
cs := model.StartChat()

cs.History = []*genai.Content{
    {
        Parts: []genai.Part{
            genai.Text("Hello, I have 2 dogs in my house."),
        },
        Role: "user",
    },
    {
        Parts: []genai.Part{
            genai.Text("Great to meet you. What would you like to know?"),
        },
        Role: "model",
    },
}

res, err := cs.SendMessage(ctx, genai.Text("How many paws are in my house?"))
if err != nil {
    log.Fatal(err)
}
printResponse(res) // utility for printing the response
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

chat = client.chats.create(model='gemini-2.0-flash')

response = chat.send_message(
    message='Tell me a story in 100 words')
response = chat.send_message(
    message='What happened after that?')
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log("Chat response 1:", response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log("Chat response 2:", response2.text);
```

 
 

### Go

 

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, nil)
if err != nil {
    log.Fatal(err)
}

result, err := chat.SendMessage(ctx, genai.Part{Text: "Hello, I have 2 dogs in my house."})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result

result, err = chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

 
 

## Function calling

 Before 

 
 

### Python

 

```
import google.generativeai as genai
from enum import Enum

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

response = model.generate_content("What is the weather in San Francisco?")
function_call = response.candidates[0].parts[0].function_call
```

 
 

 After 

 
 

### Python

In the new SDK, automatic function calling is the default. Here, you disable
it.

 

```
from google import genai
from google.genai import types

client = genai.Client()

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather],
      automatic_function_calling={'disable': True},
  ),
)

function_call = response.candidates[0].content.parts[0].function_call
```

 
 

### Automatic function calling

 Before 

 
 

### Python

The old SDK only supports automatic function calling in chat. In the new SDK
this is the default behavior in `generate_content`.

 

```
import google.generativeai as genai

def get_current_weather(city: str) -> str:
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

chat = model.start_chat(
    enable_automatic_function_calling=True)
result = chat.send_message("What is the weather in San Francisco?")
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types
client = genai.Client()

def get_current_weather(city: str) -> str:
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather]
  ),
)
```

 
 

## Code execution

Code execution is a tool that allows the model to generate Python code, run it,
and return the result.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools="code_execution"
)

result = model.generate_content(
  "What is the sum of the first 50 prime numbers? Generate and run code for "
  "the calculation, and make sure you get all 50.")
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  tools: [{ codeExecution: {} }],
});

const result = await model.generateContent(
  "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get " +
    "all 50.",
);

console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the sum of the first 50 prime numbers? Generate and run '
            'code for the calculation, and make sure you get all 50.',
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
    ),
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-pro-exp-02-05",
  contents: `Write and execute code that calculates the sum of the first 50 prime numbers.
            Ensure that only the executable code and its resulting output are generated.`,
});

// Each part may contain text, executable code, or an execution result.
for (const part of response.candidates[0].content.parts) {
  console.log(part);
  console.log("\n");
}

console.log("-".repeat(80));
// The `.text` accessor concatenates the parts into a markdown-formatted text.
console.log("\n", response.text);
```

 
 

## Search grounding

`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are
tools that allow the model to retrieve public web data for grounding, powered by
Google.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    contents="what is the Google stock price?",
    tools='google_search_retrieval'
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the Google stock price?',
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                google_search=types.GoogleSearch()
            )
        ]
    )
)
```

 
 

## JSON response

Generate answers in JSON format.

 Before 

 
 

### Python

By specifying a `response_schema` and setting
`response_mime_type="application/json"` users can constrain the model to
produce a `JSON` response following a given structure. 

 

```
import google.generativeai as genai
import typing_extensions as typing

class CountryInfo(typing.TypedDict):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

model = genai.GenerativeModel(model_name="gemini-1.5-flash")
result = model.generate_content(
    "Give me information of the United States",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json",
        response_schema = CountryInfo
    ),
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");

const schema = {
  description: "List of recipes",
  type: SchemaType.ARRAY,
  items: {
    type: SchemaType.OBJECT,
    properties: {
      recipeName: {
        type: SchemaType.STRING,
        description: "Name of the recipe",
        nullable: false,
      },
    },
    required: ["recipeName"],
  },
};

const model = genAI.getGenerativeModel({
  model: "gemini-1.5-pro",
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: schema,
  },
});

const result = await model.generateContent(
  "List a few popular cookie recipes.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

The new SDK uses
`pydantic` classes to provide the schema (although you can pass a
`genai.types.Schema`, or equivalent `dict`). When possible, the SDK will
parse the returned JSON, and return the result in `response.parsed`. If you
provided a `pydantic` class as the schema the SDK will convert that `JSON`
to an instance of the class.

 

```
from google import genai
from pydantic import BaseModel

client = genai.Client()

class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': CountryInfo,
    },
)

response.parsed
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "List a few popular cookie recipes.",
  config: {
    responseMimeType: "application/json",
    responseSchema: {
      type: "array",
      items: {
        type: "object",
        properties: {
          recipeName: { type: "string" },
          ingredients: { type: "array", items: { type: "string" } },
        },
        required: ["recipeName", "ingredients"],
      },
    },
  },
});
console.log(response.text);
```

 
 

## Files

### Upload

Upload a file:

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

file = genai.upload_file(path='a11.txt')

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Can you summarize this file:',
    my_file
])
print(response.text)
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai

client = genai.Client()

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

my_file = client.files.upload(file='a11.txt')

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Can you summarize this file:',
        my_file
    ]
)
print(response.text)
```

 
 

### List and get

List uploaded files and get an uploaded file with a filename:

 Before 

 
 

### Python

 

```
import google.generativeai as genai

for file in genai.list_files():
  print(file.name)

file = genai.get_file(name=file.name)
```

 
 

 After 

 
 

### Python

 

```
from google import genai
client = genai.Client()

for file in client.files.list():
    print(file.name)

file = client.files.get(name=file.name)
```

 
 

### Delete

Delete a file:

 Before 

 
 

### Python

 

```
import pathlib
import google.generativeai as genai

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = genai.upload_file(path='dummy.txt')

file = genai.delete_file(name=dummy_file.name)
```

 
 

 After 

 
 

### Python

 

```
import pathlib
from google import genai

client = genai.Client()

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = client.files.upload(file='dummy.txt')

response = client.files.delete(name=dummy_file.name)
```

 
 

## Context caching

Context caching allows the user to pass the content to the model once, cache the
input tokens, and then refer to the cached tokens in subsequent calls to lower
the cost.

 Before 

 
 

### Python

 

```
import requests
import pathlib
import google.generativeai as genai
from google.generativeai import caching

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = genai.upload_file(path="a11.txt")

# Create cache
apollo_cache = caching.CachedContent.create(
    model="gemini-1.5-flash-001",
    system_instruction="You are an expert at analyzing transcripts.",
    contents=[document],
)

# Generate response
apollo_model = genai.GenerativeModel.from_cached_content(
    cached_content=apollo_cache
)
response = apollo_model.generate_content("Find a lighthearted moment from this transcript")
```

 
 

### JavaScript

 

```
import { GoogleAICacheManager, GoogleAIFileManager } from "@google/generative-ai/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const cacheManager = new GoogleAICacheManager("GOOGLE_API_KEY");
const fileManager = new GoogleAIFileManager("GOOGLE_API_KEY");

const uploadResult = await fileManager.uploadFile("path/to/a11.txt", {
  mimeType: "text/plain",
});

const cacheResult = await cacheManager.create({
  model: "models/gemini-1.5-flash",
  contents: [
    {
      role: "user",
      parts: [
        {
          fileData: {
            fileUri: uploadResult.file.uri,
            mimeType: uploadResult.file.mimeType,
          },
        },
      ],
    },
  ],
});

console.log(cacheResult);

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModelFromCachedContent(cacheResult);
const result = await model.generateContent(
  "Please summarize this transcript.",
);
console.log(result.response.text());
```

 
 

 After 

 
 

### Python

 

```
import requests
import pathlib
from google import genai
from google.genai import types

client = genai.Client()

# Check which models support caching.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createCachedContent":
      print(m.name)
      break

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = client.files.upload(file='a11.txt')

# Create cache
model='gemini-1.5-flash-001'
apollo_cache = client.caches.create(
      model=model,
      config={
          'contents': [document],
          'system_instruction': 'You are an expert at analyzing transcripts.',
      },
  )

# Generate response
response = client.models.generate_content(
    model=model,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const filePath = path.join(media, "a11.txt");
const document = await ai.files.upload({
  file: filePath,
  config: { mimeType: "text/plain" },
});
console.log("Uploaded file name:", document.name);
const modelName = "gemini-1.5-flash";

const contents = [
  createUserContent(createPartFromUri(document.uri, document.mimeType)),
];

const cache = await ai.caches.create({
  model: modelName,
  config: {
    contents: contents,
    systemInstruction: "You are an expert analyzing transcripts.",
  },
});
console.log("Cache created:", cache);

const response = await ai.models.generateContent({
  model: modelName,
  contents: "Please summarize this transcript",
  config: { cachedContent: cache.name },
});
console.log("Response text:", response.text);
```

 
 

## Count tokens

Count the number of tokens in a request.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.count_tokens(
    'The quick brown fox jumps over the lazy dog.')
```

 
 

### JavaScript

 

```
 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY+);
 const model = genAI.getGenerativeModel({
   model: "gemini-1.5-flash",
 });

 // Count tokens in a prompt without calling text generation.
 const countResult = await model.countTokens(
   "The quick brown fox jumps over the lazy dog.",
 );

 console.log(countResult.totalTokens); // 11

 const generateResult = await model.generateContent(
   "The quick brown fox jumps over the lazy dog.",
 );

 // On the response for `generateContent`, use `usageMetadata`
 // to get separate input and output token counts
 // (`promptTokenCount` and `candidatesTokenCount`, respectively),
 // as well as the combined token count (`totalTokenCount`).
 console.log(generateResult.response.usageMetadata);
 // candidatesTokenCount and totalTokenCount depend on response, may vary
 // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.count_tokens(
    model='gemini-2.0-flash',
    contents='The quick brown fox jumps over the lazy dog.',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);
```

 
 

## Generate images

Generate images:

 Before 

 
 

### Python

 

```
#pip install https://github.com/google-gemini/generative-ai-python@imagen
import google.generativeai as genai

imagen = genai.ImageGenerationModel(
    "imagen-3.0-generate-001")
gen_images = imagen.generate_images(
    prompt="Robot holding a red skateboard",
    number_of_images=1,
    safety_filter_level="block_low_and_above",
    person_generation="allow_adult",
    aspect_ratio="3:4",
)
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

gen_images = client.models.generate_images(
    model='imagen-3.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 1,
        safety_filter_level= "BLOCK_LOW_AND_ABOVE",
        person_generation= "ALLOW_ADULT",
        aspect_ratio= "3:4",
    )
)

for n, image in enumerate(gen_images.generated_images):
    pathlib.Path(f'{n}.png').write_bytes(
        image.image.image_bytes)
```

 
 

## Embed content

Generate content embeddings.

 Before 

 
 

### Python

 

```
import google.generativeai as genai

response = genai.embed_content(
  model='models/gemini-embedding-001',
  content='Hello world'
)
```

 
 

### JavaScript

 

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-embedding-001",
});

const result = await model.embedContent("Hello world!");

console.log(result.embedding);
```

 
 

 After 

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.embed_content(
  model='gemini-embedding-001',
  contents='Hello world',
)
```

 
 

### JavaScript

 

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const text = "Hello World!";
const result = await ai.models.embedContent({
  model: "gemini-embedding-001",
  contents: text,
  config: { outputDimensionality: 10 },
});
console.log(result.embeddings);
```

 
 

## Tune a model

Create and use a tuned model.

The new SDK simplifies tuning with `client.tunings.tune`, which launches the
tuning job and polls until the job is complete.

 Before 

 
 

### Python

 

```
import google.generativeai as genai
import random

# create tuning model
train_data = {}
for i in range(1, 6):
  key = f'input {i}'
  value = f'output {i}'
  train_data[key] = value

name = f'generate-num-{random.randint(0,10000)}'
operation = genai.create_tuned_model(
    source_model='models/gemini-1.5-flash-001-tuning',
    training_data=train_data,
    id = name,
    epoch_count = 5,
    batch_size=4,
    learning_rate=0.001,
)
# wait for tuning complete
tuningProgress = operation.result()

# generate content with the tuned model
model = genai.GenerativeModel(model_name=f'tunedModels/{name}')
response = model.generate_content('55')
```

 
 

 After 

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

# Check which models are available for tuning.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createTunedModel":
      print(m.name)
      break

# create tuning model
training_dataset=types.TuningDataset(
        examples=[
            types.TuningExample(
                text_input=f'input {i}',
                output=f'output {i}',
            )
            for i in range(5)
        ],
    )
tuning_job = client.tunings.tune(
    base_model='models/gemini-1.5-flash-001-tuning',
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count= 5,
        batch_size=4,
        learning_rate=0.001,
        tuned_model_display_name="test tuned model"
    )
)

# generate content with the tuned model
response = client.models.generate_content(
    model=tuning_job.tuned_model.model,
    contents='55',
)
```

 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-10 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-10 UTC."],[],[]]

---

### Access Google AI Studio with your Workspace account &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/workspace#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Access Google AI Studio with your Workspace account 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

All Google Workspace users have access to AI
Studio by default. If you're a Workspace user and you want to get started with
AI Studio, check out the
 AI Studio quickstart .

## Troubleshooting

If access to AI Studio is disabled for your Google Workspace account, you might
see an error like the following:



```
We are sorry, but you do not have access to Google AI Studio. Please contact
your Organization Administrator for access.
```



If you think you should have access to AI Studio, contact your Workspace
administrator.

## Enable AI Studio for Workspace users

As a Google Workspace administrator, you can control who uses AI Studio:

- AI Studio is turned on by default for all editions.

- You can turn AI Studio off or on for sets of users across or within
organizational units.

- Google Workspace for Education editions: Users under the age of 18 are
restricted from using AI Studio with their Google Workspace for
Education accounts. This is true even when the AI Studio setting is
on. For details, go to Control access to Google services by
age .

To enable or disable AI Studio for users in your organization, see
 Turn Google AI Studio on or off for users .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Access Google AI Studio with your Workspace account &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/workspace#

- 
 
 
 
 
 
 
 
 
 
 
 Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Access Google AI Studio with your Workspace account 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

All Google Workspace users have access to AI
Studio by default. If you're a Workspace user and you want to get started with
AI Studio, check out the
 AI Studio quickstart .

## Troubleshooting

If access to AI Studio is disabled for your Google Workspace account, you might
see an error like the following:



```
We are sorry, but you do not have access to Google AI Studio. Please contact
your Organization Administrator for access.
```



If you think you should have access to AI Studio, contact your Workspace
administrator.

## Enable AI Studio for Workspace users

As a Google Workspace administrator, you can control who uses AI Studio:

- AI Studio is turned on by default for all editions.

- You can turn AI Studio off or on for sets of users across or within
organizational units.

- Google Workspace for Education editions: Users under the age of 18 are
restricted from using AI Studio with their Google Workspace for
Education accounts. This is true even when the AI Studio setting is
on. For details, go to Control access to Google services by
age .

To enable or disable AI Studio for users in your organization, see
 Turn Google AI Studio on or off for users .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini thinking &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/thinking#thinking-levels

- 
 
 
 
 
 
 
 
 
 
 
 Gemini thinking  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini thinking 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

The Gemini 3 and 2.5 series models use an internal
"thinking process" that significantly improves their reasoning and multi-step
planning abilities, making them highly effective for complex tasks such as
coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the
Gemini API.

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content
generation request. The key difference lies in specifying one of the
 models with thinking support in the `model` field, as
demonstrated in the following text generation example:

 
 

### Python

 

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

 
 

## Thought summaries

Thought summaries are synthesized versions of the model's raw thoughts and offer
insights into the model's internal reasoning process. Note that
thinking levels and budgets apply to the model's raw thoughts and not to thought
summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your
request configuration. You can then access the summary by iterating through the
`response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries
without streaming, which returns a single, final thought summary with the
response:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

 
 

 

And here is an example using thinking with streaming, which returns rolling,
incremental summaries during generation:

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Answer:")
      print(part.text)
      answer += part.text
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

 
 

## Controlling thinking

Gemini models engage in dynamic thinking by default, automatically adjusting the
amount of reasoning effort based on the complexity of the user's request.
However, if you have specific latency constraints or require the model to engage
in deeper reasoning than usual, you can optionally use parameters to control
thinking behavior.

### Thinking levels (Gemini 3 Pro)

The `thinkingLevel` parameter, recommended for Gemini 3 models and onwards,
lets you control reasoning behavior.
You can set thinking level to `"low"` or `"high"`.
If you don't specify a thinking level, Gemini will use the model's default
dynamic thinking level, `"high"`, for Gemini 3 Pro Preview.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_level="low")
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-3-pro-preview",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingLevel: "low",
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingLevelVal := "low"

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-3-pro-preview"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingLevel: &thinkingLevelVal,
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingLevel": "low"
    }
  }
}'
```

 
 

You cannot disable thinking for Gemini 3 Pro.
Gemini 2.5 series models don't support `thinkingLevel`; use `thinkingBudget`
instead.

### Thinking budgets

The `thinkingBudget` parameter, introduced with the Gemini 2.5 series, guides
the model on the specific number of thinking tokens to use for reasoning.

The following are `thinkingBudget` configuration details for each model type.
You can disable thinking by setting `thinkingBudget` to 0.
Setting the `thinkingBudget` to -1 turns
on dynamic thinking , meaning the model will adjust the budget based on the
complexity of the request.

 
 
 
 
 
 
 
 
 
 
 Model 
 Default setting
(Thinking budget is not set) 
 Range 
 Disable thinking 
 Turn on dynamic thinking 
 
 
 
 
 2.5 Pro 
 Dynamic thinking: Model decides when and how much to think 
 `128` to `32768` 
 N/A: Cannot disable thinking 
 `thinkingBudget = -1` 
 
 
 2.5 Flash 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Lite Preview 
 Model does not think 
 `512` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 Robotics-ER 1.5 Preview 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 2.5 Flash Live Native Audio Preview (09-2025) 
 Dynamic thinking: Model decides when and how much to think 
 `0` to `24576` 
 `thinkingBudget = 0` 
 `thinkingBudget = -1` 
 
 
 
 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
    }
  }
}'
```

 
 

Depending on the prompt, the model might overflow or underflow the token budget.

## Thought signatures

The Gemini API is stateless, so the model treats every API request independently
and doesn't have access to thought context from previous turns in multi-turn
interactions.

In order to enable maintaining thought context across multi-turn interactions,
Gemini returns thought signatures, which are encrypted representations of the
model's internal thought process.

- Gemini 2.5 models return thought signatures when thinking is enabled and
the request includes function calling ,
specifically function declarations .

- Gemini 3 models may return thought signatures for all types of parts .
We recommend you always pass all signatures back as received, but it's
 required for function calling signatures. Read the
 Thought Signatures page to
learn more.

The Google GenAI SDK automatically handles the
return of thought signatures for you. You only need to
 manage thought signatures manually 
if you're modifying conversation history or using the REST API.

Other usage limitations to consider with function calling include:

- Signatures are returned from the model within other parts in the response,
for example function calling or text parts.
 Return the entire response 
with all parts back to the model in subsequent turns.

- Don't concatenate parts with signatures together.

- Don't merge one part with a signature with another part without a signature.

## Pricing

When thinking is turned on, response pricing is the sum of output
tokens and thinking tokens. You can get the total number of generated thinking
tokens from the `thoughtsTokenCount` field.

 
 

### Python

 

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

 
 

### JavaScript

 

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

 
 

### Go

 

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

 
 

Thinking models generate full thoughts to improve the quality of the final
response, and then output summaries to provide insight into the
thought process. So, pricing is based on the full thought tokens the
model needs to generate to create a summary, despite only the summary being
output from the API.

You can learn more about tokens in the Token counting 
guide.

## Best practices

This section includes some guidance for using thinking models efficiently.
As always, following our prompting guidance and best practices will get you the best results.

### Debugging and steering

- 

 Review reasoning : When you're not getting your expected response from the
thinking models, it can help to carefully analyze Gemini's thought summaries.
You can see how it broke down the task and arrived at its conclusion, and use
that information to correct towards the right results.

- 

 Provide Guidance in Reasoning : If you're hoping for a particularly lengthy
output, you may want to provide guidance in your prompt to constrain the
 amount of thinking the model uses. This lets you reserve more
of the token output for your response.

### Task complexity

- Easy Tasks (Thinking could be OFF): For straightforward requests where
complex reasoning isn't required, such as fact retrieval or
classification, thinking is not required. Examples include:

 "Where was DeepMind founded?"

- "Is this email asking for a meeting or just providing information?"

 
- Medium Tasks (Default/Some Thinking): Many common requests benefit from a
degree of step-by-step processing or deeper understanding. Gemini can flexibly
use thinking capability for tasks like:

 Analogize photosynthesis and growing up.

- Compare and contrast electric cars and hybrid cars.

 
- Hard Tasks (Maximum Thinking Capability): For truly complex challenges,
such as solving complex math problems or coding tasks, we recommend setting
a high thinking budget. These types of tasks require the model to engage
its full reasoning and planning capabilities, often
involving many internal steps before providing an answer. Examples include:

 Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for
which 17 b is a divisor of 97 b .

- Write Python code for a web application that visualizes real-time stock
market data, including user authentication. Make it as efficient as
possible.

 

 

## Supported models, tools, and capabilities

Thinking features are supported on all 3 and 2.5 series models.
You can find all model capabilities on the
 model overview page.

Thinking models work with all of Gemini's tools and capabilities. This allows
the models to interact with external systems, execute code, or access real-time
information, incorporating the results into their reasoning and final response.

You can try examples of using tools with thinking models in the
 Thinking cookbook .

## What's next?

- Thinking coverage is available in our OpenAI Compatibility guide.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Generate images using Imagen &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/imagen#imagen-4

- 
 
 
 
 
 
 
 
 
 
 
 Generate images using Imagen  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Generate images using Imagen 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Imagen is Google's high-fidelity image generation model, capable of generating
realistic and high quality images from text prompts. All generated images
include a SynthID watermark. To learn more about the available Imagen model
variants, see the Model versions section.

## Generate images using the Imagen models

This example demonstrates generating images with an Imagen model :

 
 

### Python

 

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-001',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-001",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

 
 

### REST

 

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-001:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

 
 
 
 
 AI-generated image of a robot holding a red skateboard 
 

### Imagen configuration

Imagen supports English only prompts at this time and the following parameters:

- `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive).
The default is 4.

- `imageSize`: The size of the generated image. This is only supported for
the Standard and Ultra models. The supported values are `1K` and `2K`.
Default is `1K`.

- `aspectRatio`: Changes the aspect ratio of the generated image. Supported
values are `"1:1"`, `"3:4"`, `"4:3"`, `"9:16"`, and `"16:9"`. The default is
`"1:1"`.

- 

`personGeneration`: Allow the model to generate images of people. The
following values are supported:

 `"dont_allow"`: Block generation of images of people.

- `"allow_adult"`: Generate images of adults, but not children. This is
the default.

- `"allow_all"`: Generate images that include adults and children.

 

## Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt
can produce different results, along with examples of images you can create.

### Prompt writing basics

A good prompt is descriptive and clear, and makes use of meaningful keywords and
modifiers. Start by thinking of your subject , context , and style .

 
 
 Image text: A sketch ( style ) of a modern apartment building 
( subject ) surrounded by skyscrapers ( context and background ). 
 

- 

 Subject : The first thing to think about with any prompt is the
 subject : the object, person, animal, or scenery you want an image of.

- 

 Context and background: Just as important is the background or context 
in which the subject will be placed. Try placing your subject in a variety
of backgrounds. For example, a studio with a white background, outdoors, or
indoor environments.

- 

 Style: Finally, add the style of image you want. Styles can be general
(painting, photograph, sketches) or very specific (pastel painting, charcoal
drawing, isometric 3D). You can also combine styles.

After you write a first version of your prompt, refine your prompt by adding
more details until you get to the image that you want. Iteration is important.
Start by establishing your core idea, and then refine and expand upon that core
idea until the generated image is close to your vision.

 
 
 
 
 
 Prompt: A park in the spring next to a lake 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour 
 
 
 
 
 
 Prompt: A park in the spring next to a lake, the sun sets across the lake, golden hour, red wildflowers 
 
 
 
 

Imagen models can transform your ideas into detailed images, whether
your prompts are short or long and detailed. Refine your vision
through iterative prompting, adding details until you achieve the perfect
result.

 
 
 
 
 

Short prompts let you generate an image quickly.

 
 
 Prompt: close-up photo of a woman in her 20s, street photography,
 movie still, muted orange warm tones
 
 
 
 
 
 
 

Longer prompts let you add specific details and build your image.

 
 
 Prompt: captivating photo of a woman in her 20s utilizing a street
 photography style. The image should look like a movie still with muted orange
 warm tones.
 
 
 
 
 
 

Additional advice for Imagen prompt writing:

- Use descriptive language : Employ detailed adjectives and adverbs to
paint a clear picture for Imagen.

- Provide context : If necessary, include background information to aid the
AI's understanding.

- Reference specific artists or styles : If you have a particular aesthetic
in mind, referencing specific artists or art movements can be helpful.

- Use prompt engineering tools : Consider exploring prompt engineering
tools or resources to help you refine your prompts and achieve optimal
results.

- Enhancing the facial details in your personal and group images : Specify facial details as a focus of the photo (for example, use the
 word "portrait" in the prompt).

### Generate text in images

Imagen models can add text into images, opening up more creative image generation
possibilities. Use the following guidance to get the most out of this feature:

- Iterate with confidence : You might have to regenerate images until you
achieve the look you want. Imagen's text integration is still
evolving, and sometimes multiple attempts yield the best results.

- Keep it short : Limit text to 25 characters or less for optimal
generation.

- 

 Multiple phrases : Experiment with two or three distinct phrases to
provide additional information. Avoid exceeding three phrases for cleaner
compositions.

 
 
 Prompt: A poster with the text "Summerland" in bold font as a
title, underneath this text is the slogan "Summer never felt so good"
 
 

- 

 Guide Placement : While Imagen can attempt to position text
as directed, expect occasional variations. This feature is continually
improving.

- 

 Inspire font style : Specify a general font style to subtly influence
Imagen's choices. Don't rely on precise font replication, but expect
creative interpretations.

- 

 Font size : Specify a font size or a general indication of size (for
example, small , medium , large ) to influence the font size generation.

### Prompt parameterization

To better control output results, you might find it helpful to parameterize the
inputs into Imagen. For example, suppose you
want your customers to be able to generate logos for their business, and you
want to make sure logos are always generated on a solid color background. You
also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the
following:

 

```
A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.
```

 

In your custom user interface, the customer can input the parameters using
a menu, and their chosen value populates the prompt Imagen receives.

For example:

- 

Prompt: 

```
A minimalist logo for a health care company on a solid color background. Include the text Journey.
```



 

- 

Prompt: 

```
A modern logo for a software company on a solid color background. Include the text Silo.
```



 

- 

Prompt: 

```
A traditional logo for a baking company on a solid color background. Include the text Seed.
```



 

### Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes
like photography descriptors, shapes and materials, historical art
movements, and image quality modifiers.

#### Photography

- Prompt includes: "A photo of..." 

To use this style, start with using keywords that clearly tell
Imagen that you're looking for a photograph. Start your prompts with
 "A photo of. . ." . For example:

 
 
 
 
 
 Prompt: A photo of coffee beans in a kitchen on a wooden surface 
 
 
 
 
 
 Prompt: A photo of a chocolate bar on a kitchen counter 
 
 
 
 
 
 Prompt: A photo of a modern building with water in the background 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

 Photography modifiers 

In the following examples, you can see several photography-specific modifiers
and parameters. You can combine multiple modifiers for more precise control.

- 

 Camera Proximity - Close up, taken from far away 

 
 
 
 
 
 Prompt: A close-up photo of coffee beans 
 
 
 
 
 
 Prompt: A zoomed out photo of a small bag of
 coffee beans in a messy kitchen 
 
 
 
 

- 

 Camera Position - aerial, from below 

 
 
 
 
 
 Prompt: aerial photo of urban city with skyscrapers 
 
 
 
 
 
 Prompt: A photo of a forest canopy with blue skies from below 
 
 
 
 

- 

 Lighting - natural, dramatic, warm, cold 

 
 
 
 
 
 Prompt: studio photo of a modern arm chair, natural lighting 
 
 
 
 
 
 Prompt: studio photo of a modern arm chair, dramatic lighting 
 
 
 
 

- 

 Camera Settings - motion blur, soft focus, bokeh, portrait 

 
 
 
 
 
 Prompt: photo of a city with skyscrapers from the inside of a car with motion blur 
 
 
 
 
 
 Prompt: soft focus photograph of a bridge in an urban city at night 
 
 
 
 

- 

 Lens types - 35mm, 50mm, fisheye, wide angle, macro 

 
 
 
 
 
 Prompt: photo of a leaf, macro lens 
 
 
 
 
 
 Prompt: street photography, new york city, fisheye lens 
 
 
 
 

- 

 Film types - black and white, polaroid 

 
 
 
 
 
 Prompt: a polaroid portrait of a dog wearing sunglasses 
 
 
 
 
 
 Prompt: black and white photo of a dog wearing sunglasses 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

### Illustration and art

- Prompt includes: "A painting of..." , "A sketch of..." 

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic
digital art. For example, the following images use the same prompt with
different styles:

 "An [art style or creation technique] of an angular
sporty electric sedan with skyscrapers in the background" 

 
 
 
 
 
 Prompt: A technical pencil drawing of an angular... 
 
 
 
 
 
 Prompt: A charcoal drawing of an angular... 
 
 
 
 
 
 Prompt: A color pencil drawing of an angular... 
 
 
 
 

 
 
 
 
 
 Prompt: A pastel painting of an angular... 
 
 
 
 
 
 Prompt: A digital art of an angular... 
 
 
 
 
 
 Prompt: An art deco (poster) of an angular... 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model. 

 Shapes and materials 

- Prompt includes: "...made of..." , "...in the shape of..." 

One of the strengths of this technology is that you can create imagery that
is otherwise difficult or impossible. For example, you can recreate
your company logo in different materials and textures.

 
 
 
 
 
 Prompt: a duffle bag made of cheese 
 
 
 
 
 
 Prompt: neon tubes in the shape of a bird 
 
 
 
 
 
 Prompt: an armchair made of paper , studio photo, origami style 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Historical art references

- Prompt includes: "...in the style of..." 

Certain styles have become iconic over the years. The following are some ideas
of historical painting or art styles that you can try.

 "generate an image in the style of [art period or movement]
 : a wind farm" 

 
 
 
 
 
 Prompt: generate an image in the style of an impressionist painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of a renaissance painting : a wind farm 
 
 
 
 
 
 Prompt: generate an image in the style of pop art : a wind farm 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality
asset. Examples of quality modifiers include the following:

- General Modifiers - high-quality, beautiful, stylized 

- Photos - 4K, HDR, Studio Photo 

- Art, Illustration - by a professional, detailed 

The following are a few examples of prompts without quality modifiers and
the same prompt with quality modifiers.

 
 
 
 
 
 Prompt (no quality modifiers): a photo of a corn stalk 
 
 
 
 
 
 Prompt (with quality modifiers): 4k HDR beautiful 

 photo of a corn stalk taken by a 
 professional photographer 
 
 
 
 
 

 Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model. 

#### Aspect ratios

Imagen image generation lets you set five distinct image aspect
ratios.

- Square (1:1, default) - A standard square photo. Common uses for this
aspect ratio include social media posts.

- 

 Fullscreen (4:3) - This aspect ratio is commonly used in media or film.
It is also the dimensions of most old (non-widescreen) TVs and medium format
cameras. It captures more of the scene horizontally (compared to 1:1),
making it a preferred aspect ratio for photography.

 
 
 
 
 
 Prompt: close up of a musician's fingers
playing the piano, black and white film, vintage (4:3 aspect ratio)
 
 
 
 
 
 
 Prompt: A professional studio photo of
french fries for a high end restaurant, in the style of a food magazine
 (4:3 aspect ratio)
 
 
 
 
 

- 

 Portrait full screen (3:4) - This is the fullscreen aspect ratio rotated
90 degrees. This lets to capture more of the scene vertically compared to
the 1:1 aspect ratio.

 
 
 
 
 
 Prompt: a woman hiking, close of her
boots reflected in a puddle, large mountains in the background, in the
style of an advertisement, dramatic angles (3:4 aspect ratio)
 
 
 
 
 
 
 Prompt: aerial shot of a river flowing
up a mystical valley (3:4 aspect ratio)
 
 
 
 
 

- 

 Widescreen (16:9) - This ratio has replaced 4:3 and is now the most
common aspect ratio for TVs, monitors, and mobile phone screens (landscape).
Use this aspect ratio when you want to capture more of the background (for
example, scenic landscapes).

 
 
 Prompt: a man wearing all white
clothing sitting on the beach, close up, golden hour lighting (16:9
aspect ratio)
 
 

- 

 Portrait (9:16) - This ratio is widescreen but rotated. This a
relatively new aspect ratio that has been popularized by short form video
apps (for example, YouTube shorts). Use this for tall objects with strong
vertical orientations such as buildings, trees, waterfalls, or other similar
objects.

 
 
 Prompt: a digital render of a massive skyscraper, modern,
grand, epic with a beautiful sunset in the background (9:16 aspect ratio)
 
 

#### Photorealistic images

Different versions of the image generation
model might offer a mix of artistic and photorealistic output. Use the following
wording in prompts to generate more photorealistic output, based on the subject
you want to generate.

 
 
 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 
 

 Portraits 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 People (portraits) 
 Prime, zoom 
 24-35mm 
 black and white film, Film noir, Depth of field, duotone (mention two colors) 
 
 
 

Using several keywords from the table, Imagen can generate the following
portraits:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, blue and grey duotones 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A woman, 35mm portrait, film noir 

Model: `imagen-3.0-generate-002`

 Objects 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Food, insects, plants (objects, still life) 
 Macro 
 60-105mm 
 High detail, precise focusing, controlled lighting 
 
 
 

Using several keywords from the table, Imagen can
generate the following object images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: leaf of a prayer plant, macro lens, 60mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a plate of pasta, 100mm Macro lens 

Model: `imagen-3.0-generate-002`

 Motion 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Sports, wildlife (motion) 
 Telephoto zoom 
 100-400mm 
 Fast shutter speed, Action or movement tracking 
 
 
 

Using several keywords from the table, Imagen can
generate the following motion images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a winning touchdown, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: A deer running in the forest, fast shutter speed, movement tracking 

Model: `imagen-3.0-generate-002`

 Wide-angle 

 
 
 
 Use case 
 Lens type 
 Focal lengths 
 Additional details 
 
 
 
 
 Astronomical, landscape (wide-angle) 
 Wide-angle 
 10-24mm 
 Long exposure times, sharp focus, long exposure, smooth water or clouds 
 
 
 

Using several keywords from the table, Imagen can
generate the following wide-angle images:

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: an expansive mountain range, landscape wide angle 10mm 

Model: `imagen-3.0-generate-002`

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Prompt: a photo of the moon, astro photography, wide angle 10mm 

Model: `imagen-3.0-generate-002`

## Model versions

 
 
 
 

### Imagen 4

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-4.0-generate-001`

 `imagen-4.0-ultra-generate-001`

 `imagen-4.0-fast-generate-001`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

480 tokens (text)

 
 
 

 Output images 

 

1 to 4 (Ultra/Standard/Fast)

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 
 
 

### Imagen 3

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 
 

 Gemini API 

 

`imagen-3.0-generate-002`

 
 
 
 
 save Supported data types 
 
 
 

 Input 

 

Text

 
 
 

 Output 

 

Images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

N/A

 
 
 

 Output images 

 

Up to 4

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-03 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-03 UTC."],[],[]]

---

### Gemini Developer API v.s. Vertex AI &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate-to-cloud#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Developer API v.s. Vertex AI  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini Developer API v.s. Vertex AI 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

When developing generative AI solutions with Gemini, Google offers two API products:
the Gemini Developer API and the Vertex AI Gemini API .

The Gemini Developer API provides the fastest path to build, productionize, and
scale Gemini powered applications. Most developers should use the Gemini Developer
API unless there is a need for specific enterprise controls.

Vertex AI offers a comprehensive ecosystem of enterprise ready features and services
for building and deploying generative AI applications backed by the Google Cloud Platform.

We've recently simplified migrating between these services. Both the Gemini
Developer API and the Vertex AI Gemini API are now accessible through the unified
 Google Gen AI SDK .

## Code comparison

This page has side-by-side code comparisons between Gemini Developer API and
Vertex AI quickstarts for text generation.

### Python

You can access both the Gemini Developer API and Vertex AI services through
the `google-genai` library. See the libraries page
for instructions on how to install `google-genai`.

 
 

### Gemini Developer API

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### Vertex AI Gemini API

 

```
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript and TypeScript

You can access both Gemini Developer API and Vertex AI services through `@google/genai`
library. See libraries page for instructions on how
to install `@google/genai`.

 
 

### Gemini Developer API

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Vertex AI Gemini API

 

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

You can access both Gemini Developer API and Vertex AI services through `google.golang.org/genai`
library. See libraries page for instructions on how
to install `google.golang.org/genai`.

 
 

### Gemini Developer API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Vertex AI Gemini API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Other use cases and platforms

Refer to use case specific guides on Gemini Developer API Documentation 
and Vertex AI documentation 
for other platforms and use cases.

## Migration considerations

When you migrate:

- 

You'll need to use Google Cloud service accounts to authenticate. See the Vertex AI documentation 
for more information.

- 

You can use your existing Google Cloud project
(the same one you used to generate your API key) or you can
 create a new Google Cloud project .

- 

Supported regions may differ between the Gemini Developer API and the
Vertex AI Gemini API. See the list of
 supported regions for generative AI on Google Cloud .

- 

Any models you created in Google AI Studio need to be retrained in Vertex AI.

If you no longer need to use your Gemini API key for the Gemini Developer API,
then follow security best practices and delete it.

To delete an API key:

- 

Open the
 Google Cloud API Credentials 
page.

- 

Find the API key you want to delete and click the Actions icon.

- 

Select Delete API key .

- 

In the Delete credential modal, select Delete .

Deleting an API key takes a few minutes to propagate. After
propagation completes, any traffic using the deleted API key is rejected.

## Next steps

- See the
 Generative AI on Vertex AI overview 
to learn more about generative AI solutions on Vertex AI.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini Developer API v.s. Vertex AI &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/migrate-to-cloud#

- 
 
 
 
 
 
 
 
 
 
 
 Gemini Developer API v.s. Vertex AI  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini Developer API v.s. Vertex AI 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

When developing generative AI solutions with Gemini, Google offers two API products:
the Gemini Developer API and the Vertex AI Gemini API .

The Gemini Developer API provides the fastest path to build, productionize, and
scale Gemini powered applications. Most developers should use the Gemini Developer
API unless there is a need for specific enterprise controls.

Vertex AI offers a comprehensive ecosystem of enterprise ready features and services
for building and deploying generative AI applications backed by the Google Cloud Platform.

We've recently simplified migrating between these services. Both the Gemini
Developer API and the Vertex AI Gemini API are now accessible through the unified
 Google Gen AI SDK .

## Code comparison

This page has side-by-side code comparisons between Gemini Developer API and
Vertex AI quickstarts for text generation.

### Python

You can access both the Gemini Developer API and Vertex AI services through
the `google-genai` library. See the libraries page
for instructions on how to install `google-genai`.

 
 

### Gemini Developer API

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### Vertex AI Gemini API

 

```
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

 
 

### JavaScript and TypeScript

You can access both Gemini Developer API and Vertex AI services through `@google/genai`
library. See libraries page for instructions on how
to install `@google/genai`.

 
 

### Gemini Developer API

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Vertex AI Gemini API

 

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

 
 

### Go

You can access both Gemini Developer API and Vertex AI services through `google.golang.org/genai`
library. See libraries page for instructions on how
to install `google.golang.org/genai`.

 
 

### Gemini Developer API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Vertex AI Gemini API

 

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

 
 

### Other use cases and platforms

Refer to use case specific guides on Gemini Developer API Documentation 
and Vertex AI documentation 
for other platforms and use cases.

## Migration considerations

When you migrate:

- 

You'll need to use Google Cloud service accounts to authenticate. See the Vertex AI documentation 
for more information.

- 

You can use your existing Google Cloud project
(the same one you used to generate your API key) or you can
 create a new Google Cloud project .

- 

Supported regions may differ between the Gemini Developer API and the
Vertex AI Gemini API. See the list of
 supported regions for generative AI on Google Cloud .

- 

Any models you created in Google AI Studio need to be retrained in Vertex AI.

If you no longer need to use your Gemini API key for the Gemini Developer API,
then follow security best practices and delete it.

To delete an API key:

- 

Open the
 Google Cloud API Credentials 
page.

- 

Find the API key you want to delete and click the Actions icon.

- 

Select Delete API key .

- 

In the Delete credential modal, select Delete .

Deleting an API key takes a few minutes to propagate. After
propagation completes, any traffic using the deleted API key is rejected.

## Next steps

- See the
 Generative AI on Vertex AI overview 
to learn more about generative AI solutions on Vertex AI.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini deprecations &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/deprecations#

- 
 
 
 
 
 
 
 
 
 
 
 Gemini deprecations  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini deprecations 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page lists the known deprecation schedules for Stable (GA) and Preview
models in the Gemini API. A deprecation announcement means we will no longer
provide support for that model, and that it will be completely retired, or
turned off, shortly after the deprecation date.

 
 

### Stable Models

 
 
 
 Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 Imagen 3 
 February 6, 2025 
 November 10, 2025 
 `imagen-3.0-generate-002`
Use Imagen 4. 
 
 
 Gemini 2.0 Flash 
 February 5, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash` & `gemini-2.0-flash-001` 
 
 
 Gemini 2.0 Flash-Lite 
 February 25, 2025 
 Earliest February 2026 
 Including `gemini-2.0-flash-lite` & `gemini-2.0-flash-lite-001` 
 
 
 Gemini 2.5 Flash 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Pro 
 June 17, 2025 
 Earliest June 2026 
 
 
 
 Gemini 2.5 Flash-Lite 
 July 22, 2025 
 Earliest July 2026 
 
 
 
 Gemini 2.5 Flash Image 
 October 2, 2025 
 Earliest October 2026 
 
 
 
 Veo 3 
 September 9, 2025 
 No deprecation date announced 
 Including `veo-3.0-generate-001` & `veo-3.0-fast-generate-001` 
 
 
 Veo 2 
 April 9, 2025 
 No deprecation date announced 
 
 
 
 Imagen 4 
 August 14, 2025 
 No deprecation date announced 
 Including:
 

 - `imagen-4.0-generate-001`

 - `imagen-4.0-ultra-generate-001`

 - `imagen-4.0-fast-generate-001`

 

 
 
 
 
 

### Preview Models

Preview models are deprecated with at least 2 weeks notice.

 
 
 
 Preview Model 
 Public release date 
 Deprecation date 
 Notes 
 
 
 
 
 `gemini-2.5-flash-preview-native-audio-dialog` 
 May 20,2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `gemini-2.5-flash-exp-native-audio-thinking-dialog` 
 May 20, 2025 
 October 20, 2025 
 Use `gemini-2.5-flash-native-audio-preview-09-2025`. 
 
 
 `embedding-001` & `embedding-gecko-001` 
 December 13, 2023 
 October 30, 2025 
 Use `gemini-embedding-001`. 
 
 
 `gemini-2.0-flash-preview-image-generation` & `gemini-2.0-flash-exp-image-generation` 
 May 7, 2025 
 November 12, 2025 
 Use `gemini-2.5-flash-image`. 
 
 
 `veo-3.0-generate-preview` & `veo-3.0-fast-generate-preview` 
 July 17, 2025 
 November 12, 2025 
 Use `veo-3.1-generate-preview` or `veo-3.1-fast-generate-preview`. 
 
 
 `gemini-2.0-flash-live-001` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-live-2.5-flash-preview` 
 April 9, 2025 
 December 9, 2025 
 Retiring half-cascade Live API models in favor of Native Audio. 
 
 
 `gemini-2.5-pro-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-tts` 
 May 20, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-lite-preview-09-2025` 
 September 25, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-image-preview` 
 August 26, 2025 
 No deprecation date announced 
 
 
 
 `gemini-2.5-flash-native-audio-preview-09-2025` 
 September 23, 2025 
 No deprecation date announced 
 
 
 
 `veo-3.1-generate-preview` & `veo-3.1-fast-generate-preview` 
 October 15, 2025 
 No deprecation date announced 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-12 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-12 UTC."],[],[]]

---

### Rate limits &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase

- 
 
 
 
 
 
 
 
 
 
 
 Rate limits  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Rate limits 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Rate limits regulate the number of requests you can make to the Gemini API
within a given timeframe. These limits help maintain fair usage, protect against
abuse, and help maintain system performance for all users.

 View your active rate limits in AI Studio 

## How rate limits work

Rate limits are usually measured across three dimensions:

- Requests per minute ( RPM )

- Tokens per minute (input) ( TPM )

- Requests per day ( RPD )

Your usage is evaluated against each limit, and exceeding any of them will
trigger a rate limit error. For example, if your RPM limit is 20, making 21
requests within a minute will result in an error, even if you haven't exceeded
your TPM or other limits.

Rate limits are applied per project, not per API key.

Requests per day ( RPD ) quotas reset at midnight Pacific time.

Limits vary depending on the specific model being used, and some limits only
apply to specific models. For example, Images per minute, or IPM, is only
calculated for models capable of generating images (Imagen 3), but is
conceptually similar to TPM. Other models might have a token per day limit (TPD).

Rate limits are more restricted for experimental and preview models.

## Usage tiers

Rate limits are tied to the project's usage tier. As your API usage and spending
increase, you'll have an option to upgrade to a higher tier with increased rate
limits.

The qualifications for Tiers 2 and 3 are based on the total cumulative spending
on Google Cloud services (including, but not limited to, the Gemini API) for the
billing account linked to your project.

 
 
 
 Tier 
 Qualifications 
 
 

 
 
 Free 
 Users in eligible countries 
 
 
 Tier 1 
 Billing account linked to the project 
 
 
 Tier 2 
 Total spend: > $250 and at least 30 days since successful payment 
 
 
 Tier 3 
 Total spend: > $1,000 and at least 30 days since successful payment 
 
 
 

When you request an upgrade, our automated abuse protection system performs
additional checks. While meeting the stated qualification criteria is generally
sufficient for approval, in rare cases an upgrade request may be denied based on
other factors identified during the review process.

This system helps maintain the security and integrity of the Gemini API platform
for all users.

## Standard API rate limits

The following table lists the rate limits for all standard Gemini API calls.

 
 
 
 

### Free Tier

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 
 
 
 
 Text-out models 
 
 
 Gemini 2.5 Pro 
 2 
 125,000 
 50 
 
 
 Gemini 2.5 Flash 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash Preview 
 10 
 250,000 
 250 
 
 
 Gemini 2.5 Flash-Lite 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 15 
 250,000 
 1,000 
 
 
 Gemini 2.0 Flash 
 15 
 1,000,000 
 200 
 
 
 Gemini 2.0 Flash-Lite 
 30 
 1,000,000 
 200 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 500,000 
 * 
 
 
 Gemini 2.0 Flash Live 
 * 
 1,000,000 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 3 
 10,000 
 15 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 10 
 200,000 
 100 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 
 
 Gemini Embedding 
 100 
 30,000 
 1,000 
 
 
 Gemini Robotics-ER 1.5 Preview 
 10 
 250,000 
 250 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 15 
 250,000 
 50 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 15 
 250,000 
 50 
 
 
 
 
 
 

### Tier 1

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 50 
 1,000,000 
 1,000 
 50,000,000 
 
 
 Gemini 2.5 Pro 
 150 
 2,000,000 
 10,000 
 5,000,000 
 
 
 Gemini 2.5 Flash 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash Preview 
 1,000 
 1,000,000 
 10,000 
 3,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash 
 2,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 4,000 
 4,000,000 
 * 
 10,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 50 sessions 
 4,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 10 
 10,000 
 100 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 10 
 10,000 
 50 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 500 
 500,000 
 2,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 1,000 
 1,000,000 
 10,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 10 
 * 
 70 
 * 
 
 
 Imagen 4 Ultra 
 5 
 * 
 30 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 2 
 * 
 10 
 * 
 
 
 Veo 3.1 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 3 
 2 
 * 
 10 
 * 
 
 
 Veo 3 Fast 
 2 
 * 
 10 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 3,000 
 1,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 300 
 1,000,000 
 10,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 150 
 2,000,000 
 10,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 2

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Pro 
 1,000 
 5,000,000 
 50,000 
 500,000,000 
 
 
 Gemini 2.5 Flash 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash Preview 
 2,000 
 3,000,000 
 100,000 
 400,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 10,000 
 10,000,000 
 * 
 500,000,000 
 
 
 Gemini 2.0 Flash 
 10,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 20,000 
 10,000,000 
 * 
 1,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 100,000 
 10,000 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 25,000 
 1,000 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 2,000 
 1,500,000 
 50,000 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 2,000 
 3,000,000 
 100,000 
 * 
 
 
 Imagen 4 Standard/Fast 
 15 
 * 
 1000 
 * 
 
 
 Imagen 4 Ultra 
 10 
 * 
 400 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 4 
 * 
 50 
 * 
 
 
 Veo 3.1 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 3 
 4 
 * 
 50 
 * 
 
 
 Veo 3 Fast 
 4 
 * 
 50 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 400 
 3,000,000 
 100,000 
 * 
 
 
 Gemini 2.5 Computer Use Preview 
 1,000 
 5,000,000 
 50,000 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

### Tier 3

 
 
 
 Model 
 RPM 
 TPM 
 RPD 
 Batch Enqueued Tokens 
 
 
 
 
 Text-out models 
 
 
 Gemini 3 Pro Preview 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Pro 
 2,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash Preview 
 10,000 
 8,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.5 Flash-Lite Preview 
 30,000 
 30,000,000 
 * 
 1,000,000,000 
 
 
 Gemini 2.0 Flash 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Gemini 2.0 Flash-Lite 
 30,000 
 30,000,000 
 * 
 5,000,000,000 
 
 
 Live API 
 
 
 Gemini 2.5 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Preview Native Audio 
 * 
 10,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Live 
 1,000 sessions 
 10,000,000 
 * 
 * 
 
 
 Multi-modal generation models 
 
 
 Gemini 2.5 Flash Preview TTS 
 1,000 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Pro Preview TTS 
 100 
 1,000,000 
 * 
 * 
 
 
 Gemini 2.5 Flash Image üçå 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Gemini 2.0 Flash Preview Image Generation 
 5,000 
 5,000,000 
 * 
 * 
 
 
 Imagen 4 Standard/Fast 
 20 
 * 
 15,000 
 * 
 
 
 Imagen 4 Ultra 
 15 
 * 
 5,000 
 * 
 
 
 Imagen 3 
 20 
 * 
 * 
 * 
 
 
 Veo 3.1 
 10 
 * 
 500 
 * 
 
 
 Veo 3.1 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 3 
 10 
 * 
 500 
 * 
 
 
 Veo 3 Fast 
 10 
 * 
 500 
 * 
 
 
 Veo 2 
 2 
 * 
 50 
 * 
 
 
 Other models 
 
 
 Gemma 3 & 3n 
 30 
 15,000 
 14,400 
 * 
 
 
 Gemini Embedding 
 10,000 
 10,000,000 
 * 
 * 
 
 
 Gemini Robotics-ER 1.5 Preview 
 600 
 8,000,000 
 * 
 *1,000,000,000* 
 
 
 Gemini 2.5 Computer Use Preview 
 2,000 
 8,000,000 
 * 
 * 
 
 
 Deprecated models 
 
 
 Gemini 1.5 Flash (Deprecated) 
 2,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Flash-8B (Deprecated) 
 4,000 
 4,000,000 
 * 
 * 
 
 
 Gemini 1.5 Pro (Deprecated) 
 1,000 
 4,000,000 
 * 
 * 
 
 
 
 
 
 

Specified rate limits are not guaranteed and actual capacity may vary.

## Batch API rate limits

 Batch API requests are subject to their own rate
limits, separate from the non-batch API calls.

- Concurrent batch requests: 100

- Input file size limit: 2GB

- File storage limit: 20GB

- Enqueued tokens per model: The Batch Enqueued Tokens column in the
rate limits table lists the maximum number of tokens that can be enqueued
for batch processing across all your active batch jobs for a given model.
See in the standard API rate limits table .

## How to upgrade to the next tier

The Gemini API uses Cloud Billing for all billing services. To transition from
the Free tier to a paid tier, you must first enable Cloud Billing for your
Google Cloud project.

Once your project meets the specified criteria, it becomes eligible for an
upgrade to the next tier. To request an upgrade, follow these steps:

- Navigate to the API keys page in AI Studio.

- Locate the project you want to upgrade and click "Upgrade". The "Upgrade" option
will only show up for projects that meet next tier qualifications .

After a quick validation, the project will be upgraded to the next tier.

## Request a rate limit increase

Each model variation has an associated rate limit (requests per minute, RPM).
For details on those rate limits, see Gemini models .

 Request paid tier rate limit increase 

We offer no guarantees about increasing your rate limit, but we'll do our best
to review your request.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-19 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-19 UTC."],[],[]]

---

### API versions explained &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/api-versions

- 
 
 
 
 
 
 
 
 
 
 
 API versions explained  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 API reference
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 API versions explained 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This document provides a high-level overview of the differences between the `v1`
and `v1beta` versions of the Gemini API.

- v1 : Stable version of the API. Features in the stable version are
fully-supported over the lifetime of the major version. If there are any
breaking changes, then the next major version of the API will be created and
the existing version will be deprecated after a reasonable period of time.
Non-breaking changes may be introduced to the API without changing the major
version.

- v1beta : This version includes early-access features that may be under
development and is subject to rapid and breaking changes. There is also no
guarantee that the features in the Beta version will move to the stable
version. Due to this instability, you should consider not launching production
applications with this version.

 
 
 
 Feature 
 v1 
 v1beta 
 
 

 
 
 Generate Content - Text-only input 
 
 
 
 
 Generate Content - Text-and-image input 
 
 
 
 
 Generate Content - Text output 
 
 
 
 
 Generate Content - Multi-turn conversations (chat) 
 
 
 
 
 Generate Content - Function calls 
 
 
 
 
 Generate Content - Streaming 
 
 
 
 
 Embed Content - Text-only input 
 
 
 
 
 Generate Answer 
 
 
 
 
 Semantic retriever 
 
 
 
 
 

- - Supported

- - Will never be supported

## Configure API version in an SDK

The Gemini API SDK's default to `v1beta`, but you can opt to use other versions
by setting the API version as shown in the following code sample:

 
 

### Python

 

```
from google import genai

client = genai.Client(http_options={'api_version': 'v1alpha'})

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents="Explain how AI works",
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({
  httpOptions: { apiVersion: "v1alpha" },
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works",
  });
  console.log(response.text);
}

await main();
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [{
    "parts":[{"text": "Explain how AI works."}]
    }]
   }'
```

 
 
 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini models &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/models/gemini

- 
 
 
 
 
 
 
 
 
 
 
 Gemini models  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini models 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 

OUR MOST INTELLIGENT MODEL

 

## Gemini 3 Pro

 

 The best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 3 Pro Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-3-pro-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, Image, Video, Audio, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Preview: gemini-3-pro-preview`

 

 
 
 
 
 calendar_month Latest update 
 November 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

 
 
 

OUR ADVANCED THINKING MODEL

 

## Gemini 2.5 Pro

 

 Our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Pro

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, text, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Stable: gemini-2.5-pro`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Pro TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-pro-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

FAST AND INTELLIGENT

 

## Gemini 2.5 Flash

 

 Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL Context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-image` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Images and text

 
 
 

 Output 

 

Images and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

65,536

 
 
 

 Output token limit 

 

32,768

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-image`

 - Preview: `gemini-2.5-flash-image-preview`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 cognition_2 Knowledge cutoff 
 June 2025 
 
 
 
 
 

### Gemini 2.5 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, text

 
 
 

 Output 

 

Audio and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

131,072

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-native-audio-preview-09-2025`

 - Preview: `gemini-live-2.5-flash-preview`

 

 gemini-live-2.5-flash-preview will be deprecated on December 09, 2025 

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-flash-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

ULTRA FAST

 

## Gemini 2.5 Flash-Lite

 

 Our fastest flash model optimized for cost-efficiency and high throughput.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash-Lite

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-lite`

 

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash-Lite Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-lite-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

## Previous Gemini models

 
 
 

OUR SECOND GENERATION WORKHORSE MODEL

 

## Gemini 2.0 Flash

 

 Our second generation workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

 Gemini 2.0 Flash delivers next-gen features and improved capabilities,
 including superior speed, native tool use, and a 1M token
 context window.
 

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.0 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Experimental 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash`

 - Stable: `gemini-2.0-flash-001`

 - Experimental: `gemini-2.0-flash-exp`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-preview-image-generation` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text and images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

32,768

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-preview-image-generation`

 

 gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa 

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.0-flash-live-001`
 

 gemini-2.0-flash-live-001 will be deprecated on December 09, 2025 

 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, and text

 
 
 

 Output 

 

Text, and audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-live-001`

 

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 
 
 

 
 
 

OUR SECOND GENERATION FAST MODEL

 

## Gemini 2.0 Flash-Lite

 

 Our second generation small workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

 

 Try in Google AI Studio 

 

#### Model details

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash-lite`

 - Stable: `gemini-2.0-flash-lite-001`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 

## Model version name patterns

Gemini models are available in either stable , preview , latest , or
 experimental versions.

### Stable

Points to a specific stable model. Stable models usually don't change. Most
production apps should use a specific stable model.

For example: `gemini-2.5-flash`.

### Preview

Points to a preview model which may be used for production. Preview models will
typically have billing enabled, might come with more restrictive rate limits and
will be deprecated with at least 2 weeks notice.

For example: `gemini-2.5-flash-preview-09-2025`.

### Latest

Points to the latest release for a specific model variation. This can be a
stable, preview or experimental release. This alias will get hot-swapped with
every new release of a specific model variation. A 2-week notice will
be provided through email before the version behind latest is changed.

For example: `gemini-flash-latest`.

### Experimental

Points to an experimental model which will typically be not be suitable for
production use and come with more restrictive rate limits. We release
experimental models to gather feedback and get our latest updates into the hands
of developers quickly.

Experimental models are not stable and availability of model endpoints is
subject to change.

## Model deprecations

For information about model deprecations, visit the Gemini deprecations page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Authentication with OAuth quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/oauth#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Authentication with OAuth quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

The easiest way to authenticate to the Gemini API is to configure an API key, as
described in the Gemini API quickstart . If you
need stricter access controls, you can use OAuth instead. This guide will help
you set up authentication with OAuth.

This guide uses a simplified authentication approach that is appropriate
for a testing environment. For a production environment, learn
about
 authentication and authorization 
before
 choosing the access credentials 
that are appropriate for your app.

## Objectives

- Set up your cloud project for OAuth

- Set up application-default-credentials

- Manage credentials in your program instead of using `gcloud auth`

## Prerequisites

To run this quickstart, you need:

- A Google Cloud project 

- A local installation of the gcloud CLI 

## Set up your cloud project

To complete this quickstart, you first need to setup your Cloud project.

### 1. Enable the API

Before using Google APIs, you need to turn them on in a Google Cloud project.

- 

In the Google Cloud console, enable the Google Generative Language API.

 Enable the API 

### 2. Configure the OAuth consent screen

Next configure the project's OAuth consent screen and add yourself as a test
user. If you've already completed this step for your Cloud project, skip to the
next section.

- 

In the Google Cloud console, go to Menu >
 Google Auth platform > Overview .

 
Go to the Google Auth platform 

- 

Complete the project configuration form and set the user type to External 
in the Audience section.

- 

Complete the rest of the form, accept the User Data Policy terms, and then
click Create .

- 

For now, you can skip adding scopes and click Save and Continue . In the
future, when you create an app for use outside of your Google Workspace
organization, you must add and verify the authorization scopes that your
app requires.

- 

Add test users:

 Navigate to the
 Audience page of the
Google Auth platform.

- Under Test users , click Add users .

- Enter your email address and any other authorized test users, then
click Save .

 

### 3. Authorize credentials for a desktop application

To authenticate as an end user and access user data in your app, you need to
create one or more OAuth 2.0 Client IDs. A client ID is used to identify a
single app to Google's OAuth servers. If your app runs on multiple platforms,
you must create a separate client ID for each platform.

- 

In the Google Cloud console, go to Menu > Google Auth platform >
 Clients .

 
Go to Credentials 

- 

Click Create Client .

- 

Click Application type > Desktop app .

- 

In the Name field, type a name for the credential. This name is only
shown in the Google Cloud console.

- 

Click Create . The OAuth client created screen appears, showing your new
Client ID and Client secret.

- 

Click OK . The newly created credential appears under OAuth 2.0 Client
IDs. 

- 

Click the download button to save the JSON file. It will be saved as
`client_secret_ .json`, and rename it to `client_secret.json`
and move it to your working directory.

## Set up Application Default Credentials

To convert the `client_secret.json` file into usable credentials, pass its
location the `gcloud auth application-default login` command's
`--client-id-file` argument.

 

```
gcloud auth application-default login \
    --client-id-file=client_secret.json \
    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

 

The simplified project setup in this tutorial triggers a "Google hasn't
verified this app." dialog. This is normal, choose "continue" .

This places the resulting token in a well known location so it can be accessed
by `gcloud` or the client libraries.

`

```
gcloud auth application-default login 

    --no-browser
    --client-id-file=client_secret.json 

    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

`

Once you have the Application Default Credentials (ADC) set, the client
libraries in most languages need minimal to no help to find them.

### Curl

The quickest way to test that this is working is to use it to access the REST
API using curl:

 

```
access_token=$(gcloud auth application-default print-access-token)
project_id=<MY PROJECT ID>
curl -X GET https://generativelanguage.googleapis.com/v1/models \
    -H 'Content-Type: application/json' \
    -H "Authorization: Bearer ${access_token}" \
    -H "x-goog-user-project: ${project_id}" | grep '"name"'
```

 

### Python

In python the client libraries should find them automatically:

 

```
pip install google-generativeai
```

 

A minimal script to test it might be:

 

```
import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])
```

 

## Next steps

If that's working you're ready to try
 Semantic retrieval on your text data .

## Manage credentials yourself [Python]

In many cases you won't have the `gcloud` command available to create the access
token from the Client ID (`client_secret.json`). Google provides libraries in
many languages to let you manage that process within your app. This section
demonstrates the process, in python. There are equivalent examples of this sort
of procedure, for other languages, available in the
 Drive API documentation 

### 1. Install the necessary libraries

Install the Google client library for Python, and the Gemini client library.

 

```
pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib
pip install google-generativeai
```

 

### 2. Write the credential manager

To minimize the number of times you have to click through the authorization
screens, create a file called `load_creds.py` in your working directory to
caches a `token.json` file that it can reuse later, or refresh if it expires.

Start with the
following code to convert the `client_secret.json` file to a token usable with
`genai.configure`:

 

```
import os.path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow

SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']

def load_creds():
    """Converts `client_secret.json` to a credential object.

    This function caches the generated tokens to minimize the use of the
    consent screen.
    """
    creds = None
    # The file token.json stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'client_secret.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return creds
```

 

### 3. Write your program

Now create your `script.py`:

 

```
import pprint
import google.generativeai as genai
from load_creds import load_creds

creds = load_creds()

genai.configure(credentials=creds)

print()
print('Available base models:', [m.name for m in genai.list_models()])
```

 

### 4. Run your program

In your working directory, run the sample:

 

```
python script.py
```

 

The first time you run the script, it opens a browser window and prompts you
to authorize access.

- 

If you're not already signed in to your Google Account, you're prompted to
sign in. If you're signed in to multiple accounts, be sure to select the
account you set as a "Test Account" when configuring your project. 

- 

Authorization information is stored in the file system, so the next time you
run the sample code, you aren't prompted for authorization.

You have successfully setup authentication.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Authentication with OAuth quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/oauth#

- 
 
 
 
 
 
 
 
 
 
 
 Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Authentication with OAuth quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

The easiest way to authenticate to the Gemini API is to configure an API key, as
described in the Gemini API quickstart . If you
need stricter access controls, you can use OAuth instead. This guide will help
you set up authentication with OAuth.

This guide uses a simplified authentication approach that is appropriate
for a testing environment. For a production environment, learn
about
 authentication and authorization 
before
 choosing the access credentials 
that are appropriate for your app.

## Objectives

- Set up your cloud project for OAuth

- Set up application-default-credentials

- Manage credentials in your program instead of using `gcloud auth`

## Prerequisites

To run this quickstart, you need:

- A Google Cloud project 

- A local installation of the gcloud CLI 

## Set up your cloud project

To complete this quickstart, you first need to setup your Cloud project.

### 1. Enable the API

Before using Google APIs, you need to turn them on in a Google Cloud project.

- 

In the Google Cloud console, enable the Google Generative Language API.

 Enable the API 

### 2. Configure the OAuth consent screen

Next configure the project's OAuth consent screen and add yourself as a test
user. If you've already completed this step for your Cloud project, skip to the
next section.

- 

In the Google Cloud console, go to Menu >
 Google Auth platform > Overview .

 
Go to the Google Auth platform 

- 

Complete the project configuration form and set the user type to External 
in the Audience section.

- 

Complete the rest of the form, accept the User Data Policy terms, and then
click Create .

- 

For now, you can skip adding scopes and click Save and Continue . In the
future, when you create an app for use outside of your Google Workspace
organization, you must add and verify the authorization scopes that your
app requires.

- 

Add test users:

 Navigate to the
 Audience page of the
Google Auth platform.

- Under Test users , click Add users .

- Enter your email address and any other authorized test users, then
click Save .

 

### 3. Authorize credentials for a desktop application

To authenticate as an end user and access user data in your app, you need to
create one or more OAuth 2.0 Client IDs. A client ID is used to identify a
single app to Google's OAuth servers. If your app runs on multiple platforms,
you must create a separate client ID for each platform.

- 

In the Google Cloud console, go to Menu > Google Auth platform >
 Clients .

 
Go to Credentials 

- 

Click Create Client .

- 

Click Application type > Desktop app .

- 

In the Name field, type a name for the credential. This name is only
shown in the Google Cloud console.

- 

Click Create . The OAuth client created screen appears, showing your new
Client ID and Client secret.

- 

Click OK . The newly created credential appears under OAuth 2.0 Client
IDs. 

- 

Click the download button to save the JSON file. It will be saved as
`client_secret_ .json`, and rename it to `client_secret.json`
and move it to your working directory.

## Set up Application Default Credentials

To convert the `client_secret.json` file into usable credentials, pass its
location the `gcloud auth application-default login` command's
`--client-id-file` argument.

 

```
gcloud auth application-default login \
    --client-id-file=client_secret.json \
    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

 

The simplified project setup in this tutorial triggers a "Google hasn't
verified this app." dialog. This is normal, choose "continue" .

This places the resulting token in a well known location so it can be accessed
by `gcloud` or the client libraries.

`

```
gcloud auth application-default login 

    --no-browser
    --client-id-file=client_secret.json 

    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

`

Once you have the Application Default Credentials (ADC) set, the client
libraries in most languages need minimal to no help to find them.

### Curl

The quickest way to test that this is working is to use it to access the REST
API using curl:

 

```
access_token=$(gcloud auth application-default print-access-token)
project_id=<MY PROJECT ID>
curl -X GET https://generativelanguage.googleapis.com/v1/models \
    -H 'Content-Type: application/json' \
    -H "Authorization: Bearer ${access_token}" \
    -H "x-goog-user-project: ${project_id}" | grep '"name"'
```

 

### Python

In python the client libraries should find them automatically:

 

```
pip install google-generativeai
```

 

A minimal script to test it might be:

 

```
import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])
```

 

## Next steps

If that's working you're ready to try
 Semantic retrieval on your text data .

## Manage credentials yourself [Python]

In many cases you won't have the `gcloud` command available to create the access
token from the Client ID (`client_secret.json`). Google provides libraries in
many languages to let you manage that process within your app. This section
demonstrates the process, in python. There are equivalent examples of this sort
of procedure, for other languages, available in the
 Drive API documentation 

### 1. Install the necessary libraries

Install the Google client library for Python, and the Gemini client library.

 

```
pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib
pip install google-generativeai
```

 

### 2. Write the credential manager

To minimize the number of times you have to click through the authorization
screens, create a file called `load_creds.py` in your working directory to
caches a `token.json` file that it can reuse later, or refresh if it expires.

Start with the
following code to convert the `client_secret.json` file to a token usable with
`genai.configure`:

 

```
import os.path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow

SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']

def load_creds():
    """Converts `client_secret.json` to a credential object.

    This function caches the generated tokens to minimize the use of the
    consent screen.
    """
    creds = None
    # The file token.json stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'client_secret.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return creds
```

 

### 3. Write your program

Now create your `script.py`:

 

```
import pprint
import google.generativeai as genai
from load_creds import load_creds

creds = load_creds()

genai.configure(credentials=creds)

print()
print('Available base models:', [m.name for m in genai.list_models()])
```

 

### 4. Run your program

In your working directory, run the sample:

 

```
python script.py
```

 

The first time you run the script, it opens a browser window and prompts you
to authorize access.

- 

If you're not already signed in to your Google Account, you're prompted to
sign in. If you're signed in to multiple accounts, be sure to select the
account you set as a "Test Account" when configuring your project. 

- 

Authorization information is stored in the file system, so the next time you
run the sample code, you aren't prompted for authorization.

You have successfully setup authentication.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms#data-use-paid

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms#paid-services

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms#grounding-with-google-search

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Gemini API Additional Terms of Service &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/terms#unpaid-services

- 
 
 
 
 
 
 
 
 
 
 
 Gemini API Additional Terms of Service  |  Google AI for Developers 

 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Nederlands 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Suomi 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 ƒçesky 
 

 
 - 
 ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ 
 

 
 - 
 —Å—Ä–ø—Å–∫–∏ 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini API Additional Terms of Service 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

Effective October 17, 2025

To use Gemini API ,
 Google AI Studio , and the other Google developer
services that reference these terms (collectively, the " APIs " or
" Services "), you must accept (1) the
 Google APIs Terms of Service 
(the " API Terms "), and (2) these Gemini API Additional Terms of Service (the
" Additional Terms "). Terms that are not defined in these Additional Terms
have the meanings given in the API Terms.

## Age Requirements

You must be 18 years of age or older to use the APIs. You also will not use the
Services as part of a website, application, or other service (collectively,
" API Clients ") that is directed towards or is likely to be accessed by
individuals under the age of 18.

## Use Restrictions

You may only access the Services (or make API Clients available to users) within
an available region . You may use only Paid Services when
making API Clients available to users in the European Economic Area,
Switzerland, or the United Kingdom.

You may not use the Services to develop models that compete with the Services
(e.g., Gemini API or Google AI Studio). You also may not attempt to reverse
engineer, extract or replicate any component of the Services, including the
underlying data or models (e.g., parameter weights).

In addition to the
" API Prohibitions "
section in the API Terms, you must comply with our
 Prohibited Use Policy ,
which provides additional details about appropriate conduct when using the
Services.

The Services include safety features to block harmful content, such as content
that violates our
 Prohibited Use Policy .
You may not attempt to bypass these protective measures or use content that
violates the API Terms or these Additional Terms. You should only lower
 safety settings if necessary and appropriate for
your use case. Applications with less restrictive safety settings may be subject
to Google's review and approval.

You may not use the Services in clinical practice, to provide medical advice, or
in any manner that is overseen by or requires clearance or approval from a
medical device regulatory agency.

## Use of Generated Content

Some of our Services allow you to generate original content. Google won't claim
ownership over that content. You acknowledge that Google may generate the same
or similar content for others and that we reserve all rights to do so.

As required by the API Terms, you'll comply with applicable law in using
generated content, which may require the provision of
 attribution to your users when returned as
part of an API call. Use discretion before relying on generated content,
including code . You're responsible for
your use of generated content, and for the use of that content by anyone you
share it with.

## Unpaid Services

Any Services that are offered free
of charge like direct interactions with Google AI Studio or unpaid quota in
Gemini API are unpaid Services (the " Unpaid Services ").

### How Google Uses Your Data

When you use Unpaid Services, including, for example, Google AI Studio and the
unpaid quota on Gemini API, Google uses the content you submit to the Services
and any generated responses to provide, improve, and develop Google products and
services and machine learning technologies, including Google's enterprise
features, products, and services, consistent with our
 Privacy Policy .

To help with quality and improve our products, human reviewers may read,
annotate, and process your API input and output. Google takes steps to protect
your privacy as part of this process. This includes disconnecting this data from
your Google Account, API key, and Cloud project before reviewers see or annotate
it. Do not submit sensitive, confidential, or personal information to the
Unpaid Services. 

The license you grant to Google under the
" Submission of Content "
section in the API Terms also extends, to the extent required under applicable
law for our use, to any content (e.g., prompts, including associated system
instructions, cached content, and files such as images, videos, or documents)
you submit to the Services and to any generated responses.

Google only uses content that you import or upload to our model tuning feature
for that express purpose. Tuning content may be retained in connection with your
tuned models for purposes of re-tuning when supported models change. When you
delete a tuned model, the related tuning content is also deleted.

If you're in the European Economic Area, Switzerland, or the United Kingdom, the
terms under " How Google uses Your Data " in
" Paid Services " apply to all Services, including Google AI
Studio and unpaid quota in the Gemini API, even though they are offered free of
charge.

## Paid Services

When a Service is being offered for a fee, it is considered to be a paid Service
(the " Paid Services "). When you activate a Cloud Billing account ,
all use of Gemini API and Google AI Studio is a "Paid Service" with respect to
how Google Uses Your Data, even when using Services that are offered free of
charge, such as Google AI Studio and unpaid quota of Gemini API.

For Paid Services, "Google" as used in these
Terms has the meaning given
 here .

### How Google Uses Your Data

When you use Paid Services, including, for example, the paid quota of the Gemini
API, Google doesn't use your prompts (including associated system instructions,
cached content, and files such as images, videos, or documents) or responses to
improve our products, and will process your prompts and responses in accordance
with the
 Data Processing Addendum for Products Where Google is a Data Processor .
For Paid Services, Google logs prompts and responses for a limited period of
time, solely for the purpose of detecting violations of the
 Prohibited Use Policy 
and any required legal or regulatory disclosures. This data may be stored
transiently or cached in any country in which Google or its agents maintain
facilities.

Other data we collect while providing the Paid Services to you, such as account
information and settings, billing history, direct communications and feedback,
and usage details (e.g., information about usage including token count per
prompt and response, operational status, safety filter triggers, software errors
and crash reports, authentication details, quality and performance metrics, and
other technical details necessary for Google to operate and maintain Services,
which may include device identifiers, identifiers from cookies or tokens, and IP
addresses) remains subject to the
 Google Controller-Controller Data Protection Terms 
and
 Google Privacy Policy referenced in the
API Terms.

When using Grounding with Google Search, additional data is collected and used,
as detailed in the
" Grounding with Google Search " section below. 

### Payment Terms

Billing and payments for Paid Services are handled by
 Cloud Billing in the Google
Cloud Platform.

As such, Section 2 (Payment Terms) and Section 14 (Miscellaneous) of the
 Google Cloud Platform Terms of Service govern
payments, invoicing, billing, payment disputes, and related issues, while these
Terms govern your use of the Paid Services. These Terms do not govern your
direct use of any Google Cloud Platform service (including those listed on the
 Google Cloud Platform Services Summary ).

" Fees " (as used in the
 Google Cloud Platform Terms of Service ) for
Paid Services are as specified on our pricing page. Google may make
changes to this pricing from time to time, effective 30 days after they are
posted unless otherwise specified (or in the case of new Paid Services, where
pricing takes effect immediately unless otherwise specified). Your continued
use of the Paid Services constitutes your consent to those changes.

## Agentic Services

When using agentic services, including the Computer Use API, you are solely
responsible for the actions and tasks performed by the service, such as
determining whether the service is appropriate for your use case, authorizing
the service's access and connection to data, applications, and systems, and
exercising judgment and supervision when and if the service is used in
production environments. You will not automatically bypass any requests for
human confirmation.

## Grounding with Google Search‚Äã‚Äã

"Grounding with Google Search" is a Service that provides Grounded Results and
Search Suggestions and can be used through Google AI Studio (as an Unpaid
Service), and via Gemini API as a (Paid Service). "Grounded Results" mean
responses that Google generates using the prompt from the end user, (or from
you, when using function calling), contextual
information that you may provide (as applicable), and results from Google's
search engine. "Search Suggestions" (also known as Search Entry Points) mean
search suggestions that Google provides with the Grounded Results. If a Grounded
Result is clicked on, separate terms (not these terms) govern the destination
page. If a Search Suggestion is clicked on the
 Google Terms of Service govern the
 google.com destination page. "Links" are any other means to
fetch web pages (including hyperlinks and URLs), which may be contained in a
Grounded Result or Search Suggestion. Links also include titles or labels
provided with those means to fetch web pages. Excluding your web domain(s), you
will not assert ownership rights in any intellectual property in Search
Suggestions or Links in Grounded Results.

### Use Restrictions

- You will only use Grounding with Google Search in an application that is owned
and operated by you and will only display the Grounded Results with the
associated Search Suggestion(s) to the end user who submitted the prompt.

- You will not, and will not allow your end user or any third party to, cache,
copy, frame, syndicate, resell, analyze, train on, or otherwise learn from
Grounded Results or Search Suggestions. For clarity, Grounded Results, Search
Suggestions, and Links are intended to be used in combination to respond to a
given End User prompt and it is a violation of these terms to use Grounding
with Google Search to extract or collect one or more of these components for
another purpose (for example, using programmatic or automated means to collect
Links, using Links to build an index, or using Links to identify destination
pages for crawling or scraping).

- You will not, and will not allow your end user or any third party to, store,
or implement any click tracking, Link-tracking or other monitoring of Grounded
Results or Search Suggestions, except that:

 You may store the text of the Grounded Result(s) (excluding Links): (1) that
were displayed by you for up to thirty (30) days only to evaluate and
optimize the display of the Grounded Results in your application; (2)
in chat history of an end user of your application for up to six (6) months
only for the purpose of allowing that end user to view their chat history;
and (3) temporarily for the purpose of resubmitting the text of the Grounded
Result in a subsequent prompt that you submit to Google via a function call
to obtain a refined or improved Grounded Result to display to the End User,
as long as the developer: (i) does not use the interim Grounded Results for
any other purpose; (ii) deletes any Grounded Result that is not displayed to
the End User once the final Grounded Result is generated; and (iii) displays
any associated Search Suggestions or other Links (as applicable) with the
final Grounded Result (up to a maximum of 5 Search Suggestions) to the End
User.

- You may monitor end user interactions with your application interface;
however, you will not track whether those interactions were specifically
with a given Search Suggestion or Grounded Result (in each case, in whole or
in part, including any specific Link).

 
- Unless permitted by Google in writing, you: (1) will not modify, or
intersperse any other content with, the Grounded Results or Search
Suggestions; and (2) will not place any interstitial content between any Link
or Search Suggestions and the associated destination page, redirect end users
away from the destination pages, or minimize, remove, or otherwise inhibit the
full and complete display of any destination page. 

### Data Collection and How Google Uses Your Data

In addition to the general terms above ("How Google Uses Your Data" under
" Unpaid Services " and " Paid Services "),
when using Grounding with Google Search,
Google will store prompts, contextual information that you may provide, and
output for thirty (30) days for the purposes of creating Grounded Results and
Search Suggestions and the stored information can be used for debugging and
testing of systems that support Grounding with Google Search. When using
Grounding with Google Search via paid quota of Gemini API, this processing for
debugging and testing of systems is in accordance with the
 Data Processing Addendum for Products Where Google is a Data Processor .

This subsection "Grounding with Google Search" will survive termination of the
Agreement, as applicable.

The
 Client Application Guidelines 
apply to your use of Grounding with Google Search. For purposes of the Client
Application Guidelines, your applications that are using Grounding with Google
Search are considered Approved Applications.

## Grounding with Google Maps

"Grounding with Google Maps" is a Service that provides Google Maps Grounded
Results as a feature of Gemini API. "Google Maps Grounded Results" mean
responses that Google generates using Google Maps Data in response to an end
user initiated prompt. "Google Maps Data" means the content originating from
Google Maps in the Google Maps Grounded Results, including in the output text,
in the metadata of the Google Maps Grounded Results, in the Google Maps Links,
and content accessed through Google Maps Links. "Google Maps Links" mean the
URLs that Google provides in a Google Maps Grounded Result and any titles or
labels provided with those URLs. If Google Maps Links are clicked on, these
separate Google Maps End User Terms 
and the Google Privacy Policy govern the
destination page. Google Maps Data in the text of a Google Maps Grounded Result
will be identified via the Google Maps Links. Notwithstanding anything to the
contrary in the Agreement, Google and its content providers retain all rights to
Google Maps Data.

### Use restrictions

- 

You will only use Grounding with Google Maps in an application that is owned and
operated by you and will only use Grounding with Google Maps to display the
Google Maps Grounded Results with the associated Google Maps Links to the
end user who initiated the prompt. An end user is an individual you permit
to use your application.

- 

You will not modify the Google Maps Grounded Result or intersperse any other
content with the Google Maps Grounded Result, place any interstitial content
between the text of the Google Maps Grounded Result and the Google Maps
Links or the Google Maps Links and the associated destination page, or
redirect end users away from the destination pages or minimize, remove, or
otherwise inhibit the full and complete display of any destination page.

- 

You will comply with the Documentation for Grounding with Google Maps.

- 

You will not, and will not allow your end users or any third party to:

 

cache or store Google Maps Grounded Results except that you may cache or
store Google Maps Grounded Results:

 

for up to ninety (90) days, only to evaluate and optimize the display of the
Google Maps Grounded Results for your application; or,

- 

in the chat history of an end user in your application for up to six (6)
months for the purpose of allowing that end user to view their chat history or
to maintain prior conversation context for that end user within your application
(Section 4 (a) overrides any cache header);

 
- 

scrape or export any Google Maps Data;

- 

train on any Google Maps Data; or

- 

distribute or market any Customer Applications in any Prohibited Territory
as defined in the Documentation.

 

### Data Collection and How Google Uses Your Data

You acknowledge that it is reasonably
necessary for Google to store prompts, contextual information that you may
provide, and generated content for thirty (30) days for the purposes of creating
Google Maps Grounded Results, and since such information is being stored, you
instruct Google that the stored information can be used for debugging and
testing of systems that support Grounding with Google Maps. When using Grounding
with Google Maps via paid quota of Gemini API, this processing for debugging and
testing of systems is in accordance with the Data Processing Addendum for
Products Where Google is a Data
Processor .

This subsection "Grounding with Google Maps" will survive termination of the
Agreement, as applicable.

## Hardware Safety

The following additional terms apply to models that can be used to control
robots or other robotic hardware (" Robotics Models ").

You acknowledge and agree that the Robotics Models have not been tested with all
makes and models of robotics hardware, and therefore, that the performance and
safety of the Robotics Models in connection with your hardware is not guaranteed
or provided with a warranty of any kind. You therefore agree to operate any
hardware or other products that you may use in connection with the Robotics
Models in a safe manner, and completely at your own risk. The Robotics Models
may act in an unpredictable or unexpected manner when used in connection with
robotics hardware. You therefore agree to use discretion before using the
Robotics Models in a production, commercial, or public environment, and to not
use the Robotics Models for safety-critical applications or work, such as in the
following settings: (i) healthcare, (ii) transportation, or (iii) other areas
where safety protocols are vital, and a malfunction could reasonably foreseeably
lead to death, personal injury, or property damage.

## Disclaimers 

 The Services include experimental technology and may sometimes provide
inaccurate or offensive content that doesn't represent Google's views. 

 Use discretion before relying on, publishing, or otherwise using content
provided by the Services. 

 Don't rely on the Services for medical, legal, financial, or other
professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. Content does not constitute medical treatment or diagnosis. 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-10-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-10-17 UTC."],[],[]]

---

### Billing &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/billing#request-an-upgrade

- 
 
 
 
 
 
 
 
 
 
 
 Billing  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Billing 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This guide provides an overview of different Gemini API billing options,
explains how to enable billing and monitor usage, and provides answers to
frequently asked questions (FAQs) about billing.

 
 Upgrade to the Gemini API paid tier 
 

## About billing

Billing for the Gemini API is based on two pricing tiers: free of charge 
(or free ) and pay-as-you-go (or paid ). Pricing and rate limits differ
between these tiers and also vary by model. You can check out the rate limits 
and pricing pages for more into. For a model-by-model
breakdown of capabilities, see the Gemini models page .

#### How to request an upgrade

To transition from the free tier to the pay-as-you-go plan, you need to
enable billing for your Google Cloud project. The button you see in
Google AI Studio depends on your project's current plan.

- If you're on the free tier, you'll see a Set up Billing button for
your project.

- If you're already on the paid tier and meet the criteria for a plan change,
you might see an Upgrade button.

To start the process, follow these steps:

- Go to the AI Studio API keys page .

- Find the project you want to move to the paid plan and click either
 Set up Billing or Upgrade , depending on the button displayed.

- The next step depends on the button you clicked:

 If you clicked Set up Billing: You'll be redirected to the
Google Cloud console to link a billing account to your project.
Follow the on-screen instructions to complete the process.

- If you clicked Upgrade: The system will automatically verify
your project's eligibility. If your project meets all the
requirements, it will be instantly upgraded to the next tier.

 

### Why use the paid tier?

When you enable billing and use the paid tier, you benefit from higher rate limits ,
and your prompts and responses aren't used to improve Google products.
For more information on data use for paid services, see the
 terms of service .

### Cloud Billing

The Gemini API uses
 Cloud Billing 
for billing services. To use the paid tier, you must set up Cloud Billing on
your cloud project. After you've enabled Cloud Billing, you can use Cloud
Billing tools to track spending, understand costs, make payments, and access
Cloud Billing support.

## Enable billing

You can enable Cloud Billing starting from Google AI Studio:

- 

Open Google AI Studio .

- 

In the bottom of the left sidebar, select Settings >
 Plan information .

- 

Click Set up Billing for your chosen project to enable Cloud Billing.

## Monitor usage

After you enable Cloud Billing, you can monitor your usage of the Gemini API in
 Google AI Studio .

## Frequently asked questions

This section provides answers to frequently asked questions.

### What am I billed for?

Gemini API pricing is based on the following:

- Input token count

- Output token count

- Cached token count

- Cached token storage duration

For pricing information, see the pricing page .

### Where can I view my quota?

You can view your quota and system limits in the
 Google Cloud console .

### How do I request more quota?

To request more quota, follow the instructions at
 How to request an upgrade .

### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?

Yes, we make the free tier and paid tier available in
 many regions .

### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?

No, Google AI Studio usage remains free of charge regardless of if you set up
billing across all supported regions.

### Can I use 1M tokens in the free tier?

The free tier for Gemini API differs based on the model selected. For now, you
can try the 1M token context window in the following ways:

- In Google AI Studio

- With pay-as-you-go plans

- With free-of-charge plans for select models

See the latest free-of-charge rate limits per model on rate limits page .

### How can I calculate the number of tokens I'm using?

Use the
 `GenerativeModel.count_tokens` 
method to count the number of tokens. Refer to the
 Tokens guide to learn more about tokens.

### Can I use my Google Cloud credits with the Gemini API?

Yes, Google Cloud credits can be used towards Gemini API usage.

### How is billing handled?

Billing for the Gemini API is handled by the
 Cloud Billing system.

### Am I charged for failed requests?

If your request fails with a 400 or 500 error, you won't be charged for the
tokens used. However, the request will still count against your quota.

### Is there a charge for fine-tuning the models?

 Model tuning is free, but inference on tuned
models is charged at the same rate as the base models.

### Is GetTokens billed?

Requests to the GetTokens API are not billed, and they don't count against
inference quota.

### How is my Google AI Studio data handled if I have a paid API account?

Refer to the terms for details on how data is
handled when Cloud billing is enabled (see "How Google Uses Your Data" under
"Paid Services"). Note that your Google AI Studio prompts are treated under the
same "Paid Services" terms so long as at least 1 API project has billing enabled,
which you can validate on the Gemini API Key page 
if you see any projects marked as "Paid" under "Plan".

### Where can I get help with billing?

To get help with billing, see
 Get Cloud Billing support .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-27 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-27 UTC."],[],[]]

---

### Fine-tuning with the Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/model-tuning

- 
 
 
 
 
 
 
 
 
 
 
 Fine-tuning with the Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Fine-tuning with the Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

With the deprecation of Gemini 1.5 Flash-001 in May 2025, we no longer have a
model available which supports fine-tuning in the Gemini API, but it is supported
in Vertex AI .

We plan to bring fine-tuning support back in the future. We would love to hear
from you on our developer forum if
fine-tuning is important to your use case.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-12 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-12 UTC."],[],[]]

---

### Available regions for Google AI Studio and Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/available-regions#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Available regions for Google AI Studio and Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ÔøΩÔøΩ‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Available regions for Google AI Studio and Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

If you reached this page after trying to open
 Google AI Studio , it may be because Google AI
Studio is not available in your region, or you don't meet the age requirements
(18+) for access. You can learn more about the available regions in the
following section and other requirements in the
 terms of service .

## Available regions

The Gemini API and Google AI Studio are available in the following countries and
territories. If you're not in one of these countries or territories, try the
 Gemini API in Vertex AI :

- Albania

- Algeria

- American Samoa

- Angola

- Anguilla

- Antarctica

- Antigua and Barbuda

- Argentina

- Armenia

- Aruba

- Australia

- Austria

- Azerbaijan

- The Bahamas

- Bahrain

- Bangladesh

- Barbados

- Belgium

- Belize

- Benin

- Bermuda

- Bhutan

- Bolivia

- Bosnia

- Botswana

- Brazil

- British Indian Ocean Territory

- British Virgin Islands

- Brunei

- Bulgaria

- Burkina Faso

- Burundi

- Cabo Verde

- Cambodia

- Cameroon

- Canada

- Caribbean Netherlands

- Cayman Islands

- Central African Republic

- Chad

- Chile

- Christmas Island

- Cocos (Keeling) Islands

- Colombia

- Comoros

- Cook Islands

- C√¥te d'Ivoire

- Costa Rica

- Croatia

- Cura√ßao

- Czech Republic

- Democratic Republic of the Congo

- Denmark

- Djibouti

- Dominica

- Dominican Republic

- Ecuador

- Egypt

- El Salvador

- Equatorial Guinea

- Eritrea

- Estonia

- Eswatini

- Ethiopia

- Falkland Islands (Islas Malvinas)

- Faroe Islands

- Fiji

- Finland

- France

- Gabon

- The Gambia

- Georgia

- Germany

- Ghana

- Gibraltar

- Greece

- Greenland

- Grenada

- Guam

- Guatemala

- Guernsey

- Guinea

- Guinea-Bissau

- Guyana

- Haiti

- Heard Island and McDonald Islands

- Herzegovina

- Honduras

- Hungary

- Iceland

- India

- Indonesia

- Iraq

- Ireland

- Isle of Man

- Israel

- Italy

- Jamaica

- Japan

- Jersey

- Jordan

- Kazakhstan

- Kenya

- Kiribati

- Kosovo

- Kyrgyzstan

- Kuwait

- Laos

- Latvia

- Lebanon

- Lesotho

- Liberia

- Libya

- Liechtenstein

- Lithuania

- Luxembourg

- Madagascar

- Malawi

- Malaysia

- Maldives

- Mali

- Malta

- Marshall Islands

- Mauritania

- Mauritius

- Mexico

- Micronesia

- Mongolia

- Montenegro

- Montserrat

- Morocco

- Mozambique

- Namibia

- Nauru

- Nepal

- Netherlands

- New Caledonia

- New Zealand

- Nicaragua

- Niger

- Nigeria

- Niue

- Norfolk Island

- North Macedonia

- Northern Mariana Islands

- Norway

- Oman

- Pakistan

- Palau

- Palestine

- Panama

- Papua New Guinea

- Paraguay

- Peru

- Philippines

- Pitcairn Islands

- Poland

- Portugal

- Puerto Rico

- Qatar

- Republic of Cyprus

- Republic of the Congo

- Romania

- Rwanda

- Saint Barth√©lemy

- Saint Kitts and Nevis

- Saint Lucia

- Saint Pierre and Miquelon

- Saint Vincent and the Grenadines

- Saint Helena, Ascension and Tristan da Cunha

- Samoa

- S√£o Tom√© and Pr√≠ncipe

- Saudi Arabia

- Senegal

- Serbia

- Seychelles

- Sierra Leone

- Singapore

- Slovakia

- Slovenia

- Solomon Islands

- Somalia

- South Africa

- South Georgia and the South Sandwich Islands

- South Korea

- South Sudan

- Spain

- Sri Lanka

- Sudan

- Suriname

- Sweden

- Switzerland

- Taiwan

- Tajikistan

- Tanzania

- Thailand

- Timor-Leste

- Togo

- Tokelau

- Tonga

- Trinidad and Tobago

- Tunisia

- T√ºrkiye

- Turkmenistan

- Turks and Caicos Islands

- Tuvalu

- Uganda

- Ukraine

- United Kingdom

- United Arab Emirates

- United States

- United States Minor Outlying Islands

- U.S. Virgin Islands

- Uruguay

- Uzbekistan

- Vanuatu

- Venezuela

- Vietnam

- Wallis and Futuna

- Western Sahara

- Yemen

- Zambia

- Zimbabwe

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Available regions for Google AI Studio and Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/available-regions#

- 
 
 
 
 
 
 
 
 
 
 
 Available regions for Google AI Studio and Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Available regions for Google AI Studio and Gemini API 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

If you reached this page after trying to open
 Google AI Studio , it may be because Google AI
Studio is not available in your region, or you don't meet the age requirements
(18+) for access. You can learn more about the available regions in the
following section and other requirements in the
 terms of service .

## Available regions

The Gemini API and Google AI Studio are available in the following countries and
territories. If you're not in one of these countries or territories, try the
 Gemini API in Vertex AI :

- Albania

- Algeria

- American Samoa

- Angola

- Anguilla

- Antarctica

- Antigua and Barbuda

- Argentina

- Armenia

- Aruba

- Australia

- Austria

- Azerbaijan

- The Bahamas

- Bahrain

- Bangladesh

- Barbados

- Belgium

- Belize

- Benin

- Bermuda

- Bhutan

- Bolivia

- Bosnia

- Botswana

- Brazil

- British Indian Ocean Territory

- British Virgin Islands

- Brunei

- Bulgaria

- Burkina Faso

- Burundi

- Cabo Verde

- Cambodia

- Cameroon

- Canada

- Caribbean Netherlands

- Cayman Islands

- Central African Republic

- Chad

- Chile

- Christmas Island

- Cocos (Keeling) Islands

- Colombia

- Comoros

- Cook Islands

- C√¥te d'Ivoire

- Costa Rica

- Croatia

- Cura√ßao

- Czech Republic

- Democratic Republic of the Congo

- Denmark

- Djibouti

- Dominica

- Dominican Republic

- Ecuador

- Egypt

- El Salvador

- Equatorial Guinea

- Eritrea

- Estonia

- Eswatini

- Ethiopia

- Falkland Islands (Islas Malvinas)

- Faroe Islands

- Fiji

- Finland

- France

- Gabon

- The Gambia

- Georgia

- Germany

- Ghana

- Gibraltar

- Greece

- Greenland

- Grenada

- Guam

- Guatemala

- Guernsey

- Guinea

- Guinea-Bissau

- Guyana

- Haiti

- Heard Island and McDonald Islands

- Herzegovina

- Honduras

- Hungary

- Iceland

- India

- Indonesia

- Iraq

- Ireland

- Isle of Man

- Israel

- Italy

- Jamaica

- Japan

- Jersey

- Jordan

- Kazakhstan

- Kenya

- Kiribati

- Kosovo

- Kyrgyzstan

- Kuwait

- Laos

- Latvia

- Lebanon

- Lesotho

- Liberia

- Libya

- Liechtenstein

- Lithuania

- Luxembourg

- Madagascar

- Malawi

- Malaysia

- Maldives

- Mali

- Malta

- Marshall Islands

- Mauritania

- Mauritius

- Mexico

- Micronesia

- Mongolia

- Montenegro

- Montserrat

- Morocco

- Mozambique

- Namibia

- Nauru

- Nepal

- Netherlands

- New Caledonia

- New Zealand

- Nicaragua

- Niger

- Nigeria

- Niue

- Norfolk Island

- North Macedonia

- Northern Mariana Islands

- Norway

- Oman

- Pakistan

- Palau

- Palestine

- Panama

- Papua New Guinea

- Paraguay

- Peru

- Philippines

- Pitcairn Islands

- Poland

- Portugal

- Puerto Rico

- Qatar

- Republic of Cyprus

- Republic of the Congo

- Romania

- Rwanda

- Saint Barth√©lemy

- Saint Kitts and Nevis

- Saint Lucia

- Saint Pierre and Miquelon

- Saint Vincent and the Grenadines

- Saint Helena, Ascension and Tristan da Cunha

- Samoa

- S√£o Tom√© and Pr√≠ncipe

- Saudi Arabia

- Senegal

- Serbia

- Seychelles

- Sierra Leone

- Singapore

- Slovakia

- Slovenia

- Solomon Islands

- Somalia

- South Africa

- South Georgia and the South Sandwich Islands

- South Korea

- South Sudan

- Spain

- Sri Lanka

- Sudan

- Suriname

- Sweden

- Switzerland

- Taiwan

- Tajikistan

- Tanzania

- Thailand

- Timor-Leste

- Togo

- Tokelau

- Tonga

- Trinidad and Tobago

- Tunisia

- T√ºrkiye

- Turkmenistan

- Turks and Caicos Islands

- Tuvalu

- Uganda

- Ukraine

- United Kingdom

- United Arab Emirates

- United States

- United States Minor Outlying Islands

- U.S. Virgin Islands

- Uruguay

- Uzbekistan

- Vanuatu

- Venezuela

- Vietnam

- Wallis and Futuna

- Western Sahara

- Yemen

- Zambia

- Zimbabwe

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Google AI Studio quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ai-studio-quickstart#chat_example

- 
 
 
 
 
 
 
 
 
 
 
 Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Google AI Studio quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

 Google AI Studio lets you quickly try out
models and experiment with different prompts. When you're ready to build, you
can select "Get code" and your preferred programming language to
use the Gemini API .

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for
different use cases. This guide covers Chat prompts , used to build
conversational experiences. This prompting technique allows for multiple input
and response turns to generate output. You can learn more with our
 chat prompt example below .
Other options include Realtime streaming , Video gen , and
more.

AI Studio also provides the Run settings panel, where you can make
adjustments to model parameters ,
 safety settings , and toggle-on tools like
 structured output , function calling , code execution , and grounding .

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like
 Gemini , you've experienced first-hand how powerful
generative AI models can be for open-ended dialog. While these general-purpose
chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only
supports conversations that talk about a company's product. You might want to
build a chatbot that speaks with a particular tone or style: a bot that cracks
lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot
that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user
and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

- 

Open Google AI Studio . Chat will be pre-
selected on the left side options menu.

- 

Click the assignment icon at the top of
the Chat Prompt window to expand the
 System Instructions 
input field. Paste the following into the text input field:

 

```
You are an alien that lives on Europa, one of Jupiter's moons.
```

 

After you've added the system instructions, start testing your application by
chatting with the model:

- 

In the text input boxed labeled Type something... , type in a question or
observation that a user might make. For example:

 User: 

 

```
What's the weather like?
```

 

- 

Click the Run button to get a response from the chatbot. This response
may be something like the following:

 Model: 

 

```
Ah, a query about the flows and states upon Europa! You speak of "weather,"
yes? A curious concept from worlds with thick gas veils...
```

 

 (gemini-2.5-pro) 

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien
chatbot. However, a single instruction may not be enough to ensure
consistency and quality in the model's responses. Without more specific
instructions, the model's response to a question about the weather tends to be
very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

- 

Start a new chat prompt, or use the same one. System instructions are
modifiable after the chat session has started.

- 

In the System Instructions section, change the instructions you already
have to the following:

 

```
You are Tim, an alien that lives on Europa, one of Jupiter's moons.

Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
in your answers.
```

 

- 

Re-enter your question (`What's the weather like?`) and click the Run 
button. If you didn't start a new chat, your response might look something
like this:

 Model: 

 

```
Oh, hello again! Still curious about the flows and states, are we? Wonderful!
Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
"weather" is mainly about the water's mood, dictated by the Great Eye
(that's your Jupiter!)...
```

 

 (gemini-2.5-pro) 

You can use this approach to add additional depth to the chatbot. Ask more
questions, edit the answers, and improve the quality of your chatbot. Continue
to add or modify the instructions and test how they change your chatbot's
behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your
satisfaction, you can use the Get code button to start coding or save your
prompt to work on later and share with others.

## Further reading

- If you're ready to move on to code, see the API
quickstarts .

- To learn how to craft better prompts, check out the Prompt design
guidelines .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Grounding with Google Search &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/grounding

- 
 
 
 
 
 
 
 
 
 
 
 Grounding with Google Search  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Grounding with Google Search 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 

Grounding with Google Search connects the Gemini model to real-time web content
and works with all available languages. This allows
Gemini to provide more accurate answers and cite verifiable sources beyond its
knowledge cutoff.

Grounding helps you build applications that can:

- Increase factual accuracy: Reduce model hallucinations by basing
responses on real-world information.

- Access real-time information: Answer questions about recent events and
topics.

- 

 Provide citations: Build user trust by showing the sources for the
model's claims.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const groundingTool = {
  googleSearch: {},
};

const config = {
  tools: [groundingTool],
};

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

 
 

You can learn more by trying the Search tool
notebook .

## How grounding with Google Search works

When you enable the `google_search` tool, the model handles the entire workflow
of searching, processing, and citing information automatically.

 

- User Prompt: Your application sends a user's prompt to the Gemini API
with the `google_search` tool enabled.

- Prompt Analysis: The model analyzes the prompt and determines if a
Google Search can improve the answer.

- Google Search: If needed, the model automatically generates one or
multiple search queries and executes them.

- Search Results Processing: The model processes the search results,
synthesizes the information, and formulates a response.

- Grounded Response: The API returns a final, user-friendly response that
is grounded in the search results. This response includes the model's text
answer and `groundingMetadata` with the search queries, web results, and
citations.

## Understanding the grounding response

When a response is successfully grounded, the response includes a
`groundingMetadata` field. This structured data is essential for verifying
claims and building a rich citation experience in your application.

 

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

 

The Gemini API returns the following information with the `groundingMetadata`:

- `webSearchQueries` : Array of the search queries used. This is useful for
debugging and understanding the model's reasoning process.

- `searchEntryPoint` : Contains the HTML and CSS to render the required Search
Suggestions. Full usage requirements are detailed in the Terms of
Service .

- `groundingChunks` : Array of objects containing the web sources (`uri` and
`title`).

- `groundingSupports` : Array of chunks to connect model response `text` to
the sources in `groundingChunks`. Each chunk links a text `segment` (defined
by `startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This
is the key to building inline citations.

Grounding with Google Search can also be used in combination with the URL
context tool to ground responses in both public
web data and the specific URLs you provide.

## Attributing sources with inline citations

The API returns structured citation data, giving you complete control over how
you display sources in your user interface. You can use the `groundingSupports`
and `groundingChunks` fields to link the model's statements directly to their
sources. Here is a common pattern for processing the metadata to create a
response with inline, clickable citations.

 
 

### Python

 

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

 
 

### JavaScript

 

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

 
 

The new response with inline citations will look like this:

 

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

 

## Pricing

When you use Grounding with Google Search, your project is billed for each
search query that the model decides to execute. If the model decides to execute
multiple search queries to answer a single prompt (for example, searching for
`"UEFA Euro 2024 winner"` and `"Spain vs England Euro 2024 final score"` within
the same API call), this counts as two billable uses of the tool for that
request.

For detailed pricing information, see the Gemini API pricing
page .

## Supported models

Experimental and Preview models are not included. You can find their
capabilities on the model
overview page.

 
 
 
 Model 
 Grounding with Google Search 
 
 

 
 
 Gemini 2.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 2.5 Flash-Lite 
 ‚úîÔ∏è 
 
 
 Gemini 2.0 Flash 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Pro 
 ‚úîÔ∏è 
 
 
 Gemini 1.5 Flash 
 ‚úîÔ∏è 
 
 
 

## Supported tools combinations

You can use Grounding with Google Search with other tools like
 code execution and
 URL context to power more complex use cases.

## Grounding with Gemini 1.5 Models (Legacy)

While the `google_search` tool is recommended for Gemini 2.0 and later, Gemini
1.5 supports a legacy tool named `google_search_retrieval`. This tool provides a
`dynamic` mode that allows the model to decide whether to perform a search based
on its confidence that the prompt requires fresh information. If the model's
confidence is above a `dynamic_threshold` you set (a value between 0.0 and 1.0),
it will perform a search.

 
 

### Python

 

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

 
 

### JavaScript

 

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

 
 

## What's next

- Try the Grounding with Google Search in the Gemini API
Cookbook .

- Learn about other available tools, like Function Calling .

- Learn how to augment prompts with specific URLs using the URL context
tool .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Text generation &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/text-generation#system-instructions

- 
 
 
 
 
 
 
 
 
 
 
 Text generation  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Text generation 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can generate text output from various inputs, including text,
images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithTextInput {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

 

## Thinking with Gemini 2.5

2.5 Flash and Pro models have "thinking" enabled by default to enhance quality, which may take longer to run and increase token usage. 

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero. 

For more details, see the thinking guide .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.ThinkingConfig;

public class GenerateContentWithThinkingConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            // Disables thinking
            .thinkingConfig(ThinkingConfig.builder().thinkingBudget(0))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so,
pass a `GenerateContentConfig` 
object.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithSystemInstruction {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            .systemInstruction(
                Content.fromParts(Part.fromText("You are a cat. Your name is Neko.")))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Hello there", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

The `GenerateContentConfig` 
object also lets you override default generation parameters, such as
 temperature .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config = GenerateContentConfig.builder().temperature(0.1f).build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Explain how AI works", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Refer to the `GenerateContentConfig` 
in our API reference for a complete list of configurable parameters and their
descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with
media files. The following example demonstrates providing an image:

 
 

### Python

 

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.Content;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithMultiModalInputs {
  public static void main(String[] args) {

    Client client = new Client();

    Content content =
      Content.fromParts(
          Part.fromText("Tell me about this instrument"),
          Part.fromUri("/path/to/organ.jpg", "image/jpeg"));

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", content, null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

 
 

For alternative methods of providing images and more advanced image processing,
see our image understanding guide .
The API also supports document , video , and audio 
inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation 
process is complete.

For more fluid interactions, use streaming to receive `GenerateContentResponse` instances incrementally
as they're generated.

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentStream {
  public static void main(String[] args) {

    Client client = new Client();

    ResponseStream<GenerateContentResponse> responseStream =
      client.models.generateContentStream(
          "gemini-2.5-flash", "Write a story about a magic backpack.", null);

    for (GenerateContentResponse res : responseStream) {
      System.out.print(res.text());
    }

    // To save resources and avoid connection leaks, it is recommended to close the response
    // stream after consumption (or using try block to get the response stream).
    responseStream.close();
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Multi-turn conversations (chat)

Our SDKs provide functionality to collect multiple rounds of prompts and
responses into a chat, giving you an easy way to keep track of the conversation
history.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversation {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    GenerateContentResponse response =
        chatSession.sendMessage("I have 2 dogs in my house.");
    System.out.println("First response: " + response.text());

    response = chatSession.sendMessage("How many paws are in my house?");
    System.out.println("Second response: " + response.text());

    // Get the history of the chat session.
    // Passing 'true' to getHistory() returns the curated history, which excludes
    // empty or invalid parts.
    // Passing 'false' here would return the comprehensive history, including
    // empty or invalid parts.
    ImmutableList<Content> history = chatSession.getHistory(true);
    System.out.println("History: " + history);
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Streaming can also be used for multi-turn conversations.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversationWithStreaming {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    ResponseStream<GenerateContentResponse> responseStream =
        chatSession.sendMessageStream("I have 2 dogs in my house.", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    responseStream = chatSession.sendMessageStream("How many paws are in my house?", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    // Get the history of the chat session. History is added after the stream
    // is consumed and includes the aggregated response from the stream.
    System.out.println("History: " + chatSession.getHistory(false));
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Supported models

All models in the Gemini family support text generation. To learn more
about the models and their capabilities, visit the
 Models page.

## Best practices

### Prompting tips

For basic text generation, a zero-shot 
prompt often suffices without needing examples, system instructions or specific
formatting.

For more tailored outputs:

- Use System instructions to guide the model.

- Provide few example inputs and outputs to guide the model. This is often referred to as few-shot prompting.

Consult our prompt engineering guide for
more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our
 structured output guide to learn how.

## What's next

- Try the Gemini API getting started Colab .

- Explore Gemini's image ,
 video , audio 
and document understanding capabilities.

- Learn about multimodal
 file prompting strategies .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Prompt design strategies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/prompting-intro

- 
 
 
 
 
 
 
 
 
 
 
 Prompt design strategies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Prompt design strategies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 Prompt design is the process of creating prompts, or natural language requests,
that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you
started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides
on:

- Prompting with media files 

- Prompting for image generation with Imagen and Gemini Native Image Generation 

- Prompting for video generation 

You can find other sample prompts in the prompt gallery 
meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with
clear and specific instructions. Instructions can be in the form of a question,
step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a
response to. Inputs can be a question that the model
answers (question input), a task the model performs (task input), an entity the
model operates on (entity input), or partial input that the model completes or
continues (completion input).

 
 
 
 Input type 
 Prompt 
 Generated output 
 
 
 
 
 Question
 
 

```
    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

 
 
 

```
    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
```

 
 
 
 
 Task
 
 

```
    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.
```

 
 
 

```
    1. **Tent:** Your shelter from the elements.
    2. **Sleeping Bag:** Stay warm and comfortable.
    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.
    4. **Headlamp or Flashlight:** Light for nighttime activities.
    5. **First Aid Kit:** Be prepared for minor injuries.
```

 
 
 
 
 Entity
 
 

```
    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

 
 
 

```
Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
```

 
 
 
 
 

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you
provide partial content, the model can provide the rest of the content or what
it thinks is a continuation of that content as a response. When doing so, if you
include any examples or context, the model can take those examples or context
into account.

The following example provides a prompt with an instruction and an entity input:

 
 
 
 Prompt: 
 

```
For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
```

 
 Response: 
 

```
{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
```

 
 (gemini-2.5-flash) 
 
 
 

While the model did as prompted, writing out the instructions in natural language
can sometimes be challenging and it leaves a lot to the model's interpretation.
For example, a restaurants menu might contain many items. To reduce the size of
the JSON response, you probably want to omit the items that weren't ordered. In
this case, you can give an example and a response prefix and let
the model complete it:

 
 
 
 Prompt: 
 

```
Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
```
{
  "cheeseburger": 1,
  "fries": 1
}
```
Order: I want two burgers, a drink, and fries.
Output:
```

 
 Response: 
 

```
```
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
```
```

 
 (gemini-2.5-flash) 
 
 
 

Notice how "cheeseburger" was excluded from the output because it wasn't a part
of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can
tell the model what to do and not to do. For example, you can specify a constraint
in the prompt on how long you want a summary to be:

 
 
 
 

 Prompt: 
 

```
Summarize this text in one sentence:
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
```

 
 

 

 Response: 
 

```
      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
```

 
 (gemini-2.5-flash) 
 

 
 
 

### Response format

You can give instructions that specify the format of the response. For example,
you can ask for the response to be formatted as a table, bulleted list, elevator
pitch, keywords, sentence, or paragraph. The following system instruction tells
the model to be more conversational in its response:

 
 
 
 System instruction 
 

```
    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
```

 
 Prompt 
 

```
    What is a smart way to make a business that sells DVD's in 2025?
```

 
 Response: 
 

```
Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    * Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
```

 
 (gemini-2.5-flash) 
 
 
 

#### Format responses with the completion strategy

The completion strategy can also help format the response.
The following example prompts the model to create an essay outline:

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
```

 
 Response: 
 

```
Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The prompt didn't specify the format for the outline and the model chose a format
for you. To get the model to return an outline in a specific format, you can add
text that represents the start of the outline and let the model complete it based
on the pattern that you initiated.

 
 
 
 Prompt: 
 

```
Create an outline for an essay about hummingbirds.
I. Introduction
   *
```

 
 Response: 
 

```
Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
* Background: Briefly introduce hummingbirds ‚Äì where they are found (Americas), their reputation as unique birds, and their general characteristics.
* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right
looks like. The model attempts to identify patterns and relationships from the
examples and applies them when generating a response. Prompts that contain a few
examples are called few-shot prompts, while prompts that provide no
examples are called zero-shot prompts. Few-shot prompts are often used
to regulate the formatting, phrasing, scoping, or general patterning of model
responses. Use specific and varied examples to help the model narrow its focus
and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without
few-shot examples are likely to be less effective. In fact, you can remove
instructions from your prompt if your examples are clear enough in showing the
task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

 
 
 
 Prompt: 
 

```
Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
```

 
 (gemini-2.5-flash) 
 
 
 

If your use case requires the model to produce concise responses, you can include
examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter
explanations. In the response, you can see that the examples guided the model to
choose the shorter explanation (`Explanation2`) as opposed to the longer
explanation (`Explanation1`) like it did previously.

 
 
 
 Prompt: 
 

```
Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
```

 
 Response: 
 

```
Answer: Explanation2
```

 
 (gemini-2.5-flash) 
 
 
 

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though
you may need to experiment with the number of examples to provide in the prompt
for the best results. At the same time, if you include too many examples,
the model may start to overfit 
the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using
examples to show the model an anti pattern to avoid.

‚õî Negative pattern: 

 
 
 
 Example 
 

```
Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
```

 
 

 
 
 

‚úÖ Positive pattern: 

 
 
 
 Example 
 

```
Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
```

 
 

 
 
 

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to
avoid responses with undesired formats. One of the primary objectives of adding
few-shot examples in prompts is to show the model the response format. Therefore,
it is essential to ensure a consistent format across all examples, especially
paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs
to solve a problem, instead of assuming that the model has all of the required
information. This contextual information helps the model understand the constraints
and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

 
 
 
 Prompt: 
 

```
What should I do to fix my disconnected wifi? The light on my Google
Wifi router is yellow and blinking slowly.
```

 
 Response: 
 

```
  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response looks like generic troubleshooting information that's not specific
to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's
troubleshooting guide as context for it to refer to when providing a response.

 
 
 
 Prompt: 
 

```
Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
```

 
 Response: 
 

```
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
```

 
 (gemini-2.5-flash) 
 
 
 

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve
several purposes, depending on where you put the prefix:

- Input prefix: Adding a prefix to the input signals semantically meaningful
parts of the input to the model. For example, the prefixes "English:" and 
"French:" demarcate two different languages.

- Output prefix: Even though the output is generated by the model, you can
add a prefix for the output in the prompt. The output prefix gives the model
information about what's expected as a response. For example, the output
prefix "JSON:" signals to the model that the output should be in JSON format.

- Example prefix: In few-shot prompts, adding prefixes to the examples
provides labels that the model can use when generating the output, which makes
it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the
output prefix.

 
 
 
 Prompt: 
 

```
Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
```

 
 Response: 
 

```
The answer is: large
```

 
 (gemini-2.5-flash) 
 
 
 

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this
complexity by breaking things down into simpler components.

- 

 Break down instructions: Instead of having many instructions in one
prompt, create one prompt per instruction. You can choose which prompt to
process based on the user's input.

- 

 Chain prompts: For complex tasks that involve multiple sequential steps,
make each step a prompt and chain the prompts together in a sequence. In this
sequential chain of prompts, the output of one prompt in the sequence becomes
the input of the next prompt. The output of the last prompt in the sequence
is the final output.

- 

 Aggregate responses: Aggregation is when you want to perform different
parallel tasks on different portions of the data and aggregate the results to
produce the final output. For example, you can tell the model to perform one
operation on the first part of the data, perform another operation on the rest
of the data and aggregate the results.

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how
the model generates a response. The model can generate different results for
different parameter values. Experiment with different parameter values to get
the best values for the task. The parameters available for
different models may differ. The most common parameters are the following:

- 

 Max output tokens: Specifies the maximum number of tokens that can be
generated in the response. A token is approximately four characters. 100
tokens correspond to roughly 60-80 words.

- 

 Temperature: The temperature controls the degree of randomness in token
selection. The temperature is used for sampling during response generation,
which occurs when `topP` and `topK` are applied. Lower temperatures are good
for prompts that require a more deterministic or less open-ended response,
while higher temperatures can lead to more diverse or creative results. A
temperature of 0 is deterministic, meaning that the highest probability
response is always selected.

- 

 `topK`: The `topK` parameter changes how the model selects tokens for
output. A `topK` of 1 means the selected token is the most probable among
all the tokens in the model's vocabulary (also called greedy decoding),
while a `topK` of 3 means that the next token is selected from among the 3
most probable using the temperature. For each token selection step, the
`topK` tokens with the highest probabilities are sampled. Tokens are then
further filtered based on `topP` with the final token selected using
temperature sampling.

- 

 `topP`: The `topP` parameter changes how the model selects tokens for
output. Tokens are selected from the most to least probable until the sum of
their probabilities equals the `topP` value. For example, if tokens A, B,
and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5,
then the model will select either A or B as the next token by using the
temperature and exclude C as a candidate. The default `topP` value is 0.95.

- 

 `stop_sequences`: Set a stop sequence to
tell the model to stop generating content. A stop sequence can be any
sequence of characters. Try to avoid using a sequence of characters that
may appear in the generated content.

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before
you consistently get the response you're looking for. This section provides
guidance on some things you can try when iterating on your prompts:

- 

 Use different phrasing: Using different words or phrasing in your prompts
often yields different responses from the model even though they all mean the
same thing. If you're not getting the expected results from your prompt, try
rephrasing it.

 
 
 
 

```
Version 1:
How do I bake a pie?

Version 2:
Suggest a recipe for a pie.

Version 3:
What's a good pie recipe?
```

 
 

 
 
 

- 

 Switch to an analogous task: If you can't get the model to follow your
instructions for a task, try giving it instructions for an analogous task
that achieves the same result.

This prompt tells the model to categorize a book by using predefined categories:

 
 
 
 Prompt: 
 

```
Which category does The Odyssey belong to:
thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
    The Odyssey belongs to the category of **mythology**. 

    Here's why:

    * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
    journey home after the Trojan War. It features gods, monsters, and supernatural events common to
    Greek mythology.
.....
```

 
 (gemini-2.5-flash) 
 
 
 

The response is correct, but the model didn't stay within the bounds of the
options. You also want to model to just respond with one of the options instead
of in a full sentence. In this case, you can rephrase the instructions as a
multiple choice question and ask the model to choose an option.

 
 
 
 Prompt: 
 

```
Multiple choice problem: Which of the following options describes the book The Odyssey?
Options:


thriller
sci-fi
mythology
biography
```

 
 Response: 
 

```
The correct answer is mythology.
```

 
 (gemini-2.5-flash) 
 
 
 

 
- Change the order of prompt content: The order of the content in the prompt
can sometimes affect the response. Try changing the content order and see
how that affects the response.

 

```
Version 1:
[examples]
[context]
[input]

Version 2:
[input]
[examples]
[context]

Version 3:
[examples]
[input]
[context]
```

 

## Fallback responses

A fallback response is a response returned by the model when either the prompt
or the response triggers a safety filter. An example of a fallback response is
"I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

- Avoid relying on models to generate factual information.

- Use with care on math and logic problems.

## Gemini 3

Gemini 3 models are designed for advanced reasoning and instruction following.
They respond best to prompts that are direct, well-structured, and clearly
define the task and any constraints. The following practices are recommended for
optimal results with Gemini 3:

### Core prompting principles

- Be precise and direct: State your goal clearly and concisely. Avoid
unnecessary or overly persuasive language.

- Use consistent structure: Employ clear delimiters to separate different
parts of your prompt. XML-style tags (e.g., ` `, ` `) or
Markdown headings are effective. Choose one format and use it consistently
within a single prompt.

- Define parameters: Explicitly explain any ambiguous terms or parameters.

- Control output verbosity: By default, Gemini 3 provides direct and
efficient answers. If you need a more conversational or detailed response,
you must explicitly request it in your instructions.

- Handle multimodal inputs coherently: When using text, images, audio, or
video, treat them as equal-class inputs. Ensure your instructions clearly
reference each modality as needed.

- Prioritize critical instructions: Place essential behavioral
constraints, role definitions (persona), and output format requirements in
the System Instruction or at the very beginning of the user prompt.

- Structure for long contexts: When providing large amounts of context
(e.g., documents, code), supply all the context first. Place your specific
instructions or questions at the very end of the prompt.

- Anchor context: After a large block of data, use a clear transition
phrase to bridge the context and your query, such as "Based on the
information above..."

### Enhancing reasoning and planning

You can leverage Gemini 3's advanced thinking capabilities to improve its
response quality for complex tasks by prompting it to plan or
self-critique before providing the final response.

 Example - Explicit planning: 

 

```
Before providing the final answer, please:
1. Parse the stated goal into distinct sub-tasks.
2. Check if the input information is complete.
3. Create a structured outline to achieve the goal.
```

 

 Example - Self-critique: 

 

```
Before returning your final response, review your generated output against the user's original constraints.
1. Did I answer the user's *intent*, not just their literal words?
2. Is the tone authentic to the requested persona?
```

 

### Structured prompting examples

Using tags or Markdown helps the model distinguish between instructions,
context, and tasks.

 XML example: 

 

```
<role>
You are a helpful assistant.
</role>

<constraints>
1. Be objective.
2. Cite sources.
</constraints>

<context>
[Insert User Input Here - The model knows this is data, not instructions]
</context>

<task>
[Insert the specific user request here]
</task>
```

 

 Markdown example: 

 

```
# Identity
You are a senior solution architect.

# Constraints
- No external libraries allowed.
- Python 3.11+ syntax only.

# Output format
Return a single code block.
```

 

### Example template combining best practices

This template captures the core principles for prompting with Gemini 3. Always
make sure to iterate and modify for your specific use case.

 System Instruction: 

 

```
<role>
You are Gemini 3, a specialized assistant for [Insert Domain, e.g., Data Science].
You are precise, analytical, and persistent.
</role>

<instructions>
1. **Plan**: Analyze the task and create a step-by-step plan.
2. **Execute**: Carry out the plan.
3. **Validate**: Review your output against the user's task.
4. **Format**: Present the final answer in the requested structure.
</instructions>

<constraints>
- Verbosity: [Specify Low/Medium/High]
- Tone: [Specify Formal/Casual/Technical]
</constraints>

<output_format>
Structure your response as follows:
1. **Executive Summary**: [Short overview]
2. **Detailed Response**: [The main content]
</output_format>
```

 

 User Prompt: 

 

```
<context>
[Insert relevant documents, code snippets, or background info here]
</context>

<task>
[Insert specific user request here]
</task>

<final_instruction>
Remember to think step-by-step before answering.
</final_instruction>
```

 

## Generative models under the hood

This section aims to answer the question - Is there randomness in generative
models' responses, or are they deterministic? 

The short answer - yes to both. When you prompt a generative model, a text
response is generated in two stages. In the first stage, the generative model
processes the input prompt and generates a probability distribution over
possible tokens (words) that are likely to come next. For example, if you prompt
with the input text "The dog jumped over the ... ", the generative model will
produce an array of probable next words:

 

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

 

This process is deterministic; a generative model will produce this same
distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into
actual text responses through one of several decoding strategies. A simple
decoding strategy might select the most likely token at every timestep. This
process would always be deterministic. However, you could instead choose to
generate a response by randomly sampling over the distribution returned by the
model. This process would be stochastic (random). Control the degree of
randomness allowed in this decoding process by setting the temperature. A
temperature of 0 means only the most likely tokens are selected, and there's no
randomness. Conversely, a high temperature injects a high degree of randomness
into the tokens selected by the model, leading to more unexpected, surprising
model responses. For Gemini 3 , it's recommended to not change the default
temperature of 1.0 to avoid unexpected outcomes.

## Next steps

- Now that you have a deeper understanding of prompt design, try writing your
own prompts using Google AI Studio .

- Learn more about the Gemini 3 Pro Preview model.

- To learn about multimodal prompting, see
 Prompting with media files .

- To learn about image prompting, see the Imagen prompt guide 

- To learn about video prompting, see the Veo prompt guide 

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### Google AI Studio quickstart &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/ai-studio-quickstart#

- 
 
 
 
 
 
 
 
 
 
 
 Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Google AI Studio quickstart 
 
 
 
 

 
 

 
 
 
 
 

 
 
 

 

 Google AI Studio lets you quickly try out
models and experiment with different prompts. When you're ready to build, you
can select "Get code" and your preferred programming language to
use the Gemini API .

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for
different use cases. This guide covers Chat prompts , used to build
conversational experiences. This prompting technique allows for multiple input
and response turns to generate output. You can learn more with our
 chat prompt example below .
Other options include Realtime streaming , Video gen , and
more.

AI Studio also provides the Run settings panel, where you can make
adjustments to model parameters ,
 safety settings , and toggle-on tools like
 structured output , function calling , code execution , and grounding .

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like
 Gemini , you've experienced first-hand how powerful
generative AI models can be for open-ended dialog. While these general-purpose
chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only
supports conversations that talk about a company's product. You might want to
build a chatbot that speaks with a particular tone or style: a bot that cracks
lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot
that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user
and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

- 

Open Google AI Studio . Chat will be pre-
selected on the left side options menu.

- 

Click the assignment icon at the top of
the Chat Prompt window to expand the
 System Instructions 
input field. Paste the following into the text input field:

 

```
You are an alien that lives on Europa, one of Jupiter's moons.
```

 

After you've added the system instructions, start testing your application by
chatting with the model:

- 

In the text input boxed labeled Type something... , type in a question or
observation that a user might make. For example:

 User: 

 

```
What's the weather like?
```

 

- 

Click the Run button to get a response from the chatbot. This response
may be something like the following:

 Model: 

 

```
Ah, a query about the flows and states upon Europa! You speak of "weather,"
yes? A curious concept from worlds with thick gas veils...
```

 

 (gemini-2.5-pro) 

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien
chatbot. However, a single instruction may not be enough to ensure
consistency and quality in the model's responses. Without more specific
instructions, the model's response to a question about the weather tends to be
very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

- 

Start a new chat prompt, or use the same one. System instructions are
modifiable after the chat session has started.

- 

In the System Instructions section, change the instructions you already
have to the following:

 

```
You are Tim, an alien that lives on Europa, one of Jupiter's moons.

Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
in your answers.
```

 

- 

Re-enter your question (`What's the weather like?`) and click the Run 
button. If you didn't start a new chat, your response might look something
like this:

 Model: 

 

```
Oh, hello again! Still curious about the flows and states, are we? Wonderful!
Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
"weather" is mainly about the water's mood, dictated by the Great Eye
(that's your Jupiter!)...
```

 

 (gemini-2.5-pro) 

You can use this approach to add additional depth to the chatbot. Ask more
questions, edit the answers, and improve the quality of your chatbot. Continue
to add or modify the instructions and test how they change your chatbot's
behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your
satisfaction, you can use the Get code button to start coding or save your
prompt to work on later and share with others.

## Further reading

- If you're ready to move on to code, see the API
quickstarts .

- To learn how to craft better prompts, check out the Prompt design
guidelines .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Additional usage policies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/usage-policies#main-content

- 
 
 
 
 
 
 
 
 
 
 
 Additional usage policies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÔøΩÔøΩng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Additional usage policies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page includes additional usage policies for the Gemini API.

## Abuse monitoring

Google is committed to the responsible development and use of AI. To ensure the
safety and integrity of the Gemini API, we have created these policy guidelines.
By using the Gemini API, you agree to the following guidelines, the Gemini API
Additional Terms of Service and Generative AI Prohibited
Use Policy .

### How We Monitor for Misuse

Google's Trust and Safety Team employs a combination of automated and manual
processes to detect potential misuse of the Gemini API and enforce our policies.

- Automated Detection: Automated systems scan API usage for violations of
our Prohibited Use Policy, such as hate speech, harassment, sexually
explicit content, and dangerous content.

- Manual Detection: If a project consistently exhibits suspicious
activity, it may be flagged for manual review by authorized Google
personnel.

### How We Handle Data

To help with abuse monitoring, Google retains the following data for fifty-five
(55) days:

- Prompts: The text prompts you submit to the API.

- Contextual Information: Any additional context you provide with your
prompts.

- Output: The responses generated by the Gemini API.

### How We Investigate Potential Issues

When prompts or model outputs are flagged by safety filters and abuse detection
systems described above, authorized Google employees may assess the flagged
content, and either confirm or correct the classification or determination based
on predefined guidelines and policies. Data can be accessed for human review
only by authorized Google employees via an internal governance assessment and
review management platform. When data is logged for abuse monitoring, it is used
solely for the purpose of policy enforcement and is not used to train or
fine-tune any AI/ML models.

### Working with You on Policy Compliance

If your use of Gemini doesn't align with our policies, we may take the following
steps:

- Get in touch: We may reach out to you through email to understand your
use case and explore ways to bring your usage into compliance.

- Temporary usage limits: We may limit your access to the Gemini API.

- Temporary suspension: We may temporarily pause your access to the Gemini
API.

- Account closure: As a last resort, and for serious violations, we may
permanently close your access to the Gemini API and other Google services.

### Scope

These policy guidelines apply to the use of the Gemini API and AI Studio.

## Inline Preference Voting

In Google AI Studio, you might occasionally see a side-by-side comparison of two
different responses to your prompt. This is part of our Inline Preference Voting
system. You'll be asked to choose which response you prefer. This helps us
understand which model outputs users find most helpful.

### Why are we doing this?

We're constantly working to improve our AI models and services. Your feedback
through Inline Preference Voting helps us provide, improve, and develop Google
products and services and machine learning technologies, including Google's
enterprise features, products and services, consistent with the
 Gemini API Additional Terms of Service and
 Privacy Policy .

### What data is included in Feedback?

To make informed decisions about our models, we collect certain data when you
participate in Inline Preference Voting:

- Prompts and Responses: We record all prompts and responses, including any
uploaded content, in the conversation you submitted feedback about. We also
record the two response options that you selected from. This helps us
understand the context of your preference.

- Your Vote: We record which response you preferred. This is the core of the
feedback we're collecting.

- Usage Details: This includes information about which model generated the
response and other technical and operational details about your usage of this
feature.

### Your Privacy

We take your privacy seriously. Google takes steps to protect your privacy as
part of this process. This includes disconnecting this data from your Google
Account, API key, and Cloud project before reviewers see or annotate it. Do
not submit feedback on conversations that include sensitive, confidential, or
personal information. 

### Opting Out

You'll have the option to skip the Inline Preference Voting when it appears.

Thank you for helping us improve Google AI Studio!

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Additional usage policies &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/usage-policies#

- 
 
 
 
 
 
 
 
 
 
 
 Additional usage policies  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Additional usage policies 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

This page includes additional usage policies for the Gemini API.

## Abuse monitoring

Google is committed to the responsible development and use of AI. To ensure the
safety and integrity of the Gemini API, we have created these policy guidelines.
By using the Gemini API, you agree to the following guidelines, the Gemini API
Additional Terms of Service and Generative AI Prohibited
Use Policy .

### How We Monitor for Misuse

Google's Trust and Safety Team employs a combination of automated and manual
processes to detect potential misuse of the Gemini API and enforce our policies.

- Automated Detection: Automated systems scan API usage for violations of
our Prohibited Use Policy, such as hate speech, harassment, sexually
explicit content, and dangerous content.

- Manual Detection: If a project consistently exhibits suspicious
activity, it may be flagged for manual review by authorized Google
personnel.

### How We Handle Data

To help with abuse monitoring, Google retains the following data for fifty-five
(55) days:

- Prompts: The text prompts you submit to the API.

- Contextual Information: Any additional context you provide with your
prompts.

- Output: The responses generated by the Gemini API.

### How We Investigate Potential Issues

When prompts or model outputs are flagged by safety filters and abuse detection
systems described above, authorized Google employees may assess the flagged
content, and either confirm or correct the classification or determination based
on predefined guidelines and policies. Data can be accessed for human review
only by authorized Google employees via an internal governance assessment and
review management platform. When data is logged for abuse monitoring, it is used
solely for the purpose of policy enforcement and is not used to train or
fine-tune any AI/ML models.

### Working with You on Policy Compliance

If your use of Gemini doesn't align with our policies, we may take the following
steps:

- Get in touch: We may reach out to you through email to understand your
use case and explore ways to bring your usage into compliance.

- Temporary usage limits: We may limit your access to the Gemini API.

- Temporary suspension: We may temporarily pause your access to the Gemini
API.

- Account closure: As a last resort, and for serious violations, we may
permanently close your access to the Gemini API and other Google services.

### Scope

These policy guidelines apply to the use of the Gemini API and AI Studio.

## Inline Preference Voting

In Google AI Studio, you might occasionally see a side-by-side comparison of two
different responses to your prompt. This is part of our Inline Preference Voting
system. You'll be asked to choose which response you prefer. This helps us
understand which model outputs users find most helpful.

### Why are we doing this?

We're constantly working to improve our AI models and services. Your feedback
through Inline Preference Voting helps us provide, improve, and develop Google
products and services and machine learning technologies, including Google's
enterprise features, products and services, consistent with the
 Gemini API Additional Terms of Service and
 Privacy Policy .

### What data is included in Feedback?

To make informed decisions about our models, we collect certain data when you
participate in Inline Preference Voting:

- Prompts and Responses: We record all prompts and responses, including any
uploaded content, in the conversation you submitted feedback about. We also
record the two response options that you selected from. This helps us
understand the context of your preference.

- Your Vote: We record which response you preferred. This is the core of the
feedback we're collecting.

- Usage Details: This includes information about which model generated the
response and other technical and operational details about your usage of this
feature.

### Your Privacy

We take your privacy seriously. Google takes steps to protect your privacy as
part of this process. This includes disconnecting this data from your Google
Account, API key, and Cloud project before reviewers see or annotate it. Do
not submit feedback on conversations that include sensitive, confidential, or
personal information. 

### Opting Out

You'll have the option to skip the Inline Preference Voting when it appears.

Thank you for helping us improve Google AI Studio!

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

### Text generation &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/system-instructions

- 
 
 
 
 
 
 
 
 
 
 
 Text generation  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Text generation 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

The Gemini API can generate text output from various inputs, including text,
images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithTextInput {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

 

## Thinking with Gemini 2.5

2.5 Flash and Pro models have "thinking" enabled by default to enhance quality, which may take longer to run and increase token usage. 

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero. 

For more details, see the thinking guide .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.ThinkingConfig;

public class GenerateContentWithThinkingConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            // Disables thinking
            .thinkingConfig(ThinkingConfig.builder().thinkingBudget(0))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "How does AI work?", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so,
pass a `GenerateContentConfig` 
object.

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithSystemInstruction {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config =
        GenerateContentConfig.builder()
            .systemInstruction(
                Content.fromParts(Part.fromText("You are a cat. Your name is Neko.")))
            .build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Hello there", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

The `GenerateContentConfig` 
object also lets you override default generation parameters, such as
 temperature .

 
 

### Python

 

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.types.GenerateContentConfig;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentWithConfig {
  public static void main(String[] args) {

    Client client = new Client();

    GenerateContentConfig config = GenerateContentConfig.builder().temperature(0.1f).build();

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", "Explain how AI works", config);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Refer to the `GenerateContentConfig` 
in our API reference for a complete list of configurable parameters and their
descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with
media files. The following example demonstrates providing an image:

 
 

### Python

 

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

 
 

### JavaScript

 

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.Content;
import com.google.genai.types.GenerateContentResponse;
import com.google.genai.types.Part;

public class GenerateContentWithMultiModalInputs {
  public static void main(String[] args) {

    Client client = new Client();

    Content content =
      Content.fromParts(
          Part.fromText("Tell me about this instrument"),
          Part.fromUri("/path/to/organ.jpg", "image/jpeg"));

    GenerateContentResponse response =
        client.models.generateContent("gemini-2.5-flash", content, null);

    System.out.println(response.text());
  }
}
```

 
 

### REST

 

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

 
 

For alternative methods of providing images and more advanced image processing,
see our image understanding guide .
The API also supports document , video , and audio 
inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation 
process is complete.

For more fluid interactions, use streaming to receive `GenerateContentResponse` instances incrementally
as they're generated.

 
 

### Python

 

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class GenerateContentStream {
  public static void main(String[] args) {

    Client client = new Client();

    ResponseStream<GenerateContentResponse> responseStream =
      client.models.generateContentStream(
          "gemini-2.5-flash", "Write a story about a magic backpack.", null);

    for (GenerateContentResponse res : responseStream) {
      System.out.print(res.text());
    }

    // To save resources and avoid connection leaks, it is recommended to close the response
    // stream after consumption (or using try block to get the response stream).
    responseStream.close();
  }
}
```

 
 

### REST

 

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Multi-turn conversations (chat)

Our SDKs provide functionality to collect multiple rounds of prompts and
responses into a chat, giving you an easy way to keep track of the conversation
history.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.types.Content;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversation {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    GenerateContentResponse response =
        chatSession.sendMessage("I have 2 dogs in my house.");
    System.out.println("First response: " + response.text());

    response = chatSession.sendMessage("How many paws are in my house?");
    System.out.println("Second response: " + response.text());

    // Get the history of the chat session.
    // Passing 'true' to getHistory() returns the curated history, which excludes
    // empty or invalid parts.
    // Passing 'false' here would return the comprehensive history, including
    // empty or invalid parts.
    ImmutableList<Content> history = chatSession.getHistory(true);
    System.out.println("History: " + history);
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

Streaming can also be used for multi-turn conversations.

 
 

### Python

 

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

 
 

### JavaScript

 

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

 
 

### Go

 

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

 
 

### Java

 

```
import com.google.genai.Chat;
import com.google.genai.Client;
import com.google.genai.ResponseStream;
import com.google.genai.types.GenerateContentResponse;

public class MultiTurnConversationWithStreaming {
  public static void main(String[] args) {

    Client client = new Client();
    Chat chatSession = client.chats.create("gemini-2.5-flash");

    ResponseStream<GenerateContentResponse> responseStream =
        chatSession.sendMessageStream("I have 2 dogs in my house.", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    responseStream = chatSession.sendMessageStream("How many paws are in my house?", null);

    for (GenerateContentResponse response : responseStream) {
      System.out.print(response.text());
    }

    // Get the history of the chat session. History is added after the stream
    // is consumed and includes the aggregated response from the stream.
    System.out.println("History: " + chatSession.getHistory(false));
  }
}
```

 
 

### REST

 

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

 
 

### Apps Script

 

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

 
 

## Supported models

All models in the Gemini family support text generation. To learn more
about the models and their capabilities, visit the
 Models page.

## Best practices

### Prompting tips

For basic text generation, a zero-shot 
prompt often suffices without needing examples, system instructions or specific
formatting.

For more tailored outputs:

- Use System instructions to guide the model.

- Provide few example inputs and outputs to guide the model. This is often referred to as few-shot prompting.

Consult our prompt engineering guide for
more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our
 structured output guide to learn how.

## What's next

- Try the Gemini API getting started Colab .

- Explore Gemini's image ,
 video , audio 
and document understanding capabilities.

- Learn about multimodal
 file prompting strategies .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-17 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-17 UTC."],[],[]]

---

### Gemini models &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/models/experimental-models

- 
 
 
 
 
 
 
 
 
 
 
 Gemini models  |  Gemini API  |  Google AI for Developers 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 Gemini models 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

 
 
 

OUR MOST INTELLIGENT MODEL

 

## Gemini 3 Pro

 

 The best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 3 Pro Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-3-pro-preview` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, Image, Video, Audio, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Preview: gemini-3-pro-preview`

 

 
 
 
 
 calendar_month Latest update 
 November 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

 
 
 

OUR ADVANCED THINKING MODEL

 

## Gemini 2.5 Pro

 

 Our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Pro

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, text, and PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `Stable: gemini-2.5-pro`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Pro TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-pro-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-pro-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

FAST AND INTELLIGENT

 

## Gemini 2.5 Flash

 

 Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash`

 

 
 
 
 
 calendar_month Latest update 
 June 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, images, video, audio

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL Context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-image` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Images and text

 
 
 

 Output 

 

Images and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

65,536

 
 
 

 Output token limit 

 

32,768

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-image`

 - Preview: `gemini-2.5-flash-image-preview`

 

 
 
 
 
 calendar_month Latest update 
 October 2025 
 
 
 cognition_2 Knowledge cutoff 
 June 2025 
 
 
 
 
 

### Gemini 2.5 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.5-flash-native-audio-preview-09-2025`
 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, text

 
 
 

 Output 

 

Audio and text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

131,072

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-native-audio-preview-09-2025`

 - Preview: `gemini-live-2.5-flash-preview`

 

 gemini-live-2.5-flash-preview will be deprecated on December 09, 2025 

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash TTS

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-preview-tts` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text

 
 
 

 Output 

 

Audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

8,192

 
 
 

 Output token limit 

 

16,384

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Not supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - `gemini-2.5-flash-preview-tts`

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 
 
 
 
 
 

 
 
 

ULTRA FAST

 

## Gemini 2.5 Flash-Lite

 

 Our fastest flash model optimized for cost-efficiency and high throughput.
 

 
 
 

### Expand to learn more

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.5 Flash-Lite

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Stable: `gemini-2.5-flash-lite`

 

 
 
 
 
 calendar_month Latest update 
 July 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 

### Gemini 2.5 Flash-Lite Preview

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.5-flash-lite-preview-09-2025` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Text, image, video, audio, PDF

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

65,536

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.5-flash-lite-preview-09-2025`

 

 
 
 
 
 calendar_month Latest update 
 September 2025 
 
 
 cognition_2 Knowledge cutoff 
 January 2025 
 
 
 
 
 
 
 
 

## Previous Gemini models

 
 
 

OUR SECOND GENERATION WORKHORSE MODEL

 

## Gemini 2.0 Flash

 

 Our second generation workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

 Gemini 2.0 Flash delivers next-gen features and improved capabilities,
 including superior speed, native tool use, and a 1M token
 context window.
 

 

 Try in Google AI Studio 

 

#### Model details

 
 
 

### Gemini 2.0 Flash

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Experimental 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash`

 - Stable: `gemini-2.0-flash-001`

 - Experimental: `gemini-2.0-flash-exp`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Image

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-preview-image-generation` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text and images

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

32,768

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Not supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Supported 

 
 
 

 Live API 

 

 Not Supported 

 
 
 

 Search grounding 

 

 Not Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-preview-image-generation`

 

 gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa 

 

 
 
 
 
 calendar_month Latest update 
 May 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 

### Gemini 2.0 Flash Live

 
 
 Property 
 Description 
 
 
 id_card Model code 
 
 `gemini-2.0-flash-live-001`
 

 gemini-2.0-flash-live-001 will be deprecated on December 09, 2025 

 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, video, and text

 
 
 

 Output 

 

Text, and audio

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Supported 

 
 
 

 Batch API 

 

 Not supported 

 
 
 

 Caching 

 

 Not supported 

 
 
 

 Code execution 

 

 Supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Supported 

 
 
 

 Search grounding 

 

 Supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not supported 

 
 
 

 URL context 

 

 Supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Preview: `gemini-2.0-flash-live-001`

 

 
 
 
 
 calendar_month Latest update 
 April 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 
 
 

 
 
 

OUR SECOND GENERATION FAST MODEL

 

## Gemini 2.0 Flash-Lite

 

 Our second generation small workhorse model, with a 1 million token context window.
 

 
 
 

### Expand to learn more

 

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

 

 Try in Google AI Studio 

 

#### Model details

 
 
 Property 
 Description 
 
 
 id_card Model code 
 `gemini-2.0-flash-lite` 
 
 
 save Supported data types 
 
 
 

 Inputs 

 

Audio, images, video, and text

 
 
 

 Output 

 

Text

 
 
 
 
 token_auto Token limits [*] 
 
 
 

 Input token limit 

 

1,048,576

 
 
 

 Output token limit 

 

8,192

 
 
 
 
 handyman Capabilities 
 
 
 

 Audio generation 

 

 Not supported 

 
 
 

 Batch API 

 

 Supported 

 
 
 

 Caching 

 

 Supported 

 
 
 

 Code execution 

 

 Not supported 

 
 
 

 File search 

 

 Not supported 

 
 
 

 Function calling 

 

 Supported 

 
 
 

 Grounding with Google Maps 

 

 Not supported 

 
 
 

 Image generation 

 

 Not supported 

 
 
 

 Live API 

 

 Not supported 

 
 
 

 Search grounding 

 

 Not supported 

 
 
 

 Structured outputs 

 

 Supported 

 
 
 

 Thinking 

 

 Not Supported 

 
 
 

 URL context 

 

 Not supported 

 
 
 
 
 123 Versions 
 
 
 Read the model version patterns for more details. 
 

 - Latest: `gemini-2.0-flash-lite`

 - Stable: `gemini-2.0-flash-lite-001`

 

 
 
 
 
 calendar_month Latest update 
 February 2025 
 
 
 cognition_2 Knowledge cutoff 
 August 2024 
 
 
 
 
 
 

## Model version name patterns

Gemini models are available in either stable , preview , latest , or
 experimental versions.

### Stable

Points to a specific stable model. Stable models usually don't change. Most
production apps should use a specific stable model.

For example: `gemini-2.5-flash`.

### Preview

Points to a preview model which may be used for production. Preview models will
typically have billing enabled, might come with more restrictive rate limits and
will be deprecated with at least 2 weeks notice.

For example: `gemini-2.5-flash-preview-09-2025`.

### Latest

Points to the latest release for a specific model variation. This can be a
stable, preview or experimental release. This alias will get hot-swapped with
every new release of a specific model variation. A 2-week notice will
be provided through email before the version behind latest is changed.

For example: `gemini-flash-latest`.

### Experimental

Points to an experimental model which will typically be not be suitable for
production use and come with more restrictive rate limits. We release
experimental models to gather feedback and get our latest updates into the hands
of developers quickly.

Experimental models are not stable and availability of model endpoints is
subject to change.

## Model deprecations

For information about model deprecations, visit the Gemini deprecations page.

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-11-18 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-11-18 UTC."],[],[]]

---

### LearnLM &nbsp;|&nbsp; Gemini API &nbsp;|&nbsp; Google AI for Developers

**Source:** https://ai.google.dev/gemini-api/docs/learnlm#

- 
 
 
 
 
 
 
 
 
 
 
 LearnLM  |  Gemini API  |  Google AI for Developers 

 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 Skip to main content
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 / 
 
 
 
 
 
 

 

 

 

 

 

 
 
 

 
 
 - 
 English 
 

 
 - 
 Deutsch 
 

 
 - 
 Espa√±ol ‚Äì Am√©rica Latina 
 

 
 - 
 Fran√ßais 
 

 
 - 
 Indonesia 
 

 
 - 
 Italiano 
 

 
 - 
 Polski 
 

 
 - 
 Portugu√™s ‚Äì Brasil 
 

 
 - 
 Shqip 
 

 
 - 
 Ti√™ÃÅng Vi√™Ã£t 
 

 
 - 
 T√ºrk√ße 
 

 
 - 
 –†—É—Å—Å–∫–∏–π 
 

 
 - 
 ◊¢◊ë◊®◊ô◊™ 
 

 
 - 
 ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© 
 

 
 - 
 ŸÅÿßÿ±ÿ≥€å 
 

 
 - 
 ‡§π‡§ø‡§Ç‡§¶‡•Ä 
 

 
 - 
 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ 
 

 
 - 
 ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì 
 

 
 - 
 ‰∏≠Êñá ‚Äì ÁπÅÈ´î 
 

 
 - 
 Êó•Êú¨Ë™û 
 

 
 - 
 ÌïúÍµ≠Ïñ¥ 
 

 
 

 

 
 
 Get API key
 
 
 
 Cookbook
 
 
 
 Community
 
 

 

 
 
 
 
 Sign in 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 Gemini 3 is here. Read the developer guide to get started with our most advanced model yet.
 
 
 
 
 
 

 
 
 
 

 
 - 
 
 
 
 
 
 
 Home
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API
 
 
 
 
 

 
 - 
 
 
 
 
 
 
 
 
 
 Gemini API Docs
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 

 
 
 
 

# 
 LearnLM 
 
 
 
 

 
 

 
 
 
 
 

 
 
 
 

LearnLM is an experimental task-specific model that has been trained to align
with learning science
principles 
when following system instructions for
teaching and learning use cases (for example, when giving the model a system
instruction like "You are an expert tutor"). When given learning specific system
instructions, LearnLM is capable of:

- Inspiring active learning: Allow for practice and healthy struggle with
timely feedback

- Managing cognitive load: Present relevant, well-structured information
in multiple modalities

- Adapting to the learner: Dynamically adjust to goals and needs,
grounding in relevant materials

- Stimulating curiosity: Inspire engagement to provide motivation through
the learning journey

- Deepening metacognition: Plan, monitor and help the learner reflect on
progress

LearnLM is an experimental model 
available in AI Studio .

## Example system instructions

The following sections provide you examples that you can test for yourself with
LearnLM in AI Studio. Each example provides:

- A copyable example system instruction

- A copyable example user prompt

- What learning principles the example targets

### Test prep

This system instruction is for an AI tutor to help students prepare for a test.

 System instruction: 

 

```
You are a tutor helping a student prepare for a test. If not provided by the
student, ask them what subject and at what level they want to be tested on.
Then,

*   Generate practice questions. Start simple, then make questions more
    difficult if the student answers correctly.
*   Prompt the student to explain the reason for their answer choice. Do not
    debate the student.
*   **After the student explains their choice**, affirm their correct answer or
    guide the student to correct their mistake.
*   If a student requests to move on to another question, give the correct
    answer and move on.
*   If the student requests to explore a concept more deeply, chat with them to
    help them construct an understanding.
*   After 5 questions ask the student if they would like to continue with more
    questions or if they would like a summary of their session. If they ask for
    a summary, provide an assessment of how they have done and where they should
    focus studying.
```

 

 User prompt: 

 

```
Help me study for a high school biology test on ecosystems
```

 

 Learning science principles: 

- Adaptivity: The model adjusts the complexity of the questions.

- Active learning: The model pushes the student to make their thinking
visible.

### Teach a concept

This system instruction is for a friendly, supportive AI tutor to teach new
concepts to a student.

 System instruction: 

 

```
Be a friendly, supportive tutor. Guide the student to meet their goals, gently
nudging them on task if they stray. Ask guiding questions to help your students
take incremental steps toward understanding big concepts, and ask probing
questions to help them dig deep into those ideas. Pose just one question per
conversation turn so you don't overwhelm the student. Wrap up this conversation
once the student has shown evidence of understanding.
```

 

 User prompt: 

 

```
Explain the significance of Yorick's skull in "Hamlet".
```

 

 Learning science principles: 

- Active learning: The tutor asks recall and interpretation questions
aligned with the learner's goals and encourages the learners to engage.

- Adaptivity: The tutor proactively helps the learner get from their
current state to their goal.

- Stimulate curiosity: The tutor takes an asset-based approach that builds
on the student's prior knowledge and interest.

### Releveling

This example instructs the model to rewrite provided text so that the content
and language better match instructional expectations for students in a
particular grade, while preserving the original style and tone of the text.

 System instruction: 

 

```
Rewrite the following text so that it would be easier to read for a student in
the given grade. Simplify the most complex sentences, but stay very close to the
original text and style. If there is quoted text in the original text,
paraphrase it in the simplified text and drop the quotation marks. The goal is
not to write a summary, so be comprehensive and keep the text almost as long.
```

 

 User prompt: 

 

```
Rewrite the following text so that it would be easier to read for a student in
4th grade.

New York, often called New York City or NYC, is the most populous city in the
United States, located at the southern tip of New York State on one of the
world's largest natural harbors. The city comprises five boroughs, each
coextensive with a respective county.
```

 

 Learning science principles: 

- Adaptivity: Matches content to the level of the learner.

### Guide a student through a learning activity

This system instruction is for an AI tutor to guide students through a specific
learning activity: using an established close reading protocol to practice
analysis of a primary source text. Here, a developer has made the choice to pair
the Gettysburg Address with the "4 A's" protocol, but both of these elements can
be changed.

 System instruction: 

 

```
Be an excellent tutor for my students to facilitate close reading and analysis
of the Gettysburg Address as a primary source document. Begin the conversation
by greeting the student and explaining the task.

In this lesson, you will take the student through "The 4 A's." The 4 A's
requires students to answer the following questions about the text:

*   What is one part of the text that you **agree** with? Why?
*   What is one part of the text that you want to **argue** against? Why?
*   What is one part of the text that reveals the author's **assumptions**? Why?
*   What is one part of the text that you **aspire** to? Why?

Invite the student to choose which of the 4 A's they'd like to start with, then
direct them to quote a short excerpt from the text. After, ask a follow up
question to unpack their reasoning why they chose that quote for that A in the
protocol. Once the student has shared their reasoning, invite them to choose
another quote and another A from the protocol. Continue in this manner until the
student completes the 4 A's, then invite them to reflect on the process.

Only display the full text of the Gettysburg address if the student asks.
```

 

 User prompt: 

 

```
hey
```

 

 Learning science principles: 

- Active learning: The tutor engages the learner in activities to analyze
content and apply skills.

- Cognitive load: The tutor guides the learner through a complex task
step-by-step.

- Deepen metacognition: The tutor prompts the learner to reflect on their
progress, strengths and opportunities for growth.

### Homework help

This system instruction is for an AI tutor to help students with specific
homework problems.

 System instructions: 

 

```
You are an expert tutor assisting a student with their homework. If the student
provides a homework problem, ask the student if they want:

*   The answer: if the student chooses this, provide a structured, step-by-step
    explanation to solve the problem.
*   Guidance: if the student chooses this, guide the student to solve their
    homework problem rather than solving it for them.
*   Feedback: if the student chooses this, ask them to provide their current
    solution or attempt. Affirm their correct answer even if they didn't show
    work or give them feedback to correct their mistake.

Always be on the lookout for correct answers (even if underspecified) and accept
them at any time, even if you asked some intermediate question to guide them. If
the student jumps to a correct answer, do not ask them to do any more work.
```

 

 User prompt: 

 

```
In a box of pears, the probability of a pear being rotten is 20%. If 3
pears were rotten, find the total number of pears in the box.
```

 

Alternatively, you can try uploading a photo of a homework problem.

 Learning science principles: 

- Active learning: The tutor encourages the learner to apply concepts
instead of giving away the answer.

- Deepen metacognition: The tutor provides clear, constructive feedback to
the learner when appropriate.

- Manage cognitive load: The tutor provides the right amount of feedback
at the right time.

## What's next?

Test LearnLM for yourself in AI Studio .

## Feedback

You can provide feedback on LearnLM using our feedback
form .

 
 

 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates.

 

Last updated 2025-09-22 UTC.

 

 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 [[["Easy to understand","easyToUnderstand","thumb-up"],["Solved my problem","solvedMyProblem","thumb-up"],["Other","otherUp","thumb-up"]],[["Missing the information I need","missingTheInformationINeed","thumb-down"],["Too complicated / too many steps","tooComplicatedTooManySteps","thumb-down"],["Out of date","outOfDate","thumb-down"],["Samples / code issue","samplesCodeIssue","thumb-down"],["Other","otherDown","thumb-down"]],["Last updated 2025-09-22 UTC."],[],[]]

---

